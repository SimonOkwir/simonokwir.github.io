- id: _1511_
  accessed:
    - year: 2022
      month: 4
      day: 29
  citation-key: _1511_
  title: '[1511.03034] Learning with a Strong Adversary'
  type: webpage
  URL: https://arxiv.org/abs/1511.03034

- id: _anstallningsavtal_
  abstract: >-
    Share with your colleagues and friends the simplicity of using Adobe Acrobat
    Sign to sign documents electronically. Click on the buttons below to spread
    the word! Learn more at <a target='_blank'
    href='https://acrobat.adobe.com'>https:\/\/acrobat.adobe.com</a>
  accessed:
    - year: 2024
      month: 1
      day: 10
  citation-key: _anstallningsavtal_
  container-title: Acrobat Sign
  language: en-GB
  title: 'Anställningsavtal / Employment contract: Antônio Horta Riberio'
  title-short: Anställningsavtal / Employment contract
  type: webpage
  URL: >-
    https://kthsign.eu1.documents.adobe.com/public/esign?tsid=CBFCIBAACBSCTBABDUAAABACAABAAlWFtvnB7wu1XlvxJX0ziEsw7Nw_l2I_JkmS-Zm07tKJ0ezVLCryFV0unMwB1AEAVpeoVU8i1YhyGDsuXnGWuq_sR90R5OQ1-AVa2_2ssCYjwzWLLnfj1reMtHFOkcf70&

- id: _antonior92_
  abstract: >-
    PyTorch deep learning projects from CODE. Contribute to
    antonior92/code-deeplearning development by creating an account on GitHub.
  accessed:
    - year: 2019
      month: 12
      day: 28
  citation-key: _antonior92_
  container-title: GitHub
  language: en
  title: antonior92/code-deeplearning
  type: webpage
  URL: https://github.com/antonior92/code-deeplearning

- id: _antonior92_a
  abstract: >-
    Physionet 2020 challenge. Contribute to
    antonior92/physionet-12ecg-classification development by creating an account
    on GitHub.
  accessed:
    - year: 2020
      month: 6
      day: 26
  citation-key: _antonior92_a
  container-title: GitHub
  language: en
  source: github.com
  title: antonior92/physionet-12ecg-classification
  type: webpage
  URL: https://github.com/antonior92/physionet-12ecg-classification

- id: _caixa_
  accessed:
    - year: 2020
      month: 6
      day: 28
  citation-key: _caixa_
  title: Caixa de entrada - antonior92@gmail.com - Gmail
  type: webpage
  URL: https://mail.google.com/mail/u/0/#inbox

- id: _caixa_a
  accessed:
    - year: 2020
      month: 6
      day: 29
  citation-key: _caixa_a
  title: Caixa de entrada - antonior92@gmail.com - Gmail
  type: webpage
  URL: https://mail.google.com/mail/u/0/#inbox

- id: _changes_
  accessed:
    - year: 2021
      month: 8
      day: 23
  citation-key: _changes_
  container-title: Google Docs
  language: en-GB
  title: Changes
  type: webpage
  URL: >-
    https://docs.google.com/document/u/0/d/1vURop52tULBv271DKzEEOFNZFr9kY1csYlWUTYI_1OU/edit?usp=embed_facebook

- id: _cite_
  abstract: >-
    La cité des sciences et de l'industrie est un établissement public de
    diffusion de la culture scientifique, technique et industrielle située à
    Paris, La Villette. La Cité propose expositions, films, conférences et
    animations pour les enfants et leurs familles.
  accessed:
    - year: 2023
      month: 5
      day: 28
  citation-key: _cite_
  language: fr-FR
  title: >-
    Cité des sciences et de l'industrie - Accueil - Expositions, conférences,
    cinémas, activités culturelles et sorties touristiques pour les enfants, les
    parents, les familles - Paris
  type: webpage
  URL: https://www.cite-sciences.fr/fr/accueil

- id: _computing_
  accessed:
    - year: 2020
      month: 9
      day: 15
  citation-key: _computing_
  title: Computing in Cardiology 2020
  type: webpage
  URL: https://cinc2020.ibrida.io/speech/6065

- id: _ctan_
  accessed:
    - year: 2024
      month: 1
      day: 3
  citation-key: _ctan_
  title: 'CTAN: /tex-archive/macros/latex/contrib/elsarticle/doc'
  type: webpage
  URL: https://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle/doc

- id: _fairness_
  accessed:
    - year: 2024
      month: 6
      day: 10
  citation-key: _fairness_
  title: Fairness and machine learning
  type: webpage
  URL: https://fairmlbook.org/

- id: _fairness_a
  accessed:
    - year: 2024
      month: 6
      day: 10
  citation-key: _fairness_a
  title: Fairness and machine learning
  type: webpage
  URL: https://fairmlbook.org/

- id: _first_
  accessed:
    - year: 2020
      month: 2
      day: 21
  citation-key: _first_
  title: >-
    First M87 Event Horizon Telescope Results. III. Data Processing and
    Calibration - IOPscience
  type: webpage
  URL: https://iopscience.iop.org/article/10.3847/2041-8213/ab0c57

- id: _first_a
  accessed:
    - year: 2020
      month: 2
      day: 21
  citation-key: _first_a
  title: >-
    First M87 Event Horizon Telescope Results. III. Data Processing and
    Calibration - IOPscience
  type: webpage
  URL: https://iopscience.iop.org/article/10.3847/2041-8213/ab0c57

- id: _google_
  accessed:
    - year: 2020
      month: 5
      day: 25
  citation-key: _google_
  title: Google Agenda - 7 dias, a partir de segunda-feira, 25 de maio de 2020
  type: webpage
  URL: https://calendar.google.com/calendar/r

- id: _google_a
  accessed:
    - year: 2020
      month: 9
      day: 16
  citation-key: _google_a
  title: Google Calendar Hours Calculator
  type: webpage
  URL: https://google-calendar-hours.com/

- id: _inbox_
  accessed:
    - year: 2023
      month: 10
      day: 16
  citation-key: _inbox_
  title: Inbox (2) - antonior92@gmail.com - Gmail
  type: webpage
  URL: https://mail.google.com/mail/u/0/#inbox

- id: _interpolatorsunderattack_
  abstract: >-
    An online LaTeX editor that’s easy to use. No installation, real-time
    collaboration, version control, hundreds of LaTeX templates, and more.
  accessed:
    - year: 2022
      month: 2
      day: 26
  citation-key: _interpolatorsunderattack_
  language: en
  title: interpolators-under-attack
  type: webpage
  URL: https://www.overleaf.com/project/5fabd1f02a42038b23abc59f

- id: _jobtalk_
  abstract: >-
    Data-driven ECG analysis Antônio Horta Ribeiro On this slide: Hello my name
    is Antonio and I will talk about my work on data-driven analysis of the
    Electrocardiogram, the ECG About how my research enabled new uses of this
    exam. And what challenges it creates for automatic control and machine
    lear...
  accessed:
    - year: 2024
      month: 3
      day: 12
  citation-key: _jobtalk_
  container-title: Google Docs
  language: en-GB
  title: JobTalk Uppsala Automatic Control
  type: webpage
  URL: >-
    https://docs.google.com/presentation/d/1xOLtsI769Fr_7SG-k6KVABh3vDy3ea4y7pwRiEME5j8/edit?ouid=112177853345925573315&usp=slides_home&ths=true&usp=embed_facebook

- id: _lottery_2020
  abstract: >-
    Metaphors are powerful tools to transfer ideas from one mind to another.
    Alan Kay introduced the alternative meaning of the term ‘desktop’ at Xerox
    PARC in 1970. Nowadays everyone - for a glimpse of a second - has to wonder
    what is actually meant when referring to a desktop. Recently, Deep Learning
    had the pleasure to welcome a new powerful metaphor: The Lottery Ticket
    Hypothesis (LTH).
  accessed:
    - year: 2020
      month: 6
      day: 29
  citation-key: _lottery_2020
  container-title: Rob's Homepage
  issued:
    - year: 2020
      month: 6
      day: 27
  language: en
  source: roberttlange.github.io
  title: 'The Lottery Ticket Hypothesis: A Survey'
  title-short: The Lottery Ticket Hypothesis
  type: webpage
  URL: https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/

- id: _mais_
  abstract: >-
    Com aumento no trabalho informal, número de pessoas trabalhando com apps de
    transporte cresceu 30% e nas ruas, 12%
  accessed:
    - year: 2020
      month: 1
      day: 23
  citation-key: _mais_
  container-title: Estadão
  language: pt-BR
  title: >-
    Mais 1 milhão de brasileiros passaram a trabalhar como motorista de
    aplicativo ou ambulante em 2018 - Economia
  type: webpage
  URL: >-
    https://economia.estadao.com.br/noticias/geral,mais-1-milhao-de-brasileiros-passaram-a-trabalhar-como-motorista-de-aplicativo-ou-ambulante-em-2018,70003129796

- id: _medarbetarportalen_
  accessed:
    - year: 2022
      month: 11
      day: 25
  citation-key: _medarbetarportalen_
  title: Medarbetarportalen - Uppsala University
  type: webpage
  URL: https://mp.uu.se/

- id: _researcher_
  accessed:
    - year: 2024
      month: 1
      day: 9
  citation-key: _researcher_
  title: Researcher - Swedish Migration Agency
  type: webpage
  URL: >-
    https://www.migrationsverket.se/formengineweb/v2/b8361dcd-7856-4b84-869d-2bfb48c6377e/complete

- id: _sponsors_
  accessed:
    - year: 2019
      month: 10
      day: 28
  citation-key: _sponsors_
  language: en
  title: Sponsors
  type: webpage
  URL: https://www.ifac2020.org/sponsors.html

- id: _understandingexplodinggradients_
  abstract: >-
    Um editor de LaTeX online fácil de usar. Sem instalação, colaboração em
    tempo real, controle de versões, centenas de templates LaTeX e mais.
  accessed:
    - year: 2019
      month: 10
      day: 8
  citation-key: _understandingexplodinggradients_
  language: pt
  title: understanding-exploding-gradients
  type: webpage
  URL: https://pt.overleaf.com/project/5cbed10170921e14664561f1

- id: _use_
  accessed:
    - year: 2024
      month: 5
      day: 21
  citation-key: _use_
  title: >-
    THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS - FISHER - 1936 -
    Annals of Eugenics - Wiley Online Library
  type: webpage
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x

- id: 10.1001/jamacardio.2017.3180
  abstract: >-
    In approximately 20 % of atrial fibrillation (AF)–related ischemic strokes,
    stroke is the first clinical manifestation of AF. Strategies are needed to
    identify and therapeutically address previously undetected AF.To quantify
    the incidence of AF in patients at high risk for but without previously
    known AF using an insertable cardiac monitor.This prospective, single-arm,
    multicenter study was conducted from November 2012 to January 2017. Visits
    took place at 57 centers in the United States and Europe. Patients with a
    CHADS2 score of 3 or greater (or 2 with at least 1 additional risk factor)
    were enrolled. Approximately 90 % had nonspecific symptoms potentially
    compatible with AF, such as fatigue, dyspnea, and/or palpitations.Patients
    underwent monitoring with an insertable cardiac monitor for 18 to 30
    months.The primary end point was adjudicated AF lasting 6 or more minutes
    and was assessed at 18 months. Other analyses included detection rates at
    points from 30 days to 30 months and among CHADS2 score subgroups. Median
    time from insertion to detection and the percentage of patients subsequently
    prescribed oral anticoagulation therapy was also determined.A total of 446
    patients were enrolled; 233 (52.2 %) were male, and the mean (SD) age was
    71.5 (9.9) years. A total of 385 patients (86.3 %) received an insertable
    cardiac monitor, met the primary analysis cohort definition, and were
    observed for a mean (SD) period of 22.5 (7.7) months. The detection rate of
    AF lasting 6 or more minutes at 18 months was 29.3 %. Detection rates at 30
    days and 6, 12, 24, and 30 months were 6.2 %, 20.4 %, 27.1 %, 33.6 %, and
    40.0 %, respectively. At 18 months, AF incidence was similar among patients
    with CHADS2 scores of 2 (24.7 %; 95 % CI, 17.3-31.4), 3 (32.7 %; 95 % CI,
    23.8-40.7), and 4 or greater (31.7 %; 95 % CI, 22.0-40.3) (P = .23). Median
    (interquartile) time from device insertion to first AF episode detection was
    123 (41-330) days. Of patients meeting the primary end point, 13 (10.2 %)
    had 1 or more episodes lasting 24 hours or longer, and oral anticoagulation
    therapy was prescribed for 72 patients (56.3 %).The incidence of previously
    undiagnosed AF may be substantial in patients with risk factors for AF and
    stroke. Atrial fibrillation would have gone undetected in most patients had
    monitoring been limited to 30 days. Further trials regarding the value of
    detecting subclinical AF and of prophylactic therapies are
    warranted.clinicaltrials.gov Identifier: NCT01727297
  author:
    - family: Reiffel
      given: James A.
    - family: Verma
      given: Atul
    - family: Kowey
      given: Peter R.
    - family: Halperin
      given: Jonathan L.
    - family: Gersh
      given: Bernard J.
    - family: Wachter
      given: Rolf
    - family: Pouliot
      given: Erika
    - family: Ziegler
      given: Paul D.
    - family: Investigators
      given: for the REVEAL AF
  citation-key: 10.1001/jamacardio.2017.3180
  container-title: JAMA Cardiology
  DOI: 10.1001/jamacardio.2017.3180
  ISSN: 2380-6583
  issue: '10'
  issued:
    - year: 2017
      month: 10
  page: 1120-1127
  title: >-
    Incidence of previously undiagnosed atrial fibrillation using insertable
    cardiac monitors in a high-risk population: The REVEAL AF study
  type: article-journal
  URL: https://doi.org/10.1001/jamacardio.2017.3180
  volume: '2'

- id: 10.5555/3524938.3525611
  abstract: >-
    We propose to study the generalization error of a learned predictor
    undefined in terms of that of a surrogate (potentially randomized) predictor
    that is coupled to undefined and designed to trade empirical risk for
    control of generalization error. In the case where undefined interpolates
    the data, it is interesting to consider theoretical surrogate predictors
    that are partially derandomized or rerandomized, e.g., fit to the training
    data but with modified label noise. We also show that replacing undefined by
    its conditional distribution with respect to an arbitrary σ-field is a
    convenient way to derandomize. We study two examples, inspired by the work
    of Nagarajan and Kolter (2019) and Bartlett et al. (2020), where the learned
    predictor undefined interpolates the training data with high probability,
    has small risk, and, yet, does not belong to a nonrandom class with a tight
    uniform bound on two-sided generalization error. At the same time, we bound
    the risk of undefined in terms of surrogates constructed by conditioning and
    denoising, respectively, and shown to belong to nonrandom classes with
    uniformly small generalization error.
  author:
    - family: Negrea
      given: Jeffrey
    - family: Dziugaite
      given: Gintare Karolina
    - family: Roy
      given: Daniel M.
  citation-key: 10.5555/3524938.3525611
  collection-title: ICML'20
  container-title: Proceedings of the 37th international conference on machine learning
  issued:
    - year: 2020
  number-of-pages: '10'
  publisher: JMLR.org
  title: >-
    In defense of uniform convergence: Generalization via derandomization with
    an application to interpolating predictors
  type: paper-conference

- id: aastrom_numerical_1966
  author:
    - family: \AAström
      given: Karl Johan
    - family: Bohlin
      given: Torsten
  citation-key: aastrom_numerical_1966
  container-title: 'PH Hammond: Theory of Self-Adaptive Control Systems'
  issued:
    - year: 1966
  page: 96–111
  publisher: Plenum Press
  title: >-
    Numerical identification of linear dynamic systems from normal operating
    records
  type: chapter

- id: aastrom_numerical_1966a
  author:
    - family: \AAström
      given: Karl Johan
    - family: Bohlin
      given: Torsten
  citation-key: aastrom_numerical_1966a
  container-title: 'PH Hammond: Theory of Self-Adaptive Control Systems'
  issued:
    - year: 1966
  page: 96-111
  publisher: Plenum Press
  title: >-
    Numerical Identification of Linear Dynamic Systems from Normal Operating
    Records
  type: chapter

- id: abadi_tensorflow_2015
  author:
    - family: Abadi
      given: Mart́ın
    - family: Agarwal
      given: Ashish
    - family: Barham
      given: Paul
    - family: Brevdo
      given: Eugene
    - family: Chen
      given: Zhifeng
    - family: Citro
      given: Craig
    - family: Corrado
      given: Greg S.
    - family: Davis
      given: Andy
    - family: Dean
      given: Jeffrey
    - family: Devin
      given: Matthieu
    - family: Ghemawat
      given: Sanjay
    - family: Goodfellow
      given: Ian
    - family: Harp
      given: Andrew
    - family: Irving
      given: Geoffrey
    - family: Isard
      given: Michael
    - family: Jia
      given: Yangqing
    - family: Jozefowicz
      given: Rafal
    - family: Kaiser
      given: Lukasz
    - family: Kudlur
      given: Manjunath
    - family: Levenberg
      given: Josh
    - family: Mané
      given: Dan
    - family: Monga
      given: Rajat
    - family: Moore
      given: Sherry
    - family: Murray
      given: Derek
    - family: Olah
      given: Chris
    - family: Schuster
      given: Mike
    - family: Shlens
      given: Jonathon
    - family: Steiner
      given: Benoit
    - family: Sutskever
      given: Ilya
    - family: Talwar
      given: Kunal
    - family: Tucker
      given: Paul
    - family: Vanhoucke
      given: Vincent
    - family: Vasudevan
      given: Vijay
    - family: Viégas
      given: Fernanda
    - family: Vinyals
      given: Oriol
    - family: Warden
      given: Pete
    - family: Wattenberg
      given: Martin
    - family: Wicke
      given: Martin
    - family: Yu
      given: Yuan
    - family: Zheng
      given: Xiaoqiang
  citation-key: abadi_tensorflow_2015
  issued:
    - year: 2015
  title: 'TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems'
  type: report
  URL: http://tensorflow.org/

- id: abdalmoaty_application_2018
  abstract: >-
    The estimation problem of stochastic Wiener-Hammerstein models is recognized
    to be challenging, mainly due to the analytical intractability of the
    likelihood function. In this contribution, we apply a computationally
    attractive prediction error method estimator to a real-data stochastic
    Wiener-Hammerstein benchmark problem. The estimator is defined using a
    deterministic predictor that is nonlinear in the input. The prediction error
    method results in tractable expressions, and Monte Carlo approximations are
    not necessary. This allows us to tackle several issues considered
    challenging from the perspective of the current mainstream approach. Under
    mild conditions, the estimator can be shown to be consistent and
    asymptotically normal. The results of the method applied to the benchmark
    data are presented and discussed.
  accessed:
    - year: 2018
      month: 11
      day: 26
  author:
    - family: Abdalmoaty
      given: Mohamed Rasheed
    - family: Hjalmarsson
      given: Håkan
  citation-key: abdalmoaty_application_2018
  collection-title: 18th IFAC Symposium on System Identification SYSID 2018
  container-title: IFAC-PapersOnLine
  container-title-short: IFAC-PapersOnLine
  DOI: 10/gfkd8g
  ISSN: 2405-8963
  issue: '15'
  issued:
    - year: 2018
      month: 1
      day: 1
  page: 784-789
  source: ScienceDirect
  title: >-
    Application of a Linear PEM Estimator to a Stochastic Wiener-Hammerstein
    Benchmark Problem
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S2405896318317968
  volume: '51'

- id: abdulhai_reinforcement_2003
  author:
    - family: Abdulhai
      given: Baher
    - family: Kattan
      given: Lina
  citation-key: abdulhai_reinforcement_2003
  container-title: Canadian Journal of Civil Engineering
  DOI: 10.1139/l03-014
  issue: '6'
  issued:
    - year: 2003
  page: 981–991
  source: Google Scholar
  title: >-
    Reinforcement learning: Introduction to theory and potential for transport
    applications
  title-short: Reinforcement learning
  type: article-journal
  volume: '30'

- id: acharya_application_2017
  accessed:
    - year: 2018
      month: 10
      day: 21
  author:
    - family: Acharya
      given: U. Rajendra
    - family: Fujita
      given: Hamido
    - family: Oh
      given: Shu Lih
    - family: Hagiwara
      given: Yuki
    - family: Tan
      given: Jen Hong
    - family: Adam
      given: Muhammad
  citation-key: acharya_application_2017
  container-title: Information Sciences
  DOI: 10.1016/j.ins.2017.06.027
  ISSN: '00200255'
  issued:
    - year: 2017
      month: 11
  language: en
  page: 190-198
  source: Crossref
  title: >-
    Application of deep convolutional neural network for automated detection of
    myocardial infarction using ECG signals
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0020025517308009
  volume: 415-416

- id: acharya_deep_2017
  accessed:
    - year: 2018
      month: 10
      day: 21
  author:
    - family: Acharya
      given: U. Rajendra
    - family: Oh
      given: Shu Lih
    - family: Hagiwara
      given: Yuki
    - family: Tan
      given: Jen Hong
    - family: Adam
      given: Muhammad
    - family: Gertych
      given: Arkadiusz
    - family: Tan
      given: Ru San
  citation-key: acharya_deep_2017
  container-title: Computers in Biology and Medicine
  DOI: 10.1016/j.compbiomed.2017.08.022
  ISSN: '00104825'
  issued:
    - year: 2017
      month: 10
  language: en
  page: 389-396
  source: Crossref
  title: A deep convolutional neural network model to classify heartbeats
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0010482517302810
  volume: '89'

- id: achille_critical_2017
  abstract: >-
    Similar to humans and animals, deep artificial neural networks exhibit
    critical periods during which a temporary stimulus deficit can impair the
    development of a skill. The extent of the impairment depends on the onset
    and length of the deficit window, as in animal models, and on the size of
    the neural network. Deficits that do not affect low-level statistics, such
    as vertical flipping of the images, have no lasting effect on performance
    and can be overcome with further training. To better understand this
    phenomenon, we use the Fisher Information of the weights to measure the
    effective connectivity between layers of a network during training.
    Counterintuitively, information rises rapidly in the early phases of
    training, and then decreases, preventing redistribution of information
    resources in a phenomenon we refer to as a loss of "Information Plasticity".
    Our analysis suggests that the first few epochs are critical for the
    creation of strong connections that are optimal relative to the input data
    distribution. Once such strong connections are created, they do not appear
    to change during additional training. These findings suggest that the
    initial learning transient, under-scrutinized compared to asymptotic
    behavior, plays a key role in determining the outcome of the training
    process. Our findings, combined with recent theoretical results in the
    literature, also suggest that forgetting (decrease of information in the
    weights) is critical to achieving invariance and disentanglement in
    representation learning. Finally, critical periods are not restricted to
    biological systems, but can emerge naturally in learning systems, whether
    biological or artificial, due to fundamental constrains arising from
    learning dynamics and information processing.
  accessed:
    - year: 2019
      month: 6
      day: 1
  author:
    - family: Achille
      given: Alessandro
    - family: Rovere
      given: Matteo
    - family: Soatto
      given: Stefano
  citation-key: achille_critical_2017
  container-title: arXiv:1711.08856 [cs, q-bio, stat]
  issued:
    - year: 2017
      month: 11
      day: 23
  source: arXiv.org
  title: Critical Learning Periods in Deep Neural Networks
  type: article-journal
  URL: http://arxiv.org/abs/1711.08856

- id: adiwardana_humanlike_2020
  abstract: >-
    We present Meena, a multi-turn open-domain chatbot trained end-to-end on
    data mined and filtered from public domain social media conversations. This
    2.6B parameter neural network is simply trained to minimize perplexity of
    the next token. We also propose a human evaluation metric called
    Sensibleness and Specificity Average (SSA), which captures key elements of a
    human-like multi-turn conversation. Our experiments show strong correlation
    between perplexity and SSA. The fact that the best perplexity end-to-end
    trained Meena scores high on SSA (72% on multi-turn evaluation) suggests
    that a human-level SSA of 86% is potentially within reach if we can better
    optimize perplexity. Additionally, the full version of Meena (with a
    filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in
    absolute SSA than the existing chatbots we evaluated.
  accessed:
    - year: 2020
      month: 2
      day: 6
  author:
    - family: Adiwardana
      given: Daniel
    - family: Luong
      given: Minh-Thang
    - family: So
      given: David R.
    - family: Hall
      given: Jamie
    - family: Fiedel
      given: Noah
    - family: Thoppilan
      given: Romal
    - family: Yang
      given: Zi
    - family: Kulshreshtha
      given: Apoorv
    - family: Nemade
      given: Gaurav
    - family: Lu
      given: Yifeng
    - family: Le
      given: Quoc V.
  citation-key: adiwardana_humanlike_2020
  container-title: arXiv:2001.09977 [cs, stat]
  issued:
    - year: 2020
      month: 1
      day: 31
  source: arXiv.org
  title: Towards a Human-like Open-Domain Chatbot
  type: article-journal
  URL: http://arxiv.org/abs/2001.09977

- id: adlam_neural_2020
  abstract: >-
    Modern deep learning models employ considerably more parameters than
    required to ﬁt the training data. Whereas conventional statistical wisdom
    suggests such models should drastically overﬁt, in practice these models
    generalize remarkably well. An emerging paradigm for describing this
    unexpected behavior is in terms of a double descent curve, in which
    increasing a model’s capacity causes its test error to ﬁrst decrease, then
    increase to a maximum near the interpolation threshold, and then decrease
    again in the overparameterized regime. Recent efforts to explain this
    phenomenon theoretically have focused on simple settings, such as linear
    regression or kernel regression with unstructured random features, which we
    argue are too coarse to reveal important nuances of actual neural networks.
    We provide a precise high-dimensional asymptotic analysis of generalization
    under kernel regression with the Neural Tangent Kernel, which characterizes
    the behavior of wide neural networks optimized with gradient descent. Our
    results reveal that the test error has non-monotonic behavior deep in the
    overparameterized regime and can even exhibit additional peaks and descents
    when the number of parameters scales quadratically with the dataset size.
  author:
    - family: Adlam
      given: Ben
    - family: Pennington
      given: Jeffrey
  citation-key: adlam_neural_2020
  container-title: >-
    Proceedings of the 37 th International Conference on Machine Learning, PMLR
    119
  issued:
    - year: 2020
  language: en
  source: Zotero
  title: >-
    The Neural Tangent Kernel in High Dimensions: Triple Descent and a
    Multi-Scale Theory of Generalization
  type: article-journal

- id: adlam_random_2019
  abstract: >-
    One of the distinguishing characteristics of modern deep learning systems is
    that they typically employ neural network architectures that utilize
    enormous numbers of parameters, often in the millions and sometimes even in
    the billions. While this paradigm has inspired significant research on the
    properties of large networks, relatively little work has been devoted to the
    fact that these networks are often used to model large complex datasets,
    which may themselves contain millions or even billions of constraints. In
    this work, we focus on this high-dimensional regime in which both the
    dataset size and the number of features tend to infinity. We analyze the
    performance of a simple regression model trained on the random features
    $F=f(WX+B)$ for a random weight matrix $W$ and random bias vector $B$,
    obtaining an exact formula for the asymptotic training error on a noisy
    autoencoding task. The role of the bias can be understood as parameterizing
    a distribution over activation functions, and our analysis directly
    generalizes to such distributions, even those not expressible with a
    traditional additive bias. Intriguingly, we find that a mixture of
    nonlinearities can outperform the best single nonlinearity on the noisy
    autoecndoing task, suggesting that mixtures of nonlinearities might be
    useful for approximate kernel methods or neural network architecture design.
  accessed:
    - year: 2021
      month: 6
      day: 28
  author:
    - family: Adlam
      given: Ben
    - family: Levinson
      given: Jake
    - family: Pennington
      given: Jeffrey
  citation-key: adlam_random_2019
  container-title: arXiv:1912.00827 [cs, stat]
  issued:
    - year: 2019
      month: 12
      day: 2
  source: arXiv.org
  title: A Random Matrix Perspective on Mixtures of Nonlinearities for Deep Learning
  type: article-journal
  URL: http://arxiv.org/abs/1912.00827

- id: advani_highdimensional_2017
  abstract: >-
    We perform an average case analysis of the generalization dynamics of large
    neural networks trained using gradient descent. We study the
    practically-relevant "high-dimensional" regime where the number of free
    parameters in the network is on the order of or even larger than the number
    of examples in the dataset. Using random matrix theory and exact solutions
    in linear models, we derive the generalization error and training error
    dynamics of learning and analyze how they depend on the dimensionality of
    data and signal to noise ratio of the learning problem. We find that the
    dynamics of gradient descent learning naturally protect against overtraining
    and overfitting in large networks. Overtraining is worst at intermediate
    network sizes, when the effective number of free parameters equals the
    number of samples, and thus can be reduced by making a network smaller or
    larger. Additionally, in the high-dimensional regime, low generalization
    error requires starting with small initial weights. We then turn to
    non-linear neural networks, and show that making networks very large does
    not harm their generalization performance. On the contrary, it can in fact
    reduce overtraining, even without early stopping or regularization of any
    sort. We identify two novel phenomena underlying this behavior in
    overcomplete models: first, there is a frozen subspace of the weights in
    which no learning occurs under gradient descent; and second, the statistical
    properties of the high-dimensional regime yield better-conditioned input
    correlations which protect against overtraining. We demonstrate that naive
    application of worst-case theories such as Rademacher complexity are
    inaccurate in predicting the generalization performance of deep neural
    networks, and derive an alternative bound which incorporates the frozen
    subspace and conditioning effects and qualitatively matches the behavior
    observed in simulation.
  accessed:
    - year: 2020
      month: 11
      day: 26
  author:
    - family: Advani
      given: Madhu S.
    - family: Saxe
      given: Andrew M.
  citation-key: advani_highdimensional_2017
  container-title: arXiv:1710.03667 [physics, q-bio, stat]
  issued:
    - year: 2017
      month: 10
      day: 10
  source: arXiv.org
  title: High-dimensional dynamics of generalization error in neural networks
  type: article-journal
  URL: http://arxiv.org/abs/1710.03667

- id: advani_highdimensional_2020
  abstract: >-
    We perform an analysis of the average generalization dynamics of large
    neural networks trained using gradient descent. We study the
    practically-relevant “high-dimensional” regime where the number of free
    parameters in the network is on the order of or even larger than the number
    of examples in the dataset. Using random matrix theory and exact solutions
    in linear models, we derive the generalization error and training error
    dynamics of learning and analyze how they depend on the dimensionality of
    data and signal to noise ratio of the learning problem. We find that the
    dynamics of gradient descent learning naturally protect against overtraining
    and overfitting in large networks. Overtraining is worst at intermediate
    network sizes, when the effective number of free parameters equals the
    number of samples, and thus can be reduced by making a network smaller or
    larger. Additionally, in the high-dimensional regime, low generalization
    error requires starting with small initial weights. We then turn to
    non-linear neural networks, and show that making networks very large does
    not harm their generalization performance. On the contrary, it can in fact
    reduce overtraining, even without early stopping or regularization of any
    sort. We identify two novel phenomena underlying this behavior in
    overcomplete models: first, there is a frozen subspace of the weights in
    which no learning occurs under gradient descent; and second, the statistical
    properties of the high-dimensional regime yield better-conditioned input
    correlations which protect against overtraining. We demonstrate that
    standard application of theories such as Rademacher complexity are
    inaccurate in predicting the generalization performance of deep neural
    networks, and derive an alternative bound which incorporates the frozen
    subspace and conditioning effects and qualitatively matches the behavior
    observed in simulation.
  accessed:
    - year: 2021
      month: 5
      day: 28
  author:
    - family: Advani
      given: Madhu S.
    - family: Saxe
      given: Andrew M.
    - family: Sompolinsky
      given: Haim
  citation-key: advani_highdimensional_2020
  container-title: Neural Networks
  container-title-short: Neural Networks
  DOI: 10.1016/j.neunet.2020.08.022
  ISSN: 0893-6080
  issued:
    - year: 2020
      month: 12
      day: 1
  page: 428-446
  source: ScienceDirect
  title: High-dimensional dynamics of generalization error in neural networks
  type: article-journal
  volume: '132'

- id: agrawal_differentiable_2019
  abstract: >-
    Recent work has shown how to embed differentiable optimization problems
    (that is, problems whose solutions can be backpropagated through) as layers
    within deep learning architectures. This method provides a useful inductive
    bias for certain problems, but existing software for differentiable
    optimization layers is rigid and difﬁcult to apply to new settings. In this
    paper, we propose an approach to differentiating through disciplined convex
    programs, a subclass of convex optimization problems used by domain-speciﬁc
    languages (DSLs) for convex optimization. We introduce disciplined
    parametrized programming, a subset of disciplined convex programming, and we
    show that every disciplined parametrized program can be represented as the
    composition of an afﬁne map from parameters to problem data, a solver, and
    an afﬁne map from the solver’s solution to a solution of the original
    problem (a new form we refer to as afﬁne-solver-afﬁne form). We then
    demonstrate how to efﬁciently differentiate through each of these
    components, allowing for end-to-end analytical differentiation through the
    entire convex program. We implement our methodology in version 1.1 of CVXPY,
    a popular Python-embedded DSL for convex optimization, and additionally
    implement differentiable layers for disciplined convex programs in PyTorch
    and TensorFlow 2.0. Our implementation signiﬁcantly lowers the barrier to
    using convex optimization problems in differentiable programs. We present
    applications in linear machine learning models and in stochastic control,
    and we show that our layer is competitive (in execution time) compared to
    specialized differentiable solvers from past work.
  author:
    - family: Agrawal
      given: Akshay
    - family: Amos
      given: Brandon
    - family: Barratt
      given: Shane
    - family: Boyd
      given: Stephen
  citation-key: agrawal_differentiable_2019
  event-title: 33rd Conference on Neural Information Processing Systems
  issued:
    - year: 2019
  language: en
  page: '19'
  source: Zotero
  title: Differentiable Convex Optimization Layers
  type: paper-conference

- id: aguirre_bird_2019
  abstract: >-
    This text aims at providing a bird's eye view of system identification with
    special attention to nonlinear systems. The driving force is to give a
    feeling for the philosophical problems facing those that build mathematical
    models from data. Special attention will be given to grey-box approaches in
    nonlinear system identification. In this text, grey-box methods use
    auxiliary information such as the system steady-state data, possible
    symmetries, some bifurcations and the presence of hysteresis. The text ends
    with a sample of applications. No attempt is made to be thorough nor to
    survey such an extensive and mature field as system identification. In most
    parts references will be provided for a more detailed study.
  accessed:
    - year: 2020
      month: 2
      day: 13
  author:
    - family: Aguirre
      given: Luis Antonio
  citation-key: aguirre_bird_2019
  container-title: arXiv:1907.06803 [cs, eess]
  issued:
    - year: 2019
      month: 7
      day: 19
  source: arXiv.org
  title: A Bird's Eye View of Nonlinear System Identification
  type: article-journal
  URL: http://arxiv.org/abs/1907.06803

- id: aguirre_development_2017
  abstract: >-
    Downhole pressure is an important process variable in the operation of
    gas-lifted oil wells. The device installed in order to measure this variable
    is often called a Permanent Downhole Gauge (PDG). Replacing a faulty PDG is
    often not economically viable and to have an alternative estimate of the
    downhole pressure is an important goal. Using data from operating PDGs, this
    paper describes a number of issues dealt with in the development of soft
    sensors for several deepwater gas-lifted oil wells. Some of the tested
    models include nonlinear polynomials, neural networks, committee machines,
    unscented Kalman filters and filter banks. The variety of model classes used
    in addition to the diversity of oil wells considered brings to light some of
    the key-problems that have to be faced and reveal the strengths and
    weaknesses of each alternative solution. A major constraint throughout the
    work was the use of historical data, hence no specific tests were performed
    at any time. The aim of this work is to discuss the procedures, pros and
    cons of the tested solutions and to point to possible future directions of
    research.
  accessed:
    - year: 2020
      month: 2
      day: 17
  author:
    - family: Aguirre
      given: Luis A.
    - family: Teixeira
      given: Bruno O.S.
    - family: Barbosa
      given: Bruno H.G.
    - family: Teixeira
      given: Alex F.
    - family: Campos
      given: Mario C.M.M.
    - family: Mendes
      given: Eduardo M.A.M.
  citation-key: aguirre_development_2017
  container-title: Control Engineering Practice
  container-title-short: Control Engineering Practice
  DOI: 10.1016/j.conengprac.2017.06.002
  ISSN: '09670661'
  issued:
    - year: 2017
      month: 8
  language: en
  page: 83-99
  source: DOI.org (Crossref)
  title: >-
    Development of soft sensors for permanent downhole Gauges in deepwater oil
    wells
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0967066117301284
  volume: '65'

- id: aguirre_dynamical_1995
  author:
    - family: Aguirre
      given: Luis Antonio
    - family: Billings
      given: Stephen A
  citation-key: aguirre_dynamical_1995
  container-title: 'Physica D: Nonlinear Phenomena'
  DOI: 10.1016/0167-2789(95)90053-5
  issue: 1-2
  issued:
    - year: 1995
  page: 26–40
  title: Dynamical effects of overparametrization in nonlinear models
  type: article-journal
  volume: '80'

- id: aguirre_introducao_2004
  author:
    - family: Aguirre
      given: Luis Antonio
  citation-key: aguirre_introducao_2004
  issued:
    - year: 2004
  note: '00654'
  publisher: Editora UFMG
  title: >-
    Introdução à identificação de sistemas–Técnicas lineares e não-lineares
    aplicadas a sistemas reais
  type: book

- id: aguirre_prediction_2010
  author:
    - family: Aguirre
      given: Luis A
    - family: Barbosa
      given: Bruno HG
    - family: Braga
      given: Antônio P
  citation-key: aguirre_prediction_2010
  container-title: Mechanical Systems and Signal Processing
  DOI: 10.1016/j.ymssp.2010.05.003
  issue: '8'
  issued:
    - year: 2010
  page: 2855–2867
  title: >-
    Prediction and simulation errors in parameter estimation for nonlinear
    systems
  type: article-journal
  volume: '24'

- id: ahlfors_complex_1966
  author:
    - family: Ahlfors
      given: L.V.
  citation-key: ahlfors_complex_1966
  issued:
    - year: 1966
  title: Complex Analysis
  type: book
  URL: https://books.google.com.br/books?id=RfYK28TcZEwC

- id: aizenberg_multilayer_2016
  author:
    - family: Aizenberg
      given: Igor
    - family: Sheremetov
      given: Leonid
    - family: Villa-Vargas
      given: Luis
    - family: Martinez-Muñoz
      given: Jorge
  citation-key: aizenberg_multilayer_2016
  container-title: Neurocomputing
  DOI: 10.1016/j.neucom.2015.06.092
  issued:
    - year: 2016
  page: 980–989
  title: >-
    Multilayer Neural Network with Multi-Valued Neurons in time series
    forecasting of oil production
  type: article-journal
  volume: '175'

- id: akaike_new_1974
  author:
    - family: Akaike
      given: H
  citation-key: akaike_new_1974
  container-title: IEEE Transactions on Automatic Control
  container-title-short: IEEE Transactions on Automatic Control
  DOI: 10.1109/TAC.1974.1100705
  ISSN: 0018-9286
  issue: '6'
  issued:
    - year: 1974
      month: 12
  page: 716-723
  title: A new look at the statistical model identification
  type: article-journal
  volume: '19'

- id: akiyama_first_2019
  author:
    - family: Akiyama
      given: Kazunori
    - family: Alberdi
      given: Antxon
    - family: Alef
      given: Walter
    - family: Asada
      given: Keiichi
    - family: Azulay
      given: Rebecca
    - family: Baczko
      given: Anne-Kathrin
    - family: Ball
      given: David
    - family: Baloković
      given: Mislav
    - family: Barrett
      given: John
    - family: Bintley
      given: Dan
  citation-key: akiyama_first_2019
  container-title: The Astrophysical Journal Letters
  container-title-short: The Astrophysical Journal Letters
  ISSN: 2041-8205
  issue: '1'
  issued:
    - year: 2019
  page: L3
  title: >-
    First M87 event horizon telescope results. iii. data processing and
    calibration
  type: article-journal
  volume: '875'

- id: alammar_illustrated_
  abstract: >-
    Discussions:

    Hacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points,
    20 comments)



    Translations: Chinese (Simplified), Persian


    The year 2018 has been an inflection point for machine learning models
    handling text (or more accurately, Natural Language Processing or NLP for
    short). Our conceptual understanding of how best to represent words and
    sentences in a way that best captures underlying meanings and relationships
    is rapidly evolving. Moreover, the NLP community has been putting forward
    incredibly powerful components that you can freely download and use in your
    own models and pipelines (It’s been referred to as NLP’s ImageNet moment,
    referencing how years ago similar developments accelerated the development
    of machine learning in Computer Vision tasks).
  accessed:
    - year: 2019
      month: 6
      day: 8
  author:
    - family: Alammar
      given: Jay
  citation-key: alammar_illustrated_
  title: The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)
  type: webpage
  URL: http://jalammar.github.io/illustrated-bert/

- id: alammar_illustrated_a
  abstract: >-
    Discussions:

    Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3
    comments)



    Translations: Chinese (Simplified), Korean


    Watch: MIT’s Deep Learning State of the Art lecture referencing this post


    In the previous post, we looked at Attention – a ubiquitous method in modern
    deep learning models. Attention is a concept that helped improve the
    performance of neural machine translation applications. In this post, we
    will look at The Transformer – a model that uses attention to boost the
    speed with which these models can be trained. The Transformers outperforms
    the Google Neural Machine Translation model in specific tasks. The biggest
    benefit, however, comes from how The Transformer lends itself to
    parallelization. It is in fact Google Cloud’s recommendation to use The
    Transformer as a reference model to use their Cloud TPU offering. So let’s
    try to break the model apart and look at how it functions.


    The Transformer was proposed in the paper Attention is All You Need. A
    TensorFlow implementation of it is available as a part of the Tensor2Tensor
    package. Harvard’s NLP group created a guide annotating the paper with
    PyTorch implementation. In this post, we will attempt to oversimplify things
    a bit and introduce the concepts one by one to hopefully make it easier to
    understand to people without in-depth knowledge of the subject matter.


    A High-Level Look

    Let’s begin by looking at the model as a single black box. In a machine
    translation application, it would take a sentence in one language, and
    output its translation in another.
  accessed:
    - year: 2019
      month: 6
      day: 10
  author:
    - family: Alammar
      given: Jay
  citation-key: alammar_illustrated_a
  title: The Illustrated Transformer
  type: webpage
  URL: http://jalammar.github.io/illustrated-transformer/

- id: alammar_illustrated_b
  abstract: >-
    Discussions:

    Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3
    comments)



    Translations: Chinese (Simplified), Korean


    Watch: MIT’s Deep Learning State of the Art lecture referencing this post


    In the previous post, we looked at Attention – a ubiquitous method in modern
    deep learning models. Attention is a concept that helped improve the
    performance of neural machine translation applications. In this post, we
    will look at The Transformer – a model that uses attention to boost the
    speed with which these models can be trained. The Transformers outperforms
    the Google Neural Machine Translation model in specific tasks. The biggest
    benefit, however, comes from how The Transformer lends itself to
    parallelization. It is in fact Google Cloud’s recommendation to use The
    Transformer as a reference model to use their Cloud TPU offering. So let’s
    try to break the model apart and look at how it functions.


    The Transformer was proposed in the paper Attention is All You Need. A
    TensorFlow implementation of it is available as a part of the Tensor2Tensor
    package. Harvard’s NLP group created a guide annotating the paper with
    PyTorch implementation. In this post, we will attempt to oversimplify things
    a bit and introduce the concepts one by one to hopefully make it easier to
    understand to people without in-depth knowledge of the subject matter.


    A High-Level Look

    Let’s begin by looking at the model as a single black box. In a machine
    translation application, it would take a sentence in one language, and
    output its translation in another.
  accessed:
    - year: 2019
      month: 6
      day: 26
  author:
    - family: Alammar
      given: Jay
  citation-key: alammar_illustrated_b
  title: The Illustrated Transformer
  type: webpage
  URL: http://jalammar.github.io/illustrated-transformer/

- id: alday_classification_2020
  abstract: >-
    <p>The subject of the PhysioNet/Computing in Cardiology Challenge 2020 was
    the identification of cardiac abnormalities in 12-lead electrocardiogram
    (ECG) recordings. A total of 66,405 recordings were sourced from hospital
    systems from four distinct countries and annotated with clinical diagnoses,
    including 43,101 annotated recordings that were posted publicly. For this
    Challenge, we asked participants to design working, open-source algorithms
    for identifying cardiac abnormalities in 12-lead ECG recordings. This
    Challenge provided several innovations. First, we sourced data from multiple
    institutions from around the world with different demographics, allowing us
    to assess the generalizability of the algorithms. Second, we required
    participants to submit both their trained models and the code for
    reproducing their trained models from the training data, which aids the
    generalizability and reproducibility of the algorithms. Third, we proposed a
    novel evaluation metric that considers different misclassification errors
    for different cardiac abnormalities, reflecting the clinical reality that
    some diagnoses have similar outcomes and varying risks. Over 200 teams
    submitted 850 algorithms (432 of which successfully ran) during the
    unofficial and official phases of the Challenge, representing a diversity of
    approaches from both academia and industry for identifying cardiac
    abnormalities. The official phase of the Challenge is ongoing.</p>
  accessed:
    - year: 2020
      month: 9
      day: 1
  author:
    - family: Alday
      given: Erick A. Perez
    - family: Gu
      given: Annie
    - family: Shah
      given: Amit
    - family: Robichaux
      given: Chad
    - family: Wong
      given: An-Kwok Ian
    - family: Liu
      given: Chengyu
    - family: Liu
      given: Feifei
    - family: Rad
      given: Ali Bahrami
    - family: Elola
      given: Andoni
    - family: Seyedi
      given: Salman
    - family: Li
      given: Qiao
    - family: Sharma
      given: Ashish
    - family: Clifford
      given: Gari D.
    - family: Reyna
      given: Matthew A.
  citation-key: alday_classification_2020
  container-title: medRxiv
  DOI: 10.1101/2020.08.11.20172601
  issued:
    - year: 2020
      month: 8
      day: 14
  language: en
  license: >-
    © 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available
    under a Creative Commons License (Attribution 4.0 International), CC BY 4.0,
    as described at http://creativecommons.org/licenses/by/4.0/
  page: 2020.08.11.20172601
  publisher: Cold Spring Harbor Laboratory Press
  source: www.medrxiv.org
  title: >-
    Classification of 12-lead ECGs: the PhysioNet/Computing in Cardiology
    Challenge 2020
  title-short: Classification of 12-lead ECGs
  type: article-journal
  URL: https://www.medrxiv.org/content/10.1101/2020.08.11.20172601v1

- id: alemohammad_recurrent_2021
  author:
    - family: Alemohammad
      given: Sina
    - family: Wang
      given: Zichao
    - family: Balestriero
      given: Randall
    - family: Baraniuk
      given: Richard
  citation-key: alemohammad_recurrent_2021
  container-title: International conference on learning representations
  issued:
    - year: 2021
  title: The recurrent neural tangent kernel
  type: paper-conference
  URL: https://openreview.net/forum?id=3T9iFICe0Y9

- id: alkmim_improving_2012
  abstract: >-
    PROBLEM: The Brazilian population lacks equitable access to specialized
    health care and diagnostic tests, especially in remote municipalities, where
    health professionals often feel isolated and staff turnover is high.
    Telehealth has the potential to improve patients' access to specialized
    health care, but little is known about it in terms of cost-effectiveness,
    access to services or user satisfaction. APPROACH: In 2005, the State
    Government of Minas Gerais, Brazil, funded the establishment of the
    Telehealth Network, intended to connect university hospitals with the
    state's remote municipal health departments; support professionals in
    providing tele-assistance; and perform tele-electrocardiography and
    teleconsultations. The network uses low-cost equipment and has employed
    various strategies to overcome the barriers to telehealth use. LOCAL
    SETTING: The Telehealth Network connects specialists in state university
    hospitals with primary health-care professionals in 608 municipalities of
    the large state of Minas Gerais, many of them in remote areas. RELEVANT
    CHANGES: From June 2006 to October 2011, 782,773 electrocardiograms and 30
    883 teleconsultations were performed through the network, and 6000 health
    professionals were trained in its use. Most of these professionals (97%)
    were satisfied with the system, which was cost-effective, economically
    viable and averted 81% of potential case referrals to distant centres.
    LESSONS LEARNT: To succeed, a telehealth service must be part of a
    collaborative network, meet the real needs of local health professionals,
    use simple technology and have at least some face-to-face components. If
    applied to health problems for which care is in high demand, this type of
    service can be economically viable and can help to improve patient access to
    specialized health care.
  archive: PubMed
  archive_location: '22589571'
  author:
    - family: Alkmim
      given: Maria Beatriz
    - family: Figueira
      given: Renato Minelli
    - family: Marcolino
      given: Milena Soriano
    - family: Cardoso
      given: Clareci Silva
    - family: Pena de Abreu
      given: Monica
    - family: Cunha
      given: Lemuel Rodrigues
    - family: Cunha
      given: Daniel Ferreira
      non-dropping-particle: da
    - family: Antunes
      given: Andre Pires
    - family: Resende
      given: Adélson Geraldo de A
    - family: Resende
      given: Elmiro Santos
    - family: Ribeiro
      given: Antonio Luiz Pinho
  citation-key: alkmim_improving_2012
  container-title: Bulletin of the World Health Organization
  DOI: 10/f3x7px
  ISSN: 1564-0604
  issue: '5'
  issued:
    - year: 2012
      month: 5
      day: 1
  page: 373-378
  title: >-
    Improving patient access to specialized health care: the Telehealth Network
    of Minas Gerais, Brazil
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pubmed/22589571
  volume: '90'

- id: allen_assessing_1996
  author:
    - family: Allen
      given: John
    - family: Murray
      given: Alan
  citation-key: allen_assessing_1996
  container-title: Physiological measurement
  DOI: 10.1088/0967-3334/17/4/002
  issue: '4'
  issued:
    - year: 1996
  page: '249'
  source: Google Scholar
  title: Assessing ECG signal quality on a coronary care unit
  type: article-journal
  volume: '17'

- id: allen-zhu_convergence_2019
  abstract: >-
    Deep neural networks (DNNs) have demonstrated dominating performance in many
    fields; since AlexNet, networks used in practice are going wider and deeper.
    On the theoretical side, a long line of works has been focusing on training
    neural networks with one hidden layer. The theory of multi-layer networks
    remains largely unsettled. In this work, we prove why stochastic gradient
    descent (SGD) can find $\textit{global minima}$ on the training objective of
    DNNs in $\textit{polynomial time}$. We only make two assumptions: the inputs
    are non-degenerate and the network is over-parameterized. The latter means
    the network width is sufficiently large: $\textit{polynomial}$ in $L$, the
    number of layers and in $n$, the number of samples. Our key technique is to
    derive that, in a sufficiently large neighborhood of the random
    initialization, the optimization landscape is almost-convex and semi-smooth
    even with ReLU activations. This implies an equivalence between
    over-parameterized neural networks and neural tangent kernel (NTK) in the
    finite (and polynomial) width setting. As concrete examples, starting from
    randomly initialized weights, we prove that SGD can attain 100% training
    accuracy in classification tasks, or minimize regression loss in linear
    convergence speed, with running time polynomial in $n,L$. Our theory applies
    to the widely-used but non-smooth ReLU activation, and to any smooth and
    possibly non-convex loss functions. In terms of network architectures, our
    theory at least applies to fully-connected neural networks, convolutional
    neural networks (CNN), and residual neural networks (ResNet).
  author:
    - family: Allen-Zhu
      given: Zeyuan
    - family: Li
      given: Yuanzhi
    - family: Song
      given: Zhao
  citation-key: allen-zhu_convergence_2019
  container-title: Proceedings of the 36 th International Conference on Machine Learning, PMLR
  issued:
    - year: 2019
  title: A Convergence Theory for Deep Learning via Over-Parameterization
  type: article-journal
  URL: http://arxiv.org/abs/1811.03962
  volume: '97'

- id: alonso_simple_2013
  abstract: "BackgroundTools for the prediction of atrial fibrillation (AF) may identify high‐risk individuals more likely to benefit from preventive interventions and serve as a benchmark to test novel putative risk factors.Methods and ResultsIndividual‐level data from 3 large cohorts in the United States (Atherosclerosis Risk in Communities [ARIC] study, the Cardiovascular Health Study [CHS], and the Framingham Heart Study [FHS]), including 18\_556 men and women aged 46 to 94\_years (19% African Americans, 81% whites) were pooled to derive predictive models for AF using clinical variables. Validation of the derived models was performed in 7672 participants from the Age, Gene and Environment—Reykjavik study (AGES) and the Rotterdam Study (RS). The analysis included 1186 incident AF cases in the derivation cohorts and 585 in the validation cohorts. A simple 5‐year predictive model including the variables age, race, height, weight, systolic and diastolic blood pressure, current smoking, use of antihypertensive medication, diabetes, and history of myocardial infarction and heart failure had good discrimination (C‐statistic, 0.765; 95% CI, 0.748 to 0.781). Addition of variables from the electrocardiogram did not improve the overall model discrimination (C‐statistic, 0.767; 95% CI, 0.750 to 0.783; categorical net reclassification improvement, −0.0032; 95% CI, −0.0178 to 0.0113). In the validation cohorts, discrimination was acceptable (AGES C‐statistic, 0.664; 95% CI, 0.632 to 0.697 and RS C‐statistic, 0.705; 95% CI, 0.664 to 0.747) and calibration was adequate.ConclusionA risk model including variables readily available in primary care settings adequately predicted AF in diverse populations from the United States and Europe."
  accessed:
    - year: 2024
      month: 6
      day: 10
  author:
    - family: Alonso
      given: Alvaro
    - family: Krijthe
      given: Bouwe P.
    - family: Aspelund
      given: Thor
    - family: Stepas
      given: Katherine A.
    - family: Pencina
      given: Michael J.
    - family: Moser
      given: Carlee B.
    - family: Sinner
      given: Moritz F.
    - family: Sotoodehnia
      given: Nona
    - family: Fontes
      given: João D.
    - family: Janssens
      given: A. Cecile J. W.
    - family: Kronmal
      given: Richard A.
    - family: Magnani
      given: Jared W.
    - family: Witteman
      given: Jacqueline C.
    - family: Chamberlain
      given: Alanna M.
    - family: Lubitz
      given: Steven A.
    - family: Schnabel
      given: Renate B.
    - family: Agarwal
      given: Sunil K.
    - family: McManus
      given: David D.
    - family: Ellinor
      given: Patrick T.
    - family: Larson
      given: Martin G.
    - family: Burke
      given: Gregory L.
    - family: Launer
      given: Lenore J.
    - family: Hofman
      given: Albert
    - family: Levy
      given: Daniel
    - family: Gottdiener
      given: John S.
    - family: Kääb
      given: Stefan
    - family: Couper
      given: David
    - family: Harris
      given: Tamara B.
    - family: Soliman
      given: Elsayed Z.
    - family: Stricker
      given: Bruno H. C.
    - family: Gudnason
      given: Vilmundur
    - family: Heckbert
      given: Susan R.
    - family: Benjamin
      given: Emelia J.
  citation-key: alonso_simple_2013
  container-title: Journal of the American Heart Association
  DOI: 10.1161/JAHA.112.000102
  issue: '2'
  issued:
    - year: 2013
  page: e000102
  publisher: Wiley
  source: ahajournals.org (Atypon)
  title: >-
    Simple Risk Model Predicts Incidence of Atrial Fibrillation in a Racially
    and Geographically Diverse Population: the CHARGE‐AF Consortium
  title-short: >-
    Simple Risk Model Predicts Incidence of Atrial Fibrillation in a Racially
    and Geographically Diverse Population
  type: article-journal
  URL: https://www.ahajournals.org/doi/10.1161/JAHA.112.000102
  volume: '2'

- id: amorim_mais_2019
  abstract: >-
    Com aumento no trabalho informal, número de pessoas trabalhando com apps de
    transporte cresceu 30% e nas ruas, 12%
  accessed:
    - year: 2020
      month: 1
      day: 28
  author:
    - family: Amorim
      given: Daniela
  citation-key: amorim_mais_2019
  container-title: O Estado de S. Paulo
  issued:
    - year: 2019
      month: 12
      day: 18
  language: pt-BR
  title: >-
    Mais 1 milhão de brasileiros passaram a trabalhar como motorista de
    aplicativo ou ambulante em 2018
  type: article-newspaper
  URL: >-
    https://economia.estadao.com.br/noticias/geral,mais-1-milhao-de-brasileiros-passaram-a-trabalhar-como-motorista-de-aplicativo-ou-ambulante-em-2018,70003129796

- id: anand_risk_2021
  abstract: >-
    In this article, we address the problem of risk assessment of stealthy
    attacks on uncertain control systems. Considering data injection attacks
    that aim at maximizing impact while remaining undetected, we use the
    recently proposed output-to-output gain to characterize the risk associated
    with the impact of attacks in two setups: A full system knowledge attacker
    and a limited system knowledge attacker. The risk in each setup is
    formulated using a well-established risk metric, namely the Value-at-Risk
    and the maximum expected loss, respectively. Under these setups, the risk
    assessment problem corresponds to an untractable infinite non-convex
    optimization problem. To address this limitation, we adopt the framework of
    scenario-based optimization to approximate the infinite non-convex
    optimization problem by a sampled non-convex optimization problem. Then,
    based on the framework of dissipative system theory and S-procedure, the
    sampled non-convex risk assessment problem is formulated as an equivalent
    convex semi-definite program. Additionally, we derive the necessary and
    sufficient conditions for the risk to be bounded. Finally, we illustrate the
    results through numerical simulation of a hydro-turbine power system.
  accessed:
    - year: 2021
      month: 11
      day: 16
  author:
    - family: Anand
      given: Sribalaji C.
    - family: Teixeira
      given: André M. H.
    - family: Ahlén
      given: Anders
  citation-key: anand_risk_2021
  container-title: arXiv:2106.07071 [cs, eess, math]
  issued:
    - year: 2021
      month: 6
      day: 13
  source: arXiv.org
  title: Risk Assessment of Stealthy Attacks on Uncertain Control Systems
  type: article-journal
  URL: http://arxiv.org/abs/2106.07071

- id: anderson_introduction_2009
  author:
    - family: Anderson
      given: Greg W.
    - family: Guionnet
      given: Alice
    - family: Zeitouni
      given: Ofer
  citation-key: anderson_introduction_2009
  issued:
    - year: 2009
  title: An Introduction to Random Matrices
  type: book

- id: andersson_deep_2019
  abstract: >-
    Recent developments within deep learning are relevant for nonlinear system
    identification problems. In this paper, we establish connections between the
    deep learning and the system identification communities. It has recently
    been shown that convolutional architectures are at least as capable as
    recurrent architectures when it comes to sequence modeling tasks. Inspired
    by these results we explore the explicit relationships between the recently
    proposed temporal convolutional network (TCN) and two classic system
    identification model structures; Volterra series and block-oriented models.
    We end the paper with an experimental study where we provide results on two
    real-world problems, the well-known Silverbox dataset and a newer dataset
    originating from ground vibration experiments on an F-16 fighter aircraft.
  author:
    - family: Andersson
      given: Carl
    - family: Ribeiro
      given: Antônio H.
    - family: Tiels
      given: Koen
    - family: Wahlström
      given: Niklas
    - family: Schön
      given: Thomas B.
  citation-key: andersson_deep_2019
  container-title: IEEE Conference on Decision and Control (CDC)
  DOI: 10.1109/CDC40024.2019.9030219
  issued:
    - year: 2019
      month: 9
      day: 4
  license: All rights reserved
  page: 3670-3676
  title: Deep Convolutional Networks in System Identification
  type: article-journal

- id: andrieu_particle_2010
  abstract: >-
    Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as
    the two main tools to sample from high dimensional probability
    distributions. Although asymptotic convergence of Markov chain Monte Carlo
    algorithms is ensured under weak assumptions, the performance of these
    algorithms is unreliable when the proposal distributions that are used to
    explore the space are poorly chosen and/or if highly correlated variables
    are updated independently. We show here how it is possible to build efﬁcient
    high dimensional proposal distributions by using sequential Monte Carlo
    methods. This allows us not only to improve over standard Markov chain Monte
    Carlo schemes but also to make Bayesian inference feasible for a large class
    of statistical models where this was not previously so. We demonstrate these
    algorithms on a non-linear state space model and a Lévy-driven stochastic
    volatility model.
  accessed:
    - year: 2018
      month: 11
      day: 27
  author:
    - family: Andrieu
      given: Christophe
    - family: Doucet
      given: Arnaud
    - family: Holenstein
      given: Roman
  citation-key: andrieu_particle_2010
  container-title: 'Journal of the Royal Statistical Society: Series B (Statistical Methodology)'
  DOI: 10/dzzss3
  ISSN: 13697412, 14679868
  issue: '3'
  issued:
    - year: 2010
      month: 6
  language: en
  page: 269-342
  source: Crossref
  title: >-
    Particle Markov chain Monte Carlo methods: Particle Markov Chain Monte Carlo
    Methods
  title-short: Particle Markov chain Monte Carlo methods
  type: article-journal
  URL: http://doi.wiley.com/10.1111/j.1467-9868.2009.00736.x
  volume: '72'

- id: andriushchenko_sgd_2022
  abstract: >-
    We showcase important features of the dynamics of the Stochastic Gradient
    Descent (SGD) in the training of neural networks. We present empirical
    observations that commonly used large step sizes (i) lead the iterates to
    jump from one side of a valley to the other causing loss stabilization, and
    (ii) this stabilization induces a hidden stochastic dynamics orthogonal to
    the bouncing directions that biases it implicitly toward simple predictors.
    Furthermore, we show empirically that the longer large step sizes keep SGD
    high in the loss landscape valleys, the better the implicit regularization
    can operate and find sparse representations. Notably, no explicit
    regularization is used so that the regularization effect comes solely from
    the SGD training dynamics influenced by the step size schedule. Therefore,
    these observations unveil how, through the step size schedules, both
    gradient and noise drive together the SGD dynamics through the loss
    landscape of neural networks. We justify these findings theoretically
    through the study of simple neural network models as well as qualitative
    arguments inspired from stochastic processes. Finally, this analysis allows
    to shed a new light on some common practice and observed phenomena when
    training neural networks. The code of our experiments is available at
    https://github.com/tml-epfl/sgd-sparse-features.
  accessed:
    - year: 2022
      month: 10
      day: 26
  author:
    - family: Andriushchenko
      given: Maksym
    - family: Varre
      given: Aditya
    - family: Pillaud-Vivien
      given: Loucas
    - family: Flammarion
      given: Nicolas
  citation-key: andriushchenko_sgd_2022
  DOI: 10.48550/arXiv.2210.05337
  issued:
    - year: 2022
      month: 10
      day: 11
  number: arXiv:2210.05337
  publisher: arXiv
  source: arXiv.org
  title: SGD with large step sizes learns sparse features
  type: article
  URL: http://arxiv.org/abs/2210.05337

- id: angelopoulos_gentle_2022
  abstract: >-
    Black-box machine learning models are now routinely used in high-risk
    settings, like medical diagnostics, which demand uncertainty quantiﬁcation
    to avoid consequential model failures. Conformal prediction (a.k.a.
    conformal inference) is a user-friendly paradigm for creating statistically
    rigorous uncertainty sets/intervals for the predictions of such models.
    Critically, the sets are valid in a distribution-free sense: they possess
    explicit, non-asymptotic guarantees even without distributional assumptions
    or model assumptions. One can use conformal prediction with any pre-trained
    model, such as a neural network, to produce sets that are guaranteed to
    contain the ground truth with a user-speciﬁed probability, such as 90%. It
    is easy-to-understand, easy-to-use, and general, applying naturally to
    problems arising in the ﬁelds of computer vision, natural language
    processing, deep reinforcement learning, and so on.
  accessed:
    - year: 2023
      month: 9
      day: 7
  author:
    - family: Angelopoulos
      given: Anastasios N.
    - family: Bates
      given: Stephen
  citation-key: angelopoulos_gentle_2022
  issued:
    - year: 2022
      month: 12
      day: 7
  language: en
  number: arXiv:2107.07511
  publisher: arXiv
  source: arXiv.org
  title: >-
    A Gentle Introduction to Conformal Prediction and Distribution-Free
    Uncertainty Quantification
  type: article
  URL: http://arxiv.org/abs/2107.07511

- id: anton_elementary_2010
  author:
    - family: Anton
      given: H.
    - family: Rorres
      given: C.
  citation-key: anton_elementary_2010
  ISBN: 978-0-470-43205-1
  issued:
    - year: 2010
  publisher: John Wiley & Sons
  title: 'Elementary Linear Algebra: Applications Version'
  type: book
  URL: https://books.google.com.br/books?id=1PJ-WHepeBsC

- id: aquino_brazilian_2012
  abstract: >-
    Although low- and middle-income countries still bear the burden of major
    infectious diseases, chronic noncommunicable diseases are becoming
    increasingly common due to rapid demographic, epidemiologic, and nutritional
    transitions. However, information is generally scant in these countries
    regarding chronic disease incidence, social determinants, and risk factors.
    The Brazilian Longitudinal Study of Adult Health (ELSA-Brasil) aims to
    contribute relevant information with respect to the development and
    progression of clinical and subclinical chronic diseases, particularly
    cardiovascular diseases and diabetes. In this report, the authors delineate
    the study’s objectives, principal methodological features, and timeline. At
    baseline, ELSA-Brasil enrolled 15,105 civil servants from 5 universities and
    1 research institute. The baseline examination (2008–2010) included detailed
    interviews, clinical and anthropometric examinations, an oral glucose
    tolerance test, overnight urine collection, a 12-lead resting
    electrocardiogram, measurement of carotid intima-media thickness,
    echocardiography, measurement of pulse wave velocity, hepatic
    ultrasonography, retinal fundus photography, and an analysis of heart rate
    variability. Long-term biologic sample storage will allow investigation of
    biomarkers that may predict cardiovascular diseases and diabetes. Annual
    telephone surveillance, initiated in 2009, will continue for the duration of
    the study. A follow-up examination is scheduled for 2012–2013.
  author:
    - family: Aquino
      given: Estela M. L.
    - family: Barreto
      given: Sandhi Maria
    - family: Bensenor
      given: Isabela M.
    - family: Carvalho
      given: Marilia S.
    - family: Chor
      given: Dóra
    - family: Duncan
      given: Bruce B.
    - family: Lotufo
      given: Paulo A.
    - family: Mill
      given: José Geraldo
    - family: Molina
      given: Maria Del Carmen
    - family: Mota
      given: Eduardo L. A.
    - family: Azeredo Passos
      given: Valéria Maria
    - family: Schmidt
      given: Maria Inês
    - family: Szklo
      given: Moyses
  citation-key: aquino_brazilian_2012
  container-title: American Journal of Epidemiology
  DOI: 10.1093/aje/kwr294
  ISSN: 0002-9262
  issue: '4'
  issued:
    - year: 2012
      month: 1
  page: 315-324
  title: >-
    Brazilian longitudinal study of adult health (ELSA-Brasil): Objectives and
    design
  type: article-journal
  URL: https://doi.org/10.1093/aje/kwr294
  volume: '175'

- id: araujo_desenvolvimento_2012
  author:
    - family: Araujo
      given: Kim Cândido Pereira
  citation-key: araujo_desenvolvimento_2012
  genre: Projeto de Final de Curso
  issued:
    - year: 2012
      month: 6
  language: pt
  publisher: Universidade Federal de Minas Gerais
  source: Zotero
  title: >-
    Desenvolvimento de uma Ferramenta para Identificação de Sinais
    Correlacionados: Aplicação em Dados Provenientes de uma Plataforma de
    Petróleo
  type: thesis

- id: arbabi_introduction_
  author:
    - family: Arbabi
      given: Hassan
  citation-key: arbabi_introduction_
  language: en
  page: '32'
  source: Zotero
  title: Introduction to Koopman operator theory of dynamical systems
  type: article-journal

- id: ardeshir_support_2021
  abstract: >-
    The support vector machine (SVM) and minimum Euclidean norm least squares
    regression are two fundamentally different approaches to fitting linear
    models, but they have recently been connected in models for very
    high-dimensional data through a phenomenon of support vector proliferation,
    where every training example used to fit an SVM becomes a support vector. In
    this paper, we explore the generality of this phenomenon and make the
    following contributions. First, we prove a super-linear lower bound on the
    dimension (in terms of sample size) required for support vector
    proliferation in independent feature models, matching the upper bounds from
    previous works. We further identify a sharp phase transition in Gaussian
    feature models, bound the width of this transition, and give experimental
    support for its universality. Finally, we hypothesize that this phase
    transition occurs only in much higher-dimensional settings in the $\ell_1$
    variant of the SVM, and we present a new geometric characterization of the
    problem that may elucidate this phenomenon for the general $\ell_p$ case.
  accessed:
    - year: 2022
      month: 12
      day: 12
  author:
    - family: Ardeshir
      given: Navid
    - family: Sanford
      given: Clayton
    - family: Hsu
      given: Daniel
  citation-key: ardeshir_support_2021
  DOI: 10.48550/arXiv.2105.14084
  issued:
    - year: 2021
      month: 10
      day: 27
  number: arXiv:2105.14084
  publisher: arXiv
  source: arXiv.org
  title: >-
    Support vector machines and linear regression coincide with very
    high-dimensional features
  type: article
  URL: http://arxiv.org/abs/2105.14084

- id: arjovsky_unitary_2016
  author:
    - family: Arjovsky
      given: Martin
    - family: Shah
      given: Amar
    - family: Bengio
      given: Yoshua
  citation-key: arjovsky_unitary_2016
  collection-title: ICML'16
  container-title: >-
    Proceedings of the 33rd international conference on international conference
    on machine learning - volume 48
  event-place: New York, NY, USA
  issued:
    - year: 2016
  page: 1120-1128
  publisher: JMLR.org
  title: Unitary evolution recurrent neural networks
  type: paper-conference
  URL: http://dl.acm.org/citation.cfm?id=3045390.3045509

- id: arora_convergence_2019
  abstract: >-
    We analyze speed of convergence to global optimum for gradient descent
    training a deep linear neural network (parameterized as $x \mapsto W_N
    W_{N-1} \cdots W_1 x$) by minimizing the $\ell_2$ loss over whitened data.
    Convergence at a linear rate is guaranteed when the following hold: (i)
    dimensions of hidden layers are at least the minimum of the input and output
    dimensions; (ii) weight matrices at initialization are approximately
    balanced; and (iii) the initial loss is smaller than the loss of any
    rank-deficient solution. The assumptions on initialization (conditions (ii)
    and (iii)) are necessary, in the sense that violating any one of them may
    lead to convergence failure. Moreover, in the important case of output
    dimension 1, i.e. scalar regression, they are met, and thus convergence to
    global optimum holds, with constant probability under a random
    initialization scheme. Our results significantly extend previous analyses,
    e.g., of deep linear residual networks (Bartlett et al., 2018).
  accessed:
    - year: 2020
      month: 8
      day: 27
  author:
    - family: Arora
      given: Sanjeev
    - family: Cohen
      given: Nadav
    - family: Golowich
      given: Noah
    - family: Hu
      given: Wei
  citation-key: arora_convergence_2019
  container-title: arXiv:1810.02281 [cs, stat]
  issued:
    - year: 2019
      month: 10
      day: 26
  source: arXiv.org
  title: A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks
  type: article-journal
  URL: http://arxiv.org/abs/1810.02281

- id: arora_exact_2019
  author:
    - family: Arora
      given: Sanjeev
    - family: Du
      given: Simon S.
    - family: Hu
      given: Wei
    - family: Li
      given: Zhiyuan
    - family: Salakhutdinov
      given: Ruslan
    - family: Wang
      given: Ruosong
  citation-key: arora_exact_2019
  container-title: Thirty-third conference on neural information processing systems
  issued:
    - year: 2019
  title: On exact computation with an infinitely wide neural net
  type: paper-conference

- id: arora_finegrained_2019
  abstract: >-
    Recent works have cast some light on the mystery of why deep nets fit any
    data and generalize despite being very overparametrized. This paper analyzes
    training and generalization for a simple 2-layer ReLU net with random
    initialization, and provides the following improvements over recent works:
    (i) Using a tighter characterization of training speed than recent papers,
    an explanation for why training a neural net with random labels leads to
    slower training, as originally observed in [Zhang et al. ICLR'17]. (ii)
    Generalization bound independent of network size, using a data-dependent
    complexity measure. Our measure distinguishes clearly between random labels
    and true labels on MNIST and CIFAR, as shown by experiments. Moreover,
    recent papers require sample complexity to increase (slowly) with the size,
    while our sample complexity is completely independent of the network size.
    (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets
    trained via gradient descent. The key idea is to track dynamics of training
    and generalization via properties of a related kernel.
  accessed:
    - year: 2019
      month: 2
      day: 22
  author:
    - family: Arora
      given: Sanjeev
    - family: Du
      given: Simon S.
    - family: Hu
      given: Wei
    - family: Li
      given: Zhiyuan
    - family: Wang
      given: Ruosong
  citation-key: arora_finegrained_2019
  container-title: arXiv:1901.08584 [cs, stat]
  issued:
    - year: 2019
      month: 1
      day: 24
  source: arXiv.org
  title: >-
    Fine-Grained Analysis of Optimization and Generalization for
    Overparameterized Two-Layer Neural Networks
  type: article-journal
  URL: http://arxiv.org/abs/1901.08584

- id: arora_stronger_2018
  abstract: >-
    Deep nets generalize well despite having more parameters than the number of
    training samples. Recent works try to give an explanation using PAC-Bayes
    and Margin-based analyses, but do not as yet result in sample complexity
    bounds better than naive parameter counting. The current paper shows
    generalization bounds that’re orders of magnitude better in practice. These
    rely upon new succinct reparametrizations of the trained net — a compression
    that is explicit and eﬃcient. These yield generalization bounds via a simple
    compression-based framework introduced here. Our results also provide some
    theoretical justiﬁcation for widespread empirical success in compressing
    deep nets.
  accessed:
    - year: 2023
      month: 11
      day: 27
  author:
    - family: Arora
      given: Sanjeev
    - family: Ge
      given: Rong
    - family: Neyshabur
      given: Behnam
    - family: Zhang
      given: Yi
  citation-key: arora_stronger_2018
  issued:
    - year: 2018
      month: 11
      day: 26
  language: en
  number: arXiv:1802.05296
  publisher: arXiv
  source: arXiv.org
  title: Stronger generalization bounds for deep nets via a compression approach
  type: article
  URL: http://arxiv.org/abs/1802.05296

- id: arreckx_regularized_2018
  accessed:
    - year: 2018
      month: 5
      day: 23
  author:
    - family: Arreckx
      given: Sylvain
    - family: Orban
      given: Dominique
  citation-key: arreckx_regularized_2018
  container-title: SIAM Journal on Optimization
  DOI: 10.1137/16M1088570
  ISSN: 1052-6234, 1095-7189
  issue: '2'
  issued:
    - year: 2018
      month: 1
  language: en
  page: 1613-1639
  source: Crossref
  title: >-
    A Regularized Factorization-Free Method for Equality-Constrained
    Optimization
  type: article-journal
  URL: https://epubs.siam.org/doi/10.1137/16M1088570
  volume: '28'

- id: artin_algebra_1991
  author:
    - family: Artin
      given: M.
  citation-key: artin_algebra_1991
  ISBN: 978-0-13-004763-2
  issued:
    - year: 1991
  publisher: Prentice Hall
  title: Algebra
  type: book
  URL: https://books.google.com.br/books?id=C_juAAAAMAAJ

- id: ash_probability_2000
  author:
    - family: Ash
      given: Robert B.
    - family: Doléans-Dale
      given: Catherine
  citation-key: ash_probability_2000
  edition: 2nd
  issued:
    - year: 2000
  publisher: Harcourt/Academic Press
  title: Probability and Measure Theory
  type: book

- id: ashton_that_2009
  author:
    - family: Ashton
      given: Kevin
  citation-key: ashton_that_2009
  container-title: RFID journal
  container-title-short: RFID journal
  issue: '7'
  issued:
    - year: 2009
  page: 97-114
  title: That ‘internet of things’ thing
  type: article-journal
  volume: '22'

- id: astrom_maximum_1979
  author:
    - family: Astrom
      given: K. J.
  citation-key: astrom_maximum_1979
  container-title: IFAC Proceedings Volumes
  DOI: 10.1016/S1474-6670(17)53976-2
  issue: '8'
  issued:
    - year: 1979
  page: 551–574
  source: Google Scholar
  title: Maximum likelihood and prediction error methods
  type: article-journal
  volume: '12'

- id: astrom_system_1971
  author:
    - family: Åström
      given: K.J.
    - family: Eykhoff
      given: P.
  citation-key: astrom_system_1971
  container-title: Automatica
  DOI: 10/dw3ktq
  ISSN: 0005-1098
  issue: '2'
  issued:
    - year: 1971
      month: 3
  page: 123-162
  title: System Identification—A Survey
  type: article-journal
  volume: '7'

- id: astrom_system_1971a
  author:
    - family: Åström
      given: K.J.
    - family: Eykhoff
      given: P.
  citation-key: astrom_system_1971a
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10.1016/0005-1098(71)90059-8
  ISSN: 0005-1098
  issue: '2'
  issued:
    - year: 1971
      month: 3
      day: 1
  page: 123-162
  title: System identification—A survey
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/0005109871900598
  volume: '7'

- id: attia_artificial_2019
  abstract: >-
    Summary

    Background

    Atrial fibrillation is frequently asymptomatic and thus underdetected but is
    associated with stroke, heart failure, and death. Existing screening methods
    require prolonged monitoring and are limited by cost and low yield. We aimed
    to develop a rapid, inexpensive, point-of-care means of identifying patients
    with atrial fibrillation using machine learning.

    Methods

    We developed an artificial intelligence (AI)-enabled electrocardiograph
    (ECG) using a convolutional neural network to detect the
    electrocardiographic signature of atrial fibrillation present during normal
    sinus rhythm using standard 10-second, 12-lead ECGs. We included all
    patients aged 18 years or older with at least one digital, normal sinus
    rhythm, standard 10-second, 12-lead ECG acquired in the supine position at
    the Mayo Clinic ECG laboratory between Dec 31, 1993, and July 21, 2017, with
    rhythm labels validated by trained personnel under cardiologist supervision.
    We classified patients with at least one ECG with a rhythm of atrial
    fibrillation or atrial flutter as positive for atrial fibrillation. We
    allocated ECGs to the training, internal validation, and testing datasets in
    a 7:1:2 ratio. We calculated the area under the curve (AUC) of the receiver
    operatoring characteristic curve for the internal validation dataset to
    select a probability threshold, which we applied to the testing dataset. We
    evaluated model performance on the testing dataset by calculating the AUC
    and the accuracy, sensitivity, specificity, and F1 score with two-sided 95%
    CIs.

    Findings

    We included 180 922 patients with 649 931 normal sinus rhythm ECGs for
    analysis: 454 789 ECGs recorded from 126 526 patients in the training
    dataset, 64 340 ECGs from 18 116 patients in the internal validation
    dataset, and 130 802 ECGs from 36 280 patients in the testing dataset. 3051
    (8·4%) patients in the testing dataset had verified atrial fibrillation
    before the normal sinus rhythm ECG tested by the model. A single AI-enabled
    ECG identified atrial fibrillation with an AUC of 0·87 (95% CI 0·86–0·88),
    sensitivity of 79·0% (77·5–80·4), specificity of 79·5% (79·0–79·9), F1 score
    of 39·2% (38·1–40·3), and overall accuracy of 79·4% (79·0–79·9). Including
    all ECGs acquired during the first month of each patient's window of
    interest (ie, the study start date or 31 days before the first recorded
    atrial fibrillation ECG) increased the AUC to 0·90 (0·90–0·91), sensitivity
    to 82·3% (80·9–83·6), specificity to 83·4% (83·0–83·8), F1 score to 45·4%
    (44·2–46·5), and overall accuracy to 83·3% (83·0–83·7).

    Interpretation

    An AI-enabled ECG acquired during normal sinus rhythm permits identification
    at point of care of individuals with atrial fibrillation.

    Funding

    None.
  accessed:
    - year: 2019
      month: 9
      day: 2
  author:
    - family: Attia
      given: Zachi I
    - family: Noseworthy
      given: Peter A
    - family: Lopez-Jimenez
      given: Francisco
    - family: Asirvatham
      given: Samuel J
    - family: Deshmukh
      given: Abhishek J
    - family: Gersh
      given: Bernard J
    - family: Carter
      given: Rickey E
    - family: Yao
      given: Xiaoxi
    - family: Rabinstein
      given: Alejandro A
    - family: Erickson
      given: Brad J
    - family: Kapa
      given: Suraj
    - family: Friedman
      given: Paul A
  citation-key: attia_artificial_2019
  container-title: The Lancet
  container-title-short: The Lancet
  DOI: 10/gf7d9h
  ISSN: 0140-6736
  issued:
    - year: 2019
      month: 8
      day: 1
  source: ScienceDirect
  title: >-
    An artificial intelligence-enabled ECG algorithm for the identification of
    patients with atrial fibrillation during sinus rhythm: a retrospective
    analysis of outcome prediction
  title-short: >-
    An artificial intelligence-enabled ECG algorithm for the identification of
    patients with atrial fibrillation during sinus rhythm
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0140673619317210

- id: attia_screening_2019
  abstract: >-
    Asymptomatic left ventricular dysfunction (ALVD) is present in 3–6% of the
    general population, is associated with reduced quality of life and
    longevity, and is treatable when found1–4. An inexpensive, noninvasive
    screening tool for ALVD in the doctor’s office is not available. We tested
    the hypothesis that application of artificial intelligence (AI) to the
    electrocardiogram (ECG), a routine method of measuring the heart’s
    electrical activity, could identify ALVD. Using paired 12-lead ECG and
    echocardiogram data, including the left ventricular ejection fraction (a
    measure of contractile function), from 44,959 patients at the Mayo Clinic,
    we trained a convolutional neural network to identify patients with
    ventricular dysfunction, defined as ejection fraction ≤35%, using the ECG
    data alone. When tested on an independent set of 52,870 patients, the
    network model yielded values for the area under the curve, sensitivity,
    specificity, and accuracy of 0.93, 86.3%, 85.7%, and 85.7%, respectively. In
    patients without ventricular dysfunction, those with a positive AI screen
    were at 4 times the risk (hazard ratio, 4.1; 95% confidence interval, 3.3 to
    5.0) of developing future ventricular dysfunction compared with those with a
    negative screen. Application of AI to the ECG—a ubiquitous, low-cost
    test—permits the ECG to serve as a powerful screening tool in asymptomatic
    individuals to identify ALVD.
  author:
    - family: Attia
      given: Zachi I.
    - family: Kapa
      given: Suraj
    - family: Lopez-Jimenez
      given: Francisco
    - family: McKie
      given: Paul M.
    - family: Ladewig
      given: Dorothy J.
    - family: Satam
      given: Gaurav
    - family: Pellikka
      given: Patricia A.
    - family: Enriquez-Sarano
      given: Maurice
    - family: Noseworthy
      given: Peter A.
    - family: Munger
      given: Thomas M.
    - family: Asirvatham
      given: Samuel J.
    - family: Scott
      given: Christopher G.
    - family: Carter
      given: Rickey E.
    - family: Friedman
      given: Paul A.
  citation-key: attia_screening_2019
  container-title: Nature Medicine
  container-title-short: Nature Medicine
  DOI: 10.1038/s41591-018-0240-2
  ISSN: 1546-170X
  issue: '1'
  issued:
    - year: 2019
      month: 1
      day: 1
  page: 70-74
  title: >-
    Screening for cardiac contractile dysfunction using an artificial
    intelligence–enabled electrocardiogram
  type: article-journal
  URL: https://doi.org/10.1038/s41591-018-0240-2
  volume: '25'

- id: attiazachii._age_2019
  abstract: >-
    Background:Sex and age have long been known to affect the ECG. Several
    biologic variables and anatomic factors may contribute to sex and
    age-related differences on the ECG. We hypothesized that a convolutional
    neural network (CNN) could be trained through a process called deep learning
    to predict a person’s age and self-reported sex using only 12-lead ECG
    signals. We further hypothesized that discrepancies between CNN-predicted
    age and chronological age may serve as a physiological measure of
    health.Methods:We trained CNNs using 10-second samples of 12-lead ECG
    signals from 499 727 patients to predict sex and age. The networks were
    tested on a separate cohort of 275 056 patients. Subsequently, 100 randomly
    selected patients with multiple ECGs over the course of decades were
    identified to assess within-individual accuracy of CNN age
    estimation.Results:Of 275 056 patients tested, 52% were males and mean age
    was 58.6±16.2 years. For sex classification, the model obtained 90.4%
    classification accuracy with an area under the curve of 0.97 in the
    independent test data. Age was estimated as a continuous variable with an
    average error of 6.9±5.6 years (R-squared =0.7). Among 100 patients with
    multiple ECGs over the course of at least 2 decades of life, most patients
    (51%) had an average error between real age and CNN-predicted age of <7
    years. Major factors seen among patients with a CNN-predicted age that
    exceeded chronologic age by >7 years included: low ejection fraction,
    hypertension, and coronary disease (P<0.01). In the 27% of patients where
    correlation was >0.8 between CNN-predicted and chronologic age, no incident
    events occurred over follow-up (33±12 years).Conclusions:Applying artificial
    intelligence to the ECG allows prediction of patient sex and estimation of
    age. The ability of an artificial intelligence algorithm to determine
    physiological age, with further validation, may serve as a measure of
    overall health.
  accessed:
    - year: 2019
      month: 9
      day: 2
  author:
    - literal: Attia Zachi I.
    - family: A.
      given: Friedman Paul
    - literal: Noseworthy Peter A.
    - literal: Lopez-Jimenez Francisco
    - literal: Ladewig Dorothy J.
    - literal: Satam Gaurav
    - literal: Pellikka Patricia A.
    - literal: Munger Thomas M.
    - literal: Asirvatham Samuel J.
    - literal: Scott Christopher G.
    - literal: Carter Rickey E.
    - literal: Kapa Suraj
  citation-key: attiazachii._age_2019
  container-title: 'Circulation: Arrhythmia and Electrophysiology'
  container-title-short: 'Circulation: Arrhythmia and Electrophysiology'
  DOI: 10/gf7d9g
  issue: '9'
  issued:
    - year: 2019
      month: 9
      day: 1
  page: e007284
  source: ahajournals.org (Atypon)
  title: >-
    Age and Sex Estimation Using Artificial Intelligence From Standard 12-Lead
    ECGs
  type: article-journal
  URL: https://www.ahajournals.org/doi/full/10.1161/CIRCEP.119.007284
  volume: '12'

- id: augustyniak_equivalence_
  abstract: >-
    The electrocardiography (ECG) and vectocardiography (VCG) both describe the
    same phenomena: the temporal changes of the surface potentials resulting
    from cardiac electrical field. The ECG uses 12 leads positioned as it was
    found optimal from the medical point of view during a hundred years of
    practice. The VCG uses 3 channels connected to the psuedoorthogonal leads
    that placement is determined by the orthogonality of main Cartesian axes -
    in consequence a three-dimensional recording is performed. This paper is
    devoted to the experimental verification of the likeness of the data
    provided by both recording techniques. Two different transforms re-mapping
    the ECG to the VCG domain and viceversa were studied with use of the set of
    125 simultaneous ECG and VCG signals from the CSE Multilead Database. One of
    the possible technical interests of transforming the ECG signals to the VCG
    domain is reducing the data volume thanks to eliminating the information
    redundancy typical for ECG. Our results demonstrate that the forward and
    inverse transform has no perfect reconstruction property and some extent of
    distortion should be considered when applying this technique to the signal
    compression.
  author:
    - family: Augustyniak
      given: Piotr
  citation-key: augustyniak_equivalence_
  source: CiteSeer
  title: >-
    On The Equivalence Of The 12-Lead Ecg And The Vcg Representations Of The
    Cardiac Electrical Activity
  type: book

- id: ayalasolares_modeling_2016
  author:
    - family: Ayala Solares
      given: Jose Roberto
    - family: Wei
      given: Hua-Liang
    - family: Boynton
      given: R. J.
    - family: Walker
      given: Simon N
    - family: Billings
      given: Stephen A
  citation-key: ayalasolares_modeling_2016
  container-title: Space Weather
  issue: '10'
  issued:
    - year: 2016
  page: 899–916
  title: >-
    Modeling and prediction of global magnetic disturbance in near-Earth space:
    A case study for Kp index using NARX models
  type: article-journal
  volume: '14'

- id: ayalasolares_modeling_2016a
  author:
    - family: Ayala Solares
      given: Jose Roberto
    - family: Wei
      given: Hua-Liang
    - family: Boynton
      given: R. J.
    - family: Walker
      given: Simon N
    - family: Billings
      given: Stephen A
  citation-key: ayalasolares_modeling_2016a
  container-title: Space Weather
  DOI: 10/gfjwmm
  issue: '10'
  issued:
    - year: 2016
  page: 899-916
  title: >-
    Modeling and Prediction of Global Magnetic Disturbance in Near-Earth Space:
    A Case Study for Kp Index Using NARX Models
  type: article-journal
  volume: '14'

- id: azad_longterm_2014
  author:
    - family: Azad
      given: Hanieh Borhan
    - family: Mekhilef
      given: Saad
    - family: Ganapathy
      given: Vellapa Gounder
  citation-key: azad_longterm_2014
  container-title: IEEE Transactions on Sustainable Energy
  DOI: 10.1109/TSTE.2014.2300150
  issue: '2'
  issued:
    - year: 2014
  page: 546–553
  title: >-
    Long-term wind speed forecasting and general pattern recognition using
    neural networks
  type: article-journal
  volume: '5'

- id: azulay_why_2019
  abstract: >-
    Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to
    small image transformations: either because of the convolutional
    architecture or because they were trained using data augmentation. Recently,
    several authors have shown that this is not the case: small translations or
    rescalings of the input image can drastically change the network’s
    prediction. In this paper, we quantify this phenomena and ask why neither
    the convolutional architecture nor data augmentation are suﬃcient to achieve
    the desired invariance. Speciﬁcally, we show that the convolutional
    architecture does not give invariance since architectures ignore the
    classical sampling theorem, and data augmentation does not give invariance
    because the CNNs learn to be invariant to transformations only for images
    that are very similar to typical images from the training set. We discuss
    two possible solutions to this problem: (1) antialiasing the intermediate
    representations and (2) increasing data augmentation and show that they
    provide only a partial solution at best. Taken together, our results
    indicate that the problem of insuring invariance to small image
    transformations in neural networks while preserving high accuracy remains
    unsolved.
  author:
    - family: Azulay
      given: Aharon
    - family: Weiss
      given: Yair
  citation-key: azulay_why_2019
  container-title: Journal of Machine Learning Research
  issued:
    - year: 2019
  language: en
  page: 1-25
  source: Zotero
  title: >-
    Why do deep convolutional networks generalize so poorly to small image
    transformations?
  type: article-journal
  volume: '20'

- id: ba_layer_2016
  abstract: >-
    Training state-of-the-art, deep neural networks is computationally
    expensive. One way to reduce the training time is to normalize the
    activities of the neurons. A recently introduced technique called batch
    normalization uses the distribution of the summed input to a neuron over a
    mini-batch of training cases to compute a mean and variance which are then
    used to normalize the summed input to that neuron on each training case.
    This significantly reduces the training time in feed-forward neural
    networks. However, the effect of batch normalization is dependent on the
    mini-batch size and it is not obvious how to apply it to recurrent neural
    networks. In this paper, we transpose batch normalization into layer
    normalization by computing the mean and variance used for normalization from
    all of the summed inputs to the neurons in a layer on a single training
    case. Like batch normalization, we also give each neuron its own adaptive
    bias and gain which are applied after the normalization but before the
    non-linearity. Unlike batch normalization, layer normalization performs
    exactly the same computation at training and test times. It is also
    straightforward to apply to recurrent neural networks by computing the
    normalization statistics separately at each time step. Layer normalization
    is very effective at stabilizing the hidden state dynamics in recurrent
    networks. Empirically, we show that layer normalization can substantially
    reduce the training time compared with previously published techniques.
  accessed:
    - year: 2019
      month: 5
      day: 29
  author:
    - family: Ba
      given: Jimmy Lei
    - family: Kiros
      given: Jamie Ryan
    - family: Hinton
      given: Geoffrey E.
  citation-key: ba_layer_2016
  container-title: arXiv:1607.06450 [cs, stat]
  issued:
    - year: 2016
      month: 7
      day: 21
  source: arXiv.org
  title: Layer Normalization
  type: article-journal
  URL: http://arxiv.org/abs/1607.06450

- id: baake_fitting_1992
  author:
    - family: Baake
      given: Ellen
    - family: Baake
      given: Michael
    - family: Bock
      given: HG
    - family: Briggs
      given: KM
  citation-key: baake_fitting_1992
  container-title: Physical Review A
  DOI: 10.1103/PhysRevA.45.5524
  issue: '8'
  issued:
    - year: 1992
  page: '5524'
  title: Fitting ordinary differential equations to chaotic data
  type: article-journal
  volume: '45'

- id: baake_modelling_1992
  author:
    - family: Baake
      given: Ellen
    - family: Schlöder
      given: Johannes P
  citation-key: baake_modelling_1992
  container-title: Bulletin of mathematical biology
  issue: '6'
  issued:
    - year: 1992
  page: 999–1021
  title: Modelling the fast fluorescence rise of photosynthesis
  type: article-journal
  volume: '54'

- id: bach_breaking_2017
  author:
    - family: Bach
      given: Francis
  citation-key: bach_breaking_2017
  container-title: Journal of Machine Learning Research
  issue: '19'
  issued:
    - year: 2017
  page: 1-53
  title: Breaking the curse of dimensionality with convex neural networks
  type: article-journal
  URL: http://jmlr.org/papers/v18/14-546.html
  volume: '18'

- id: bach_equivalence_2017
  author:
    - family: Bach
      given: Francis
  citation-key: bach_equivalence_2017
  container-title: Journal of Machine Learning Research
  issue: '21'
  issued:
    - year: 2017
  page: 1-38
  title: >-
    On the equivalence between kernel quadrature rules and random feature
    expansions
  type: article-journal
  URL: http://jmlr.org/papers/v18/15-178.html
  volume: '18'

- id: bach_etatrick_2019
  accessed:
    - year: 2023
      month: 6
      day: 2
  author:
    - family: Bach
      given: Francis
  citation-key: bach_etatrick_2019
  issued:
    - year: 2019
      month: 7
      day: 1
  language: en-US
  title: >-
    The “eta-trick” or the effectiveness of reweighted least-squares – Machine
    Learning Research Blog
  type: post-weblog
  URL: >-
    https://francisbach.com/the-%ce%b7-trick-or-the-effectiveness-of-reweighted-least-squares/

- id: bach_highdimensional_2023
  abstract: >-
    We consider linear regression problems with a varying number of random
    projections, where we provably exhibit a double descent curve for a fixed
    prediction problem, with a high-dimensional analysis based on random matrix
    theory. We first consider the ridge regression estimator and re-interpret
    earlier results using classical notions from non-parametric statistics,
    namely degrees of freedom, also known as effective dimensionality. In
    particular, we show that the random design performance of ridge regression
    with a specific regularization parameter matches the classical bias and
    variance expressions coming from the easier fixed design analysis but for
    another larger implicit regularization parameter. We then compute asymptotic
    equivalents of the generalization performance (in terms of bias and
    variance) of the minimum norm least-squares fit with random projections,
    providing simple expressions for the double descent phenomenon.
  author:
    - family: Bach
      given: Francis
  citation-key: bach_highdimensional_2023
  container-title: arXiv:2303.01372
  issued:
    - year: 2023
      month: 3
      day: 2
  title: >-
    High-dimensional analysis of double descent for linear regression with
    random projections
  type: article-journal
  URL: http://arxiv.org/abs/2303.01372

- id: bach_learning_2023
  author:
    - family: Bach
      given: Francis
  citation-key: bach_learning_2023
  issued:
    - year: 2023
  language: en
  source: Zotero
  title: Learning Theory from First Principles
  type: book

- id: bach_optimization_2011
  abstract: >-
    Sparse estimation methods are aimed at using or obtaining parsimonious
    representations of data or models. They were ﬁrst dedicated to linear
    variable selection but numerous extensions have now emerged such as
    structured sparsity or kernel selection. It turns out that many of the
    related estimation problems can be cast as convex optimization problems by
    regularizing the empirical risk with appropriate nonsmooth norms. The goal
    of this monograph is to present from a general perspective optimization
    tools and techniques dedicated to such sparsity-inducing penalties. We cover
    proximal methods, block-coordinate descent, reweighted 2-penalized
    techniques, workingset and homotopy methods, as well as non-convex
    formulations and extensions, and provide an extensive set of experiments to
    compare various algorithms from a computational point of view.
  accessed:
    - year: 2023
      month: 6
      day: 12
  author:
    - family: Bach
      given: Francis
  citation-key: bach_optimization_2011
  container-title: Foundations and Trends in Machine Learning
  container-title-short: FNT in Machine Learning
  DOI: 10.1561/2200000015
  ISSN: 1935-8237, 1935-8245
  issue: '1'
  issued:
    - year: 2011
  language: en
  page: 1-106
  source: DOI.org (Crossref)
  title: Optimization with Sparsity-Inducing Penalties
  type: article-journal
  URL: http://www.nowpublishers.com/article/Details/MAL-015
  volume: '4'

- id: bach_relationship_2023
  abstract: >-
    We consider multivariate splines and show that they have a random feature
    expansion as infinitely wide neural networks with one-hidden layer and a
    homogeneous activation function which is the power of the rectified linear
    unit. We show that the associated function space is a Sobolev space on a
    Euclidean ball, with an explicit bound on the norms of derivatives. This
    link provides a new random feature expansion for multivariate splines that
    allow efficient algorithms. This random feature expansion is numerically
    better behaved than usual random Fourier features, both in theory and
    practice. In particular, in dimension one, we compare the associated
    leverage scores to compare the two random expansions and show a better
    scaling for the neural network expansion.
  accessed:
    - year: 2023
      month: 5
      day: 25
  author:
    - family: Bach
      given: Francis
  citation-key: bach_relationship_2023
  DOI: 10.48550/arXiv.2302.03459
  issued:
    - year: 2023
      month: 3
      day: 1
  number: arXiv:2302.03459
  publisher: arXiv
  source: arXiv.org
  title: >-
    On the relationship between multivariate splines and infinitely-wide neural
    networks
  type: article
  URL: http://arxiv.org/abs/2302.03459

- id: bahdanau_neural_2014
  abstract: >-
    Neural machine translation is a recently proposed approach to machine
    translation. Unlike the traditional statistical machine translation, the
    neural machine translation aims at building a single neural network that can
    be jointly tuned to maximize the translation performance. The models
    proposed recently for neural machine translation often belong to a family of
    encoder-decoders and consists of an encoder that encodes a source sentence
    into a fixed-length vector from which a decoder generates a translation. In
    this paper, we conjecture that the use of a fixed-length vector is a
    bottleneck in improving the performance of this basic encoder-decoder
    architecture, and propose to extend this by allowing a model to
    automatically (soft-)search for parts of a source sentence that are relevant
    to predicting a target word, without having to form these parts as a hard
    segment explicitly. With this new approach, we achieve a translation
    performance comparable to the existing state-of-the-art phrase-based system
    on the task of English-to-French translation. Furthermore, qualitative
    analysis reveals that the (soft-)alignments found by the model agree well
    with our intuition.
  author:
    - family: Bahdanau
      given: Dzmitry
    - family: Cho
      given: Kyunghyun
    - family: Bengio
      given: Yoshua
  citation-key: bahdanau_neural_2014
  container-title: arXiv:1409.0473 [cs, stat]
  issued:
    - year: 2014
      month: 9
      day: 1
  source: arXiv.org
  title: Neural Machine Translation by Jointly Learning to Align and Translate
  type: article-journal
  URL: http://arxiv.org/abs/1409.0473

- id: bai_asymptotics_2007
  abstract: >-
    Let \{$X_{ij}$\}, $i,j=...,$ be a double array of i.i.d. complex random
    variables with $EX_{11}=0,E|X_{11}|\^2=1$ and $E|X_{11}|\^4<\infty$, and let
    $A_n=\frac{1}{N}T_n\^{{1}/{2}}X_nX_n\^\*T_n\^{{1}/{2}}$, where
    $T_n\^{{1}/{2}}$ is the square root of a nonnegative definite matrix $T_n$
    and $X_n$ is the $n\times N$ matrix of the upper-left corner of the double
    array. The matrix $A_n$ can be considered as a sample covariance matrix of
    an i.i.d. sample from a population with mean zero and covariance matrix
    $T_n$, or as a multivariate $F$ matrix if $T_n$ is the inverse of another
    sample covariance matrix. To investigate the limiting behavior of the
    eigenvectors of $A_n$, a new form of empirical spectral distribution is
    defined with weights defined by eigenvectors and it is then shown that this
    has the same limiting spectral distribution as the empirical spectral
    distribution defined by equal weights. Moreover, if \{$X_{ij}$\} and $T_n$
    are either real or complex and some additional moment assumptions are made
    then linear spectral statistics defined by the eigenvectors of $A_n$ are
    proved to have Gaussian limits, which suggests that the eigenvector matrix
    of $A_n$ is nearly Haar distributed when $T_n$ is a multiple of the identity
    matrix, an easy consequence for a Wishart matrix.
  accessed:
    - year: 2020
      month: 12
      day: 22
  author:
    - family: Bai
      given: Z. D.
    - family: Miao
      given: B. Q.
    - family: Pan
      given: G. M.
  citation-key: bai_asymptotics_2007
  container-title: The Annals of Probability
  container-title-short: Ann. Probab.
  DOI: 10.1214/009117906000001079
  ISSN: 0091-1798
  issue: '4'
  issued:
    - year: 2007
      month: 7
  page: 1532-1572
  source: arXiv.org
  title: On asymptotics of eigenvectors of large sample covariance matrix
  type: article-journal
  URL: http://arxiv.org/abs/0708.1720
  volume: '35'

- id: bai_deep_2019
  abstract: >-
    We present a new approach to modeling sequential data: the deep equilibrium
    model (DEQ). Motivated by an observation that the hidden layers of many
    existing deep sequence models converge towards some fixed point, we propose
    the DEQ approach that directly finds these equilibrium points via
    root-finding. Such a method is equivalent to running an infinite depth
    (weight-tied) feedforward network, but has the notable advantage that we can
    analytically backpropagate through the equilibrium point using implicit
    differentiation. Using this approach, training and prediction in these
    networks require only constant memory, regardless of the effective "depth"
    of the network. We demonstrate how DEQs can be applied to two
    state-of-the-art deep sequence models: self-attention transformers and
    trellis networks. On large-scale language modeling tasks, such as the
    WikiText-103 benchmark, we show that DEQs 1) often improve performance over
    these state-of-the-art models (for similar parameter counts); 2) have
    similar computational requirements as existing models; and 3) vastly reduce
    memory consumption (often the bottleneck for training large sequence
    models), demonstrating an up-to 88% memory reduction in our experiments. The
    code is available at https://github. com/locuslab/deq .
  accessed:
    - year: 2019
      month: 10
      day: 14
  author:
    - family: Bai
      given: Shaojie
    - family: Kolter
      given: J. Zico
    - family: Koltun
      given: Vladlen
  citation-key: bai_deep_2019
  container-title: arXiv:1909.01377 [cs, stat]
  issued:
    - year: 2019
      month: 9
      day: 3
  source: arXiv.org
  title: Deep Equilibrium Models
  type: article-journal
  URL: http://arxiv.org/abs/1909.01377

- id: bai_empirical_2018
  abstract: >-
    For most deep learning practitioners, sequence modeling is synonymous with
    recurrent networks. Yet recent results indicate that convolutional
    architectures can outperform recurrent networks on tasks such as audio
    synthesis and machine translation. Given a new sequence modeling task or
    dataset, which architecture should one use? We conduct a systematic
    evaluation of generic convolutional and recurrent architectures for sequence
    modeling. The models are evaluated across a broad range of standard tasks
    that are commonly used to benchmark recurrent networks. Our results indicate
    that a simple convolutional architecture outperforms canonical recurrent
    networks such as LSTMs across a diverse range of tasks and datasets, while
    demonstrating longer effective memory. We conclude that the common
    association between sequence modeling and recurrent networks should be
    reconsidered, and convolutional networks should be regarded as a natural
    starting point for sequence modeling tasks.
  author:
    - family: Bai
      given: Shaojie
    - family: Kolter
      given: J Zico
    - family: Koltun
      given: Vladlen
  citation-key: bai_empirical_2018
  container-title: arXiv:1803.01271
  issued:
    - year: 2018
  title: >-
    An Empirical Evaluation of Generic Convolutional and Recurrent Networks for
    Sequence Modeling
  type: article-journal

- id: bai_recent_2021
  abstract: >-
    Adversarial training is one of the most effective approaches to defending
    deep learning models against adversarial examples. Unlike other defense
    strategies, adversarial training aims to enhance the robustness of models
    intrinsically. During the last few years, adversarial training has been
    studied and discussed from various aspects. A variety of improvements and
    developments of adversarial training are proposed, which were, however,
    neglected in existing surveys. For the ﬁrst time in this survey, we
    systematically review the recent progress on adversarial training for
    adversarial robustness with a novel taxonomy. Then we discuss the
    generalization problems in adversarial training from three perspectives and
    highlight the challenges which are not fully tackled. Finally, we present
    potential future directions.
  accessed:
    - year: 2022
      month: 4
      day: 29
  author:
    - family: Bai
      given: Tao
    - family: Luo
      given: Jinqi
    - family: Zhao
      given: Jun
    - family: Wen
      given: Bihan
    - family: Wang
      given: Qian
  citation-key: bai_recent_2021
  container-title: arXiv:2102.01356
  issued:
    - year: 2021
      month: 4
      day: 20
  language: en
  source: arXiv.org
  title: Recent Advances in Adversarial Training for Adversarial Robustness
  type: article-journal
  URL: http://arxiv.org/abs/2102.01356

- id: bai_spectral_2010
  author:
    - family: Bai
      given: Zhidong
    - family: Silverstein
      given: Jack W
  citation-key: bai_spectral_2010
  collection-title: Springer Series in Statistics
  issued:
    - year: 2010
  publisher: Springer
  title: Spectral analysis of large dimensional random matrices
  type: book
  volume: '20'

- id: bai_trellis_2018
  abstract: >-
    We present trellis networks, a new architecture for sequence modeling. On
    the one hand, a trellis network is a temporal convolutional network with
    special structure, characterized by weight tying across depth and direct
    injection of the input into deep layers. On the other hand, we show that
    truncated recurrent networks are equivalent to trellis networks with special
    sparsity structure in their weight matrices. Thus trellis networks with
    general weight matrices generalize truncated recurrent networks. We leverage
    these connections to design high-performing trellis networks that absorb
    structural and algorithmic elements from both recurrent and convolutional
    models. Experiments demonstrate that trellis networks outperform the current
    state of the art methods on a variety of challenging benchmarks, including
    word-level language modeling and character-level language modeling tasks,
    and stress tests designed to evaluate long-term memory retention. The code
    is available at https://github.com/locuslab/trellisnet .
  accessed:
    - year: 2019
      month: 6
      day: 6
  author:
    - family: Bai
      given: Shaojie
    - family: Kolter
      given: J. Zico
    - family: Koltun
      given: Vladlen
  citation-key: bai_trellis_2018
  container-title: arXiv:1810.06682 [cs, stat]
  issued:
    - year: 2018
      month: 10
      day: 15
  source: arXiv.org
  title: Trellis Networks for Sequence Modeling
  type: article-journal
  URL: http://arxiv.org/abs/1810.06682

- id: bailey_sizenoise_2018
  accessed:
    - year: 2018
      month: 12
      day: 4
  author:
    - family: Bailey
      given: Bolton
    - family: Telgarsky
      given: Matus J
  citation-key: bailey_sizenoise_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 6489–6499
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Size-Noise Tradeoffs in Generative Networks
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/7884-size-noise-tradeoffs-in-generative-networks.pdf

- id: baldi_neuronal_2018
  accessed:
    - year: 2018
      month: 12
      day: 4
  author:
    - family: Baldi
      given: Pierre
    - family: Vershynin
      given: Roman
  citation-key: baldi_neuronal_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 7739–7748
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: On Neuronal Capacity
  type: chapter
  URL: http://papers.nips.cc/paper/7999-on-neuronal-capacity.pdf

- id: banerjee_diagnosis_2017
  abstract: >-
    Accurate identification of prostate cancer in frozen sections at the time of
    surgery can be challenging, limiting the surgeon’s ability to best determine
    resection margins during prostatectomy. We performed desorption electrospray
    ionization mass spectrometry imaging (DESI-MSI) on 54 banked human cancerous
    and normal prostate tissue specimens to investigate the spatial distribution
    of a wide variety of small metabolites, carbohydrates, and lipids. In
    contrast to several previous studies, our method included Krebs cycle
    intermediates (m/z <200), which we found to be highly informative in
    distinguishing cancer from benign tissue. Malignant prostate cells showed
    marked metabolic derangements compared with their benign counterparts. Using
    the “Least absolute shrinkage and selection operator” (Lasso), we analyzed
    all metabolites from the DESI-MS data and identified parsimonious sets of
    metabolic profiles for distinguishing between cancer and normal tissue. In
    an independent set of samples, we could use these models to classify
    prostate cancer from benign specimens with nearly 90% accuracy per patient.
    Based on previous work in prostate cancer showing that glucose levels are
    high while citrate is low, we found that measurement of the glucose/citrate
    ion signal ratio accurately predicted cancer when this ratio exceeds 1.0 and
    normal prostate when the ratio is less than 0.5. After brief tissue
    preparation, the glucose/citrate ratio can be recorded on a tissue sample in
    1 min or less, which is in sharp contrast to the 20 min or more required by
    histopathological examination of frozen tissue specimens.
  accessed:
    - year: 2017
      month: 9
      day: 18
  author:
    - family: Banerjee
      given: Shibdas
    - family: Zare
      given: Richard N.
    - family: Tibshirani
      given: Robert J.
    - family: Kunder
      given: Christian A.
    - family: Nolley
      given: Rosalie
    - family: Fan
      given: Richard
    - family: Brooks
      given: James D.
    - family: Sonn
      given: Geoffrey A.
  citation-key: banerjee_diagnosis_2017
  container-title: Proceedings of the National Academy of Sciences
  container-title-short: PNAS
  DOI: 10.1073/pnas.1700677114
  ISSN: 0027-8424, 1091-6490
  issue: '13'
  issued:
    - year: 2017
      month: 3
      day: 28
  language: en
  page: 3334-3339
  PMID: '28292895'
  source: www.pnas.org
  title: >-
    Diagnosis of prostate cancer by desorption electrospray ionization mass
    spectrometric imaging of small metabolites and lipids
  type: article-journal
  URL: http://www.pnas.org/content/114/13/3334
  volume: '114'

- id: banerjee_largest_2017
  abstract: >-
    We study the largest eigenvalue of certain block matrices where the number
    of blocks and the block size both increase with suitable conditions on their
    relative growth. In one of them, we employ a symmetric block structure with
    large independent Wigner blocks and in the other we have the Wigner block
    structure with large independent symmetric blocks. The entries are assumed
    to be independent and identically distributed with mean [Formula: see text]
    variance [Formula: see text] with an appropriate growth condition on the
    moments. Under our conditions the limit spectral distribution of these
    matrices is the standard semi-circle law. It is natural to ask if the
    extreme eigenvalues converge to the extreme points of its support, namely
    [Formula: see text]. We exhibit models where this indeed happens as well as
    models where the spectral norm converges to [Formula: see text]. Our proofs
    are based on combinatorial analysis of the behavior of the trace of large
    powers of the matrix.
  accessed:
    - year: 2020
      month: 11
      day: 23
  author:
    - family: Banerjee
      given: Debapratim
    - family: Bose
      given: Arup
  citation-key: banerjee_largest_2017
  container-title: 'Random Matrices: Theory and Applications'
  container-title-short: 'Random Matrices: Theory Appl.'
  DOI: 10.1142/S2010326317500083
  ISSN: 2010-3263, 2010-3271
  issue: '02'
  issued:
    - year: 2017
      month: 4
  language: en
  page: '1750008'
  source: DOI.org (Crossref)
  title: 'Largest eigenvalue of large random block matrices: A combinatorial approach'
  title-short: Largest eigenvalue of large random block matrices
  type: article-journal
  URL: https://www.worldscientific.com/doi/abs/10.1142/S2010326317500083
  volume: '06'

- id: barakat_simultaneous_1999
  author:
    - family: Barakat
      given: Richard
    - family: Sandler
      given: Barbara H.
  citation-key: barakat_simultaneous_1999
  container-title: 'Journal of Optics A: Pure and Applied Optics'
  DOI: 10.1088/1464-4258/1/5/309
  issue: '5'
  issued:
    - year: 1999
  page: '629'
  title: >-
    Simultaneous determination of the modulus and phase of a coherently
    illuminated object from its measured diffraction image
  type: article-journal
  volume: '1'

- id: barber_bayesian_2012
  author:
    - family: Barber
      given: David
  citation-key: barber_bayesian_2012
  issued:
    - year: 2012
  publisher: Cambridge University Press
  source: Google Scholar
  title: Bayesian reasoning and machine learning
  type: book

- id: barbosa_downhole_2015
  author:
    - family: Barbosa
      given: Bruno HG
    - family: Gomes
      given: Lucas P
    - family: Teixeira
      given: Alex F
    - family: Aguirre
      given: Luis A
  citation-key: barbosa_downhole_2015
  container-title: IFAC-PapersOnLine
  DOI: 10.1016/j.ifacol.2015.08.045
  issue: '6'
  issued:
    - year: 2015
  page: 286–291
  title: Downhole Pressure Estimation Using Committee Machines and Neural Networks
  type: article-journal
  volume: '48'

- id: barbosa_downhole_2015a
  author:
    - family: Barbosa
      given: Bruno HG
    - family: Gomes
      given: Lucas P
    - family: Teixeira
      given: Alex F
    - family: Aguirre
      given: Luis A
  citation-key: barbosa_downhole_2015a
  container-title: IFAC-PapersOnLine
  DOI: 10/gfjwq5
  issue: '6'
  issued:
    - year: 2015
  note: '00003'
  page: 286-291
  title: Downhole Pressure Estimation Using Committee Machines and Neural Networks
  type: article-journal
  volume: '48'

- id: barocas_fairness_2023
  author:
    - family: Barocas
      given: Solon
    - family: Hardt
      given: Moritz
    - family: Narayanan
      given: Arvind
  citation-key: barocas_fairness_2023
  issued:
    - year: 2023
  language: en
  source: Zotero
  title: Fairness and Machine Learning
  type: book

- id: barsbey_heavy_2021
  abstract: >-
    Neural network compression techniques have become increasingly popular as
    they can drastically reduce the storage and computation requirements for
    very large networks. Recent empirical studies have illustrated that even
    simple pruning strategies can be surprisingly effective, and several
    theoretical studies have shown that compressible networks (in specific
    senses) should achieve a low generalization error. Yet, a theoretical
    characterization of the underlying cause that makes the networks amenable to
    such simple compression schemes is still missing. In this study, we address
    this fundamental question and reveal that the dynamics of the training
    algorithm has a key role in obtaining such compressible networks. Focusing
    our attention on stochastic gradient descent (SGD), our main contribution is
    to link compressibility to two recently established properties of SGD: (i)
    as the network size goes to infinity, the system can converge to a
    mean-field limit, where the network weights behave independently, (ii) for a
    large step-size/batch-size ratio, the SGD iterates can converge to a
    heavy-tailed stationary distribution. In the case where these two phenomena
    occur simultaneously, we prove that the networks are guaranteed to be
    '$\ell_p$-compressible', and the compression errors of different pruning
    techniques (magnitude, singular value, or node pruning) become arbitrarily
    small as the network size increases. We further prove generalization bounds
    adapted to our theoretical framework, which indeed confirm that the
    generalization error will be lower for more compressible networks. Our
    theory and numerical study on various neural networks show that large
    step-size/batch-size ratios introduce heavy-tails, which, in combination
    with overparametrization, result in compressibility.
  accessed:
    - year: 2023
      month: 6
      day: 15
  author:
    - family: Barsbey
      given: Melih
    - family: Sefidgaran
      given: Milad
    - family: Erdogdu
      given: Murat A.
    - family: Richard
      given: Gaël
    - family: Şimşekli
      given: Umut
  citation-key: barsbey_heavy_2021
  DOI: 10.48550/arXiv.2106.03795
  issued:
    - year: 2021
      month: 6
      day: 7
  number: arXiv:2106.03795
  publisher: arXiv
  source: arXiv.org
  title: Heavy Tails in SGD and Compressibility of Overparametrized Neural Networks
  type: article
  URL: http://arxiv.org/abs/2106.03795

- id: bartlett_adversarial_2021
  abstract: >-
    We consider the phenomenon of adversarial examples in ReLU networks with
    independent Gaussian parameters. For networks of constant depth and with a
    large range of widths (for instance, it sufﬁces if the width of each layer
    is polynomial in that of any other layer), small perturbations of input
    vectors lead to large changes of outputs. This generalizes results of
    Daniely and Schacham (2020) for networks of rapidly decreasing width and of
    Bubeck et al (2021) for two-layer networks. Our proof shows that adversarial
    examples arise in these networks because the functions they compute are
    locally very similar to random linear functions. Bottleneck layers play a
    key role: the minimal width up to some point in the network determines
    scales and sensitivities of mappings computed up to that point. The main
    result is for networks with constant depth, but we also show that some
    constraint on depth is necessary for a result of this kind, because there
    are suitably deep networks that, with constant probability, compute a
    function that is close to constant.
  author:
    - family: Bartlett
      given: Peter L
    - family: Bubeck
      given: Sébastien
    - family: Cherapanamjeri
      given: Yeshwanth
  citation-key: bartlett_adversarial_2021
  container-title: Neural Information Processing Systems  (NeurIPS)
  issued:
    - year: 2021
  language: en
  source: Zotero
  title: Adversarial Examples in Multi-Layer Random ReLU Networks
  type: article-journal

- id: bartlett_benign_2020
  abstract: >-
    The phenomenon of benign overfitting is one of the key mysteries uncovered
    by deep learning methodology: deep neural networks seem to predict well,
    even with a perfect fit to noisy training data. Motivated by this
    phenomenon, we consider when a perfect fit to training data in linear
    regression is compatible with accurate prediction. We give a
    characterization of linear regression problems for which the minimum norm
    interpolating prediction rule has near-optimal prediction accuracy. The
    characterization is in terms of two notions of the effective rank of the
    data covariance. It shows that overparameterization is essential for benign
    overfitting in this setting: the number of directions in parameter space
    that are unimportant for prediction must significantly exceed the sample
    size. By studying examples of data covariance properties that this
    characterization shows are required for benign overfitting, we find an
    important role for finite-dimensional data: the accuracy of the minimum norm
    interpolating prediction rule approaches the best possible accuracy for a
    much narrower range of properties of the data distribution when the data lie
    in an infinite-dimensional space vs. when the data lie in a
    finite-dimensional space with dimension that grows faster than the sample
    size.
  author:
    - family: Bartlett
      given: Peter L.
    - family: Long
      given: Philip M.
    - family: Lugosi
      given: Gábor
    - family: Tsigler
      given: Alexander
  citation-key: bartlett_benign_2020
  container-title: Proceedings of the National Academy of Sciences
  DOI: 10.1073/pnas.1907378117
  ISSN: 0027-8424, 1091-6490
  issue: '48'
  issued:
    - year: 2020
      month: 4
      day: 24
  page: 30063--30070
  title: Benign overfitting in linear regression
  type: article-journal
  volume: '117'

- id: bartlett_deep_2021
  abstract: >-
    The remarkable practical success of deep learning has revealed some major
    surprises from a theoretical perspective. In particular, simple gradient
    methods easily find near-optimal solutions to non-convex optimization
    problems, and despite giving a near-perfect fit to training data without any
    explicit effort to control model complexity, these methods exhibit excellent
    predictive accuracy. We conjecture that specific principles underlie these
    phenomena: that overparametrization allows gradient methods to find
    interpolating solutions, that these methods implicitly impose
    regularization, and that overparametrization leads to benign overfitting. We
    survey recent theoretical progress that provides examples illustrating these
    principles in simpler settings. We first review classical uniform
    convergence results and why they fall short of explaining aspects of the
    behavior of deep learning methods. We give examples of implicit
    regularization in simple settings, where gradient methods lead to minimal
    norm functions that perfectly fit the training data. Then we review
    prediction methods that exhibit benign overfitting, focusing on regression
    problems with quadratic loss. For these methods, we can decompose the
    prediction rule into a simple component that is useful for prediction and a
    spiky component that is useful for overfitting but, in a favorable setting,
    does not harm prediction accuracy. We focus specifically on the linear
    regime for neural networks, where the network can be approximated by a
    linear model. In this regime, we demonstrate the success of gradient flow,
    and we consider benign overfitting with two-layer networks, giving an exact
    asymptotic analysis that precisely demonstrates the impact of
    overparametrization. We conclude by highlighting the key challenges that
    arise in extending these insights to realistic deep learning settings.
  accessed:
    - year: 2021
      month: 3
      day: 22
  author:
    - family: Bartlett
      given: Peter L.
    - family: Montanari
      given: Andrea
    - family: Rakhlin
      given: Alexander
  citation-key: bartlett_deep_2021
  container-title: arXiv:2103.09177
  issued:
    - year: 2021
      month: 3
      day: 16
  source: arXiv.org
  title: 'Deep learning: a statistical viewpoint'
  title-short: Deep learning
  type: article-journal
  URL: http://arxiv.org/abs/2103.09177

- id: bartlett_spectrallynormalized_2017
  author:
    - family: Bartlett
      given: Peter L
    - family: Foster
      given: Dylan J
    - family: Telgarsky
      given: Matus J
  citation-key: bartlett_spectrallynormalized_2017
  container-title: Advances in neural information processing systems
  editor:
    - family: Guyon
      given: I.
    - family: Luxburg
      given: U. Von
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Fergus
      given: R.
    - family: Vishwanathan
      given: S.
    - family: Garnett
      given: R.
  issued:
    - year: 2017
  publisher: Curran Associates, Inc.
  title: Spectrally-normalized margin bounds for neural networks
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper_files/paper/2017/file/b22b257ad0519d4500539da3c8bcf4dd-Paper.pdf
  volume: '30'

- id: basu_regularized_2015
  abstract: >-
    Many scientific and economic problems involve the analysis of
    high-dimensional time series datasets. However, theoretical studies in
    high-dimensional statistics to date rely primarily on the assumption of
    independent and identically distributed (i.i.d.) samples. In this work, we
    focus on stable Gaussian processes and investigate the theoretical
    properties of $\ell _1$-regularized estimates in two important statistical
    problems in the context of high-dimensional time series: (a) stochastic
    regression with serially correlated errors and (b) transition matrix
    estimation in vector autoregressive (VAR) models. We derive nonasymptotic
    upper bounds on the estimation errors of the regularized estimates and
    establish that consistent estimation under high-dimensional scaling is
    possible via $\ell_1$-regularization for a large class of stable processes
    under sparsity constraints. A key technical contribution of the work is to
    introduce a measure of stability for stationary processes using their
    spectral properties that provides insight into the effect of dependence on
    the accuracy of the regularized estimates. With this proposed stability
    measure, we establish some useful deviation bounds for dependent data, which
    can be used to study several important regularized estimates in a time
    series setting.
  author:
    - family: Basu
      given: Sumanta
    - family: Michailidis
      given: George
  citation-key: basu_regularized_2015
  container-title: The Annals of Statistics
  DOI: 10.1214/15-AOS1315
  ISSN: 0090-5364
  issue: '4'
  issued:
    - year: 2015
      month: 8
  page: 1535-1567
  source: arXiv.org
  title: Regularized estimation in sparse high-dimensional time series models
  type: article-journal
  URL: http://arxiv.org/abs/1311.4175
  volume: '43'

- id: batselier_canonical_2014
  abstract: >-
    This article introduces the canonical decomposition of the vector space of
    multivariate polynomials for a given monomial ordering. Its importance lies
    in solving multivariate polynomial systems, computing Gröbner bases, and
    solving the ideal membership problem. An SVD-based algorithm is presented
    that numerically computes the canonical decomposition. It is then shown how,
    by introducing the notion of divisibility into this algorithm, a numerical
    Gröbner basis can also be computed. In addition, we demonstrate how the
    canonical decomposition can be used to decide whether the affine solution
    set of a multivariate polynomial system is zero-dimensional and to solve the
    ideal membership problem numerically. The SVD-based canonical decomposition
    algorithm is also extended to numerically compute border bases. A tolerance
    for each of the algorithms is derived using perturbation theory of principal
    angles. This derivation shows that the condition number of computing the
    canonical decomposition and numerical Gröbner basis is essentially the
    condition number of the Macaulay matrix. Numerical experiments with both
    exact and noisy coefficients are presented and discussed.
  author:
    - family: Batselier
      given: K.
    - family: Dreesen
      given: P.
    - family: De Moor
      given: B.
  citation-key: batselier_canonical_2014
  container-title: SIAM Journal on Matrix Analysis and Applications
  container-title-short: SIAM. J. Matrix Anal. & Appl.
  DOI: 10.1137/130927176
  ISSN: 0895-4798
  issue: '4'
  issued:
    - year: 2014
      month: 1
      day: 1
  page: 1242-1264
  source: epubs.siam.org (Atypon)
  title: >-
    The Canonical Decomposition of $\mathcal{C}^n_d$ and Numerical Gröbner and
    Border Bases
  type: article-journal
  URL: http://epubs.siam.org/doi/abs/10.1137/130927176
  volume: '35'

- id: batselier_geometry_2013
  abstract: >-
    Multivariate polynomials are usually discussed in the framework of algebraic
    geometry. Solving problems in algebraic geometry usually involves the use of
    a Gröbner basis. This article shows that linear algebra without any Gröbner
    basis computation suffices to solve basic problems from algebraic geometry
    by describing three operations: multiplication, division, and elimination.
    This linear algebra framework will also allow us to give a geometric
    interpretation. Multivariate division will involve oblique projections, and
    a link between elimination and principal angles between subspaces (CS
    decomposition) is revealed. The main computational tool in this approach is
    the QR decomposition.
  author:
    - family: Batselier
      given: K.
    - family: Dreesen
      given: P.
    - family: Moor
      given: B.
  citation-key: batselier_geometry_2013
  container-title: SIAM Journal on Matrix Analysis and Applications
  container-title-short: SIAM. J. Matrix Anal. & Appl.
  DOI: 10.1137/120863782
  ISSN: 0895-4798
  issue: '1'
  issued:
    - year: 2013
      month: 1
      day: 1
  page: 102-125
  source: epubs.siam.org (Atypon)
  title: The Geometry of Multivariate Polynomial Division and Elimination
  type: article-journal
  URL: http://epubs.siam.org/doi/abs/10.1137/120863782
  volume: '34'

- id: battiti_first_1992
  abstract: >-
    On-line first-order backpropagation is sufficiently fast and effective for
    many large-scale classification problems but for very high precision
    mappings, batch processing may be the method of choice. This paper reviews
    first- and second-order optimization methods for learning in feedforward
    neural networks. The viewpoint is that of optimization: many methods can be
    cast in the language of optimization techniques, allowing the transfer to
    neural nets of detailed results about computational complexity and safety
    procedures to ensure convergence and to avoid numerical problems. The review
    is not intended to deliver detailed prescriptions for the most appropriate
    methods in specific applications, but to illustrate the main characteristics
    of the different methods and their mutual relations.
  author:
    - family: Battiti
      given: R.
  citation-key: battiti_first_1992
  container-title: Neural Computation
  DOI: 10.1162/neco.1992.4.2.141
  ISSN: 0899-7667
  issue: '2'
  issued:
    - year: 1992
      month: 3
  page: 141-166
  source: IEEE Xplore
  title: >-
    First- and Second-Order Methods for Learning: Between Steepest Descent and
    Newton's Method
  title-short: First- and Second-Order Methods for Learning
  type: article-journal
  volume: '4'

- id: bauschke_convex_2011
  accessed:
    - year: 2023
      month: 3
      day: 21
  author:
    - family: Bauschke
      given: Heinz H.
    - family: Combettes
      given: Patrick L.
  citation-key: bauschke_convex_2011
  collection-title: CMS Books in Mathematics
  DOI: 10.1007/978-1-4419-9467-7
  event-place: New York, NY
  ISBN: 978-1-4419-9466-0 978-1-4419-9467-7
  issued:
    - year: 2011
  language: en
  publisher: Springer New York
  publisher-place: New York, NY
  source: DOI.org (Crossref)
  title: Convex Analysis and Monotone Operator Theory in Hilbert Spaces
  type: book
  URL: https://link.springer.com/10.1007/978-1-4419-9467-7

- id: beale_neural_2017
  author:
    - family: Beale
      given: Mark Hudson
    - family: Hagan
      given: Martin T.
    - family: Demuth
      given: Howard B.
  citation-key: beale_neural_2017
  issued:
    - year: 2017
  publisher: Mathworks
  title: Neural network toolbox for use with MATLAB
  type: report

- id: beck_fast_2009
  abstract: >-
    We consider the class of iterative shrinkage-thresholding algorithms (ISTA)
    for solving linear inverse problems arising in signal/image processing. This
    class of methods, which can be viewed as an extension of the classical
    gradient algorithm, is attractive due to its simplicity and thus is adequate
    for solving large-scale problems even with dense matrix data. However, such
    methods are also known to converge quite slowly. In this paper we present a
    new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves
    the computational simplicity of ISTA but with a global rate of convergence
    which is proven to be signiﬁcantly better, both theoretically and
    practically. Initial promising numerical results for wavelet-based image
    deblurring demonstrate the capabilities of FISTA which is shown to be faster
    than ISTA by several orders of magnitude.
  accessed:
    - year: 2024
      month: 1
      day: 16
  author:
    - family: Beck
      given: Amir
    - family: Teboulle
      given: Marc
  citation-key: beck_fast_2009
  container-title: SIAM Journal on Imaging Sciences
  container-title-short: SIAM J. Imaging Sci.
  DOI: 10.1137/080716542
  ISSN: 1936-4954
  issue: '1'
  issued:
    - year: 2009
      month: 1
  language: en
  page: 183-202
  source: DOI.org (Crossref)
  title: >-
    A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse
    Problems
  type: article-journal
  URL: http://epubs.siam.org/doi/10.1137/080716542
  volume: '2'

- id: beck_protecting_2016
  abstract: >-
    BACKGROUND: As increasing amounts of personal information are being
    collected through a plethora of electronic modalities by statutory and
    non-statutory organizations, ensuring the confidentiality and security of
    such information has become a major issue globally. While the use of many of
    these media can be beneficial to individuals or populations, they can also
    be open to abuse by individuals or statutory and non-statutory
    organizations. Recent examples include collection of personal information by
    national security systems and the development of national programs like the
    Chinese Social Credit System. In many low- and middle-income countries, an
    increasing amount of personal health information is being collected. The
    collection of personal health information is necessary, in order to develop
    longitudinal medical records and to monitor and evaluate the use, cost,
    outcome, and impact of health services at facility, sub-national, and
    national levels. However, if personal health information is not held
    confidentially and securely, individuals with communicable or
    non-communicable diseases (NCDs) may be reluctant to use preventive or
    therapeutic health services, due to fear of being stigmatized or
    discriminated against. While policymakers and other stakeholders in these
    countries recognize the need to develop and implement policies for
    protecting the privacy, confidentiality and security of personal health
    information, to date few of these countries have developed, let alone
    implemented, coherent policies. The global HIV response continues to
    emphasize the importance of collecting HIV-health information, recently
    re-iterated by the Fast Track to End AIDS by 2030 program and the recent
    changes in the Guidelines on When to Start Antiretroviral Therapy and on
    Pre-exposure Prophylaxis for HIV. The success of developing HIV treatment
    cascades in low- and middle-income countries will require the development of
    National Health Identification Systems. The success of programs like
    Universal Health Coverage, under the recently ratified Sustainable
    Development Goals is also contingent on the availability of personal health
    information for communicable and non-communicable diseases.

    DESIGN: Guidance for countries to develop and implement their own guidelines
    for protecting HIV-information formed the basis of identifying a number of
    fundamental principles, governing the areas of privacy, confidentiality and
    security. The use of individual-level data must balance maximizing the
    benefits from their most effective and fullest use, and minimizing harm
    resulting from their malicious or inadvertent release.

    DISCUSSION: These general principles are described in this paper, as along
    with a bibliography referring to more detailed technical information. A
    country assessment tool and user's manual, based on these principles, have
    been developed to support countries to assess the privacy, confidentiality,
    and security of personal health information at facility, data
    warehouse/repository, and national levels. The successful development and
    implementation of national guidance will require strong collaboration at
    local, regional, and national levels, and this is a pre-condition for the
    successful implementation of a range of national and global programs.

    CONCLUSION: This paper is a call for action for stakeholders in low- and
    middle-income countries to develop and implement such coherent policies and
    provides fundamental principles governing the areas of privacy,
    confidentiality, and security of personal health information being collected
    in low- and middle-income countries.
  author:
    - family: Beck
      given: Eduard J.
    - family: Gill
      given: Wayne
    - family: De Lay
      given: Paul R.
  citation-key: beck_protecting_2016
  container-title: Global Health Action
  container-title-short: Glob Health Action
  ISSN: 1654-9880
  issued:
    - year: 2016
  language: eng
  page: '32089'
  PMCID: PMC5123209
  PMID: '27885972'
  source: PubMed
  title: >-
    Protecting the confidentiality and security of personal health information
    in low- and middle-income countries in the era of SDGs and Big Data
  type: article-journal
  volume: '9'

- id: beekmans_linux_2012
  author:
    - family: Beekmans
      given: G.
  citation-key: beekmans_linux_2012
  ISBN: 978-1-300-01983-1
  issued:
    - year: 2012
  publisher: Lulu Press, Incorporated
  title: Linux From Scratch
  type: book
  URL: https://books.google.com.br/books?id=N7vJAwAAQBAJ

- id: bejnordi_diagnostic_2017
  accessed:
    - year: 2017
      month: 12
      day: 13
  author:
    - family: Bejnordi
      given: Babak Ehteshami
    - family: Veta
      given: Mitko
    - family: Johannes van Diest
      given: Paul
    - family: Ginneken
      given: Bram
      non-dropping-particle: van
    - family: Karssemeijer
      given: Nico
    - family: Litjens
      given: Geert
    - family: Laak
      given: Jeroen A. W. M.
      non-dropping-particle: van der
    - literal: and the CAMELYON16 Consortium
    - family: Hermsen
      given: Meyke
    - family: Manson
      given: Quirine F
    - family: Balkenhol
      given: Maschenka
    - family: Geessink
      given: Oscar
    - family: Stathonikos
      given: Nikolaos
    - family: Dijk
      given: Marcory CRF
      non-dropping-particle: van
    - family: Bult
      given: Peter
    - family: Beca
      given: Francisco
    - family: Beck
      given: Andrew H
    - family: Wang
      given: Dayong
    - family: Khosla
      given: Aditya
    - family: Gargeya
      given: Rishab
    - family: Irshad
      given: Humayun
    - family: Zhong
      given: Aoxiao
    - family: Dou
      given: Qi
    - family: Li
      given: Quanzheng
    - family: Chen
      given: Hao
    - family: Lin
      given: Huang-Jing
    - family: Heng
      given: Pheng-Ann
    - family: Haß
      given: Christian
    - family: Bruni
      given: Elia
    - family: Wong
      given: Quincy
    - family: Halici
      given: Ugur
    - family: Öner
      given: Mustafa Ümit
    - family: Cetin-Atalay
      given: Rengul
    - family: Berseth
      given: Matt
    - family: Khvatkov
      given: Vitali
    - family: Vylegzhanin
      given: Alexei
    - family: Kraus
      given: Oren
    - family: Shaban
      given: Muhammad
    - family: Rajpoot
      given: Nasir
    - family: Awan
      given: Ruqayya
    - family: Sirinukunwattana
      given: Korsuk
    - family: Qaiser
      given: Talha
    - family: Tsang
      given: Yee-Wah
    - family: Tellez
      given: David
    - family: Annuscheit
      given: Jonas
    - family: Hufnagl
      given: Peter
    - family: Valkonen
      given: Mira
    - family: Kartasalo
      given: Kimmo
    - family: Latonen
      given: Leena
    - family: Ruusuvuori
      given: Pekka
    - family: Liimatainen
      given: Kaisa
    - family: Albarqouni
      given: Shadi
    - family: Mungal
      given: Bharti
    - family: George
      given: Ami
    - family: Demirci
      given: Stefanie
    - family: Navab
      given: Nassir
    - family: Watanabe
      given: Seiryo
    - family: Seno
      given: Shigeto
    - family: Takenaka
      given: Yoichi
    - family: Matsuda
      given: Hideo
    - family: Ahmady Phoulady
      given: Hady
    - family: Kovalev
      given: Vassili
    - family: Kalinovsky
      given: Alexander
    - family: Liauchuk
      given: Vitali
    - family: Bueno
      given: Gloria
    - family: Fernandez-Carrobles
      given: M. Milagro
    - family: Serrano
      given: Ismael
    - family: Deniz
      given: Oscar
    - family: Racoceanu
      given: Daniel
    - family: Venâncio
      given: Rui
  citation-key: bejnordi_diagnostic_2017
  container-title: JAMA
  DOI: 10.1001/jama.2017.14585
  ISSN: 0098-7484
  issue: '22'
  issued:
    - year: 2017
      month: 12
      day: 12
  language: en
  page: '2199'
  source: CrossRef
  title: >-
    Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph
    Node Metastases in Women With Breast Cancer
  type: article-journal
  URL: http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2017.14585
  volume: '318'

- id: belkin_reconciling_2019
  abstract: >-
    Breakthroughs in machine learning are rapidly changing science and society,
    yet our fundamental understanding of this technology has lagged far behind.
    Indeed, one of the central tenets of the field, the bias–variance trade-off,
    appears to be at odds with the observed behavior of methods used in modern
    machine-learning practice. The bias–variance trade-off implies that a model
    should balance underfitting and overfitting: Rich enough to express
    underlying structure in data and simple enough to avoid fitting spurious
    patterns. However, in modern practice, very rich models such as neural
    networks are trained to exactly fit (i.e., interpolate) the data.
    Classically, such models would be considered overfitted, and yet they often
    obtain high accuracy on test data. This apparent contradiction has raised
    questions about the mathematical foundations of machine learning and their
    relevance to practitioners. In this paper, we reconcile the classical
    understanding and the modern practice within a unified performance curve.
    This “double-descent” curve subsumes the textbook U-shaped bias–variance
    trade-off curve by showing how increasing model capacity beyond the point of
    interpolation results in improved performance. We provide evidence for the
    existence and ubiquity of double descent for a wide spectrum of models and
    datasets, and we posit a mechanism for its emergence. This connection
    between the performance and the structure of machine-learning models
    delineates the limits of classical analyses and has implications for both
    the theory and the practice of machine learning.
  accessed:
    - year: 2020
      month: 8
      day: 7
  author:
    - family: Belkin
      given: Mikhail
    - family: Hsu
      given: Daniel
    - family: Ma
      given: Siyuan
    - family: Mandal
      given: Soumik
  citation-key: belkin_reconciling_2019
  container-title: Proceedings of the National Academy of Sciences
  DOI: 10.1073/pnas.1903070116
  ISSN: 0027-8424, 1091-6490
  issue: '32'
  issued:
    - year: 2019
  page: 15849-15854
  title: >-
    Reconciling modern machine-learning practice and the classical bias–variance
    trade-off
  type: article-journal
  volume: '116'

- id: belkin_two_2020
  abstract: >-
    The “double descent” risk curve was proposed to qualitatively describe the
    out-of-sample prediction accuracy of variably parameterized machine learning
    models. This article provides a precise mathematical analysis for the shape
    of this curve in two simple data models with the least squares/least norm
    predictor. Specifically, it is shown that the risk peaks when the number of
    features $p$ is close to the sample size $n$ but also that the risk
    sometimes decreases toward its minimum as $p$ increases beyond $n$. This
    behavior parallels some key patterns observed in large models, including
    modern neural networks, and is contrasted with that of “prescient” models
    that select features in an a priori optimal order.
  author:
    - family: Belkin
      given: Mikhail
    - family: Hsu
      given: Daniel
    - family: Xu
      given: Ji
  citation-key: belkin_two_2020
  container-title: SIAM Journal on Mathematics of Data Science
  DOI: 10.1137/20M1336072
  issue: '4'
  issued:
    - year: 2020
      month: 1
      day: 1
  page: 1167-1180
  source: epubs.siam.org (Atypon)
  title: Two Models of Double Descent for Weak Features
  type: article-journal
  volume: '2'

- id: belkin_understand_2018
  abstract: >-
    Generalization performance of classifiers in deep learning has recently
    become a subject of intense study. Deep models, typically over-parametrized,
    tend to fit the training data exactly. Despite this "overfitting", they
    perform well on test data, a phenomenon not yet fully understood. The first
    point of our paper is that strong performance of overfitted classifiers is
    not a unique feature of deep learning. Using six real-world and two
    synthetic datasets, we establish experimentally that kernel machines trained
    to have zero classification or near zero regression error perform very well
    on test data, even when the labels are corrupted with a high level of noise.
    We proceed to give a lower bound on the norm of zero loss solutions for
    smooth kernels, showing that they increase nearly exponentially with data
    size. We point out that this is difficult to reconcile with the existing
    generalization bounds. Moreover, none of the bounds produce non-trivial
    results for interpolating solutions. Second, we show experimentally that
    (non-smooth) Laplacian kernels easily fit random labels, a finding that
    parallels results for ReLU neural networks. In contrast, fitting noisy data
    requires many more epochs for smooth Gaussian kernels. Similar performance
    of overfitted Laplacian and Gaussian classifiers on test, suggests that
    generalization is tied to the properties of the kernel function rather than
    the optimization process. Certain key phenomena of deep learning are
    manifested similarly in kernel methods in the modern "overfitted" regime.
    The combination of the experimental and theoretical results presented in
    this paper indicates a need for new theoretical ideas for understanding
    properties of classical kernel methods. We argue that progress on
    understanding deep learning will be difficult until more tractable "shallow"
    kernel methods are better understood.
  accessed:
    - year: 2020
      month: 8
      day: 7
  author:
    - family: Belkin
      given: Mikhail
    - family: Ma
      given: Siyuan
    - family: Mandal
      given: Soumik
  citation-key: belkin_understand_2018
  container-title: arXiv:1802.01396 [cs, stat]
  issued:
    - year: 2018
      month: 6
      day: 14
  source: arXiv.org
  title: To understand deep learning we need to understand kernel learning
  type: article-journal
  URL: http://arxiv.org/abs/1802.01396

- id: belloni_squareroot_2011
  abstract: >-
    We propose a pivotal method for estimating high-dimensional sparse linear
    regression models, where the overall number of regressors $p$ is large,
    possibly much larger than $n$, but only $s$ regressors are significant. The
    method is a modification of the lasso, called the square-root lasso. The
    method is pivotal in that it neither relies on the knowledge of the standard
    deviation $\sigma$ or nor does it need to pre-estimate $\sigma$. Moreover,
    the method does not rely on normality or sub-Gaussianity of noise. It
    achieves near-oracle performance, attaining the convergence rate $\sigma
    \{(s/n)\log p\}^{1/2}$ in the prediction norm, and thus matching the
    performance of the lasso with known $\sigma$. These performance results are
    valid for both Gaussian and non-Gaussian errors, under some mild moment
    restrictions. We formulate the square-root lasso as a solution to a convex
    conic programming problem, which allows us to implement the estimator using
    efficient algorithmic methods, such as interior-point and first-order
    methods.
  accessed:
    - year: 2022
      month: 10
      day: 10
  author:
    - family: Belloni
      given: Alexandre
    - family: Chernozhukov
      given: Victor
    - family: Wang
      given: Lie
  citation-key: belloni_squareroot_2011
  container-title: Biometrika
  container-title-short: Biometrika
  DOI: 10.1093/biomet/asr043
  ISSN: 0006-3444, 1464-3510
  issue: '4'
  issued:
    - year: 2011
      month: 12
      day: 1
  page: 791-806
  source: arXiv.org
  title: 'Square-Root Lasso: Pivotal Recovery of Sparse Signals via Conic Programming'
  title-short: Square-Root Lasso
  type: article-journal
  URL: http://arxiv.org/abs/1009.5689
  volume: '98'

- id: bengio_learning_1994
  abstract: >-
    Recurrent neural networks can be used to map input sequences to output
    sequences, such as for recognition, production or prediction problems.
    However, practical difficulties have been reported in training recurrent
    neural networks to perform tasks in which the temporal contingencies present
    in the input/output sequences span long intervals. We show why gradient
    based learning algorithms face an increasingly difficult problem as the
    duration of the dependencies to be captured increases. These results expose
    a trade-off between efficient learning by gradient descent and latching on
    information for long periods. Based on an understanding of this problem,
    alternatives to standard gradient descent are considered
  author:
    - family: Bengio
      given: Y.
    - family: Simard
      given: P.
    - family: Frasconi
      given: P.
  citation-key: bengio_learning_1994
  container-title: IEEE Transactions on Neural Networks
  DOI: 10.1109/72.279181
  ISSN: 1045-9227
  issue: '2'
  issued:
    - year: 1994
      month: 3
  page: 157-166
  source: IEEE Xplore
  title: Learning long-term dependencies with gradient descent is difficult
  type: article-journal
  volume: '5'

- id: bengio_neural_2003
  author:
    - family: Bengio
      given: Yoshua
    - family: Ducharme
      given: Réjean
    - family: Vincent
      given: Pascal
    - family: Jauvin
      given: Christian
  citation-key: bengio_neural_2003
  container-title: Journal of machine learning research
  issue: Feb
  issued:
    - year: 2003
  page: 1137–1155
  source: Google Scholar
  title: A neural probabilistic language model
  type: article-journal
  volume: '3'

- id: bengio_problem_1993
  abstract: >-
    The authors seek to train recurrent neural networks in order to map input
    sequences to output sequences, for applications in sequence recognition or
    production. Results are presented showing that learning long-term
    dependencies in such recurrent networks using gradient descent is a very
    difficult task. It is shown how this difficulty arises when robustly
    latching bits of information with certain attractors. The derivatives of the
    output at time t with respect to the unit activations at time zero tend
    rapidly to zero as t increases for most input values. In such a situation,
    simple gradient descent techniques appear inappropriate. The consideration
    of alternative optimization methods and architectures is suggested.<<ETX>>
  author:
    - family: Bengio
      given: Y.
    - family: Frasconi
      given: P.
    - family: Simard
      given: P.
  citation-key: bengio_problem_1993
  container-title: IEEE International Conference on Neural Networks
  DOI: 10/d7zs24
  event-title: IEEE International Conference on Neural Networks
  issued:
    - year: 1993
      month: 3
  page: 1183-1188 vol.3
  source: IEEE Xplore
  title: The problem of learning long-term dependencies in recurrent networks
  type: paper-conference

- id: bensrhair_stereo_2002
  author:
    - family: Bensrhair
      given: Abdelaziz
    - family: Bertozzi
      given: Massimo
    - family: Broggi
      given: Alberto
    - family: Fascioli
      given: Alessandra
    - family: Mousset
      given: Stephane
    - family: Toulminet
      given: Gwenadelle
  citation-key: bensrhair_stereo_2002
  container-title: Intelligent Vehicle Symposium, 2002. IEEE
  issued:
    - year: 2002
  page: 465–470
  publisher: IEEE
  title: Stereo vision-based feature extraction for vehicle detection
  type: paper-conference
  volume: '2'

- id: berger_formal_2009
  abstract: >-
    Reference analysis produces objective Bayesian inference, in the sense that
    inferential statements depend only on the assumed model and the available
    data, and the prior distribution used to make an inference is least
    informative in a certain information-theoretic sense. Reference priors have
    been rigorously defined in specific contexts and heuristically defined in
    general, but a rigorous general definition has been lacking. We produce a
    rigorous general definition here and then show how an explicit expression
    for the reference prior can be obtained under very weak regularity
    conditions. The explicit expression can be used to derive new reference
    priors both analytically and numerically.
  accessed:
    - year: 2018
      month: 10
      day: 7
  author:
    - family: Berger
      given: James O.
    - family: Bernardo
      given: José M.
    - family: Sun
      given: Dongchu
  citation-key: berger_formal_2009
  container-title: The Annals of Statistics
  DOI: 10.1214/07-AOS587
  ISSN: 0090-5364
  issue: '2'
  issued:
    - year: 2009
      month: 4
  page: 905-938
  source: arXiv.org
  title: The formal definition of reference priors
  type: article-journal
  URL: http://arxiv.org/abs/0904.0156
  volume: '37'

- id: bern_chagas_2015
  author:
    - family: Bern
      given: Caryn
  citation-key: bern_chagas_2015
  container-title: The New England Journal of Medicine
  container-title-short: N Engl J Med
  DOI: 10.1056/NEJMra1410150
  ISSN: 1533-4406
  issue: '5'
  issued:
    - year: 2015
      month: 7
      day: 30
  language: eng
  page: 456-466
  PMID: '26222561'
  source: PubMed
  title: Chagas' Disease
  type: article-journal
  volume: '373'

- id: bert_phantom_2005
  author:
    - family: Bert
      given: Christoph
    - family: Metheany
      given: Katherine G
    - family: Doppke
      given: Karen
    - family: Chen
      given: George TY
  citation-key: bert_phantom_2005
  container-title: Medical physics
  DOI: 10.1118/1.1984263
  issue: '9'
  issued:
    - year: 2005
  page: 2753–2762
  title: >-
    A phantom evaluation of a stereo-vision surface imaging system for
    radiotherapy patient setup
  type: article-journal
  volume: '32'

- id: bertozzi_gold_1998
  author:
    - family: Bertozzi
      given: Massimo
    - family: Broggi
      given: Alberto
  citation-key: bertozzi_gold_1998
  container-title: Image Processing, IEEE Transactions on
  DOI: 10.1109/83.650851
  issue: '1'
  issued:
    - year: 1998
  page: 62–81
  title: >-
    GOLD: A parallel real-time stereo vision system for generic obstacle and
    lane detection
  type: article-journal
  volume: '7'

- id: bertsekas_convex_2003
  author:
    - family: Bertsekas
      given: Dimitri P
    - family: Nedi
      given: Angelia
    - family: Ozdaglar
      given: Asuman E
  citation-key: bertsekas_convex_2003
  issued:
    - year: 2003
  title: Convex Analysis and Optimization
  type: book

- id: bertsekas_nonlinear_1999
  author:
    - family: Bertsekas
      given: Dimitri P
  citation-key: bertsekas_nonlinear_1999
  ISBN: 1-886529-00-0
  issued:
    - year: 1999
  publisher: Athena scientific Belmont
  title: Nonlinear programming
  type: book

- id: bertsimas_introduction_1997
  author:
    - family: Bertsimas
      given: Dimitris
    - family: Tsitsiklis
      given: John N
  citation-key: bertsimas_introduction_1997
  issued:
    - year: 1997
  publisher: Athena Scientific Belmont, MA
  title: Introduction to linear optimization
  type: book
  volume: '6'

- id: betancourt_conceptual_2017
  abstract: >-
    Hamiltonian Monte Carlo has proven a remarkable empirical success, but only
    recently have we begun to develop a rigorous understanding of why it
    performs so well on difficult problems and how it is best applied in
    practice. Unfortunately, that understanding is confined within the
    mathematics of differential geometry which has limited its dissemination,
    especially to the applied communities for which it is particularly
    important. In this review I provide a comprehensive conceptual account of
    these theoretical foundations, focusing on developing a principled intuition
    behind the method and its optimal implementations rather of any exhaustive
    rigor. Whether a practitioner or a statistician, the dedicated reader will
    acquire a solid grasp of how Hamiltonian Monte Carlo works, when it
    succeeds, and, perhaps most importantly, when it fails.
  author:
    - family: Betancourt
      given: Michael
  citation-key: betancourt_conceptual_2017
  container-title: arXiv:1701.02434 [stat]
  issued:
    - year: 2017
      month: 1
      day: 9
  source: arXiv.org
  title: A Conceptual Introduction to Hamiltonian Monte Carlo
  type: article-journal
  URL: http://arxiv.org/abs/1701.02434

- id: bhagoji_lower_2019
  accessed:
    - year: 2021
      month: 5
      day: 16
  author:
    - family: Bhagoji
      given: Arjun Nitin
    - family: Cullina
      given: Daniel
    - family: Mittal
      given: Prateek
  citation-key: bhagoji_lower_2019
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2019
  language: en
  source: proceedings.neurips.cc
  title: Lower Bounds on Adversarial Robustness from Optimal Transport
  type: article-journal
  URL: >-
    https://proceedings.neurips.cc/paper/2019/hash/02bf86214e264535e3412283e817deaa-Abstract.html
  volume: '32'

- id: bhojanapalli_lowrank_2020
  abstract: >-
    Attention based Transformer architecture has enabled significant advances in
    the field of natural language processing. In addition to new pre-training
    techniques, recent improvements crucially rely on working with a relatively
    larger embedding dimension for tokens. Unfortunately, this leads to models
    that are prohibitively large to be employed in the downstream tasks. In this
    paper we identify one of the important factors contributing to the large
    embedding size requirement. In particular, our analysis highlights that the
    scaling between the number of heads and the size of each head in the current
    architecture gives rise to a low-rank bottleneck in attention heads, causing
    this limitation. We further validate this in our experiments. As a solution
    we propose to set the head size of an attention unit to input sequence
    length, and independent of the number of heads, resulting in multi-head
    attention layers with provably more expressive power. We empirically show
    that this allows us to train models with a relatively smaller embedding
    dimension and with better performance scaling.
  accessed:
    - year: 2021
      month: 11
      day: 11
  author:
    - family: Bhojanapalli
      given: Srinadh
    - family: Yun
      given: Chulhee
    - family: Rawat
      given: Ankit Singh
    - family: Reddi
      given: Sashank
    - family: Kumar
      given: Sanjiv
  citation-key: bhojanapalli_lowrank_2020
  container-title: Proceedings of the 37th International Conference on Machine Learning
  event-title: International Conference on Machine Learning
  ISSN: 2640-3498
  issued:
    - year: 2020
      month: 11
      day: 21
  language: en
  page: 864-873
  publisher: PMLR
  source: proceedings.mlr.press
  title: Low-Rank Bottleneck in Multi-head Attention Models
  type: paper-conference
  URL: https://proceedings.mlr.press/v119/bhojanapalli20a.html

- id: biau_highdimensional_2015
  author:
    - family: Biau
      given: Gérard
    - family: Mason
      given: David M
  citation-key: biau_highdimensional_2015
  container-title: Mathematical statistics and limit theorems
  issued:
    - year: 2015
  page: 21-40
  publisher: Springer
  title: High-Dimensional p-Norms
  type: chapter

- id: biggio_wild_2018
  abstract: >-
    Learning-based pattern classifiers, including deep networks, have shown
    impressive performance in several application domains, ranging from computer
    vision to cybersecurity. However, it has also been shown that adversarial
    input perturbations carefully crafted either at training or at test time can
    easily subvert their predictions. The vulnerability of machine learning to
    such wild patterns (also referred to as adversarial examples), along with
    the design of suitable countermeasures, have been investigated in the
    research field of adversarial machine learning. In this work, we provide a
    thorough overview of the evolution of this research area over the last ten
    years and beyond, starting from pioneering, earlier work on the security of
    non-deep learning algorithms up to more recent work aimed to understand the
    security properties of deep learning algorithms, in the context of computer
    vision and cybersecurity tasks. We report interesting connections between
    these apparently-different lines of work, highlighting common misconceptions
    related to the security evaluation of machine-learning algorithms. We review
    the main threat models and attacks defined to this end, and discuss the main
    limitations of current work, along with the corresponding future challenges
    towards the design of more secure learning algorithms.
  accessed:
    - year: 2021
      month: 5
      day: 15
  author:
    - family: Biggio
      given: Battista
    - family: Roli
      given: Fabio
  citation-key: biggio_wild_2018
  container-title: Pattern Recognition
  container-title-short: Pattern Recognition
  DOI: 10.1016/j.patcog.2018.07.023
  ISSN: 0031-3203
  issued:
    - year: 2018
      month: 12
      day: 1
  language: en
  page: 317-331
  source: ScienceDirect
  title: 'Wild patterns: Ten years after the rise of adversarial machine learning'
  title-short: Wild patterns
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0031320318302565
  volume: '84'

- id: billings_correlation_1986
  author:
    - family: Billings
      given: S. A.
    - family: Voon
      given: W. S. F.
  citation-key: billings_correlation_1986
  container-title: International journal of Control
  DOI: 10.1080/00207179108934155
  issue: '1'
  issued:
    - year: 1986
  page: 235–244
  title: Correlation based model validity tests for non-linear models
  type: article-journal
  volume: '44'

- id: billings_correlation_1986a
  author:
    - family: Billings
      given: S. A.
    - family: Voon
      given: W. S. F.
  citation-key: billings_correlation_1986a
  container-title: International journal of Control
  DOI: 10/dkj53r
  issue: '1'
  issued:
    - year: 1986
  note: '00002'
  page: 235-244
  title: Correlation Based Model Validity Tests for Non-Linear Models
  type: article-journal
  volume: '44'

- id: billings_identification_1989
  author:
    - family: Billings
      given: S. A.
    - family: Chen
      given: S
    - family: Korenberg
      given: M. J.
  citation-key: billings_identification_1989
  container-title: International Journal of Control
  DOI: 10.1080/00207178908559767
  issue: '6'
  issued:
    - year: 1989
  page: 2157–2189
  title: >-
    Identification of MIMO non-linear systems using a forward-regression
    orthogonal estimator
  type: article-journal
  volume: '49'

- id: billings_nonlinear_1994
  author:
    - family: Billings
      given: S. A.
    - family: Zhu
      given: Q. M.
  citation-key: billings_nonlinear_1994
  container-title: International Journal of Control
  DOI: 10.1080/00207179408921513
  issue: '6'
  issued:
    - year: 1994
  page: 1107–1120
  title: Nonlinear model validation using correlation tests
  type: article-journal
  volume: '60'

- id: billings_nonlinear_2013
  author:
    - family: Billings
      given: S. A.
  call-number: QA402.5 .B55 2013
  citation-key: billings_nonlinear_2013
  event-place: Chichester, West Sussex, United Kingdom
  ISBN: 978-1-119-94359-4
  issued:
    - year: 2013
  number-of-pages: '555'
  publisher: Wiley
  publisher-place: Chichester, West Sussex, United Kingdom
  source: Library of Congress ISBN
  title: >-
    Nonlinear system identification: NARMAX methods in the time, frequency, and
    spatio-temporal domains
  title-short: Nonlinear system identification
  type: book

- id: birchfield_depth_1999
  author:
    - family: Birchfield
      given: Stan
    - family: Tomasi
      given: Carlo
  citation-key: birchfield_depth_1999
  container-title: International Journal of Computer Vision
  DOI: 10.1023/A:1008160311296
  issue: '3'
  issued:
    - year: 1999
  page: 269–293
  title: Depth discontinuities by pixel-to-pixel stereo
  type: article-journal
  volume: '35'

- id: birodkar_closedform_2019
  abstract: >-
    In modern computer vision tasks, convolutional neural networks (CNNs) are
    indispensable for image classification tasks due to their efficiency and
    effectiveness. Part of their superiority compared to other architectures,
    comes from the fact that a single, local filter is shared across the entire
    image. However, there are scenarios where we may need to treat spatial
    locations in non-uniform manner. We see this in nature when considering how
    humans have evolved foveation to process different areas in their field of
    vision with varying levels of detail. In this paper we propose a way to
    enable CNNs to learn different pooling weights for each pixel location. We
    do so by introducing an extended definition of a pooling operator. This
    operator can learn a strict super-set of what can be learned by average
    pooling or convolutions. It has the benefit of being shared across feature
    maps and can be encouraged to be local or diffuse depending on the data. We
    show that for fixed network weights, our pooling operator can be computed in
    closed-form by spectral decomposition of matrices associated with class
    separability. Through experiments, we show that this operator benefits
    generalization for ResNets and CNNs on the CIFAR-10, CIFAR-100 and SVHN
    datasets and improves robustness to geometric corruptions and perturbations
    on the CIFAR-10-C and CIFAR-10-P test sets.
  accessed:
    - year: 2020
      month: 3
      day: 23
  author:
    - family: Birodkar
      given: Vighnesh
    - family: Mobahi
      given: Hossein
    - family: Krishnan
      given: Dilip
    - family: Bengio
      given: Samy
  citation-key: birodkar_closedform_2019
  container-title: arXiv:1906.03808 [cs, stat]
  issued:
    - year: 2019
      month: 6
      day: 10
  source: arXiv.org
  title: A Closed-Form Learned Pooling for Deep Classification Networks
  type: article-journal
  URL: http://arxiv.org/abs/1906.03808

- id: bishop_pattern_2006
  author:
    - family: Bishop
      given: Christopher M.
  call-number: Q327 .B52 2006
  citation-key: bishop_pattern_2006
  collection-title: Information science and statistics
  event-place: New York
  ISBN: 978-0-387-31073-2
  issued:
    - year: 2006
  number-of-pages: '738'
  publisher: Springer
  publisher-place: New York
  source: Library of Congress ISBN
  title: Pattern recognition and machine learning
  type: book

- id: biton_atrial_2021
  abstract: >-
    This study aims to assess whether information derived from the raw 12-lead
    electrocardiogram (ECG) combined with clinical information is predictive of
    atrial fibrillation (AF) development.We use a subset of the Telehealth
    Network of Minas Gerais (TNMG) database consisting of patients that had
    repeated 12-lead ECG measurements between 2010-2017 that is 1,130,404
    recordings from 415,389 unique patients. Median and interquartile of age for
    the recordings were 58 (46-69) and 38% of the patients were males.
    Recordings were assigned to train-validation and test sets in an 80:20%
    split which was stratified by class, age and gender. A random forest
    classifier was trained to predict, for a given recording, the risk of AF
    development within 5-years. We use features obtained from different
    modalities, namely demographics, clinical information, engineered features,
    and features from deep representation learning.The best model performance on
    the test set was obtained for the model combining features from all
    modalities with an AUROC=0.909 against the best single modality model which
    had an AUROC=0.839.Our study has important clinical implications for AF
    management. It is the first study integrating feature engineering, deep
    learning and EMR metadata to create a risk prediction tool for the
    management of patients at risk of AF. The best model that includes features
    from all modalities demonstrates that human knowledge in electrophysiology
    combined with deep learning outperforms any single modality approach. The
    high performance obtained suggest that structural changes in the 12-lead ECG
    are associated with existing or impending AF.
  accessed:
    - year: 2021
      month: 8
      day: 10
  author:
    - family: Biton
      given: Shany
    - family: Gendelman
      given: Sheina
    - family: Ribeiro
      given: Antônio H
    - family: Miana
      given: Gabriela
    - family: Moreira
      given: Carla
    - family: Ribeiro
      given: Antonio Luiz P
    - family: Behar
      given: Joachim A
  citation-key: biton_atrial_2021
  container-title: European Heart Journal - Digital Health
  container-title-short: European Heart Journal - Digital Health
  DOI: 10.1093/ehjdh/ztab071
  ISSN: 2634-3916
  issued:
    - year: 2021
  license: All rights reserved
  title: >-
    Atrial fibrillation risk prediction from the 12-lead ECG using digital
    biomarkers and deep representation learning
  type: article-journal
  URL: https://doi.org/10.1093/ehjdh/ztab071

- id: bjorck_understanding_2018
  abstract: >-
    Batch normalization (BN) is a technique to normalize activations in
    intermediate layers of deep neural networks. Its tendency to improve
    accuracy and speed up training have established BN as a favorite technique
    in deep learning. Yet, despite its enormous success, there remains little
    consensus on the exact reason and mechanism behind these improvements. In
    this paper we take a step towards a better understanding of BN, following an
    empirical approach. We conduct several experiments, and show that BN
    primarily enables training with larger learning rates, which is the cause
    for faster convergence and better generalization. For networks without BN we
    demonstrate how large gradient updates can result in diverging loss and
    activations growing uncontrollably with network depth, which limits possible
    learning rates. BN avoids this problem by constantly correcting activations
    to be zero-mean and of unit standard deviation, which enables larger
    gradient steps, yields faster convergence and may help bypass sharp local
    minima. We further show various ways in which gradients and activations of
    deep unnormalized networks are ill-behaved. We contrast our results against
    recent findings in random matrix theory, shedding new light on classical
    initialization schemes and their consequences.
  accessed:
    - year: 2018
      month: 12
      day: 13
  author:
    - family: Bjorck
      given: Johan
    - family: Gomes
      given: Carla
    - family: Selman
      given: Bart
    - family: Weinberger
      given: Kilian Q.
  citation-key: bjorck_understanding_2018
  container-title: arXiv:1806.02375 [cs, stat]
  issued:
    - year: 2018
      month: 5
      day: 31
  source: arXiv.org
  title: Understanding Batch Normalization
  type: article-journal
  URL: http://arxiv.org/abs/1806.02375

- id: blei_variational_2017
  abstract: >-
    One of the core problems of modern statistics is to approximate
    difficult-to-compute probability densities. This problem is especially
    important in Bayesian statistics, which frames all inference about unknown
    quantities as a calculation involving the posterior density. In this paper,
    we review variational inference (VI), a method from machine learning that
    approximates probability densities through optimization. VI has been used in
    many applications and tends to be faster than classical methods, such as
    Markov chain Monte Carlo sampling. The idea behind VI is to first posit a
    family of densities and then to find the member of that family which is
    close to the target. Closeness is measured by Kullback-Leibler divergence.
    We review the ideas behind mean-field variational inference, discuss the
    special case of VI applied to exponential family models, present a full
    example with a Bayesian mixture of Gaussians, and derive a variant that uses
    stochastic optimization to scale up to massive data. We discuss modern
    research in VI and highlight important open problems. VI is powerful, but it
    is not yet well understood. Our hope in writing this paper is to catalyze
    statistical research on this class of algorithms.
  accessed:
    - year: 2019
      month: 1
      day: 10
  author:
    - family: Blei
      given: David M.
    - family: Kucukelbir
      given: Alp
    - family: McAuliffe
      given: Jon D.
  citation-key: blei_variational_2017
  container-title: Journal of the American Statistical Association
  DOI: 10/gb2dc6
  ISSN: 0162-1459, 1537-274X
  issue: '518'
  issued:
    - year: 2017
      month: 4
      day: 3
  page: 859-877
  source: arXiv.org
  title: 'Variational Inference: A Review for Statisticians'
  title-short: Variational Inference
  type: article-journal
  URL: http://arxiv.org/abs/1601.00670
  volume: '112'

- id: blog_understanding_
  abstract: >-
    My attempt at distilling the ideas behind the neural tangent kernel that is
    making waves in recent theoretical deep learning research.
  accessed:
    - year: 2020
      month: 8
      day: 9
  author:
    - family: Blog
      given: Rajat's
  citation-key: blog_understanding_
  title: Understanding the Neural Tangent Kernel
  type: webpage
  URL: https://rajatvd.github.io/NTK/

- id: bock_multiple_1984
  author:
    - family: Bock
      given: Hans Georg
    - family: Plitt
      given: Karl-Josef
  citation-key: bock_multiple_1984
  container-title: IFAC Proceedings Volumes
  container-title-short: IFAC Proceedings Volumes
  DOI: 10.1016/S1474-6670(17)61205-9
  ISSN: 1474-6670
  issue: '2'
  issued:
    - year: 1984
  page: 1603-1608
  title: >-
    A multiple shooting algorithm for direct solution of optimal control
    problems
  type: article-journal
  volume: '17'

- id: bock_multiple_1984a
  author:
    - family: Bock
      given: Hans Georg
    - family: Plitt
      given: Karl-Josef
  citation-key: bock_multiple_1984a
  container-title: IFAC Proceedings Volumes
  DOI: 10/gfjwmq
  ISSN: 1474-6670
  issue: '2'
  issued:
    - year: 1984
  page: 1603-1608
  title: >-
    A Multiple Shooting Algorithm for Direct Solution of Optimal Control
    Problems
  type: article-journal
  volume: '17'

- id: bock_numerical_1981
  author:
    - family: Bock
      given: Hans Georg
  citation-key: bock_numerical_1981
  container-title: Modelling of chemical reaction systems
  issued:
    - year: 1981
  page: 102–125
  publisher: Springer
  title: Numerical treatment of inverse problems in chemical reaction kinetics
  type: chapter

- id: bock_numerical_1981a
  author:
    - family: Bock
      given: Hans Georg
  citation-key: bock_numerical_1981a
  container-title: Modelling of Chemical Reaction Systems
  issued:
    - year: 1981
  page: 102-125
  publisher: Springer
  title: Numerical Treatment of Inverse Problems in Chemical Reaction Kinetics
  type: chapter

- id: bock_recent_1983
  author:
    - family: Bock
      given: HG
  citation-key: bock_recent_1983
  container-title: >-
    Numerical Treatment of Inverse Problems in Differential and Integral
    Equations
  DOI: 10.1007/978-1-4684-7324-7_7
  issued:
    - year: 1983
  page: 95–121
  title: Recent advances in parameter identification problems for ODE
  type: article-journal

- id: bock_recent_1983a
  author:
    - family: Bock
      given: HG
  citation-key: bock_recent_1983a
  container-title: >-
    Numerical Treatment of Inverse Problems in Differential and Integral
    Equations
  DOI: 10/gfjwmg
  issued:
    - year: 1983
  page: 95-121
  title: Recent Advances in Parameter Identification Problems for ODE
  type: article-journal

- id: bohlin_case_1994
  author:
    - family: Bohlin
      given: Torsten
  citation-key: bohlin_case_1994
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10.1016/0005-1098(94)90032-9
  ISSN: 0005-1098
  issue: '2'
  issued:
    - year: 1994
  page: 307-318
  title: A case study of grey box identification
  type: article-journal
  volume: '30'

- id: bollapragada_exact_2016
  abstract: >-
    The paper studies the solution of stochastic optimization problems in which
    approximations to the gradient and Hessian are obtained through subsampling.
    We first consider Newton-like methods that employ these approximations and
    discuss how to coordinate the accuracy in the gradient and Hessian to yield
    a superlinear rate of convergence in expectation. The second part of the
    paper analyzes an inexact Newton method that solves linear systems
    approximately using the conjugate gradient (CG) method, and that samples the
    Hessian and not the gradient (the gradient is assumed to be exact). We
    provide a complexity analysis for this method based on the properties of the
    CG iteration and the quality of the Hessian approximation, and compare it
    with a method that employs a stochastic gradient iteration instead of the CG
    method. We report preliminary numerical results that illustrate the
    performance of inexact subsampled Newton methods on machine learning
    applications based on logistic regression.
  author:
    - family: Bollapragada
      given: Raghu
    - family: Byrd
      given: Richard
    - family: Nocedal
      given: Jorge
  citation-key: bollapragada_exact_2016
  container-title: arXiv:1609.08502 [math, stat]
  issued:
    - year: 2016
      month: 9
      day: 27
  source: arXiv.org
  title: Exact and Inexact Subsampled Newton Methods for Optimization
  type: article-journal
  URL: http://arxiv.org/abs/1609.08502

- id: bolukbasi_man_2016
  author:
    - family: Bolukbasi
      given: Tolga
    - family: Chang
      given: Kai-Wei
    - family: Zou
      given: James Y
    - family: Saligrama
      given: Venkatesh
    - family: Kalai
      given: Adam T
  citation-key: bolukbasi_man_2016
  event-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2016
  page: 4349-4357
  title: >-
    Man is to computer programmer as woman is to homemaker? debiasing word
    embeddings
  type: paper-conference

- id: bond_assessing_2014
  abstract: >-
    INTRODUCTION: It is well known that accurate interpretation of the 12-lead
    electrocardiogram (ECG) requires a high degree of skill. There is also a
    moderate degree of variability among those who interpret the ECG. While this
    is the case, there are no best practice guidelines for the actual ECG
    interpretation process. Hence, this study adopts computerized eye tracking
    technology to investigate whether eye-gaze can be used to gain a deeper
    insight into how expert annotators interpret the ECG. Annotators were
    recruited in San Jose, California at the 2013 International Society of
    Computerised Electrocardiology (ISCE).

    METHODS: Each annotator was recruited to interpret a number of 12-lead ECGs
    (N=12) while their eye gaze was recorded using a Tobii X60 eye tracker. The
    device is based on corneal reflection and is non-intrusive. With a sampling
    rate of 60Hz, eye gaze coordinates were acquired every 16.7ms. Fixations
    were determined using a predefined computerized classification algorithm,
    which was then used to generate heat maps of where the annotators looked.
    The ECGs used in this study form four groups (3=ST elevation myocardial
    infarction [STEMI], 3=hypertrophy, 3=arrhythmias and 3=exhibiting unique
    artefacts). There was also an equal distribution of difficulty levels
    (3=easy to interpret, 3=average and 3=difficult). ECGs were displayed using
    the 4x3+1 display format and computerized annotations were concealed.

    RESULTS: Precisely 252 expert ECG interpretations (21 annotators×12 ECGs)
    were recorded. Average duration for ECG interpretation was 58s (SD=23).
    Fleiss' generalized kappa coefficient (Pa=0.56) indicated a moderate
    inter-rater reliability among the annotators. There was a 79% inter-rater
    agreement for STEMI cases, 71% agreement for arrhythmia cases, 65% for the
    lead misplacement and dextrocardia cases and only 37% agreement for the
    hypertrophy cases. In analyzing the total fixation duration, it was found
    that on average annotators study lead V1 the most (4.29s), followed by leads
    V2 (3.83s), the rhythm strip (3.47s), II (2.74s), V3 (2.63s), I (2.53s), aVL
    (2.45s), V5 (2.27s), aVF (1.74s), aVR (1.63s), V6 (1.39s), III (1.32s) and
    V4 (1.19s). It was also found that on average the annotator spends an equal
    amount of time studying leads in the frontal plane (15.89s) when compared to
    leads in the transverse plane (15.70s). It was found that on average the
    annotators fixated on lead I first followed by leads V2, aVL, V1, II, aVR,
    V3, rhythm strip, III, aVF, V5, V4 and V6. We found a strong correlation
    (r=0.67) between time to first fixation on a lead and the total fixation
    duration on each lead. This indicates that leads studied first are studied
    the longest. There was a weak negative correlation between duration and
    accuracy (r=-0.2) and a strong correlation between age and accuracy
    (r=0.67).

    CONCLUSIONS: Eye tracking facilitated a deeper insight into how expert
    annotators interpret the 12-lead ECG. As a result, the authors recommend ECG
    annotators to adopt an initial first impression/pattern recognition approach
    followed by a conventional systematic protocol to ECG interpretation. This
    recommendation is based on observing misdiagnoses given due to first
    impression only. In summary, this research presents eye gaze results from
    expert ECG annotators and provides scope for future work that involves
    exploiting computerized eye tracking technology to further the science of
    ECG interpretation.
  author:
    - family: Bond
      given: R. R.
    - family: Zhu
      given: T.
    - family: Finlay
      given: D. D.
    - family: Drew
      given: B.
    - family: Kligfield
      given: P. D.
    - family: Guldenring
      given: D.
    - family: Breen
      given: C.
    - family: Gallagher
      given: A. G.
    - family: Daly
      given: M. J.
    - family: Clifford
      given: G. D.
  citation-key: bond_assessing_2014
  container-title: Journal of Electrocardiology
  container-title-short: J Electrocardiol
  DOI: 10.1016/j.jelectrocard.2014.07.011
  ISSN: 1532-8430
  issue: '6'
  issued:
    - literal: 2014 Nov-Dec
  language: eng
  page: 895-906
  PMID: '25110276'
  source: PubMed
  title: >-
    Assessing computerized eye tracking technology for gaining insight into
    expert interpretation of the 12-lead electrocardiogram: an objective
    quantitative approach
  title-short: >-
    Assessing computerized eye tracking technology for gaining insight into
    expert interpretation of the 12-lead electrocardiogram
  type: article-journal
  volume: '47'

- id: bonin_lassoenhanced_2010
  accessed:
    - year: 2017
      month: 9
      day: 13
  author:
    - family: Bonin
      given: Mariangela
    - family: Seghezza
      given: Valerio
    - family: Piroddi
      given: Luigi
  citation-key: bonin_lassoenhanced_2010
  container-title: American Control Conference (ACC), 2010
  issued:
    - year: 2010
  page: 4522–4527
  publisher: IEEE
  source: Google Scholar
  title: LASSO-enhanced simulation error minimization method for NARX model selection
  type: paper-conference
  URL: http://ieeexplore.ieee.org/abstract/document/5530859/

- id: bonin_narx_2010
  author:
    - family: Bonin
      given: M
    - family: Seghezza
      given: V
    - family: Piroddi
      given: L
  citation-key: bonin_narx_2010
  container-title: IET Control Theory & Applications
  DOI: 10.1049/iet-cta.2009.0217
  issue: '7'
  issued:
    - year: 2010
  page: 1157–1168
  title: NARX model selection based on simulation error minimisation and LASSO
  type: article-journal
  volume: '4'

- id: boolos_computability_2002
  author:
    - family: Boolos
      given: George S.
    - family: Burgess
      given: John P.
    - family: Jeffrey
      given: Richard C.
  citation-key: boolos_computability_2002
  issued:
    - year: 2002
  publisher: Cambridge university press
  source: Google Scholar
  title: Computability and logic
  type: book

- id: bottou_optimization_2018
  accessed:
    - year: 2018
      month: 5
      day: 8
  author:
    - family: Bottou
      given: Léon
    - family: Curtis
      given: Frank E.
    - family: Nocedal
      given: Jorge
  citation-key: bottou_optimization_2018
  container-title: SIAM Review
  DOI: 10.1137/16M1080173
  ISSN: 0036-1445, 1095-7200
  issue: '2'
  issued:
    - year: 2018
      month: 1
  language: en
  page: 223-311
  source: Crossref
  title: Optimization Methods for Large-Scale Machine Learning
  type: article-journal
  URL: https://epubs.siam.org/doi/10.1137/16M1080173
  volume: '60'

- id: bouc_mathematical_1971
  author:
    - family: Bouc
      given: R
  citation-key: bouc_mathematical_1971
  container-title: Acta Acustica united with Acustica
  container-title-short: Acta Acustica united with Acustica
  ISSN: 1610-1928
  issue: '1'
  issued:
    - year: 1971
  page: 16-25
  publisher: S. Hirzel Verlag
  title: A mathematical model for hysteresis
  type: article-journal
  volume: '24'

- id: bousquet_tradeoffs_2008
  accessed:
    - year: 2017
      month: 9
      day: 11
  author:
    - family: Bousquet
      given: Olivier
    - family: Bottou
      given: Léon
  citation-key: bousquet_tradeoffs_2008
  container-title: Advances in neural information processing systems
  issued:
    - year: 2008
  page: 161–168
  source: Google Scholar
  title: The tradeoffs of large scale learning
  type: paper-conference
  URL: http://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning

- id: bousseljot_nutzung_1995
  author:
    - family: Bousseljot
      given: R
    - family: Kreiseler
      given: D
    - family: Schnabel
      given: A
  citation-key: bousseljot_nutzung_1995
  container-title: Biomedizinische Technik/Biomedical Engineering
  issue: s1
  issued:
    - year: 1995
  page: 317–318
  publisher: Walter de Gruyter, Berlin/New York
  title: Nutzung der EKG-Signaldatenbank CARDIODAT der PTB über das internet
  type: article-journal
  volume: '40'

- id: box_time_2015
  author:
    - family: Box
      given: George EP
    - family: Jenkins
      given: Gwilym M.
    - family: Reinsel
      given: Gregory C.
    - family: Ljung
      given: Greta M.
  citation-key: box_time_2015
  issued:
    - year: 2015
  publisher: John Wiley & Sons
  source: Google Scholar
  title: 'Time series analysis: forecasting and control'
  title-short: Time series analysis
  type: book

- id: boyd_convex_2004
  author:
    - family: Boyd
      given: Stephen P.
    - family: Vandenberghe
      given: Lieven
  call-number: QA402.5 .B69 2004
  citation-key: boyd_convex_2004
  ISBN: 978-0-521-83378-3
  issued:
    - year: 2004
  publisher: Cambridge University Press
  source: Library of Congress ISBN
  title: Convex optimization
  type: book

- id: boyd_fading_1985
  abstract: >-
    Using the notion of fading memory we prove very strong versions of two folk
    theorems. The first is that any time-invariant (TI) continuous nonlinear
    operator can be approximated by a Volterra series operator, and the second
    is that the approximating operator can be realized as a finite-dimensional
    linear dynamical system with a nonlinear readout map. While previous
    approximation results are valid over finite time intervals and for signals
    in compact sets, the approximations presented here hold for all time and for
    signals in useful (noncompact) sets. The discretetime analog of the second
    theorem asserts that any TI operator with fading memory can be approximated
    (in our strong sense) by a nonlinear moving- average operator. Some further
    discussion of the notion of fading memory is given.
  author:
    - family: Boyd
      given: S.
    - family: Chua
      given: L.
  citation-key: boyd_fading_1985
  container-title: IEEE Transactions on Circuits and Systems
  DOI: 10/fkqmc5
  ISSN: 0098-4094
  issue: '11'
  issued:
    - year: 1985
      month: 11
  page: 1150-1161
  source: IEEE Xplore
  title: >-
    Fading memory and the problem of approximating nonlinear operators with
    Volterra series
  type: article-journal
  volume: '32'

- id: boyd_subgradients_2022
  author:
    - family: Boyd
      given: S.
    - family: Duchi
      given: J.
    - family: Pilanci
      given: M.
    - family: Vandenberghe
      given: L.
  citation-key: boyd_subgradients_2022
  issued:
    - year: 2022
      month: 4
      day: 13
  number: Notes for EE364b, Stanford University, Spring 2021-22
  title: Subgradients (Lecture Notes)
  type: report

- id: boynton_analysis_2013
  author:
    - family: Boynton
      given: R. J.
    - family: Balikhin
      given: M. A.
    - family: Billings
      given: S. A.
    - family: Reeves
      given: G. D.
    - family: Ganushkina
      given: 'N'
    - family: Gedalin
      given: M
    - family: Amariutei
      given: OA
    - family: Borovsky
      given: J. E.
    - family: Walker
      given: S. N.
  citation-key: boynton_analysis_2013
  container-title: 'Journal of Geophysical Research: Space Physics'
  DOI: 10.1002/jgra.50192
  issue: '4'
  issued:
    - year: 2013
  page: 1500–1513
  title: >-
    The analysis of electron fluxes at geosynchronous orbit employing a NARMAX
    approach
  type: article-journal
  volume: '118'

- id: boynton_analysis_2013a
  author:
    - family: Boynton
      given: R. J.
    - family: Balikhin
      given: M. A.
    - family: Billings
      given: S. A.
    - family: Reeves
      given: G. D.
    - family: Ganushkina
      given: 'N'
    - family: Gedalin
      given: M
    - family: Amariutei
      given: OA
    - family: Borovsky
      given: J. E.
    - family: Walker
      given: S. N.
  citation-key: boynton_analysis_2013a
  container-title: 'Journal of Geophysical Research: Space Physics'
  DOI: 10/f42b3m
  issue: '4'
  issued:
    - year: 2013
  page: 1500-1513
  title: >-
    The Analysis of Electron Fluxes at Geosynchronous Orbit Employing a NARMAX
    Approach
  type: article-journal
  volume: '118'

- id: bradski_learning_2008
  author:
    - family: Bradski
      given: Gary
    - family: Kaehler
      given: Adrian
  citation-key: bradski_learning_2008
  issued:
    - year: 2008
  publisher: ' O''Reilly Media, Inc.'
  title: 'Learning OpenCV: Computer vision with the OpenCV library'
  type: book

- id: bradski_learning_2011
  author:
    - family: Bradski
      given: Gary R.
    - family: Kaehler
      given: Adrian
  citation-key: bradski_learning_2011
  collection-title: Software that sees
  edition: 1. ed., [Nachdr.]
  event-place: Beijing
  ISBN: 978-0-596-51613-0
  issued:
    - year: 2011
  language: eng
  note: 'OCLC: 838472784'
  number-of-pages: '555'
  publisher: O'Reilly
  publisher-place: Beijing
  source: Gemeinsamer Bibliotheksverbund ISBN
  title: 'Learning OpenCV: computer vision with the OpenCV library'
  title-short: Learning OpenCV
  type: book

- id: branch_subspace_1999
  author:
    - family: Branch
      given: Mary Ann
    - family: Coleman
      given: Thomas F
    - family: Li
      given: Yuying
  citation-key: branch_subspace_1999
  container-title: SIAM Journal on Scientific Computing
  DOI: 10.1137/S1064827595289108
  issue: '1'
  issued:
    - year: 1999
  page: 1–23
  title: >-
    A subspace, interior, and conjugate gradient method for large-scale
    bound-constrained minimization problems
  type: article-journal
  volume: '21'

- id: brant_electrocardiographic_2023
  author:
    - family: Brant
      given: Luisa C C
    - family: Ribeiro
      given: Antônio H
    - family: Pinto-Filho
      given: Marcelo M
    - family: Kornej
      given: Jelena
    - family: Preis
      given: Sarah R.
    - family: Eromosele
      given: Benjamin
    - family: Magnani
      given: Jared W.
    - family: Murabito
      given: Joanne M.
    - family: Larson
      given: Martin G
    - family: Benjamin
      given: Emelia J
    - family: Ribeiro
      given: Antonio L P
    - family: Lin
      given: Honghuang
  citation-key: brant_electrocardiographic_2023
  container-title: 'Circulation: Cardiovascular Quality and Outcomes'
  DOI: 10.1161/CIRCOUTCOMES.122.009821
  issued:
    - year: 2023
  license: All rights reserved
  title: >-
    Electrocardiographic Age Predicts Cardiovascular Events in Community: The
    Framingham Heart Study
  type: article-journal

- id: brant_reproducibility_2013
  abstract: >-
    Objectives: Endothelial dysfunction is associated to cardiovascular risk
    factors and predicts cardiovascular events. Peripheral arterial tonometry
    (PAT) is a novel noninvasive method to assess endothelial function. However,
    there is a paucity of data about its reproducibility. The aim of this study
    was to assess the feasibility and reproducibility of PAT in adults.

    Methods: PAT exams were performed twice in the same day in 123 participants
    of a cohort about the determinants of diabetes and cardiovascular diseases
    (Brazilian Longitudinal Study of Adult Health – ELSA-Brasil). The interval
    between the exams was 2–6 h (mean ¼ 4 h). Endothelial function in PAT method
    is measured by reactive hyperemia index (RHI), which evaluates arterial
    pulsatile volume changes in response to hyperemia. Agreement of RHI values
    was compared by Bland–Altman method, coefficient of variation and
    coefficient of repeatability. Reliability was assessed by intraclass
    correlation coefficient (ICC).

    Results: Mean values of RHI did not differ significantly between the exams
    of each participant (1.92 Æ 0.56 vs. 1.96 Æ 0.58, P ¼ 0.48). There were no
    systematic errors between the exams (mean of differences ¼ À0.03 Æ 0.5).
    Measurement error was 0.35, coefficient of variation was 18.0% and ICC was
    0.61. Sex, age or the presence of obesity did not have a considerable
    influence on the reproducibility of PAT.

    Conclusion: PAT exam is feasible and has acceptable reproducibility in
    adults when compared with other noninvasive methods for endothelial function
    assessment. This performance makes PAT a promising method for future
    clinical and epidemiological studies.
  accessed:
    - year: 2023
      month: 8
      day: 30
  author:
    - family: Brant
      given: Luisa C.C.
    - family: Barreto
      given: Sandhi M.
    - family: Passos
      given: Valéria M.A.
    - family: Ribeiro
      given: Antônio L.P.
  citation-key: brant_reproducibility_2013
  container-title: Journal of Hypertension
  DOI: 10.1097/HJH.0b013e328362d913
  ISSN: 0263-6352
  issue: '10'
  issued:
    - year: 2013
      month: 10
  language: en
  page: 1984-1990
  source: DOI.org (Crossref)
  title: >-
    Reproducibility of peripheral arterial tonometry for the assessment of
    endothelial function in adults
  type: article-journal
  URL: https://journals.lww.com/00004872-201310000-00011
  volume: '31'

- id: braun_enzoiia_1994
  author:
    - family: Braun
      given: Heinrich
    - family: Zagorski
      given: Peter
  citation-key: braun_enzoiia_1994
  container-title: >-
    Evolutionary Computation, 1994. IEEE World Congress on Computational
    Intelligence., Proceedings of the First IEEE Conference on
  issued:
    - year: 1994
  page: 278–283
  publisher: IEEE
  title: ENZO-II-A powerful design tool to evolve multilayer feed forward networks
  type: paper-conference

- id: breiman_bagging_1996
  abstract: >-
    Bagging predictors is a method for generating multiple versions of a
    predictor and using these to get an aggregated predictor. The aggregation
    averages over the versions when predicting a numerical outcome and does a
    plurality vote when predicting a class. The multiple versions are formed by
    making bootstrap replicates of the learning set and using these as new
    learning sets. Tests on real and simulated data sets using classification
    and regression trees and subset selection in linear regression show that
    bagging can give substantial gains in accuracy. The vital element is the
    instability of the prediction method. If perturbing the learning set can
    cause significant changes in the predictor constructed, then bagging can
    improve accuracy.
  accessed:
    - year: 2020
      month: 11
      day: 26
  author:
    - family: Breiman
      given: Leo
  citation-key: breiman_bagging_1996
  container-title: Machine Learning
  container-title-short: Mach Learn
  DOI: 10.1007/BF00058655
  ISSN: 0885-6125, 1573-0565
  issue: '2'
  issued:
    - year: 1996
      month: 8
  language: en
  page: 123-140
  source: DOI.org (Crossref)
  title: Bagging predictors
  type: article-journal
  URL: http://link.springer.com/10.1007/BF00058655
  volume: '24'

- id: breiman_heuristics_1996
  author:
    - family: Breiman
      given: Leo
  citation-key: breiman_heuristics_1996
  container-title: The annals of statistics
  issue: '6'
  issued:
    - year: 1996
  page: 2350–2383
  source: Google Scholar
  title: Heuristics of instability and stabilization in model selection
  type: article-journal
  volume: '24'

- id: breiman_random_2001
  author:
    - family: Breiman
      given: Leo
  citation-key: breiman_random_2001
  container-title: Machine learning
  issue: '1'
  issued:
    - year: 2001
  page: 5–32
  publisher: Springer
  title: Random forests
  type: article-journal
  volume: '45'

- id: breiman_statistical_2001
  author:
    - family: Breiman
      given: Leo
  citation-key: breiman_statistical_2001
  container-title: Statistical science
  DOI: 10.1214/ss/1009213726
  issue: '3'
  issued:
    - year: 2001
  page: 199–231
  source: Google Scholar
  title: >-
    Statistical modeling: The two cultures (with comments and a rejoinder by the
    author)
  title-short: Statistical modeling
  type: article-journal
  volume: '16'

- id: bronson_matrix_1991
  author:
    - family: Bronson
      given: Richard
  call-number: QA188 .B758 1990
  citation-key: bronson_matrix_1991
  edition: 2nd ed
  event-place: Boston
  ISBN: 978-0-12-135251-6
  issued:
    - year: 1991
  number-of-pages: '503'
  publisher: Academic Press
  publisher-place: Boston
  source: Library of Congress ISBN
  title: 'Matrix methods: an introduction'
  title-short: Matrix methods
  type: book

- id: brown_complex_2009
  author:
    - family: Brown
      given: James Ward
    - family: Churchill
      given: Ruel V.
  call-number: QA331.7 .C524 2009
  citation-key: brown_complex_2009
  collection-title: Brown and Churchill series
  edition: 8th ed
  event-place: Boston
  ISBN: 978-0-07-305194-9
  issued:
    - year: 2009
  note: 'OCLC: ocn176648981'
  number-of-pages: '468'
  publisher: McGraw-Hill Higher Education
  publisher-place: Boston
  source: Library of Congress ISBN
  title: Complex variables and applications
  type: book

- id: broyden_convergence_1970
  author:
    - family: Broyden
      given: Charles George
  citation-key: broyden_convergence_1970
  container-title: IMA Journal of Applied Mathematics
  DOI: 10.1093/imamat/6.1.76
  issue: '1'
  issued:
    - year: 1970
  page: 76–90
  title: >-
    The convergence of a class of double-rank minimization algorithms 1. general
    considerations
  type: article-journal
  volume: '6'

- id: bruna_intriguing_2014
  author:
    - family: Bruna
      given: Joan
    - family: Szegedy
      given: Christian
    - family: Sutskever
      given: Ilya
    - family: Goodfellow
      given: Ian
    - family: Zaremba
      given: Wojciech
    - family: Fergus
      given: Rob
    - family: Erhan
      given: Dumitru
  citation-key: bruna_intriguing_2014
  container-title: International Conference on Learning Representations (ICLR)
  event-title: International Conference on Learning Representations (ICLR)
  issued:
    - year: 2014
  title: Intriguing properties of neural networks
  type: paper-conference

- id: bryc_spectral_2006
  abstract: >-
    We study the limiting spectral measure of large symmetric random matrices of
    linear algebraic structure. For Hankel and Toeplitz matrices generated by
    i.i.d. random variables $\{X_k\}$ of unit variance, and for symmetric Markov
    matrices generated by i.i.d. random variables $\{X_{ij}\}_{j>i}$ of zero
    mean and unit variance, scaling the eigenvalues by $\sqrt{n}$ we prove the
    almost sure, weak convergence of the spectral measures to universal,
    nonrandom, symmetric distributions $\gamma_H$, $\gamma_M$ and $\gamma_T$ of
    unbounded support. The moments of $\gamma_H$ and $\gamma_T$ are the sum of
    volumes of solids related to Eulerian numbers, whereas $\gamma_M$ has a
    bounded smooth density given by the free convolution of the semicircle and
    normal densities. For symmetric Markov matrices generated by i.i.d. random
    variables $\{X_{ij}\}_{j>i}$ of mean $m$ and finite variance, scaling the
    eigenvalues by ${n}$ we prove the almost sure, weak convergence of the
    spectral measures to the atomic measure at $-m$. If $m=0$, and the fourth
    moment is finite, we prove that the spectral norm of $\mathbf {M}_n$ scaled
    by $\sqrt{2n\log n}$ converges almost surely to 1.
  accessed:
    - year: 2020
      month: 12
      day: 14
  author:
    - family: Bryc
      given: Włodzimierz
    - family: Dembo
      given: Amir
    - family: Jiang
      given: Tiefeng
  citation-key: bryc_spectral_2006
  container-title: arXiv:math/0307330
  DOI: 10.1214/009117905000000495
  issued:
    - year: 2006
      month: 2
      day: 27
  source: arXiv.org
  title: Spectral measure of large random Hankel, Markov and Toeplitz matrices
  type: article-journal
  URL: http://arxiv.org/abs/math/0307330

- id: bubeck_convex_2015
  abstract: >-
    This monograph presents the main complexity theorems in convex optimization
    and their corresponding algorithms. Starting from the fundamental theory of
    black-box optimization, the material progresses towards recent advances in
    structural optimization and stochastic optimization. Our presentation of
    black-box optimization, strongly influenced by Nesterov's seminal book and
    Nemirovski's lecture notes, includes the analysis of cutting plane methods,
    as well as (accelerated) gradient descent schemes. We also pay special
    attention to non-Euclidean settings (relevant algorithms include
    Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance
    in machine learning. We provide a gentle introduction to structural
    optimization with FISTA (to optimize a sum of a smooth and a simple
    non-smooth term), saddle-point mirror prox (Nemirovski's alternative to
    Nesterov's smoothing), and a concise description of interior point methods.
    In stochastic optimization we discuss stochastic gradient descent,
    mini-batches, random coordinate descent, and sublinear algorithms. We also
    briefly touch upon convex relaxation of combinatorial problems and the use
    of randomness to round solutions, as well as random walks based methods.
  accessed:
    - year: 2024
      month: 1
      day: 16
  author:
    - family: Bubeck
      given: Sébastien
  citation-key: bubeck_convex_2015
  container-title: Foundations and Trends in Machine Learning
  issue: 3-4
  issued:
    - year: 2015
  page: 231-357
  source: arXiv.org
  title: 'Convex Optimization: Algorithms and Complexity'
  title-short: Convex Optimization
  type: article-journal
  URL: http://arxiv.org/abs/1405.4980
  volume: '8'

- id: bubeck_law_2021
  abstract: >-
    We initiate the study of the inherent tradeoffs between the size of a neural
    network and its robustness, as measured by its Lipschitz constant. We make a
    precise conjecture that, for any Lipschitz activation function and for most
    datasets, any two-layers neural network with $k$ neurons that perfectly fit
    the data must have its Lipschitz constant larger (up to a constant) than
    $\sqrt{n/k}$ where $n$ is the number of datapoints. In particular, this
    conjecture implies that overparametrization is necessary for robustness,
    since it means that one needs roughly one neuron per datapoint to ensure a
    $O(1)$-Lipschitz network, while mere data fitting of $d$-dimensional data
    requires only one neuron per $d$ datapoints. We prove a weaker version of
    this conjecture when the Lipschitz constant is replaced by an upper bound on
    it based on the spectral norm of the weight matrix. We also prove the
    conjecture in the high-dimensional regime $n \approx d$ (which we also refer
    to as the undercomplete case, since only $k \leq d$ is relevant here).
    Finally we prove the conjecture for polynomial activation functions of
    degree $p$ when $n \approx d^p$. We complement these findings with
    experimental evidence supporting the conjecture.
  author:
    - family: Bubeck
      given: Sébastien
    - family: Li
      given: Yuanzhi
    - family: Nagaraj
      given: Dheeraj
  citation-key: bubeck_law_2021
  container-title: >-
    134 of Proceedings of Machine Learning Research, Conference on Learning
    Theory (COLT)
  issued:
    - year: 2021
  page: 804–820
  title: A law of robustness for two-layers neural networks
  type: article-journal
  volume: '134'

- id: bubeck_universal_2021
  abstract: >-
    Classically, data interpolation with a parametrized model class is possible
    as long as the number of parameters is larger than the number of equations
    to be satisfied. A puzzling phenomenon in deep learning is that models are
    trained with many more parameters than what this classical theory would
    suggest. We propose a theoretical explanation for this phenomenon. We prove
    that for a broad class of data distributions and model classes,
    overparametrization is necessary if one wants to interpolate the data
    smoothly. Namely we show that smooth interpolation requires $d$ times more
    parameters than mere interpolation, where $d$ is the ambient data dimension.
    We prove this universal law of robustness for any smoothly parametrized
    function class with polynomial size weights, and any covariate distribution
    verifying isoperimetry. In the case of two-layers neural networks and
    Gaussian covariates, this law was conjectured in prior work by Bubeck, Li
    and Nagaraj. We also give an interpretation of our result as an improved
    generalization bound for model classes consisting of smooth functions.
  accessed:
    - year: 2021
      month: 8
      day: 26
  author:
    - family: Bubeck
      given: Sébastien
    - family: Sellke
      given: Mark
  citation-key: bubeck_universal_2021
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2021
  source: arXiv.org
  title: A Universal Law of Robustness via Isoperimetry
  type: article-journal
  URL: http://arxiv.org/abs/2105.12806

- id: buda_systematic_2018
  abstract: >-
    In this study, we systematically investigate the impact of class imbalance
    on classification performance of convolutional neural networks (CNNs) and
    compare frequently used methods to address the issue. Class imbalance is a
    common problem that has been comprehensively studied in classical machine
    learning, yet very limited systematic research is available in the context
    of deep learning. In our study, we use three benchmark datasets of
    increasing complexity, MNIST, CIFAR-10 and ImageNet, to investigate the
    effects of imbalance on classification and perform an extensive comparison
    of several methods to address the issue: oversampling, undersampling,
    two-phase training, and thresholding that compensates for prior class
    probabilities. Our main evaluation metric is area under the receiver
    operating characteristic curve (ROC AUC) adjusted to multi-class tasks since
    overall accuracy metric is associated with notable difficulties in the
    context of imbalanced data. Based on results from our experiments we
    conclude that (i) the effect of class imbalance on classification
    performance is detrimental; (ii) the method of addressing class imbalance
    that emerged as dominant in almost all analyzed scenarios was oversampling;
    (iii) oversampling should be applied to the level that completely eliminates
    the imbalance, whereas the optimal undersampling ratio depends on the extent
    of imbalance; (iv) as opposed to some classical machine learning models,
    oversampling does not cause overfitting of CNNs; (v) thresholding should be
    applied to compensate for prior class probabilities when overall number of
    properly classified cases is of interest.
  accessed:
    - year: 2019
      month: 1
      day: 28
  author:
    - family: Buda
      given: Mateusz
    - family: Maki
      given: Atsuto
    - family: Mazurowski
      given: Maciej A.
  citation-key: buda_systematic_2018
  container-title: Neural Networks
  DOI: 10/gfbfz3
  ISSN: '08936080'
  issued:
    - year: 2018
      month: 10
  page: 249-259
  source: arXiv.org
  title: >-
    A systematic study of the class imbalance problem in convolutional neural
    networks
  type: article-journal
  URL: http://arxiv.org/abs/1710.05381
  volume: '106'

- id: burden_numerical_2011
  author:
    - family: Burden
      given: Richard L.
    - family: Faires
      given: J. Douglas
  call-number: QA297 .B84 2011
  citation-key: burden_numerical_2011
  edition: 9th ed
  event-place: Boston, MA
  ISBN: 978-0-538-73351-9
  issued:
    - year: 2011
  note: 'OCLC: ocn496962633'
  number-of-pages: '872'
  publisher: Brooks/Cole, Cengage Learning
  publisher-place: Boston, MA
  source: Library of Congress ISBN
  title: Numerical analysis
  type: book

- id: byrd_approximate_1988
  author:
    - family: Byrd
      given: Richard H
    - family: Schnabel
      given: Robert B
    - family: Shultz
      given: Gerald A
  citation-key: byrd_approximate_1988
  container-title: Mathematical Programming
  DOI: 10.1007/BF01580735
  issue: 1-3
  issued:
    - year: 1988
  page: 247–263
  title: >-
    Approximate solution of the trust region problem by minimization over
    two-dimensional subspaces
  type: article-journal
  volume: '40'

- id: byrd_interior_1999
  accessed:
    - year: 2017
      month: 8
      day: 20
  author:
    - family: Byrd
      given: Richard H.
    - family: Hribar
      given: Mary E.
    - family: Nocedal
      given: Jorge
  citation-key: byrd_interior_1999
  container-title: SIAM Journal on Optimization
  DOI: 10.1137/S1052623497325107
  issue: '4'
  issued:
    - year: 1999
  page: 877–900
  source: Google Scholar
  title: An interior point algorithm for large-scale nonlinear programming
  type: article-journal
  URL: http://epubs.siam.org/doi/abs/10.1137/S1052623497325107
  volume: '9'

- id: byrd_interior_1999a
  accessed:
    - year: 2017
      month: 8
      day: 20
  author:
    - family: Byrd
      given: Richard H.
    - family: Hribar
      given: Mary E.
    - family: Nocedal
      given: Jorge
  citation-key: byrd_interior_1999a
  container-title: SIAM Journal on Optimization
  DOI: 10/b4r783
  issue: '4'
  issued:
    - year: 1999
  page: 877-900
  title: An Interior Point Algorithm for Large-Scale Nonlinear Programming
  type: article-journal
  volume: '9'

- id: byrd_knitro_2006
  author:
    - family: Byrd
      given: Richard H
    - family: Nocedal
      given: Jorge
    - family: Waltz
      given: Richard A
  citation-key: byrd_knitro_2006
  container-title: Large-scale nonlinear optimization
  issued:
    - year: 2006
  page: 35–59
  publisher: Springer
  title: 'KNITRO: An integrated package for nonlinear optimization'
  type: chapter

- id: byrd_knitro_2006a
  author:
    - family: Byrd
      given: Richard H
    - family: Nocedal
      given: Jorge
    - family: Waltz
      given: Richard A
  citation-key: byrd_knitro_2006a
  container-title: Large-Scale Nonlinear Optimization
  issued:
    - year: 2006
  page: 35-59
  publisher: Springer
  title: 'KNITRO: An Integrated Package for Nonlinear Optimization'
  type: chapter

- id: byrd_limited_1995
  author:
    - family: Byrd
      given: Richard H
    - family: Lu
      given: Peihuang
    - family: Nocedal
      given: Jorge
    - family: Zhu
      given: Ciyou
  citation-key: byrd_limited_1995
  container-title: SIAM Journal on Scientific Computing
  container-title-short: SIAM Journal on Scientific Computing
  DOI: 10.1137/0916069
  ISSN: 1064-8275
  issue: '5'
  issued:
    - year: 1995
  page: 1190-1208
  title: A limited memory algorithm for bound constrained optimization
  type: article-journal
  volume: '16'

- id: byrd_local_1997
  author:
    - family: Byrd
      given: Richard H
    - family: Liu
      given: Guanghui
    - family: Nocedal
      given: Jorge
  citation-key: byrd_local_1997
  container-title: Numerical analysis
  container-title-short: Numerical analysis
  issued:
    - year: 1997
  page: 37-56
  title: On the local behavior of an interior point method for nonlinear programming
  type: article-journal
  volume: '1997'

- id: byrd_local_1997a
  author:
    - family: Byrd
      given: Richard H
    - family: Liu
      given: Guanghui
    - family: Nocedal
      given: Jorge
  citation-key: byrd_local_1997a
  container-title: Numerical analysis
  issued:
    - year: 1997
  page: 37-56
  title: On the Local Behavior of an Interior Point Method for Nonlinear Programming
  type: article-journal
  volume: '1997'

- id: byrd_representations_1994
  abstract: >-
    We derive compact representations of BFGS and symmetric rank-one matrices
    for optimization. These representations allow us to efficiently implement
    limited memory methods for large constrained optimization problems. In
    particular, we discuss how to compute projections of limited memory matrices
    onto subspaces. We also present a compact representation of the matrices
    generated by Broyden's update for solving systems of nonlinear equations.
  accessed:
    - year: 2017
      month: 10
      day: 30
  author:
    - family: Byrd
      given: Richard H.
    - family: Nocedal
      given: Jorge
    - family: Schnabel
      given: Robert B.
  citation-key: byrd_representations_1994
  container-title: Mathematical Programming
  container-title-short: Mathematical Programming
  DOI: 10.1007/BF01582063
  ISSN: 0025-5610, 1436-4646
  issue: 1-3
  issued:
    - year: 1994
      month: 1
      day: 1
  language: en
  page: 129-156
  source: link.springer.com
  title: >-
    Representations of quasi-Newton matrices and their use in limited memory
    methods
  type: article-journal
  URL: https://link.springer.com/article/10.1007/BF01582063
  volume: '63'

- id: byrd_trust_1987
  author:
    - family: Byrd
      given: Richard H
    - family: Schnabel
      given: Robert B
    - family: Shultz
      given: Gerald A
  citation-key: byrd_trust_1987
  container-title: SIAM Journal on Numerical Analysis
  DOI: 10.1137/0724076
  issue: '5'
  issued:
    - year: 1987
  page: 1152–1170
  title: A trust region algorithm for nonlinearly constrained optimization
  type: article-journal
  volume: '24'

- id: byrd_trust_2000
  author:
    - family: Byrd
      given: Richard H
    - family: Gilbert
      given: Jean Charles
    - family: Nocedal
      given: Jorge
  citation-key: byrd_trust_2000
  container-title: Mathematical Programming
  container-title-short: Mathematical Programming
  DOI: 10.1007/PL00011391
  ISSN: 0025-5610
  issue: '1'
  issued:
    - year: 2000
  page: 149-185
  title: >-
    A trust region method based on interior point techniques for nonlinear
    programming
  type: article-journal
  volume: '89'

- id: byrd_trust_2000a
  author:
    - family: Byrd
      given: Richard H
    - family: Gilbert
      given: Jean Charles
    - family: Nocedal
      given: Jorge
  citation-key: byrd_trust_2000a
  container-title: Mathematical Programming
  DOI: 10/dmkzpw
  ISSN: 0025-5610
  issue: '1'
  issued:
    - year: 2000
  page: 149-185
  title: >-
    A Trust Region Method Based on Interior Point Techniques for Nonlinear
    Programming
  type: article-journal
  volume: '89'

- id: cadenas_wind_2016
  author:
    - family: Cadenas
      given: Erasmo
    - family: Rivera
      given: Wilfrido
    - family: Campos-Amezcua
      given: Rafael
    - family: Cadenas
      given: Roberto
  citation-key: cadenas_wind_2016
  container-title: Neural Computing and Applications
  DOI: 10/f9k443
  issue: '8'
  issued:
    - year: 2016
  note: '00010'
  page: 2417-2428
  title: 'Wind Speed Forecasting Using the NARX Model, Case: La Mata, Oaxaca, México'
  type: article-journal
  volume: '27'

- id: cadenas_wind_2016a
  author:
    - family: Cadenas
      given: Erasmo
    - family: Rivera
      given: Wilfrido
    - family: Campos-Amezcua
      given: Rafael
    - family: Cadenas
      given: Roberto
  citation-key: cadenas_wind_2016a
  container-title: Neural Computing and Applications
  DOI: 10/f9k443
  issue: '8'
  issued:
    - year: 2016
  license: All rights reserved
  page: 2417-2428
  title: 'Wind Speed Forecasting Using the NARX Model, Case: La Mata, Oaxaca, México'
  type: article-journal
  volume: '27'

- id: calafiore_leading_2016
  abstract: >-
    This paper deals with the problem of finding a low-complexity estimate of
    the impulse response of a linear time-invariant discrete-time dynamic system
    from noise-corrupted input-output data. To this purpose, we introduce an
    identification criterion formed by the average (over the input
    perturbations) of a standard prediction error cost, plus a weighted ℓ1
    regularization term which promotes sparse solutions. While it is well known
    that such criteria do provide solutions with many zeros, a critical issue in
    our identification context is where these zeros are located, since sensible
    low-order models should be zero in the tail of the impulse response. The
    flavor of the key results in this paper is that, under quite standard
    assumptions (such as i.i.d. input and noise sequences and system stability),
    the estimate of the impulse response resulting from the proposed criterion
    is indeed identically zero from a certain time index nl (named the leading
    order) onwards, with arbitrarily high probability, for a sufficiently large
    data cardinality N. Numerical experiments are reported that support the
    theoretical results, and comparisons are made with some other
    state-of-the-art methodologies.
  author:
    - family: Calafiore
      given: G. C.
    - family: Novara
      given: C.
    - family: Taragna
      given: M.
  citation-key: calafiore_leading_2016
  container-title: 2016 IEEE 55th Conference on Decision and Control (CDC)
  DOI: 10.1109/CDC.2016.7798705
  event-title: 2016 IEEE 55th Conference on Decision and Control (CDC)
  issued:
    - year: 2016
      month: 12
  page: 2926-2931
  source: IEEE Xplore
  title: >-
    Leading impulse response identification via the weighted elastic net
    criterion
  type: paper-conference

- id: calafiore_leading_2017
  abstract: "This paper deals with the problem of finding a low-complexity estimate of the impulse response of a linear time-invariant discrete-time dynamic system from noise-corrupted input–output data. To this purpose, we introduce an identification criterion formed by the average (over the input perturbations) of a standard prediction error cost, plus an ℓ1 regularization term which promotes sparse solutions. While it is well known that such criteria do provide solutions with many zeros, a critical issue in our identification context is where these zeros are located, since sensible low-order models should be zero in the tail of the impulse response. The flavor of the key results in this paper is that, under quite standard assumptions (such as i.i.d.\_input and noise sequences and system stability), the estimate of the impulse response resulting from the proposed criterion is indeed identically zero from a certain time index nl (named the leading order) onwards, with arbitrarily high probability, for a sufficiently large data cardinality N. Numerical experiments are reported that support the theoretical results, and comparisons are made with some other state-of-the-art methodologies."
  author:
    - family: Calafiore
      given: Giuseppe C.
    - family: Novara
      given: Carlo
    - family: Taragna
      given: Michele
  citation-key: calafiore_leading_2017
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10.1016/j.automatica.2017.01.011
  ISSN: 0005-1098
  issue: Supplement C
  issued:
    - year: 2017
      month: 6
      day: 1
  page: 75-87
  source: ScienceDirect
  title: Leading impulse response identification via the Elastic Net criterion
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0005109817300213
  volume: '80'

- id: calafiore_sparse_2014
  author:
    - family: Calafiore
      given: Giuseppe Carlo
    - family: Ghaoui
      given: Laurent El
    - family: Novara
      given: Carlo
  citation-key: calafiore_sparse_2014
  container-title: 19th IFAC World Congress
  container-title-short: IFAC Proceedings Volumes
  DOI: 10.3182/20140824-6-ZA-1003.01549
  ISSN: 1474-6670
  issue: '3'
  issued:
    - year: 2014
      month: 1
      day: 1
  page: 3238-3243
  title: Sparse Identification of Polynomial and Posynomial Models
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S1474667016421063
  volume: '47'

- id: calafiore_sparse_2015
  abstract: >-
    Posynomials are nonnegative combinations of monomials with possibly
    fractional and both positive and negative exponents. Posynomial models are
    widely used in various engineering design endeavors, such as circuits,
    aerospace and structural design, mainly due to the fact that design problems
    cast in terms of posynomial objectives and constraints can be solved
    efficiently by means of a convex optimization technique known as geometric
    programming (GP). However, while quite a vast literature exists on GP-based
    design, very few contributions can yet be found on the problem of
    identifying posynomial models from experimental data. Posynomial
    identification amounts to determining not only the coefficients of the
    combination, but also the exponents in the monomials, which renders the
    identification problem hard. In this paper, we propose an approach to the
    identification of multivariate posynomial models based on the expansion on a
    given large-scale basis of monomials. The model is then identified by
    seeking coefficients of the combination that minimize a mixed objective,
    composed by a term representing the fitting error and a term inducing
    sparsity in the representation, which results in a problem formulation of
    the “square-root LASSO” type, with nonnegativity constraints on the
    variables. We propose to solve the problem via a sequential
    coordinate-minimization scheme, which is suitable for large-scale
    implementations. A numerical example is finally presented, dealing with the
    identification of a posynomial model for a NACA 4412 airfoil.
  author:
    - family: Calafiore
      given: Giuseppe C.
    - family: El Ghaoui
      given: Laurent M.
    - family: Novara
      given: Carlo
  citation-key: calafiore_sparse_2015
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10.1016/j.automatica.2015.06.003
  ISSN: 0005-1098
  issue: Supplement C
  issued:
    - year: 2015
      month: 9
      day: 1
  page: 27-34
  source: ScienceDirect
  title: Sparse identification of posynomial models
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0005109815002319
  volume: '59'

- id: candes_compressive_2006
  author:
    - family: Candès
      given: Emmanuel J
  citation-key: candes_compressive_2006
  container-title: Proceedings of the international congress of mathematicians
  issued:
    - year: 2006
  page: 1433–1452
  title: Compressive sampling
  type: paper-conference
  volume: '3'

- id: candes_conformalized_2023
  abstract: >-
    Existing survival analysis techniques heavily rely on strong modelling
    assumptions and are, therefore, prone to model misspecification errors. In
    this paper, we develop an inferential method based on ideas from conformal
    prediction, which can wrap around any survival prediction algorithm to
    produce calibrated, covariate-dependent lower predictive bounds on survival
    times. In the Type I right-censoring setting, when the censoring times are
    completely exogenous, the lower predictive bounds have guaranteed coverage
    in finite samples without any assumptions other than that of operating on
    independent and identically distributed data points. Under a more general
    conditionally independent censoring assumption, the bounds satisfy a doubly
    robust property which states the following: marginal coverage is
    approximately guaranteed if either the censoring mechanism or the
    conditional survival function is estimated well. Further, we demonstrate
    that the lower predictive bounds remain valid and informative for other
    types of censoring. The validity and efficiency of our procedure are
    demonstrated on synthetic data and real COVID-19 data from the UK Biobank.
  accessed:
    - year: 2023
      month: 9
      day: 8
  author:
    - family: Candès
      given: Emmanuel J.
    - family: Lei
      given: Lihua
    - family: Ren
      given: Zhimei
  citation-key: candes_conformalized_2023
  DOI: 10.48550/arXiv.2103.09763
  issued:
    - year: 2023
      month: 4
      day: 23
  number: arXiv:2103.09763
  publisher: arXiv
  source: arXiv.org
  title: Conformalized Survival Analysis
  type: article
  URL: http://arxiv.org/abs/2103.09763

- id: candes_dantzig_2007
  abstract: >-
    In many important statistical applications, the number of variables or
    parameters p is much larger than the number of observations n. Suppose then
    that we have observations y = Xβ + z, where $\beta \in {\bf R}^{p}$ is a
    parameter vector of interest, X is a data matrix with possibly far fewer
    rows than columns, n « p, and the $z_{i}\text{'}{\rm s}$ are i.i.d. N(0,
    σ²). Is it possible to estimate β reliably based on the noisy data y? To
    estimate β, we introduce a new estimator-we call it the Dantzig
    selector-which is a solution to the l₁-regularization problem $\underset
    \tilde{\beta}\in {\bf R}^{p}\to{{\rm min}}\|\tilde{\beta}\|_{\ell _{1}}$
    subject to $\|X^{\ast }r\|_{\ell _{\infty}}\leq (1+t^{-1})\sqrt{2\,{\rm
    log}\,p}\cdot \sigma $, where r is the residual vector $y-X\tilde{\beta}$
    and t is a positive scalar. We show that if X obeys a uniform uncertainty
    principle (with unit-normed columns) and if the true parameter vector β is
    sufficiently sparse (which here roughly guarantees that the model is
    identifiable), then with very large probability, $\|\hat{\beta}-\beta
    \|_{\ell _{2}}^{2}\leq C^{2}\cdot 2\,{\rm log}\,p\cdot \left(\sigma
    ^{2}+\sum_{i}{\rm min}(\beta _{i}^{2},\sigma ^{2})\right)$. Our results are
    nonasymptotic and we give values for the constant C. Even though n may be
    much smaller than p, our estimator achieves a loss within a logarithmic
    factor of the ideal mean squared error one would achieve with an oracle
    which would supply perfect information about which coordinates are nonzero,
    and which were above the noise level. In multivariate regression and from a
    model selection viewpoint, our result says that it is possible nearly to
    select the best subset of variables by solving a very simple convex program,
    which, in fact, can easily be recast as a convenient linear program (LP).
  author:
    - family: Candes
      given: Emmanuel
    - family: Tao
      given: Terence
  citation-key: candes_dantzig_2007
  container-title: The Annals of Statistics
  ISSN: 0090-5364
  issue: '6'
  issued:
    - year: 2007
  page: 2313-2351
  source: JSTOR
  title: 'The Dantzig Selector: Statistical Estimation When p Is Much Larger than n'
  title-short: The Dantzig Selector
  type: article-journal
  URL: http://www.jstor.org/stable/25464587
  volume: '35'

- id: candes_decoding_2005
  author:
    - family: Candes
      given: Emmanuel J
    - family: Tao
      given: Terence
  citation-key: candes_decoding_2005
  container-title: IEEE transactions on information theory
  issue: '12'
  issued:
    - year: 2005
  page: 4203–4215
  publisher: IEEE
  title: Decoding by linear programming
  type: article-journal
  volume: '51'

- id: candes_modern_2006
  abstract: >-
    A number of fundamental results in modern statistical theory involve
    thresholding estimators. This survey paper aims at reconstructing the
    history of how thresholding rules came to be popular in statistics and
    describing, in a not overly technical way, the domain of their application.
    Two notions play a fundamental role in our narrative: sparsity and oracle
    inequalities. Sparsity is a property of the object to estimate, which seems
    to be characteristic of many modern problems, in statistics as well as
    applied mathematics and theoretical computer science, to name a few. ‘Oracle
    inequalities’ are a powerful decision-theoretic tool which has served to
    understand the optimality of thresholding rules, but which has many other
    potential applications, some of which we will discuss.
                Our story is also the story of the dialogue between statistics and applied harmonic analysis. Starting with the work of Wiener, we will see that certain representations emerge as being optimal for estimation. A leitmotif throughout our exposition is that efficient representations lead to efficient estimation.
  accessed:
    - year: 2022
      month: 12
      day: 28
  author:
    - family: Candès
      given: Emmanuel J.
  citation-key: candes_modern_2006
  container-title: Acta Numerica
  container-title-short: Acta Numerica
  DOI: 10.1017/S0962492906230010
  ISSN: 0962-4929, 1474-0508
  issued:
    - year: 2006
      month: 5
  language: en
  page: 257-325
  source: DOI.org (Crossref)
  title: Modern statistical estimation via oracle inequalities
  type: article-journal
  URL: >-
    https://www.cambridge.org/core/product/identifier/S0962492906230010/type/journal_article
  volume: '15'

- id: cantelmo_adaptive_2010
  accessed:
    - year: 2017
      month: 9
      day: 13
  author:
    - family: Cantelmo
      given: C.
    - family: Piroddi
      given: L.
  citation-key: cantelmo_adaptive_2010
  container-title: IET Control Theory & Applications
  DOI: 10.1049/iet-cta.2009.0581
  ISSN: 1751-8644, 1751-8652
  issue: '12'
  issued:
    - year: 2010
      month: 12
      day: 1
  language: en
  page: 2693-2706
  source: CrossRef
  title: Adaptive model selection for polynomial NARX models
  type: article-journal
  URL: http://digital-library.theiet.org/content/journals/10.1049/iet-cta.2009.0581
  volume: '4'

- id: cantwell_rethinking_2018
  abstract: >-
    We review some of the latest approaches to analysing cardiac
    electrophysiology data using machine learning and predictive modelling.
    Cardiac arrhythmias, particularly atrial fibrillation, are a major global
    healthcare challenge. Treatment is often through catheter ablation, which
    involves the targeted localized destruction of regions of the myocardium
    responsible for initiating or perpetuating the arrhythmia. Ablation targets
    are either anatomically defined, or identified based on their functional
    properties as determined through the analysis of contact intracardiac
    electrograms acquired with increasing spatial density by modern
    electroanatomic mapping systems. While numerous quantitative approaches have
    been investigated over the past decades for identifying these critical
    curative sites, few have provided a reliable and reproducible advance in
    success rates. Machine learning techniques, including recent deep-learning
    approaches, offer a potential route to gaining new insight from this wealth
    of highly complex spatio-temporal information that existing methods struggle
    to analyse. Coupled with predictive modelling, these techniques offer
    exciting opportunities to advance the field and produce more accurate
    diagnoses and robust personalised treatment. We outline some of these
    methods and illustrate their use in making predictions from the contact
    electrogram and augmenting predictive modelling tools, both by more rapidly
    predicting future states of the system and by inferring the parameters of
    these models from experimental observations.
  accessed:
    - year: 2018
      month: 10
      day: 20
  author:
    - family: Cantwell
      given: Chris D.
    - family: Mohamied
      given: Yumnah
    - family: Tzortzis
      given: Konstantinos N.
    - family: Garasto
      given: Stef
    - family: Houston
      given: Charles
    - family: Chowdhury
      given: Rasheda A.
    - family: Ng
      given: Fu Siong
    - family: Bharath
      given: Anil A.
    - family: Peters
      given: Nicholas S.
  citation-key: cantwell_rethinking_2018
  container-title: arXiv:1810.04227
  issued:
    - year: 2018
      month: 10
      day: 9
  source: arXiv.org
  title: >-
    Rethinking multiscale cardiac electrophysiology with machine learning and
    predictive modelling
  type: article-journal
  URL: http://arxiv.org/abs/1810.04227

- id: cardoso_longitudinal_2016
  abstract: >-
    Purpose We have established a prospective cohort of 1959 patients with
    chronic Chagas cardiomyopathy to evaluate if a clinical prediction rule
    based on ECG, brain natriuretic peptide (BNP) levels, and other biomarkers
    can be useful in clinical practice. This paper outlines the study and
    baseline characteristics of the participants.

    Participants The study is being conducted in 21 municipalities of the
    northern part of Minas Gerais State in Brazil, and includes a follow-up of 2
    years. The baseline evaluation included collection of sociodemographic
    information, social determinants of health, health-related behaviours,
    comorbidities, medicines in use, history of previous treatment for Chagas
    disease, functional class, quality of life, blood sample collection, and
    ECG. Patients were mostly female, aged 50–74 years, with low family income
    and educational level, with known Chagas disease for >10 years; 46%
    presented with functional class >II. Previous use of benznidazole was
    reported by 25.2% and permanent use of pacemaker by 6.2%. Almost half of the
    patients presented with high blood cholesterol and hypertension, and
    one-third of them had diabetes mellitus. N-terminal of the prohormone BNP
    (NT-ProBNP) level was >300 pg/mL in 30% of the sample.

    Findings to date Clinical and laboratory markers predictive of severe and
    progressive Chagas disease were identified as high NT-ProBNP levels, as well
    as symptoms of advanced heart failure. These results confirm the important
    residual morbidity of Chagas disease in the remote areas, thus supporting
    political decisions that should prioritise in addition to epidemiological
    surveillance the medical treatment of chronic Chagas cardiomyopathy in the
    coming years. The São Paulo-Minas Gerais Tropical Medicine Research Center
    (SaMi-Trop) represents a major challenge for focused research in neglected
    diseases, with knowledge that can be applied in primary healthcare.

    Future plans We will continue following this patients’ cohort to provide
    relevant information about the development and progression of Chagas disease
    in remotes areas, with social and economic inequalities.

    Trial registration number NCT02646943; Pre-results.
  accessed:
    - year: 2021
      month: 11
      day: 25
  author:
    - family: Cardoso
      given: Clareci Silva
    - family: Sabino
      given: Ester Cerdeira
    - family: Oliveira
      given: Claudia Di Lorenzo
    - family: Oliveira
      given: Lea Campos
      dropping-particle: de
    - family: Ferreira
      given: Ariela Mota
    - family: Cunha-Neto
      given: Edécio
    - family: Bierrenbach
      given: Ana Luiza
    - family: Ferreira
      given: João Eduardo
    - family: Haikal
      given: Desirée Sant'Ana
    - family: Reingold
      given: Arthur L.
    - family: Ribeiro
      given: Antonio Luiz P.
  citation-key: cardoso_longitudinal_2016
  container-title: BMJ Open
  DOI: 10.1136/bmjopen-2016-011181
  ISSN: 2044-6055, 2044-6055
  issue: '5'
  issued:
    - year: 2016
      month: 5
      day: 1
  language: en
  license: >-
    Published by the BMJ Publishing Group Limited. For permission to use (where
    not already granted under a licence) please go to
    http://www.bmj.com/company/products-services/rights-and-licensing/. This is
    an Open Access article distributed in accordance with the terms of the
    Creative Commons Attribution (CC BY 4.0) license, which permits others to
    distribute, remix, adapt and build upon this work, for commercial use,
    provided the original work is properly cited. See:
    http://creativecommons.org/licenses/by/4.0/
  page: e011181
  PMID: '27147390'
  publisher: British Medical Journal Publishing Group
  section: Cardiovascular medicine
  source: bmjopen.bmj.com
  title: >-
    Longitudinal study of patients with chronic Chagas cardiomyopathy in Brazil
    (SaMi-Trop project): a cohort profile
  title-short: >-
    Longitudinal study of patients with chronic Chagas cardiomyopathy in Brazil
    (SaMi-Trop project)
  type: article-journal
  URL: https://bmjopen.bmj.com/content/6/5/e011181
  volume: '6'

- id: carin_deep_2018
  abstract: >-
    This JAMA Guide to Statistics and Methods explains the basic concepts
    underlying convolutional neural networks (CNNs), a type of machine learning
    being used to automate the reading of medical images.
  accessed:
    - year: 2018
      month: 10
      day: 20
  author:
    - family: Carin
      given: Lawrence
    - family: Pencina
      given: Michael J.
  citation-key: carin_deep_2018
  container-title: JAMA
  container-title-short: JAMA
  DOI: 10.1001/jama.2018.13316
  ISSN: 0098-7484
  issue: '11'
  issued:
    - year: 2018
      month: 9
      day: 18
  language: en
  page: 1192-1193
  source: jamanetwork.com
  title: On Deep Learning for Medical Image Analysis
  type: article-journal
  URL: https://jamanetwork.com/journals/jama/fullarticle/2702856
  volume: '320'

- id: carlini_certified_2023
  abstract: >-
    In this paper we show how to achieve state-of-the-art certified adversarial
    robustness to 2-norm bounded perturbations by relying exclusively on
    off-the-shelf pretrained models. To do so, we instantiate the denoised
    smoothing approach of Salman et al. 2020 by combining a pretrained denoising
    diffusion probabilistic model and a standard high-accuracy classifier. This
    allows us to certify 71% accuracy on ImageNet under adversarial
    perturbations constrained to be within an 2-norm of 0.5, an improvement of
    14 percentage points over the prior certified SoTA using any approach, or an
    improvement of 30 percentage points over denoised smoothing. We obtain these
    results using only pretrained diffusion models and image classifiers,
    without requiring any fine tuning or retraining of model parameters.
  accessed:
    - year: 2023
      month: 4
      day: 13
  author:
    - family: Carlini
      given: Nicholas
    - family: Tramer
      given: Florian
    - family: Dvijotham
      given: Krishnamurthy Dj
    - family: Rice
      given: Leslie
    - family: Sun
      given: Mingjie
    - family: Kolter
      given: J. Zico
  citation-key: carlini_certified_2023
  container-title: International Conference on Learning Representations (ICLR)
  DOI: 10.48550/arXiv.2206.10550
  issued:
    - year: 2023
      month: 3
      day: 6
  publisher: arXiv
  source: arXiv.org
  title: (Certified!!) Adversarial Robustness for Free!
  type: paper-conference
  URL: http://arxiv.org/abs/2206.10550

- id: carraro_indirect_2014
  author:
    - family: Carraro
      given: T
    - family: Geiger
      given: Michael
    - family: Rannacher
      given: Rolf
  citation-key: carraro_indirect_2014
  container-title: SIAM Journal on Scientific Computing
  container-title-short: SIAM Journal on Scientific Computing
  DOI: 10.1137/120895809
  ISSN: 1064-8275
  issue: '2'
  issued:
    - year: 2014
  page: A452-A481
  title: >-
    Indirect multiple shooting for nonlinear parabolic optimal control problems
    with control constraints
  type: article-journal
  volume: '36'

- id: carraro_indirect_2014a
  author:
    - family: Carraro
      given: T
    - family: Geiger
      given: Michael
    - family: Rannacher
      given: Rolf
  citation-key: carraro_indirect_2014a
  container-title: SIAM Journal on Scientific Computing
  DOI: 10/f54rgr
  ISSN: 1064-8275
  issue: '2'
  issued:
    - year: 2014
  page: A452-A481
  title: >-
    Indirect Multiple Shooting for Nonlinear Parabolic Optimal Control Problems
    with Control Constraints
  type: article-journal
  volume: '36'

- id: carratino_learning_2019
  abstract: >-
    Sketching and stochastic gradient methods are arguably the most common
    techniques to derive efficient large scale learning algorithms. In this
    paper, we investigate their application in the context of nonparametric
    statistical learning. More precisely, we study the estimator defined by
    stochastic gradient with mini batches and random features. The latter can be
    seen as form of nonlinear sketching and used to define approximate kernel
    methods. The considered estimator is not explicitly penalized/constrained
    and regularization is implicit. Indeed, our study highlights how different
    parameters, such as number of features, iterations, step-size and mini-batch
    size control the learning properties of the solutions. We do this by
    deriving optimal finite sample bounds, under standard assumptions. The
    obtained results are corroborated and illustrated by numerical experiments.
  accessed:
    - year: 2020
      month: 8
      day: 10
  author:
    - family: Carratino
      given: Luigi
    - family: Rudi
      given: Alessandro
    - family: Rosasco
      given: Lorenzo
  citation-key: carratino_learning_2019
  container-title: arXiv:1807.06343 [cs, stat]
  issued:
    - year: 2019
      month: 1
      day: 24
  source: arXiv.org
  title: Learning with SGD and Random Features
  type: article-journal
  URL: http://arxiv.org/abs/1807.06343

- id: castro_causality_2020
  abstract: >-
    Causal reasoning can shed new light on the major challenges in machine
    learning for medical imaging: scarcity of high-quality annotated data and
    mismatch between the development dataset and the target environment. A
    causal perspective on these issues allows decisions about data collection,
    annotation, preprocessing, and learning strategies to be made and
    scrutinized more transparently, while providing a detailed categorisation of
    potential biases and mitigation techniques. Along with worked clinical
    examples, we highlight the importance of establishing the causal
    relationship between images and their annotations, and offer step-by-step
    recommendations for future studies.
  accessed:
    - year: 2023
      month: 4
      day: 4
  author:
    - family: Castro
      given: Daniel C.
    - family: Walker
      given: Ian
    - family: Glocker
      given: Ben
  citation-key: castro_causality_2020
  container-title: Nature Communications
  container-title-short: Nat Commun
  DOI: 10.1038/s41467-020-17478-w
  ISSN: 2041-1723
  issue: '1'
  issued:
    - year: 2020
      month: 7
      day: 22
  language: en
  license: 2020 The Author(s)
  number: '1'
  page: '3673'
  publisher: Nature Publishing Group
  source: www.nature.com
  title: Causality matters in medical imaging
  type: article-journal
  URL: https://www.nature.com/articles/s41467-020-17478-w
  volume: '11'

- id: cauchy_methode_1847
  author:
    - family: Cauchy
      given: Augustin
  citation-key: cauchy_methode_1847
  container-title: Comp. Rend. Sci. Paris
  issue: '1847'
  issued:
    - year: 1847
  page: 536–538
  title: Méthode générale pour la résolution des systemes déquations simultanées
  type: article-journal
  volume: '25'

- id: chandar_nonsaturating_2019
  abstract: >-
    Modelling long-term dependencies is a challenge for recurrent neural
    networks. This is primarily due to the fact that gradients vanish during
    training, as the sequence length increases. Gradients can be attenuated by
    transition operators and are attenuated or dropped by activation functions.
    Canonical architectures like LSTM alleviate this issue by skipping
    information through a memory mechanism. We propose a new recurrent
    architecture (Non-saturating Recurrent Unit; NRU) that relies on a memory
    mechanism but forgoes both saturating activation functions and saturating
    gates, in order to further alleviate vanishing gradients. In a series of
    synthetic and real world tasks, we demonstrate that the proposed model is
    the only model that performs among the top 2 models across all tasks with
    and without long-term dependencies, when compared against a range of other
    architectures.
  accessed:
    - year: 2019
      month: 9
      day: 18
  author:
    - family: Chandar
      given: Sarath
    - family: Sankar
      given: Chinnadhurai
    - family: Vorontsov
      given: Eugene
    - family: Kahou
      given: Samira Ebrahimi
    - family: Bengio
      given: Yoshua
  citation-key: chandar_nonsaturating_2019
  container-title: Proceedings of the AAAI Conference on Artificial Intelligence
  container-title-short: AAAI
  DOI: 10/gf8gdm
  ISSN: 2374-3468, 2159-5399
  issued:
    - year: 2019
      month: 7
      day: 17
  language: en
  page: 3280-3287
  source: DOI.org (Crossref)
  title: Towards Non-Saturating Recurrent Units for Modelling Long-Term Dependencies
  type: article-journal
  URL: https://aaai.org/ojs/index.php/AAAI/article/view/4200
  volume: '33'

- id: chandra_coevolutionary_2017
  accessed:
    - year: 2018
      month: 5
      day: 7
  author:
    - family: Chandra
      given: Rohitash
    - family: Ong
      given: Yew-Soon
    - family: Goh
      given: Chi-Keong
  citation-key: chandra_coevolutionary_2017
  container-title: Neurocomputing
  DOI: 10.1016/j.neucom.2017.02.065
  ISSN: '09252312'
  issued:
    - year: 2017
      month: 6
  language: en
  page: 21-34
  source: Crossref
  title: >-
    Co-evolutionary multi-task learning with predictive recurrence for
    multi-step chaotic time series prediction
  type: article-journal
  URL: http://linkinghub.elsevier.com/retrieve/pii/S0925231217303892
  volume: '243'

- id: chandra_competition_2015
  accessed:
    - year: 2018
      month: 5
      day: 7
  author:
    - family: Chandra
      given: Rohitash
  citation-key: chandra_competition_2015
  container-title: IEEE Transactions on Neural Networks and Learning Systems
  DOI: 10.1109/TNNLS.2015.2404823
  ISSN: 2162-237X, 2162-2388
  issue: '12'
  issued:
    - year: 2015
      month: 12
  page: 3123-3136
  source: Crossref
  title: >-
    Competition and Collaboration in Cooperative Coevolution of Elman Recurrent
    Neural Networks for Time-Series Prediction
  type: article-journal
  URL: http://ieeexplore.ieee.org/document/7055352/
  volume: '26'

- id: chang_realtime_2007
  author:
    - family: Chang
      given: Nelson
    - family: Lin
      given: Ting-Min
    - family: Tsai
      given: Tsung-Hsien
    - family: Tseng
      given: Yu-Cheng
    - family: Chang
      given: Tian-Sheuan
  citation-key: chang_realtime_2007
  container-title: Multimedia and Expo, 2007 IEEE International Conference on
  issued:
    - year: 2007
  page: 2090–2093
  publisher: IEEE
  title: Real-time DSP implementation on local stereo matching
  type: paper-conference

- id: chatterji_foolish_2022
  abstract: >-
    We prove a lower bound on the excess risk of sparse interpolating procedures
    for linear regression with Gaussian data in the overparameterized regime. We
    apply this result to obtain a lower bound for basis pursuit (the minimum
    $\ell_1$-norm interpolant) that implies that its excess risk can converge at
    an exponentially slower rate than OLS (the minimum $\ell_2$-norm
    interpolant), even when the ground truth is sparse. Our analysis exposes the
    benefit of an effect analogous to the "wisdom of the crowd", except here the
    harm arising from fitting the $\textit{noise}$ is ameliorated by spreading
    it among many directions -- the variance reduction arises from a
    $\textit{foolish}$ crowd.
  accessed:
    - year: 2022
      month: 5
      day: 18
  author:
    - family: Chatterji
      given: Niladri S.
    - family: Long
      given: Philip M.
  citation-key: chatterji_foolish_2022
  container-title: arXiv:2110.02914
  issued:
    - year: 2022
      month: 3
      day: 17
  source: arXiv.org
  title: Foolish Crowds Support Benign Overfitting
  type: article-journal
  URL: http://arxiv.org/abs/2110.02914

- id: chattopadhyay_datadriven_2020
  abstract: >-
    In this paper, the performance of three deep learning methods for predicting
    short-term evolution and for reproducing the long-term statistics of a
    multi-scale spatio-temporal Lorenz 96 system is examined. The methods are:
    echo state network (a type of reservoir computing, RC-ESN), deep
    feed-forward artificial neural network (ANN), and recurrent neural network
    with long short-term memory (RNN-LSTM). This Lorenz 96 system has three
    tiers of nonlinearly interacting variables representing slow/large-scale
    ($X$), intermediate ($Y$), and fast/small-scale ($Z$) processes. For
    training or testing, only $X$ is available; $Y$ and $Z$ are never known or
    used. We show that RC-ESN substantially outperforms ANN and RNN-LSTM for
    short-term prediction, e.g., accurately forecasting the chaotic trajectories
    for hundreds of numerical solver's time steps, equivalent to several
    Lyapunov timescales. The RNN-LSTM and ANN show some prediction skills as
    well; RNN-LSTM bests ANN. Furthermore, even after losing the trajectory,
    data predicted by RC-ESN and RNN-LSTM have probability density functions
    (PDFs) that closely match the true PDF, even at the tails. The PDF of the
    data predicted using ANN, however, deviates from the true PDF. Implications,
    caveats, and applications to data-driven and data-assisted surrogate
    modeling of complex nonlinear dynamical systems such as weather/climate are
    discussed.
  accessed:
    - year: 2021
      month: 4
      day: 1
  author:
    - family: Chattopadhyay
      given: Ashesh
    - family: Hassanzadeh
      given: Pedram
    - family: Subramanian
      given: Devika
  citation-key: chattopadhyay_datadriven_2020
  container-title: Nonlinear Processes in Geophysics
  container-title-short: Nonlin. Processes Geophys.
  DOI: 10.5194/npg-27-373-2020
  ISSN: 1607-7946
  issue: '3'
  issued:
    - year: 2020
      month: 7
      day: 2
  page: 373-389
  source: arXiv.org
  title: >-
    Data-driven prediction of a multi-scale Lorenz 96 chaotic system using deep
    learning methods: Reservoir computing, ANN, and RNN-LSTM
  title-short: >-
    Data-driven prediction of a multi-scale Lorenz 96 chaotic system using deep
    learning methods
  type: article-journal
  URL: http://arxiv.org/abs/1906.08829
  volume: '27'

- id: chaudhari_stochastic_2018
  abstract: >-
    Stochastic gradient descent (SGD) is widely believed to perform implicit
    regularization when used to train deep neural networks, but the precise
    manner in which this occurs has thus far been elusive. We prove that SGD
    minimizes an average potential over the posterior distribution of weights
    along with an entropic regularization term. This potential is however not
    the original loss function in general. So SGD does perform variational
    inference, but for a different loss than the one used to compute the
    gradients. Even more surprisingly, SGD does not even converge in the
    classical sense: we show that the most likely trajectories of SGD for deep
    networks do not behave like Brownian motion around critical points. Instead,
    they resemble closed loops with deterministic components. We prove that such
    “out-of-equilibrium” behavior is a consequence of highly non-isotropic
    gradient noise in SGD; the covariance matrix of mini-batch gradients for
    deep networks has a rank as small as 1% of its dimension. We provide
    extensive empirical validation of these claims, proven in the appendix.
  accessed:
    - year: 2019
      month: 6
      day: 1
  author:
    - family: Chaudhari
      given: Pratik
    - family: Soatto
      given: Stefano
  citation-key: chaudhari_stochastic_2018
  container-title: 2018 Information Theory and Applications Workshop (ITA)
  DOI: 10/gf3d8t
  event-place: San Diego, CA
  event-title: 2018 Information Theory and Applications Workshop (ITA)
  ISBN: 978-1-72810-124-8
  issued:
    - year: 2018
      month: 2
  language: en
  page: 1-10
  publisher: IEEE
  publisher-place: San Diego, CA
  source: DOI.org (Crossref)
  title: >-
    Stochastic Gradient Descent Performs Variational Inference, Converges to
    Limit Cycles for Deep Networks
  type: paper-conference
  URL: https://ieeexplore.ieee.org/document/8503224/

- id: chen_atomic_1998
  abstract: >-
    The time-frequency and time-scale communities have recently developed a
    large number of overcomplete waveform dictionaries — stationary wavelets,
    wavelet packets, cosine packets, chirplets, and warplets, to name a few.
    Decomposition into overcomplete systems is not unique, and several methods
    for decomposition have been proposed, including the method of frames (MOF),
    Matching pursuit (MP), and, for special dictionaries, the best orthogonal
    basis (BOB).Basis Pursuit (BP) is a principle for decomposing a signal into
    an "optimal" superposition of dictionary elements, where optimal means
    having the smallest l1 norm of coefficients among all such decompositions.
    We give examples exhibiting several advantages over MOF, MP, and BOB,
    including better sparsity and superresolution. BP has interesting relations
    to ideas in areas as diverse as ill-posed problems, in abstract harmonic
    analysis, total variation denoising, and multiscale edge denoising.BP in
    highly overcomplete dictionaries leads to large-scale optimization problems.
    With signals of length 8192 and a wavelet packet dictionary, one gets an
    equivalent linear program of size 8192 by 212,992. Such problems can be
    attacked successfully only because of recent advances in linear programming
    by interior-point methods. We obtain reasonable success with a primal-dual
    logarithmic barrier method and conjugate-gradient solver.
  author:
    - family: Chen
      given: Scott Shaobing
    - family: Donoho
      given: David L.
    - family: Saunders
      given: Michael A.
  citation-key: chen_atomic_1998
  container-title: SIAM Journal on Scientific Computing
  DOI: 10.1137/S1064827596304010
  issue: '1'
  issued:
    - year: 1998
  page: 33-61
  title: Atomic decomposition by basis pursuit
  type: article-journal
  URL: https://doi.org/10.1137/S1064827596304010
  volume: '20'

- id: chen_detection_2020
  abstract: >-
    Electrocardiograms (ECGs) are widely used to clinically detect cardiac
    arrhythmias (CAs). They are also being used to develop computer-assisted
    methods for heart disease diagnosis. We have developed a convolution neural
    network model to detect and classify CAs, using a large 12-lead ECG dataset
    (6,877 recordings) provided by the China Physiological Signal Challenge
    (CPSC) 2018. Our model, which was ranked ﬁrst in the challenge competition,
    achieved a median overall F1-score of 0.84 for the nine-type CA
    classiﬁcation of CPSC2018’s hidden test set of 2,954 ECG recordings. Further
    analysis showed that concurrent CAs were adequately predictive for 476
    patients with multiple types of CA diagnoses in the dataset. Using only
    single-lead data yielded a performance that was only slightly worse than
    using the full 12-lead data, with leads aVR and V1 being the most prominent.
    We extensively consider these results in the context of their agreement with
    and relevance to clinical observations.
  accessed:
    - year: 2020
      month: 6
      day: 25
  author:
    - family: Chen
      given: Tsai-Min
    - family: Huang
      given: Chih-Han
    - family: Shih
      given: Edward S.C.
    - family: Hu
      given: Yu-Feng
    - family: Hwang
      given: Ming-Jing
  citation-key: chen_detection_2020
  container-title: iScience
  container-title-short: iScience
  DOI: 10.1016/j.isci.2020.100886
  ISSN: '25890042'
  issue: '3'
  issued:
    - year: 2020
      month: 3
  language: en
  page: '100886'
  source: DOI.org (Crossref)
  title: >-
    Detection and Classification of Cardiac Arrhythmias by a Challenge-Best Deep
    Learning Neural Network Model
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S2589004220300705
  volume: '23'

- id: chen_linear_1998
  author:
    - family: Chen
      given: C.T.
  citation-key: chen_linear_1998
  edition: 3rd
  issued:
    - year: 1998
  title: Linear system theory and design, holt, winehart and winston
  type: book

- id: chen_multiple_2021
  author:
    - family: Chen
      given: Lin
    - family: Min
      given: Yifei
    - family: Belkin
      given: Mikhail
    - family: Karbasi
      given: Amin
  citation-key: chen_multiple_2021
  container-title: Advances in Neural Information Processing Systems
  editor:
    - family: Ranzato
      given: M.
    - family: Beygelzimer
      given: A.
    - family: Dauphin
      given: Y.
    - family: Liang
      given: P. S.
    - family: Vaughan
      given: J. Wortman
  issued:
    - year: 2021
  page: 8898–8912
  publisher: Curran Associates, Inc.
  title: 'Multiple Descent: Design Your Own Generalization Curve'
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2021/file/4ae67a7dd7e491f8fb6f9ea0cf25dfdb-Paper.pdf
  volume: '34'

- id: chen_neural_2018
  accessed:
    - year: 2018
      month: 12
      day: 4
  author:
    - family: Chen
      given: Tian Qi
    - family: Rubanova
      given: Yulia
    - family: Bettencourt
      given: Jesse
    - family: Duvenaud
      given: David K
  citation-key: chen_neural_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 6571–6582
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Neural Ordinary Differential Equations
  type: chapter
  URL: http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf

- id: chen_nonlinear_1990
  author:
    - family: Chen
      given: Sheng
    - family: Billings
      given: S. A.
    - family: Grant
      given: P. M.
  citation-key: chen_nonlinear_1990
  container-title: International Journal of Control
  issue: '6'
  issued:
    - year: 1990
  page: 1191–1214
  title: Non-linear system identification using neural networks
  type: article-journal
  volume: '51'

- id: chen_nonlinear_1990a
  author:
    - family: Chen
      given: Sheng
    - family: Billings
      given: S. A.
    - family: Grant
      given: P. M.
  citation-key: chen_nonlinear_1990a
  container-title: International Journal of Control
  DOI: 10/cg8bhx
  issue: '6'
  issued:
    - year: 1990
  note: '01127'
  page: 1191-1214
  title: Non-Linear System Identification Using Neural Networks
  type: article-journal
  volume: '51'

- id: chen_orthogonal_1989
  author:
    - family: Chen
      given: Sheng
    - family: Billings
      given: Stephen A
    - family: Luo
      given: Wan
  citation-key: chen_orthogonal_1989
  container-title: International Journal of Control
  DOI: 10.1080/00207178908953472
  issue: '5'
  issued:
    - year: 1989
  page: 1873–1896
  title: >-
    Orthogonal least squares methods and their application to non-linear system
    identification
  type: article-journal
  volume: '50'

- id: chen_orthogonal_1989a
  author:
    - family: Chen
      given: Sheng
    - family: Billings
      given: Stephen A
    - family: Luo
      given: Wan
  citation-key: chen_orthogonal_1989a
  container-title: International Journal of Control
  DOI: 10/dd4g9h
  issue: '5'
  issued:
    - year: 1989
  note: '01591'
  page: 1873-1896
  title: >-
    Orthogonal Least Squares Methods and Their Application to Non-Linear System
    Identification
  type: article-journal
  volume: '50'

- id: chen_orthogonalleastsquares_2009
  author:
    - family: Chen
      given: Sheng
    - family: Hong
      given: Xia
    - family: Luk
      given: Bing Lam
    - family: Harris
      given: Chris J
  citation-key: chen_orthogonalleastsquares_2009
  container-title: Neurocomputing
  issue: '10'
  issued:
    - year: 2009
  page: 2670–2681
  title: 'Orthogonal-least-squares regression: A unified approach for data modelling'
  type: article-journal
  volume: '72'

- id: chen_practical_1990
  author:
    - family: Chen
      given: S.
    - family: Billings
      given: S. A.
    - family: Cowan
      given: C. F. N.
    - family: Grant
      given: P. M.
  citation-key: chen_practical_1990
  container-title: International Journal of Control
  DOI: 10.1080/00207179008953599
  issue: '6'
  issued:
    - year: 1990
  page: 1327–1350
  title: Practical identification of NARMAX models using radial basis functions
  type: article-journal
  volume: '52'

- id: chen_representations_1989
  author:
    - family: Chen
      given: S
    - family: Billings
      given: SA
  citation-key: chen_representations_1989
  container-title: International Journal of Control
  DOI: 10.1080/00207178908559683
  issue: '3'
  issued:
    - year: 1989
  page: 1013–1032
  title: 'Representations of non-linear systems: the NARMAX model'
  type: article-journal
  volume: '49'

- id: chen_system_2014
  author:
    - family: Chen
      given: Tianshi
    - family: Andersen
      given: Martin S.
    - family: Ljung
      given: Lennart
    - family: Chiuso
      given: Alessandro
    - family: Pillonetto
      given: Gianluigi
  citation-key: chen_system_2014
  container-title: IEEE Transactions on Automatic Control
  DOI: 10.1109/TAC.2014.2351851
  issue: '11'
  issued:
    - year: 2014
  page: 2933–2945
  source: Google Scholar
  title: >-
    System identification via sparse multiple kernel-based regularization using
    sequential convex optimization techniques
  type: article-journal
  volume: '59'

- id: cheng_adaptive_2009
  abstract: >-
    A neural-network-based adaptive controller is proposed for the tracking
    problem of manipulators with uncertain kinematics, dynamics and actuator
    model. The adaptive Jacobian scheme is used to estimate the unknown
    kinematics parameters. Uncertainties in the manipulator dynamics and
    actuator model are compensated by three-layer neural networks. External
    disturbances and approximation errors are counteracted by robust signals.
    The actuator controller is designed based on the backstepping scheme.
    Compared with the existing work, the proposed method considers the
    manipulator kinematics uncertainty, does not need the
    “linearity-in-parameters” assumption for the uncertain terms in the dynamics
    of manipulator and actuator, and guarantees the tracking error to be as
    small as desired. Finally, the performance of the proposed approach is
    illustrated by the simulation example.
  author:
    - family: Cheng
      given: Long
    - family: Hou
      given: Zeng-Guang
    - family: Tan
      given: Min
  citation-key: cheng_adaptive_2009
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10/cghh6t
  ISSN: 0005-1098
  issue: '10'
  issued:
    - year: 2009
      month: 10
      day: 1
  page: 2312-2318
  title: >-
    Adaptive neural network tracking control for manipulators with uncertain
    kinematics, dynamics and actuator model
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0005109809002982
  volume: '45'

- id: cheng_errors_1998
  abstract: >-
    This article discusses the use of instrumental variables and grouping
    methods in the linear errors-in-variables or measurement error model.
    Comparisons are made between these methods, standard measurement error model
    methods with side conditions, least squares methods, and replicated models.
    It is demonstrated that there are close relationships between these
    apparently diverse estimation techniques.
  author:
    - family: Cheng
      given: Chi-Lun
    - family: Van Ness
      given: John W.
  citation-key: cheng_errors_1998
  container-title: 'Econometrics in theory and practice: Festschrift for hans schneeweiß'
  DOI: 10.1007/978-3-642-47027-1_1
  editor:
    - family: Galata
      given: Robert
    - family: Küchenhoff
      given: Helmut
  event-place: Heidelberg
  ISBN: 978-3-642-47027-1
  issued:
    - year: 1998
  page: 3–13
  publisher: Physica-Verlag HD
  publisher-place: Heidelberg
  title: Errors in variables in econometrics
  type: chapter
  URL: https://doi.org/10.1007/978-3-642-47027-1_1

- id: chernozhukov_double_2017
  abstract: >-
    We revisit the classic semiparametric problem of inference on a low
    dimensional parameter θ0 in the presence of high-dimensional nuisance
    parameters η0. We depart from the classical setting by allowing for η0 to be
    so high-dimensional that the traditional assumptions, such as Donsker
    properties, that limit complexity of the parameter space for this object
    break down. To estimate η0, we consider the use of statistical or machine
    learning (ML) methods which are particularly well-suited to estimation in
    modern, very high-dimensional cases. ML methods perform well by employing
    regularization to reduce variance and trading oﬀ regularization bias with
    overﬁtting in practice. However, both regularization bias and overﬁtting in
    estimating η0 cause a heavy bias in estimators of θ0 that are obtained by
    naively plugging ML estimators of η0 into estimating equations for θ0. This
    bias results in the naive estimator failing to be N −1/2 consistent, where N
    is the sample size. We show that the impact of regularization bias and
    overﬁtting on estimation of the parameter of interest θ0 can be removed by
    using two simple, yet critical, ingredients: (1) using Neyman-orthogonal
    moments/scores that have reduced sensitivity with respect to nuisance
    parameters to estimate θ0, and (2) making use of cross-ﬁtting which provides
    an eﬃcient form of data-splitting. We call the resulting set of methods
    double or debiased ML (DML). We verify that DML delivers point estimators
    that concentrate in a N −1/2-neighborhood of the true parameter values and
    are approximately unbiased and normally distributed, which allows
    construction of valid conﬁdence statements. The generic statistical theory
    of DML is elementary and simultaneously relies on only weak theoretical
    requirements which will admit the use of a broad array of modern ML methods
    for estimating the nuisance parameters such as random forests, lasso, ridge,
    deep neural nets, boosted trees, and various hybrids and ensembles of these
    methods. We illustrate the general theory by applying it to provide
    theoretical properties of DML applied to learn the main regression parameter
    in a partially linear regression model, DML applied to learn the coeﬃcient
    on an endogenous variable in a partially linear instrumental variables
    model, DML applied to learn the average treatment eﬀect and the average
    treatment eﬀect on the treated under unconfoundedness, and DML applied to
    learn the local average treatment eﬀect in an instrumental variables
    setting. In addition to these theoretical applications, we also illustrate
    the use of DML in three empirical examples.
  accessed:
    - year: 2023
      month: 8
      day: 30
  author:
    - family: Chernozhukov
      given: Victor
    - family: Chetverikov
      given: Denis
    - family: Demirer
      given: Mert
    - family: Duflo
      given: Esther
    - family: Hansen
      given: Christian
    - family: Newey
      given: Whitney
    - family: Robins
      given: James
  citation-key: chernozhukov_double_2017
  issued:
    - year: 2017
      month: 12
      day: 12
  language: en
  number: arXiv:1608.00060
  publisher: arXiv
  source: arXiv.org
  title: Double/Debiased Machine Learning for Treatment and Causal Parameters
  type: article
  URL: http://arxiv.org/abs/1608.00060

- id: chiarugi_adaptive_2007
  abstract: >-
    QRS detection performance can depend on the type of noise present in each
    lead involved in the overall processing. A common approach to QRS detection
    is based on a QRS enhanced signal obtained from the derivatives of the
    pre-filtered leads. However, the signal pre-filtering cannot be able to
    perform a complete noise rejection and the use of derivatives can enhance
    the noise as well. In many cases the noise occurs only on one lead and the
    addition of a noisy lead to the QRS enhanced signal decreases the overall
    detection performances of the QRS detector. For this reason the noise
    estimation on each channel, providing information for the channel inclusion
    or rejection in building the QRS enhanced signal, can improve the overall
    performances of the QRS detector. The results have been evaluated on the 48
    records of the MIT-BIH Arrhythmia Database where each ECG record is composed
    by 2 leads sampled at 360 Hz for a total duration of about 30 minutes. The
    annotated QRSs are 109494 in total. The results have been very satisfying on
    all the annotated QRSs and, with the inclusion of an automatic criterion for
    ventricular flutter detection, a sensitivity=99.76% and a positive
    predictive value=99.81% have been obtained.
  author:
    - family: Chiarugi
      given: F.
    - family: Sakkalis
      given: V.
    - family: Emmanouilidou
      given: D.
    - family: Krontiris
      given: T.
    - family: Varanini
      given: M.
    - family: Tollis
      given: I.
  citation-key: chiarugi_adaptive_2007
  container-title: 2007 Computers in Cardiology
  DOI: 10.1109/CIC.2007.4745445
  event-title: 2007 Computers in Cardiology
  issued:
    - year: 2007
      month: 9
  page: 157-160
  source: IEEE Xplore
  title: >-
    Adaptive threshold QRS detector with best channel selection based on a noise
    rating system
  type: paper-conference

- id: chicco_machine_2020
  abstract: >-
    Cardiovascular diseases kill approximately 17 million people globally every
    year, and they mainly exhibit as myocardial infarctions and heart failures.
    Heart failure (HF) occurs when the heart cannot pump enough blood to meet
    the needs of the body.Available electronic medical records of patients
    quantify symptoms, body features, and clinical laboratory test values, which
    can be used to perform biostatistics analysis aimed at highlighting patterns
    and correlations otherwise undetectable by medical doctors. Machine
    learning, in particular, can predict patients’ survival from their data and
    can individuate the most important features among those included in their
    medical records.
  author:
    - family: Chicco
      given: Davide
    - family: Jurman
      given: Giuseppe
  citation-key: chicco_machine_2020
  container-title: BMC Medical Informatics and Decision Making
  container-title-short: BMC Medical Informatics and Decision Making
  DOI: 10.1186/s12911-020-1023-5
  ISSN: 1472-6947
  issue: '1'
  issued:
    - year: 2020
      month: 2
      day: 3
  page: '16'
  title: >-
    Machine learning can predict survival of patients with heart failure from
    serum creatinine and ejection fraction alone
  type: article-journal
  URL: https://doi.org/10.1186/s12911-020-1023-5
  volume: '20'

- id: chiu_stateoftheart_2017
  abstract: >-
    Attention-based encoder-decoder architectures such as Listen, Attend, and
    Spell (LAS), subsume the acoustic, pronunciation and language model
    components of a traditional automatic speech recognition (ASR) system into a
    single neural network. In previous work, we have shown that such
    architectures are comparable to state-of-theart ASR systems on dictation
    tasks, but it was not clear if such architectures would be practical for
    more challenging tasks such as voice search. In this work, we explore a
    variety of structural and optimization improvements to our LAS model which
    significantly improve performance. On the structural side, we show that word
    piece models can be used instead of graphemes. We also introduce a
    multi-head attention architecture, which offers improvements over the
    commonly-used single-head attention. On the optimization side, we explore
    synchronous training, scheduled sampling, label smoothing, and minimum word
    error rate optimization, which are all shown to improve accuracy. We present
    results with a unidirectional LSTM encoder for streaming recognition. On a
    12, 500 hour voice search task, we find that the proposed changes improve
    the WER from 9.2% to 5.6%, while the best conventional system achieves 6.7%;
    on a dictation task our model achieves a WER of 4.1% compared to 5% for the
    conventional system.
  author:
    - family: Chiu
      given: Chung-Cheng
    - family: Sainath
      given: Tara N.
    - family: Wu
      given: Yonghui
    - family: Prabhavalkar
      given: Rohit
    - family: Nguyen
      given: Patrick
    - family: Chen
      given: Zhifeng
    - family: Kannan
      given: Anjuli
    - family: Weiss
      given: Ron J.
    - family: Rao
      given: Kanishka
    - family: Gonina
      given: Ekaterina
    - family: Jaitly
      given: Navdeep
    - family: Li
      given: Bo
    - family: Chorowski
      given: Jan
    - family: Bacchiani
      given: Michiel
  citation-key: chiu_stateoftheart_2017
  container-title: arXiv:1712.01769 [cs, eess, stat]
  issued:
    - year: 2017
      month: 12
      day: 5
  source: arXiv.org
  title: State-of-the-art Speech Recognition With Sequence-to-Sequence Models
  type: article-journal
  URL: http://arxiv.org/abs/1712.01769

- id: chiuso_nonparametric_2010
  abstract: >-
    Identification of sparse high dimensional linear systems pose sever
    challenges to off-the-shelf techniques for system identification. This is
    particularly so when relatively small data sets, as compared to the number
    of inputs and outputs, have to be used. In this paper we introduce a new
    nonparametric technique which borrows ideas from a recently introduced
    Kernel estimator called “stable-spline” as well as from sparsity inducing
    priors which use ℓ1 penalty. We compare the new method with a group LAR-type
    of algorithm applied to estimation of sparse Vector Autoregressive models
    and to standard PEM methods.
  author:
    - family: Chiuso
      given: A.
    - family: Pillonetto
      given: G.
  citation-key: chiuso_nonparametric_2010
  container-title: 49th IEEE Conference on Decision and Control (CDC)
  DOI: 10.1109/CDC.2010.5717169
  event-title: 49th IEEE Conference on Decision and Control (CDC)
  issued:
    - year: 2010
      month: 12
  page: 2942-2947
  source: IEEE Xplore
  title: >-
    Nonparametric sparse estimators for identification of large scale linear
    systems
  type: paper-conference

- id: chizat_global_2018
  author:
    - family: Chizat
      given: Lénaïc
    - family: Bach
      given: Francis
  citation-key: chizat_global_2018
  container-title: Advances in neural information processing systems
  issued:
    - year: 2018
  title: >-
    On the global convergence of gradient descent for over-parameterized models
    using optimal transport
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2018/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf
  volume: '31'

- id: chizat_lazy_2019
  abstract: >-
    In a series of recent theoretical works, it was shown that strongly
    over-parameterized neural networks trained with gradient-based methods could
    converge exponentially fast to zero training loss, with their parameters
    hardly varying. In this work, we show that this "lazy training" phenomenon
    is not specific to over-parameterized neural networks, and is due to a
    choice of scaling, often implicit, that makes the model behave as its
    linearization around the initialization, thus yielding a model equivalent to
    learning with positive-definite kernels. Through a theoretical analysis, we
    exhibit various situations where this phenomenon arises in non-convex
    optimization and we provide bounds on the distance between the lazy and
    linearized optimization paths. Our numerical experiments bring a critical
    note, as we observe that the performance of commonly used non-linear deep
    convolutional neural networks in computer vision degrades when trained in
    the lazy regime. This makes it unlikely that "lazy training" is behind the
    many successes of neural networks in difficult high dimensional tasks.
  author:
    - family: Chizat
      given: Lenaic
    - family: Oyallon
      given: Edouard
    - family: Bach
      given: Francis
  citation-key: chizat_lazy_2019
  container-title: Advances in Neural Information Processing Systems 32
  issued:
    - year: 2019
  title: On Lazy Training in Differentiable Programming
  type: article-journal
  URL: http://arxiv.org/abs/1812.07956

- id: cho_kernel_2009
  author:
    - family: Cho
      given: Youngmin
    - family: Saul
      given: Lawrence
  citation-key: cho_kernel_2009
  container-title: Advances in neural information processing systems
  editor:
    - family: Bengio
      given: Y.
    - family: Schuurmans
      given: D.
    - family: Lafferty
      given: J.
    - family: Williams
      given: C.
    - family: Culotta
      given: A.
  issued:
    - year: 2009
  publisher: Curran Associates, Inc.
  title: Kernel methods for deep learning
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf
  volume: '22'

- id: cho_learning_2014
  accessed:
    - year: 2019
      month: 11
      day: 22
  author:
    - family: Cho
      given: Kyunghyun
    - family: Merriënboer
      given: Bart
      non-dropping-particle: van
    - family: Gulcehre
      given: Caglar
    - family: Bahdanau
      given: Dzmitry
    - family: Bougares
      given: Fethi
    - family: Schwenk
      given: Holger
    - family: Bengio
      given: Yoshua
  citation-key: cho_learning_2014
  container-title: >-
    Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)
  DOI: 10.3115/v1/D14-1179
  event-place: Doha, Qatar
  event-title: EMNLP 2014
  issued:
    - year: 2014
      month: 10
  page: 1724–1734
  publisher: Association for Computational Linguistics
  publisher-place: Doha, Qatar
  source: ACLWeb
  title: >-
    Learning Phrase Representations using RNN Encoder–Decoder for Statistical
    Machine Translation
  type: paper-conference
  URL: https://www.aclweb.org/anthology/D14-1179

- id: cho_properties_2014
  abstract: >-
    Neural machine translation is a relatively new approach to statistical
    machine translation based purely on neural networks. The neural machine
    translation models often consist of an encoder and a decoder. The encoder
    extracts a fixed-length representation from a variable-length input
    sentence, and the decoder generates a correct translation from this
    representation. In this paper, we focus on analyzing the properties of the
    neural machine translation using two models; RNN Encoder--Decoder and a
    newly proposed gated recursive convolutional neural network. We show that
    the neural machine translation performs relatively well on short sentences
    without unknown words, but its performance degrades rapidly as the length of
    the sentence and the number of unknown words increase. Furthermore, we find
    that the proposed gated recursive convolutional network learns a grammatical
    structure of a sentence automatically.
  author:
    - family: Cho
      given: Kyunghyun
    - family: Merrienboer
      given: Bart
      non-dropping-particle: van
    - family: Bahdanau
      given: Dzmitry
    - family: Bengio
      given: Yoshua
  citation-key: cho_properties_2014
  container-title: arXiv:1409.1259 [cs, stat]
  issued:
    - year: 2014
      month: 9
      day: 3
  source: arXiv.org
  title: 'On the Properties of Neural Machine Translation: Encoder-Decoder Approaches'
  title-short: On the Properties of Neural Machine Translation
  type: article-journal
  URL: http://arxiv.org/abs/1409.1259

- id: choi_tutorial_2020
  abstract: >-
    A polygenic score (PGS) or polygenic risk score (PRS) is an estimate of an
    individual’s genetic liability to a trait or disease, calculated according
    to their genotype profile and relevant genome-wide association study (GWAS)
    data. While present PRSs typically explain only a small fraction of trait
    variance, their correlation with the single largest contributor to
    phenotypic variation—genetic liability—has led to the routine application of
    PRSs across biomedical research. Among a range of applications, PRSs are
    exploited to assess shared etiology between phenotypes, to evaluate the
    clinical utility of genetic data for complex disease and as part of
    experimental studies in which, for example, experiments are performed that
    compare outcomes (e.g., gene expression and cellular response to treatment)
    between individuals with low and high PRS values. As GWAS sample sizes
    increase and PRSs become more powerful, PRSs are set to play a key role in
    research and stratified medicine. However, despite the importance and
    growing application of PRSs, there are limited guidelines for performing PRS
    analyses, which can lead to inconsistency between studies and
    misinterpretation of results. Here, we provide detailed guidelines for
    performing and interpreting PRS analyses. We outline standard quality
    control steps, discuss different methods for the calculation of PRSs,
    provide an introductory online tutorial, highlight common misconceptions
    relating to PRS results, offer recommendations for best practice and discuss
    future challenges.
  accessed:
    - year: 2024
      month: 1
      day: 24
  author:
    - family: Choi
      given: Shing Wan
    - family: Mak
      given: Timothy Shin-Heng
    - family: O’Reilly
      given: Paul F.
  citation-key: choi_tutorial_2020
  container-title: Nature Protocols
  container-title-short: Nat Protoc
  DOI: 10.1038/s41596-020-0353-1
  ISSN: 1750-2799
  issue: '9'
  issued:
    - year: 2020
      month: 9
  language: en
  license: 2020 The Author(s), under exclusive licence to Springer Nature Limited
  number: '9'
  page: 2759-2772
  publisher: Nature Publishing Group
  source: www.nature.com
  title: 'Tutorial: a guide to performing polygenic risk score analyses'
  title-short: Tutorial
  type: article-journal
  URL: https://www.nature.com/articles/s41596-020-0353-1
  volume: '15'

- id: chua_deep_2018
  abstract: >-
    Model-based reinforcement learning (RL) algorithms can attain excellent
    sample efficiency, but often lag behind the best model-free algorithms in
    terms of asymptotic performance. This is especially true with high-capacity
    parametric function approximators, such as deep networks. In this paper, we
    study how to bridge this gap, by employing uncertainty-aware dynamics
    models. We propose a new algorithm called probabilistic ensembles with
    trajectory sampling (PETS) that combines uncertainty-aware deep network
    dynamics models with sampling-based uncertainty propagation. Our comparison
    to state-of-the-art model-based and model-free deep RL algorithms shows that
    our approach matches the asymptotic performance of model-free algorithms on
    several challenging benchmark tasks, while requiring significantly fewer
    samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and
    Proximal Policy Optimization respectively on the half-cheetah task).
  accessed:
    - year: 2019
      month: 2
      day: 6
  author:
    - family: Chua
      given: Kurtland
    - family: Calandra
      given: Roberto
    - family: McAllister
      given: Rowan
    - family: Levine
      given: Sergey
  citation-key: chua_deep_2018
  container-title: arXiv:1805.12114 [cs, stat]
  issued:
    - year: 2018
      month: 5
      day: 30
  source: arXiv.org
  title: >-
    Deep Reinforcement Learning in a Handful of Trials using Probabilistic
    Dynamics Models
  type: article-journal
  URL: http://arxiv.org/abs/1805.12114

- id: chung_empirical_2014
  abstract: >-
    In this paper we compare different types of recurrent units in recurrent
    neural networks (RNNs). Especially, we focus on more sophisticated units
    that implement a gating mechanism, such as a long short-term memory (LSTM)
    unit and a recently proposed gated recurrent unit (GRU). We evaluate these
    recurrent units on the tasks of polyphonic music modeling and speech signal
    modeling. Our experiments revealed that these advanced recurrent units are
    indeed better than more traditional recurrent units such as tanh units.
    Also, we found GRU to be comparable to LSTM.
  author:
    - family: Chung
      given: Junyoung
    - family: Gulcehre
      given: Caglar
    - family: Cho
      given: KyungHyun
    - family: Bengio
      given: Yoshua
  citation-key: chung_empirical_2014
  container-title: arXiv:1412.3555 [cs]
  issued:
    - year: 2014
      month: 12
      day: 11
  source: arXiv.org
  title: Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
  type: article-journal
  URL: http://arxiv.org/abs/1412.3555

- id: cisse_parseval_2017
  archive: http://arxiv.org/abs/1704.08847
  author:
    - family: Cisse
      given: Moustapha
    - family: Bojanowski
      given: Piotr
    - family: Grave
      given: Edouard
    - family: Dauphin
      given: Yann
    - family: Usunier
      given: Nicolas
  citation-key: cisse_parseval_2017
  collection-title: ICML’17
  container-title: >-
    Proceedings of the 34th international conference on machine learning -
    volume 70
  issued:
    - year: 2017
  page: 854–863
  publisher: JMLR
  title: 'Parseval networks: Improving robustness to adversarial examples'
  type: paper-conference

- id: clark_what_2019
  abstract: >-
    Large pre-trained neural networks such as BERT have had great recent success
    in NLP, motivating a growing body of research investigating what aspects of
    language they are able to learn from unlabeled data. Most recent analysis
    has focused on model outputs (e.g., language model surprisal) or internal
    vector representations (e.g., probing classifiers). Complementary to these
    works, we propose methods for analyzing the attention mechanisms of
    pre-trained models and apply them to BERT. BERT's attention heads exhibit
    patterns such as attending to delimiter tokens, specific positional offsets,
    or broadly attending over the whole sentence, with heads in the same layer
    often exhibiting similar behaviors. We further show that certain attention
    heads correspond well to linguistic notions of syntax and coreference. For
    example, we find heads that attend to the direct objects of verbs,
    determiners of nouns, objects of prepositions, and coreferent mentions with
    remarkably high accuracy. Lastly, we propose an attention-based probing
    classifier and use it to further demonstrate that substantial syntactic
    information is captured in BERT's attention.
  accessed:
    - year: 2019
      month: 6
      day: 25
  author:
    - family: Clark
      given: Kevin
    - family: Khandelwal
      given: Urvashi
    - family: Levy
      given: Omer
    - family: Manning
      given: Christopher D.
  citation-key: clark_what_2019
  container-title: arXiv:1906.04341 [cs]
  issued:
    - year: 2019
      month: 6
      day: 10
  source: arXiv.org
  title: What Does BERT Look At? An Analysis of BERT's Attention
  title-short: What Does BERT Look At?
  type: article-journal
  URL: http://arxiv.org/abs/1906.04341

- id: clarke_jeffreys_1994
  abstract: >-
    We provide a rigorous proof that Jeffreys’ prior asymptotically maximizes
    Shannon’s mutual information between a sample of size n and the parameter.
    This was conjectured by Bernard0 (1979) and, despite the absence of a proof,
    forms the basis of the reference prior method in Bayesian statistical
    analysis. Our proof rests on an examination of large sample decision
    theoretic properties associated with the relative entropy or the
    Kullback-Leibler distance between probability density functions for
    independent and identically distributed random variables. For smooth
    finite-dimensional parametric families we derive an asymptotic expression
    for the minimax risk and for the related maximin risk. As a result, we show
    that, among continuous positive priors, Jeffreys’ prior uniquely achieves
    the asymptotic maximin value. In the discrete parameter case we show that,
    asymptotically, the Bayes risk reduces to the entropy of the prior so that
    the reference prior is seen to be the maximum entropy prior. We identify the
    physical significance of the risks by giving two information-theoretic
    interpretations in terms of probabilistic coding.
  accessed:
    - year: 2018
      month: 10
      day: 7
  author:
    - family: Clarke
      given: Bertrand S.
    - family: Barron
      given: Andrew R.
  citation-key: clarke_jeffreys_1994
  container-title: Journal of Statistical Planning and Inference
  DOI: 10.1016/0378-3758(94)90153-8
  ISSN: '03783758'
  issue: '1'
  issued:
    - year: 1994
      month: 8
  language: en
  page: 37-60
  source: Crossref
  title: Jeffreys' prior is asymptotically least favorable under entropy risk
  type: article-journal
  URL: http://linkinghub.elsevier.com/retrieve/pii/0378375894901538
  volume: '41'

- id: clarke_optimization_1990
  author:
    - family: Clarke
      given: Frank H.
  citation-key: clarke_optimization_1990
  DOI: 10.1137/1.9781611971309
  issued:
    - year: 1990
  publisher: Society for Industrial and Applied Mathematics
  title: Optimization and Nonsmooth Analysis
  type: book
  URL: https://epubs.siam.org/doi/abs/10.1137/1.9781611971309

- id: clifford_af_2017
  abstract: >-
    The PhysioNet/Computing in Cardiology (CinC) Challenge 2017 focused on
    differentiating AF from noise, normal or other rhythms in short term (from
    9–61 s) ECG recordings performed by patients. A total of 12,186 ECGs were
    used: 8,528 in the public training set and 3,658 in the private hidden test
    set. Due to the high degree of inter-expert disagreement between a
    significant fraction of the expert labels we implemented a mid-competition
    bootstrap approach to expert relabeling of the data, levering the best
    performing Challenge entrants’ algorithms to identify contentious labels., A
    total of 75 independent teams entered the Challenge using a variety of
    traditional and novel methods, ranging from random forests to a deep
    learning approach applied to the raw data in the spectral domain. Four teams
    won the Challenge with an equal high F1 score (averaged across all classes)
    of 0.83, although the top 11 algorithms scored within 2% of this. A
    combination of 45 algorithms identified using LASSO achieved an F1 of 0.87,
    indicating that a voting approach can boost performance.
  accessed:
    - year: 2018
      month: 10
      day: 21
  author:
    - family: Clifford
      given: Gari D
    - family: Liu
      given: Chengyu
    - family: Moody
      given: Benjamin
    - family: Lehman
      given: Li-wei H.
    - family: Silva
      given: Ikaro
    - family: Li
      given: Qiao
    - family: Johnson
      given: A E
    - family: Mark
      given: Roger G.
  citation-key: clifford_af_2017
  container-title: Computing in Cardiology
  container-title-short: Comput Cardiol (2010)
  ISSN: 2325-8861
  issued:
    - year: 2017
      month: 9
  PMCID: PMC5978770
  PMID: '29862307'
  source: PubMed Central
  title: >-
    AF Classification from a Short Single Lead ECG Recording: the
    PhysioNet/Computing in Cardiology Challenge 2017
  title-short: AF Classification from a Short Single Lead ECG Recording
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5978770/
  volume: '44'

- id: cohen_certified_2019
  abstract: >-
    We show how to turn any classifier that classifies well under Gaussian noise
    into a new classifier that is certifiably robust to adversarial
    perturbations under the $\ell_2$ norm. This "randomized smoothing" technique
    has been proposed recently in the literature, but existing guarantees are
    loose. We prove a tight robustness guarantee in $\ell_2$ norm for smoothing
    with Gaussian noise. We use randomized smoothing to obtain an ImageNet
    classifier with e.g. a certified top-1 accuracy of 49% under adversarial
    perturbations with $\ell_2$ norm less than 0.5 (=127/255). No certified
    defense has been shown feasible on ImageNet except for smoothing. On
    smaller-scale datasets where competing approaches to certified $\ell_2$
    robustness are viable, smoothing delivers higher certified accuracies. Our
    strong empirical results suggest that randomized smoothing is a promising
    direction for future research into adversarially robust classification. Code
    and models are available at http://github.com/locuslab/smoothing.
  accessed:
    - year: 2023
      month: 4
      day: 14
  author:
    - family: Cohen
      given: Jeremy M.
    - family: Rosenfeld
      given: Elan
    - family: Kolter
      given: J. Zico
  citation-key: cohen_certified_2019
  container-title: Proceedings of the International Conference on Machine Learning
  DOI: 10.48550/arXiv.1902.02918
  issued:
    - year: 2019
      month: 6
      day: 15
  page: 1310-1320
  publisher: arXiv
  source: arXiv.org
  title: Certified Adversarial Robustness via Randomized Smoothing
  type: paper-conference
  URL: http://arxiv.org/abs/1902.02918
  volume: '97'

- id: cohen_coefficient_1960
  accessed:
    - year: 2019
      month: 2
      day: 19
  author:
    - family: Cohen
      given: Jacob
  citation-key: cohen_coefficient_1960
  container-title: Educational and Psychological Measurement
  DOI: 10/dghsrr
  ISSN: 0013-1644, 1552-3888
  issue: '1'
  issued:
    - year: 1960
      month: 4
  language: en
  page: 37-46
  source: Crossref
  title: A Coefficient of Agreement for Nominal Scales
  type: article-journal
  URL: http://journals.sagepub.com/doi/10.1177/001316446002000104
  volume: '20'

- id: cohen_gradient_2020
  abstract: >-
    We empirically demonstrate that full-batch gradient descent on neural
    network training objectives typically operates in a regime we call the Edge
    of Stability. In this regime, the maximum...
  accessed:
    - year: 2021
      month: 3
      day: 19
  author:
    - family: Cohen
      given: Jeremy
    - family: Kaur
      given: Simran
    - family: Li
      given: Yuanzhi
    - family: Kolter
      given: J. Zico
    - family: Talwalkar
      given: Ameet
  citation-key: cohen_gradient_2020
  event-title: International Conference on Learning Representations
  issued:
    - year: 2020
      month: 9
      day: 28
  language: en
  source: openreview.net
  title: >-
    Gradient Descent on Neural Networks Typically Occurs at the Edge of
    Stability
  type: paper-conference
  URL: https://openreview.net/forum?id=jh-rTtvkGeM

- id: coleman_efficient_1995
  author:
    - family: Coleman
      given: Thomas F
    - family: Liao
      given: Aiping
  citation-key: coleman_efficient_1995
  container-title: Computational Optimization and Applications
  issue: '1'
  issued:
    - year: 1995
  page: 47–66
  title: >-
    An efficient trust region method for unconstrained discrete-time optimal
    control problems
  type: article-journal
  volume: '4'

- id: coleman_efficient_1995a
  author:
    - family: Coleman
      given: Thomas F
    - family: Liao
      given: Aiping
  citation-key: coleman_efficient_1995a
  container-title: Computational Optimization and Applications
  DOI: 10/dxsjw9
  issue: '1'
  issued:
    - year: 1995
  note: '00041'
  page: 47-66
  title: >-
    An Efficient Trust Region Method for Unconstrained Discrete-Time Optimal
    Control Problems
  type: article-journal
  volume: '4'

- id: combettes_lipschitz_2020
  abstract: >-
    Obtaining sharp Lipschitz constants for feed-forward neural networks is
    essential to assess their robustness in the face of perturbations of their
    inputs. We derive such constants in the context of a general layered network
    model involving compositions of nonexpansive averaged operators and affine
    operators. By exploiting this architecture, our analysis finely captures the
    interactions between the layers, yielding tighter Lipschitz constants than
    those resulting from the product of individual bounds for groups of layers.
    The proposed framework is shown to cover in particular many practical
    instances encountered in feed-forward neural networks. Our Lipschitz
    constant estimates are further improved in the case of structures employing
    scalar nonlinear functions, which include standard convolutional networks as
    special cases.
  accessed:
    - year: 2021
      month: 8
      day: 17
  author:
    - family: Combettes
      given: Patrick L.
    - family: Pesquet
      given: Jean-Christophe
  citation-key: combettes_lipschitz_2020
  container-title: arXiv:1903.01014 [math]
  issued:
    - year: 2020
      month: 6
      day: 20
  source: arXiv.org
  title: >-
    Lipschitz Certificates for Layered Network Structures Driven by Averaged
    Activation Operators
  type: article-journal
  URL: http://arxiv.org/abs/1903.01014

- id: condat_fast_2016
  abstract: >-
    A new algorithm is proposed to project, exactly and in finite time, a vector
    of arbitrary size onto a simplex or an $$l_1$$-norm ball. It can be viewed
    as a Gauss–Seidel-like variant of Michelot’s variable fixing algorithm; that
    is, the threshold used to fix the variables is updated after each element is
    read, instead of waiting for a full reading pass over the list of non-fixed
    elements. This algorithm is empirically demonstrated to be faster than
    existing methods.
  accessed:
    - year: 2024
      month: 5
      day: 22
  author:
    - family: Condat
      given: Laurent
  citation-key: condat_fast_2016
  container-title: Mathematical Programming
  container-title-short: Math. Program.
  DOI: 10.1007/s10107-015-0946-6
  ISSN: 1436-4646
  issue: '1'
  issued:
    - year: 2016
      month: 7
      day: 1
  language: en
  page: 575-585
  source: Springer Link
  title: Fast projection onto the simplex and the $$\pmb {l}_\mathbf {1}$$ball
  type: article-journal
  URL: https://doi.org/10.1007/s10107-015-0946-6
  volume: '158'

- id: conn_trustregion_2000
  author:
    - family: Conn
      given: A. R.
    - family: Gould
      given: Nicholas I. M.
    - family: Toint
      given: Ph L.
  call-number: QA402.5 .C6485 2000
  citation-key: conn_trustregion_2000
  collection-title: MPS-SIAM series on optimization
  event-place: Philadelphia, PA
  ISBN: 978-0-89871-460-9
  issued:
    - year: 2000
  number-of-pages: '959'
  publisher: Society for Industrial and Applied Mathematics
  publisher-place: Philadelphia, PA
  source: Library of Congress ISBN
  title: Trust-region methods
  type: book

- id: connally_predictionand_2007
  author:
    - family: Connally
      given: Patrick
    - family: Li
      given: Kang
    - family: Irwin
      given: George W
  citation-key: connally_predictionand_2007
  container-title: Neurocomputing
  issue: '4'
  issued:
    - year: 2007
  page: 819–827
  title: >-
    Prediction-and simulation-error based perceptron training: solution space
    analysis and a novel combined training scheme
  type: article-journal
  volume: '70'

- id: cooper_computability_2017
  author:
    - family: Cooper
      given: S.B.
  citation-key: cooper_computability_2017
  collection-title: Chapman Hall/CRC Mathematics Series
  ISBN: 978-1-351-99196-4
  issued:
    - year: 2017
  publisher: CRC Press
  title: Computability Theory
  type: book
  URL: https://books.google.com.br/books?id=hlc0DwAAQBAJ

- id: corbet_linux_2005
  author:
    - family: Corbet
      given: Jonathan
    - family: Rubini
      given: Alessandro
    - family: Kroah-Hartman
      given: Greg
    - family: Rubini
      given: Alessandro
  call-number: QA76.76.D49 R92 2005
  citation-key: corbet_linux_2005
  edition: 3rd ed
  event-place: Beijing ; Sebastopol, CA
  ISBN: 978-0-596-00590-0
  issued:
    - year: 2005
  number-of-pages: '615'
  publisher: O'Reilly
  publisher-place: Beijing ; Sebastopol, CA
  source: Library of Congress ISBN
  title: Linux device drivers
  type: book

- id: cormen_introduction_2009
  call-number: QA76.6 .C662 2009
  citation-key: cormen_introduction_2009
  edition: 3rd ed
  editor:
    - family: Cormen
      given: Thomas H.
  event-place: Cambridge, Mass
  ISBN: 978-0-262-03384-8 978-0-262-53305-8
  issued:
    - year: 2009
  note: 'OCLC: ocn311310321'
  number-of-pages: '1292'
  publisher: MIT Press
  publisher-place: Cambridge, Mass
  source: Library of Congress ISBN
  title: Introduction to algorithms
  type: book

- id: cortes_impact_2010
  abstract: >-
    Kernel approximation is commonly used to scale kernel-based algorithms to
    applications containing as many as several million instances. This paper
    analyzes the effect of such approximations in the kernel matrix on the
    hypothesis generated by several widely used learning algorithms. We give
    stability bounds based on the norm of the kernel approximation for these
    algorithms, including SVMs, KRR, and graph Laplacian-based regularization
    algorithms. These bounds help determine the degree of approximation that can
    be tolerated in the estimation of the kernel matrix. Our analysis is general
    and applies to arbitrary approximations of the kernel matrix. However, we
    also give a specific analysis of the Nystrom low-rank approximation in this
    context and report the results of experiments evaluating the quality of the
    Nystrom low-rank kernel approximation when used with ridge regression.
  author:
    - family: Cortes
      given: Corinna
    - family: Mohri
      given: Mehryar
    - family: Talwalkar
      given: Ameet
  citation-key: cortes_impact_2010
  collection-title: Proceedings of machine learning research
  container-title: >-
    Proceedings of the thirteenth international conference on artificial
    intelligence and statistics
  editor:
    - family: Teh
      given: Yee Whye
    - family: Titterington
      given: Mike
  event-place: Chia Laguna Resort, Sardinia, Italy
  issued:
    - year: 2010
      month: 5
      day: 13
    - year: 2010
      month: 5
      day: 15
  page: 113–120
  publisher: PMLR
  publisher-place: Chia Laguna Resort, Sardinia, Italy
  title: On the impact of kernel approximation on learning accuracy
  type: paper-conference
  URL: http://proceedings.mlr.press/v9/cortes10a.html
  volume: '9'

- id: cortez_modeling_2009
  abstract: >-
    We propose a data mining approach to predict human wine taste preferences
    that is based on easily available analytical tests at the certification
    step. A large dataset (when compared to other studies in this domain) is
    considered, with white and red vinho verde samples (from Portugal). Three
    regression techniques were applied, under a computationally efficient
    procedure that performs simultaneous variable and model selection. The
    support vector machine achieved promising results, outperforming the
    multiple regression and neural network methods. Such model is useful to
    support the oenologist wine tasting evaluations and improve wine production.
    Furthermore, similar techniques can help in target marketing by modeling
    consumer tastes from niche markets.
  accessed:
    - year: 2024
      month: 5
      day: 16
  author:
    - family: Cortez
      given: Paulo
    - family: Cerdeira
      given: António
    - family: Almeida
      given: Fernando
    - family: Matos
      given: Telmo
    - family: Reis
      given: José
  citation-key: cortez_modeling_2009
  collection-title: 'Smart Business Networks: Concepts and Empirical Evidence'
  container-title: Decision Support Systems
  container-title-short: Decision Support Systems
  DOI: 10.1016/j.dss.2009.05.016
  ISSN: 0167-9236
  issue: '4'
  issued:
    - year: 2009
      month: 11
      day: 1
  page: 547-553
  source: ScienceDirect
  title: Modeling wine preferences by data mining from physicochemical properties
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0167923609001377
  volume: '47'

- id: cover_elements_2006
  author:
    - family: Cover
      given: T. M.
    - family: Thomas
      given: Joy A.
  call-number: Q360 .C68 2006
  citation-key: cover_elements_2006
  edition: 2nd ed
  event-place: Hoboken, N.J
  ISBN: 978-0-471-24195-9
  issued:
    - year: 2006
  note: 'OCLC: ocm59879802'
  number-of-pages: '748'
  publisher: Wiley-Interscience
  publisher-place: Hoboken, N.J
  source: Library of Congress ISBN
  title: Elements of information theory
  type: book

- id: croce_robustbench_2021
  abstract: >-
    As a research community, we are still lacking a systematic understanding of
    the progress on adversarial robustness which often makes it hard to identify
    the most promising ideas in training robust models. A key challenge in
    benchmarking robustness is that its evaluation is often error-prone leading
    to robustness overestimation. Our goal is to establish a standardized
    benchmark of adversarial robustness, which as accurately as possible
    reflects the robustness of the considered models within a reasonable
    computational budget. To this end, we start by considering the image
    classification task and introduce restrictions (possibly loosened in the
    future) on the allowed models. We evaluate adversarial robustness with
    AutoAttack, an ensemble of white- and black-box attacks, which was recently
    shown in a large-scale study to improve almost all robustness evaluations
    compared to the original publications. To prevent overadaptation of new
    defenses to AutoAttack, we welcome external evaluations based on adaptive
    attacks, especially where AutoAttack flags a potential overestimation of
    robustness. Our leaderboard, hosted at https://robustbench.github.io/,
    contains evaluations of 120+ models and aims at reflecting the current state
    of the art in image classification on a set of well-defined tasks in
    $\ell_\infty$- and $\ell_2$-threat models and on common corruptions, with
    possible extensions in the future. Additionally, we open-source the library
    https://github.com/RobustBench/robustbench that provides unified access to
    80+ robust models to facilitate their downstream applications. Finally,
    based on the collected models, we analyze the impact of robustness on the
    performance on distribution shifts, calibration, out-of-distribution
    detection, fairness, privacy leakage, smoothness, and transferability.
  accessed:
    - year: 2023
      month: 4
      day: 13
  author:
    - family: Croce
      given: Francesco
    - family: Andriushchenko
      given: Maksym
    - family: Sehwag
      given: Vikash
    - family: Debenedetti
      given: Edoardo
    - family: Flammarion
      given: Nicolas
    - family: Chiang
      given: Mung
    - family: Mittal
      given: Prateek
    - family: Hein
      given: Matthias
  citation-key: croce_robustbench_2021
  container-title: NeurIPS Datasets and Benchmarks track
  DOI: 10.48550/arXiv.2010.09670
  issued:
    - year: 2021
      month: 10
      day: 31
  source: arXiv.org
  title: 'RobustBench: a standardized adversarial robustness benchmark'
  title-short: RobustBench
  type: paper-conference
  URL: http://arxiv.org/abs/2010.09670

- id: cubanski_neural_1994
  abstract: >-
    Neural Network for Detecting AF. Introduction: A neural network classifier
    has been designed, which is able to distinguish atrial fibrillation (AF)
    from other supraventricular arrhythmias in ambulatory (Holter) ECGs. Method
    and Results: The classification algorithm uses a rhythm analysts that
    considers the ECG to be a time series of RR interval durations. This is
    combined with an analysis of baseline morphology that considers the
    morphological characteristics of the non-QRS portions of the waveform. A
    back propagation-based neural network has been used as part of the
    classifier implementation. When applied to a library consisting exclusively
    of 42,970 examples of AF and other supraventricular rhythm disturbances
    validated by an experienced cardiologist, the algorithm demonstrated a
    sensitivity of 82.4% for 10-beat runs of paroxysmal atrial fibrillation
    (PAF) and a specificity of 96.6%. Since this system has been implemented as
    a postprocessor to a conventional automated Holter system, operating only on
    segments of ECG that are known to contain supraventricular arrhythmias
    rather than ventricular arrhythmias or sinus rhythm, it can be added to most
    existing Holter processing systems without significantly increasing the
    average time to process a tape. Conclusion: A neural network system has been
    designed, which can potentially provide, for the first time, an accurate,
    quantitative technique to determine the natural history of PAF and to
    evaluate potential treatments for PAF.
  accessed:
    - year: 2019
      month: 1
      day: 15
  author:
    - family: CUBANSKI
      given: DAVID
    - family: CYGANSKI
      given: DAVID
    - family: ANTMAN
      given: ELLIOTT M.
    - family: FELDMAN
      given: CHARLES L.
  citation-key: cubanski_neural_1994
  container-title: Journal of Cardiovascular Electrophysiology
  container-title-short: Journal of Cardiovascular Electrophysiology
  DOI: 10/crkd95
  ISSN: 1045-3873
  issue: '7'
  issued:
    - year: 1994
      month: 7
      day: 1
  page: 602-608
  title: >-
    A Neural Network System for Detection of Atrial Fibrillation in Ambulatory
    Electrocardiograms
  type: article-journal
  URL: https://doi.org/10.1111/j.1540-8167.1994.tb01301.x
  volume: '5'

- id: cucker_mathematical_2001
  accessed:
    - year: 2019
      month: 12
      day: 31
  author:
    - family: Cucker
      given: Felipe
    - family: Smale
      given: Steve
  citation-key: cucker_mathematical_2001
  container-title: Bulletin of the American Mathematical Society
  container-title-short: Bull. Amer. Math. Soc.
  DOI: 10.1090/S0273-0979-01-00923-5
  ISSN: 0273-0979
  issue: '01'
  issued:
    - year: 2001
      month: 10
      day: 5
  language: en
  page: 1-50
  source: DOI.org (Crossref)
  title: On the mathematical foundations of learning
  type: article-journal
  URL: http://www.ams.org/journal-getitem?pii=S0273-0979-01-00923-5
  volume: '39'

- id: cucker_mathematical_2002
  author:
    - family: Cucker
      given: Felipe
    - family: Smale
      given: Steve
  citation-key: cucker_mathematical_2002
  container-title: Bulletin of the American mathematical society
  DOI: 10.1090/S0273-0979-01-00923-5
  issue: '1'
  issued:
    - year: 2002
  page: 1–49
  source: Google Scholar
  title: On the mathematical foundations of learning
  type: article-journal
  volume: '39'

- id: curth_uturn_2023
  abstract: >-
    Conventional statistical wisdom established a well-understood relationship
    between model complexity and prediction error, typically presented as a
    _U-shaped curve_ reflecting a transition between under- and overfitting
    regimes. However, motivated by the success of overparametrized neural
    networks, recent influential work has suggested this theory to be generally
    incomplete, introducing an additional regime that exhibits a second descent
    in test error as the parameter count $p$ grows past sample size $n$ -- a
    phenomenon dubbed _double descent_. While most attention has naturally been
    given to the deep-learning setting, double descent was shown to emerge more
    generally across non-neural models: known cases include _linear regression,
    trees, and boosting_. In this work, we take a closer look at the evidence
    surrounding these more classical statistical machine learning methods and
    challenge the claim that observed cases of double descent truly extend the
    limits of a traditional U-shaped complexity-generalization curve therein. We
    show that once careful consideration is given to _what is being plotted_ on
    the x-axes of their double descent plots, it becomes apparent that there are
    implicitly multiple, distinct complexity axes along which the parameter
    count grows. We demonstrate that the second descent appears exactly (and
    _only_) when and where the transition between these underlying axes occurs,
    and that its location is thus _not_ inherently tied to the interpolation
    threshold $p=n$. We then gain further insight by adopting a classical
    nonparametric statistics perspective. We interpret the investigated methods
    as _smoothers_ and propose a generalized measure for the _effective_ number
    of parameters they use _on unseen examples_, using which we find that their
    apparent double descent curves do indeed fold back into more traditional
    convex shapes -- providing a resolution to the ostensible tension between
    double descent and traditional statistical intuition.
  accessed:
    - year: 2023
      month: 12
      day: 12
  author:
    - family: Curth
      given: Alicia
    - family: Jeffares
      given: Alan
    - family: Schaar
      given: Mihaela
      dropping-particle: van der
  citation-key: curth_uturn_2023
  event-title: Thirty-seventh Conference on Neural Information Processing Systems
  issued:
    - year: 2023
      month: 11
      day: 2
  language: en
  source: openreview.net
  title: >-
    A U-turn on Double Descent: Rethinking Parameter Counting in Statistical
    Learning
  title-short: A U-turn on Double Descent
  type: paper-conference
  URL: https://openreview.net/forum?id=O0Lz8XZT2b

- id: dacostalopes_controloriented_2015
  author:
    - family: Costa Lopes
      given: Francisco
      non-dropping-particle: da
    - family: Watanabe
      given: Edson H
    - family: Rolim
      given: Luís Guilherme B
  citation-key: dacostalopes_controloriented_2015
  container-title: IEEE Transactions on Industrial Electronics
  DOI: 10.1109/TIE.2015.2412519
  issue: '8'
  issued:
    - year: 2015
  page: 5155–5163
  title: >-
    A control-oriented model of a PEM fuel cell stack based on NARX and NOE
    neural networks
  type: article-journal
  volume: '62'

- id: dacostalopes_controloriented_2015a
  author:
    - family: Costa Lopes
      given: Francisco
      non-dropping-particle: da
    - family: Watanabe
      given: Edson H
    - family: Rolim
      given: Luís Guilherme B
  citation-key: dacostalopes_controloriented_2015a
  container-title: IEEE Transactions on Industrial Electronics
  DOI: 10/gfjwmk
  issue: '8'
  issued:
    - year: 2015
  page: 5155-5163
  title: >-
    A Control-Oriented Model of a PEM Fuel Cell Stack Based on NARX and NOE
    Neural Networks
  type: article-journal
  volume: '62'

- id: daehlen_nonlinear_2014
  author:
    - family: D\a ehlen
      given: Jon S
    - family: Eikrem
      given: Gisle Otto
    - family: Johansen
      given: Tor Arne
  citation-key: daehlen_nonlinear_2014
  container-title: Journal of Process Control
  issue: '7'
  issued:
    - year: 2014
  page: 1106–1120
  title: >-
    Nonlinear model predictive control using trust-region derivative-free
    optimization
  type: article-journal
  volume: '24'

- id: dai_coupled_2018
  accessed:
    - year: 2019
      month: 3
      day: 7
  author:
    - family: Dai
      given: Bo
    - family: Dai
      given: Hanjun
    - family: He
      given: Niao
    - family: Liu
      given: Weiyang
    - family: Liu
      given: Zhen
    - family: Chen
      given: Jianshu
    - family: Xiao
      given: Lin
    - family: Song
      given: Le
  citation-key: dai_coupled_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 9690–9700
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Coupled Variational Bayes via Optimization Embedding
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/8177-coupled-variational-bayes-via-optimization-embedding.pdf

- id: dai_semisupervised_2015
  abstract: >-
    We present two approaches that use unlabeled data to improve sequence
    learning with recurrent networks. The first approach is to predict what
    comes next in a sequence, which is a conventional language model in natural
    language processing. The second approach is to use a sequence autoencoder,
    which reads the input sequence into a vector and predicts the input sequence
    again. These two algorithms can be used as a "pretraining" step for a later
    supervised sequence learning algorithm. In other words, the parameters
    obtained from the unsupervised step can be used as a starting point for
    other supervised training models. In our experiments, we find that long
    short term memory recurrent networks after being pretrained with the two
    approaches are more stable and generalize better. With pretraining, we are
    able to train long short term memory recurrent networks up to a few hundred
    timesteps, thereby achieving strong performance in many text classification
    tasks, such as IMDB, DBpedia and 20 Newsgroups.
  accessed:
    - year: 2019
      month: 6
      day: 8
  author:
    - family: Dai
      given: Andrew M.
    - family: Le
      given: Quoc V.
  citation-key: dai_semisupervised_2015
  container-title: arXiv:1511.01432 [cs]
  issued:
    - year: 2015
      month: 11
      day: 4
  source: arXiv.org
  title: Semi-supervised Sequence Learning
  type: article-journal
  URL: http://arxiv.org/abs/1511.01432

- id: dai_transformerxl_2019
  abstract: >-
    Transformers have a potential of learning longer-term dependency, but are
    limited by a fixed-length context in the setting of language modeling. We
    propose a novel neural architecture Transformer-XL that enables learning
    dependency beyond a fixed length without disrupting temporal coherence. It
    consists of a segment-level recurrence mechanism and a novel positional
    encoding scheme. Our method not only enables capturing longer-term
    dependency, but also resolves the context fragmentation problem. As a
    result, Transformer-XL learns dependency that is 80% longer than RNNs and
    450% longer than vanilla Transformers, achieves better performance on both
    short and long sequences, and is up to 1,800+ times faster than vanilla
    Transformers during evaluation. Notably, we improve the state-of-the-art
    results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on
    WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without
    finetuning). When trained only on WikiText-103, Transformer-XL manages to
    generate reasonably coherent, novel text articles with thousands of tokens.
    Our code, pretrained models, and hyperparameters are available in both
    Tensorflow and PyTorch.
  accessed:
    - year: 2020
      month: 5
      day: 27
  author:
    - family: Dai
      given: Zihang
    - family: Yang
      given: Zhilin
    - family: Yang
      given: Yiming
    - family: Carbonell
      given: Jaime
    - family: Le
      given: Quoc V.
    - family: Salakhutdinov
      given: Ruslan
  citation-key: dai_transformerxl_2019
  container-title: arXiv:1901.02860 [cs, stat]
  issued:
    - year: 2019
      month: 6
      day: 2
  source: arXiv.org
  title: 'Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context'
  title-short: Transformer-XL
  type: article-journal
  URL: http://arxiv.org/abs/1901.02860

- id: dalvi_adversarial_2004
  abstract: >-
    Essentially all data mining algorithms assume that the data-generating
    process is independent of the data miner's activities. However, in many
    domains, including spam detection, intrusion detection, fraud detection,
    surveillance and counter-terrorism, this is far from the case: the data is
    actively manipulated by an adversary seeking to make the classifier produce
    false negatives. In these domains, the performance of a classifier can
    degrade rapidly after it is deployed, as the adversary learns to defeat it.
    Currently the only solution to this is repeated, manual, ad hoc
    reconstruction of the classifier. In this paper we develop a formal
    framework and algorithms for this problem. We view classification as a game
    between the classifier and the adversary, and produce a classifier that is
    optimal given the adversary's optimal strategy. Experiments in a spam
    detection domain show that this approach can greatly outperform a classifier
    learned in the standard way, and (within the parameters of the problem)
    automatically adapt the classifier to the adversary's evolving
    manipulations.
  author:
    - family: Dalvi
      given: Nilesh
    - family: Domingos
      given: Pedro
    - literal: Mausam
    - family: Sanghai
      given: Sumit
    - family: Verma
      given: Deepak
  citation-key: dalvi_adversarial_2004
  container-title: >-
    Proceedings of the tenth ACM SIGKDD international conference on knowledge
    discovery and data mining
  DOI: 10.1145/1014052.1014066
  ISBN: 1-58113-888-1
  issued:
    - year: 2004
  title: Adversarial classification
  type: paper-conference

- id: damour_underspecification_2020
  abstract: >-
    ML models often exhibit unexpectedly poor behavior when they are deployed in
    real-world domains. We identify underspecification as a key reason for these
    failures. An ML pipeline is underspecified when it can return many
    predictors with equivalently strong held-out performance in the training
    domain. Underspecification is common in modern ML pipelines, such as those
    based on deep learning. Predictors returned by underspecified pipelines are
    often treated as equivalent based on their training domain performance, but
    we show here that such predictors can behave very differently in deployment
    domains. This ambiguity can lead to instability and poor model behavior in
    practice, and is a distinct failure mode from previously identified issues
    arising from structural mismatch between training and deployment domains. We
    show that this problem appears in a wide variety of practical ML pipelines,
    using examples from computer vision, medical imaging, natural language
    processing, clinical risk prediction based on electronic health records, and
    medical genomics. Our results show the need to explicitly account for
    underspecification in modeling pipelines that are intended for real-world
    deployment in any domain.
  accessed:
    - year: 2020
      month: 11
      day: 11
  author:
    - family: D'Amour
      given: Alexander
    - family: Heller
      given: Katherine
    - family: Moldovan
      given: Dan
    - family: Adlam
      given: Ben
    - family: Alipanahi
      given: Babak
    - family: Beutel
      given: Alex
    - family: Chen
      given: Christina
    - family: Deaton
      given: Jonathan
    - family: Eisenstein
      given: Jacob
    - family: Hoffman
      given: Matthew D.
    - family: Hormozdiari
      given: Farhad
    - family: Houlsby
      given: Neil
    - family: Hou
      given: Shaobo
    - family: Jerfel
      given: Ghassen
    - family: Karthikesalingam
      given: Alan
    - family: Lucic
      given: Mario
    - family: Ma
      given: Yian
    - family: McLean
      given: Cory
    - family: Mincu
      given: Diana
    - family: Mitani
      given: Akinori
    - family: Montanari
      given: Andrea
    - family: Nado
      given: Zachary
    - family: Natarajan
      given: Vivek
    - family: Nielson
      given: Christopher
    - family: Osborne
      given: Thomas F.
    - family: Raman
      given: Rajiv
    - family: Ramasamy
      given: Kim
    - family: Sayres
      given: Rory
    - family: Schrouff
      given: Jessica
    - family: Seneviratne
      given: Martin
    - family: Sequeira
      given: Shannon
    - family: Suresh
      given: Harini
    - family: Veitch
      given: Victor
    - family: Vladymyrov
      given: Max
    - family: Wang
      given: Xuezhi
    - family: Webster
      given: Kellie
    - family: Yadlowsky
      given: Steve
    - family: Yun
      given: Taedong
    - family: Zhai
      given: Xiaohua
    - family: Sculley
      given: D.
  citation-key: damour_underspecification_2020
  container-title: arXiv:2011.03395
  issued:
    - year: 2020
      month: 11
      day: 6
  source: arXiv.org
  title: >-
    Underspecification Presents Challenges for Credibility in Modern Machine
    Learning
  type: article-journal
  URL: http://arxiv.org/abs/2011.03395

- id: dan_sharp_2020
  abstract: >-
    Adversarial robustness has become a fundamental requirement in modern
    machine learning applications. Yet, there has been surprisingly little
    statistical understanding so far. In this paper, we provide the first result
    of the *optimal* minimax guarantees for the excess risk for adversarially
    robust classification, under Gaussian mixture model proposed by
    schmidt2018adversarially. The results are stated in terms of the
    *Adversarial Signal-to-Noise Ratio (AdvSNR)*, which generalizes a similar
    notion for standard linear classification to the adversarial setting. For
    the Gaussian mixtures with AdvSNR value of r, we prove an excess risk lower
    bound of order Θ(e^-(½+o(1)) r²^ ᵈ⁄ₙ) and design a computationally efficient
    estimator that achieves this optimal rate. Our results built upon minimal
    assumptions while cover a wide spectrum of adversarial perturbations
    including ₚ balls for any p 1.
  author:
    - family: Dan
      given: Chen
    - family: Wei
      given: Yuting
    - family: Ravikumar
      given: Pradeep
  citation-key: dan_sharp_2020
  collection-title: Proceedings of Machine Learning Research
  container-title: International Conference on Machine Learning (ICML)
  editor:
    - family: III
      given: Hal Daumé
    - family: Singh
      given: Aarti
  issued:
    - year: 2020
      month: 7
      day: 13
    - year: 2020
      month: 7
      day: 18
  page: 2345–2355
  title: Sharp statistical guaratees for adversarially robust Gaussian classification
  type: paper-conference
  URL: https://proceedings.mlr.press/v119/dan20b.html
  volume: '119'

- id: daniel_realtime_2007
  accessed:
    - year: 2018
      month: 7
      day: 17
  author:
    - family: Daniel
      given: G
    - family: Lissa
      given: G
    - family: Redondo
      given: D Medina
    - family: Vásquez
      given: L
    - family: Zapata
      given: D
  citation-key: daniel_realtime_2007
  container-title: 'Journal of Physics: Conference Series'
  DOI: 10.1088/1742-6596/90/1/012013
  ISSN: 1742-6596
  issued:
    - year: 2007
      month: 11
      day: 1
  page: '012013'
  source: Crossref
  title: 'Real-time 3D vectorcardiography: an application for didactic use'
  title-short: Real-time 3D vectorcardiography
  type: article-journal
  URL: >-
    http://stacks.iop.org/1742-6596/90/i=1/a=012013?key=crossref.695ba585841996824e286ada8e1662f0
  volume: '90'

- id: daniely_most_2020
  accessed:
    - year: 2022
      month: 4
      day: 12
  author:
    - family: Daniely
      given: Amit
    - family: Shacham
      given: Hadas
  citation-key: daniely_most_2020
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2020
  page: 6629–6636
  source: Neural Information Processing Systems
  title: Most ReLU Networks Suffer from ell 2 Adversarial Perturbations
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2020/hash/497476fe61816251905e8baafdf54c23-Abstract.html
  volume: '33'

- id: dascoli_triple_2020
  author:
    - family: Ascoli
      given: Stéphane
      non-dropping-particle: 'd'' '
    - family: Sagun
      given: Levent
    - family: Biroli
      given: Giulio
  citation-key: dascoli_triple_2020
  container-title: Advances in Neural Information Processing Systems
  editor:
    - family: Larochelle
      given: H.
    - family: Ranzato
      given: M.
    - family: Hadsell
      given: R.
    - family: Balcan
      given: M. F.
    - family: Lin
      given: H.
  issued:
    - year: 2020
  page: 3058–3069
  publisher: Curran Associates, Inc.
  title: >-
    Triple descent and the two kinds of overfitting: where &amp; why do they
    appear?
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2020/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf
  volume: '33'

- id: daskalakis_limit_2018
  abstract: >-
    Motivated by applications in Optimization, Game Theory, and the training of
    Generative Adversarial Networks, the convergence properties of first order
    methods in min-max problems have received extensive study. It has been
    recognized that they may cycle, and there is no good understanding of their
    limit points when they do not. When they converge, do they converge to local
    min-max solutions? We characterize the limit points of two basic first order
    methods, namely Gradient Descent/Ascent (GDA) and Optimistic Gradient
    Descent Ascent (OGDA). We show that both dynamics avoid unstable critical
    points for almost all initializations. Moreover, for small step sizes and
    under mild assumptions, the set of \{OGDA\}-stable critical points is a
    superset of \{GDA\}-stable critical points, which is a superset of local
    min-max solutions (strict in some cases). The connecting thread is that the
    behavior of these dynamics can be studied from a dynamical systems
    perspective.
  accessed:
    - year: 2018
      month: 12
      day: 13
  author:
    - family: Daskalakis
      given: Constantinos
    - family: Panageas
      given: Ioannis
  citation-key: daskalakis_limit_2018
  container-title: arXiv:1807.03907 [math, stat]
  issued:
    - year: 2018
      month: 7
      day: 10
  source: arXiv.org
  title: The Limit Points of (Optimistic) Gradient Descent in Min-Max Optimization
  type: article-journal
  URL: http://arxiv.org/abs/1807.03907

- id: daubechies_iteratively_2010
  author:
    - family: Daubechies
      given: Ingrid
    - family: DeVore
      given: Ronald
    - family: Fornasier
      given: Massimo
    - family: Güntürk
      given: C Sinan
  citation-key: daubechies_iteratively_2010
  container-title: Communications on Pure and Applied Mathematics
  container-title-short: >-
    Communications on Pure and Applied Mathematics: A Journal Issued by the
    Courant Institute of Mathematical Sciences
  ISSN: 0010-3640
  issue: '1'
  issued:
    - year: 2010
  page: 1-38
  publisher: Wiley Online Library
  title: Iteratively reweighted least squares minimization for sparse recovery
  type: article-journal
  volume: '63'

- id: daubechies_ten_1992
  author:
    - family: Daubechies
      given: Ingrid
  call-number: QA403.3 .D38 1992
  citation-key: daubechies_ten_1992
  collection-number: '61'
  collection-title: CBMS-NSF regional conference series in applied mathematics
  event-place: Philadelphia, Pa
  ISBN: 978-0-89871-274-2
  issued:
    - year: 1992
  number-of-pages: '357'
  publisher: Society for Industrial and Applied Mathematics
  publisher-place: Philadelphia, Pa
  source: Library of Congress ISBN
  title: Ten lectures on wavelets
  type: book

- id: dauphin_language_2017
  author:
    - family: Dauphin
      given: Yann N
    - family: Fan
      given: Angela
    - family: Auli
      given: Michael
    - family: Grangier
      given: David
  citation-key: dauphin_language_2017
  event-title: >-
    Proceedings of the 34th International Conference on Machine Learning-Volume
    70
  issued:
    - year: 2017
  page: 933-941
  publisher: JMLR. org
  title: Language modeling with gated convolutional networks
  type: paper-conference

- id: davidon_variable_1991
  author:
    - family: Davidon
      given: William C
  citation-key: davidon_variable_1991
  container-title: SIAM Journal on Optimization
  DOI: 10.1137/0801001
  issue: '1'
  issued:
    - year: 1991
  page: 1–17
  title: Variable metric method for minimization
  type: article-journal
  volume: '1'

- id: davidon_variable_1991a
  author:
    - family: Davidon
      given: William C
  citation-key: davidon_variable_1991a
  container-title: SIAM Journal on Optimization
  DOI: 10/cjnhbp
  issue: '1'
  issued:
    - year: 1991
  note: '01872'
  page: 1-17
  title: Variable Metric Method for Minimization
  type: article-journal
  volume: '1'

- id: day_unsupervised_2007
  abstract: >-
    Abstract.  Summary: The advent of high-density, high-volume genomic data has
    created the need for tools to summarize large datasets at multiple scales.
    HMMSeg i
  accessed:
    - year: 2019
      month: 4
      day: 1
  author:
    - family: Day
      given: Nathan
    - family: Hemmaplardh
      given: Andrew
    - family: Thurman
      given: Robert E.
    - family: Stamatoyannopoulos
      given: John A.
    - family: Noble
      given: William S.
  citation-key: day_unsupervised_2007
  container-title: Bioinformatics
  container-title-short: Bioinformatics
  DOI: 10/bbjzck
  ISSN: 1367-4803
  issue: '11'
  issued:
    - year: 2007
      month: 6
      day: 1
  language: en
  page: 1424-1426
  source: academic.oup.com
  title: Unsupervised segmentation of continuous genomic data
  type: article-journal
  URL: https://academic.oup.com/bioinformatics/article/23/11/1424/200706
  volume: '23'

- id: dayan_qlearning_1992
  author:
    - family: Dayan
      given: Peter
    - family: Watkins
      given: CJCH
  citation-key: dayan_qlearning_1992
  container-title: Machine learning
  DOI: 10.1023/A:1022657612745
  issue: '3'
  issued:
    - year: 1992
  page: 279–292
  source: Google Scholar
  title: Q-learning
  type: article-journal
  volume: '8'

- id: deazevedo_does_2020
  author:
    - family: Azevedo
      given: Rodrigo
      non-dropping-particle: de
  citation-key: deazevedo_does_2020
  issued:
    - year: 2020
      month: 10
  title: >-
    Does gradient descent converge to a minimum-norm solution in least-squares
    problems?
  type: post-weblog
  URL: https://math.stackexchange.com/q/3499305

- id: decuyper_selecting_
  abstract: >-
    In this work we study the effect of using different types of excitation
    signals as training data when constructing black box nonlinear models. We
    focus in particular on the class of nonlinear systems which exhibit
    autonomous oscillations. Three type of excitation signals are considered:
    random-phase multisines, ﬁltered white noise and swept sines. It is shown
    that depending on the excitation signal, the resulting model can fail to
    reproduce the autonomous output. Results are presented for the forced Van
    der Pol oscillator and the oscillatory wake of an submerged circular
    cylinder in a ﬂow. It is moreover shown that broadband excitations such as
    multisines or noise, are appropriate signals when intending to capture the
    synchronisation principle observed for autonomous oscillators. The latter is
    shown on computational ﬂuid dynamic data of a submerged cylinder in a ﬂow.
  author:
    - family: Decuyper
      given: J
    - family: Troyer
      given: T De
    - family: Runacres
      given: M C
    - family: Tiels
      given: K
    - family: Schoukens
      given: J
  citation-key: decuyper_selecting_
  language: en
  page: '15'
  source: Zotero
  title: On selecting appropriate training data to model an au- tonomous oscillator
  type: article-journal

- id: defauw_clinically_2018
  abstract: >-
    The volume and complexity of diagnostic imaging is increasing at a pace
    faster than the availability of human expertise to interpret it. Artificial
    intelligence has shown great promise in classifying two-dimensional
    photographs of some common diseases and typically relies on databases of
    millions of annotated images. Until now, the challenge of reaching the
    performance of expert clinicians in a real-world clinical pathway with
    three-dimensional diagnostic scans has remained unsolved. Here, we apply a
    novel deep learning architecture to a clinically heterogeneous set of
    three-dimensional optical coherence tomography scans from patients referred
    to a major eye hospital. We demonstrate performance in making a referral
    recommendation that reaches or exceeds that of experts on a range of
    sight-threatening retinal diseases after training on only 14,884 scans.
    Moreover, we demonstrate that the tissue segmentations produced by our
    architecture act as a device-independent representation; referral accuracy
    is maintained when using tissue segmentations from a different type of
    device. Our work removes previous barriers to wider clinical use without
    prohibitive training data requirements across multiple pathologies in a
    real-world setting.
  author:
    - family: De  Fauw
      given: Jeffrey
    - family: Ledsam
      given: Joseph R.
    - family: Romera-Paredes
      given: Bernardino
    - family: Nikolov
      given: Stanislav
    - family: Tomasev
      given: Nenad
    - family: Blackwell
      given: Sam
    - family: Askham
      given: Harry
    - family: Glorot
      given: Xavier
    - family: O’Donoghue
      given: Brendan
    - family: Visentin
      given: Daniel
    - family: Driessche
      given: George
      non-dropping-particle: van den
    - family: Lakshminarayanan
      given: Balaji
    - family: Meyer
      given: Clemens
    - family: Mackinder
      given: Faith
    - family: Bouton
      given: Simon
    - family: Ayoub
      given: Kareem
    - family: Chopra
      given: Reena
    - family: King
      given: Dominic
    - family: Karthikesalingam
      given: Alan
    - family: Hughes
      given: Cían O.
    - family: Raine
      given: Rosalind
    - family: Hughes
      given: Julian
    - family: Sim
      given: Dawn A.
    - family: Egan
      given: Catherine
    - family: Tufail
      given: Adnan
    - family: Montgomery
      given: Hugh
    - family: Hassabis
      given: Demis
    - family: Rees
      given: Geraint
    - family: Back
      given: Trevor
    - family: Khaw
      given: Peng T.
    - family: Suleyman
      given: Mustafa
    - family: Cornebise
      given: Julien
    - family: Keane
      given: Pearse A.
    - family: Ronneberger
      given: Olaf
  citation-key: defauw_clinically_2018
  container-title: Nature Medicine
  container-title-short: Nature Medicine
  DOI: 10.1038/s41591-018-0107-6
  ISSN: 1546-170X
  issue: '9'
  issued:
    - year: 2018
      month: 9
      day: 1
  page: 1342-1350
  title: >-
    Clinically applicable deep learning for diagnosis and referral in retinal
    disease
  type: article-journal
  URL: https://doi.org/10.1038/s41591-018-0107-6
  volume: '24'

- id: defazio_saga_2014
  author:
    - family: Defazio
      given: Aaron
    - family: Bach
      given: Francis
    - family: Lacoste-Julien
      given: Simon
  citation-key: defazio_saga_2014
  container-title: Advances in Neural Information Processing Systems
  editor:
    - family: Ghahramani
      given: Z.
    - family: Welling
      given: M.
    - family: Cortes
      given: C.
    - family: Lawrence
      given: N.
    - family: Weinberger
      given: K.Q.
  issued:
    - year: 2014
  title: >-
    SAGA: A fast incremental gradient method with support for non-strongly
    convex composite objectives
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper_files/paper/2014/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf
  volume: '27'

- id: degroot_probability_2012
  author:
    - family: DeGroot
      given: Morris H.
    - family: Schervish
      given: Mark J.
  call-number: QA273 .D35 2012
  citation-key: degroot_probability_2012
  edition: 4th ed
  event-place: Boston
  ISBN: 978-0-321-50046-5
  issued:
    - year: 2012
  note: 'OCLC: ocn502674206'
  number-of-pages: '893'
  publisher: Addison-Wesley
  publisher-place: Boston
  source: Library of Congress ISBN
  title: Probability and statistics
  type: book

- id: dejesusrubio_stable_2017
  abstract: >-
    In this research, a modified Kalman filter is introduced for the adaptation
    of a neural network. The modified Kalman filter is an improved version of
    the extended Kalman filter based in the following two changes: (1) a term of
    the weights adaptation is modified in the modified algorithm to assure the
    uniform stability, convergence of the weights error, and local minimums
    avoidance, (2) the activation functions are used instead of the Jacobian
    terms in the modified algorithm to assure the boundedness of the weights
    error. The suggested algorithm is applied for the chaotic systems
    identification.
  author:
    - family: Jesús Rubio
      given: José
      non-dropping-particle: de
  citation-key: dejesusrubio_stable_2017
  container-title: Journal of the Franklin Institute
  container-title-short: Journal of the Franklin Institute
  DOI: 10/gcjcxf
  ISSN: 0016-0032
  issue: '16'
  issued:
    - year: 2017
      month: 11
      day: 1
  page: 7444-7462
  title: >-
    Stable Kalman filter and neural network for the chaotic systems
    identification
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0016003217304325
  volume: '354'

- id: delathauwer_computation_2004
  abstract: >-
    The canonical decomposition of higher-order tensors is a key tool in
    multilinear algebra. First we review the state of the art. Then we show
    that, under certain conditions, the problem can be rephrased as the
    simultaneous diagonalization, by equivalence or congruence, of a set of
    matrices. Necessary and sufficient conditions for the uniqueness of these
    simultaneous matrix decompositions are derived. In a next step, the problem
    can be translated into a simultaneous generalized Schur decomposition, with
    orthogonal unknowns [A.-J. van der Veen and A. Paulraj, IEEE Trans. Signal
    Process., 44 (1996), pp. 1136--1155]. A first-order perturbation analysis of
    the simultaneous generalized Schur decomposition is carried out. We discuss
    some computational techniques (including a new Jacobi algorithm) and
    illustrate their behavior by means of a number of numerical experiments.
  author:
    - family: De Lathauwer
      given: L.
    - family: De Moor
      given: B.
    - family: Vandewalle
      given: J.
  citation-key: delathauwer_computation_2004
  container-title: SIAM Journal on Matrix Analysis and Applications
  container-title-short: SIAM. J. Matrix Anal. & Appl.
  DOI: 10.1137/S089547980139786X
  ISSN: 0895-4798
  issue: '2'
  issued:
    - year: 2004
      month: 1
      day: 1
  page: 295-327
  source: epubs.siam.org (Atypon)
  title: >-
    Computation of the Canonical Decomposition by Means of a Simultaneous
    Generalized Schur Decomposition
  type: article-journal
  URL: http://epubs.siam.org/doi/abs/10.1137/S089547980139786X
  volume: '26'

- id: delathauwer_multilinear_2000
  abstract: >-
    We discuss a multilinear generalization of the singular value decomposition.
    There is a strong analogy between several properties of the matrix and the
    higher-order tensor decomposition; uniqueness, link with the matrix
    eigenvalue decomposition, first-order perturbation effects, etc., are
    analyzed. We investigate how tensor symmetries affect the decomposition and
    propose a multilinear generalization of the symmetric eigenvalue
    decomposition for pair-wise symmetric tensors.
  author:
    - family: De Lathauwer
      given: L.
    - family: De Moor
      given: B.
    - family: Vandewalle
      given: J.
  citation-key: delathauwer_multilinear_2000
  container-title: SIAM Journal on Matrix Analysis and Applications
  container-title-short: SIAM. J. Matrix Anal. & Appl.
  DOI: 10.1137/S0895479896305696
  ISSN: 0895-4798
  issue: '4'
  issued:
    - year: 2000
      month: 1
      day: 1
  page: 1253-1278
  source: epubs.siam.org (Atypon)
  title: A Multilinear Singular Value Decomposition
  type: article-journal
  URL: http://epubs.siam.org/doi/abs/10.1137/S0895479896305696
  volume: '21'

- id: delima_um_2010
  author:
    - family: Lima
      given: Danilo Alves
      non-dropping-particle: de
    - family: Pereira
      given: Guilherme AS
  citation-key: delima_um_2010
  issued:
    - year: 2010
  title: >-
    Um sistema de visao estéreo para navegaçao de um carro autônomo em ambientes
    com obstáculos
  type: article-journal

- id: delmoral_sequential_2006
  abstract: >-
    We propose a methodology to sample sequentially from a sequence of
    probability distributions that are deﬁned on a common space, each
    distribution being known up to a normalizing constant. These probability
    distributions are approximated by a cloud of weighted random samples which
    are propagated over time by using sequential Monte Carlo methods. This
    methodology allows us to derive simple algorithms to make parallel Markov
    chain Monte Carlo algorithms interact to perform global optimization and
    sequential Bayesian estimation and to compute ratios of normalizing
    constants. We illustrate these algorithms for various integration tasks
    arising in the context of Bayesian inference.
  accessed:
    - year: 2018
      month: 12
      day: 12
  author:
    - family: Del Moral
      given: Pierre
    - family: Doucet
      given: Arnaud
    - family: Jasra
      given: Ajay
  citation-key: delmoral_sequential_2006
  container-title: 'Journal of the Royal Statistical Society: Series B (Statistical Methodology)'
  DOI: 10/cfbsfg
  ISSN: 1369-7412, 1467-9868
  issue: '3'
  issued:
    - year: 2006
      month: 6
  language: en
  page: 411-436
  source: Crossref
  title: Sequential Monte Carlo samplers
  type: article-journal
  URL: http://doi.wiley.com/10.1111/j.1467-9868.2006.00553.x
  volume: '68'

- id: delrio_assessment_2011
  author:
    - family: Río
      given: B. Aldecoa Sánchez
      non-dropping-particle: del
    - family: Lopetegi
      given: T.
    - family: Romero
      given: I.
  citation-key: delrio_assessment_2011
  container-title: Computing in Cardiology, 2011
  issued:
    - year: 2011
  page: 609–612
  publisher: IEEE
  source: Google Scholar
  title: Assessment of different methods to estimate electrocardiogram signal quality
  type: paper-conference

- id: demoor_restricted_1991
  abstract: >-
    The restricted singular value decomposition (RSVD) is the factorization of a
    given matrix, relative to two other given matrices. It can be interpreted as
    the ordinary singular value decomposition with different inner products in
    row and column spaces. Its properties and structure, as well as its
    connection to generalized eigenvalue problems, canonical correlation
    analysis, and other generalizations of the singular value decomposition, are
    investigated in detail.Applications that are discussed include the analysis
    of the extended shorted operator, unitarily invariant norm minimization with
    rank constraints, rank minimization in matrix balls, the analysis and
    solution of linear matrix equations, rank minimization of a partitioned
    matrix, and the connection with generalized Schur complements, constrained
    linear and total linear least squares problems with mixed exact and noisy
    data, including a generalized Gauss–Markov estimation scheme.
  author:
    - family: De Moor
      given: B.
    - family: Golub
      given: G.
  citation-key: demoor_restricted_1991
  container-title: SIAM Journal on Matrix Analysis and Applications
  container-title-short: SIAM. J. Matrix Anal. & Appl.
  DOI: 10.1137/0612029
  ISSN: 0895-4798
  issue: '3'
  issued:
    - year: 1991
      month: 7
      day: 1
  page: 401-425
  source: epubs.siam.org (Atypon)
  title: 'The Restricted Singular Value Decomposition: Properties and Applications'
  title-short: The Restricted Singular Value Decomposition
  type: article-journal
  URL: http://epubs.siam.org/doi/abs/10.1137/0612029
  volume: '12'

- id: deng_comparison_2023
  abstract: >-
    The Pooled Cohort Equations (PCEs) are race- and sex-specific Cox
    proportional hazards (PH)-based models used for 10-year atherosclerotic
    cardiovascular disease (ASCVD) risk prediction with acceptable
    discrimination. In recent years, neural network models have gained
    increasing popularity with their success in image recognition and text
    classification. Various survival neural network models have been proposed by
    combining survival analysis and neural network architecture to take
    advantage of the strengths from both. However, the performance of these
    survival neural network models compared to each other and to PCEs in ASCVD
    prediction is unknown.
  accessed:
    - year: 2023
      month: 7
      day: 6
  author:
    - family: Deng
      given: Yu
    - family: Liu
      given: Lei
    - family: Jiang
      given: Hongmei
    - family: Peng
      given: Yifan
    - family: Wei
      given: Yishu
    - family: Zhou
      given: Zhiyang
    - family: Zhong
      given: Yizhen
    - family: Zhao
      given: Yun
    - family: Yang
      given: Xiaoyun
    - family: Yu
      given: Jingzhi
    - family: Lu
      given: Zhiyong
    - family: Kho
      given: Abel
    - family: Ning
      given: Hongyan
    - family: Allen
      given: Norrina B.
    - family: Wilkins
      given: John T.
    - family: Liu
      given: Kiang
    - family: Lloyd-Jones
      given: Donald M.
    - family: Zhao
      given: Lihui
  citation-key: deng_comparison_2023
  container-title: BMC Medical Research Methodology
  container-title-short: BMC Medical Research Methodology
  DOI: 10.1186/s12874-022-01829-w
  ISSN: 1471-2288
  issue: '1'
  issued:
    - year: 2023
      month: 1
      day: 24
  page: '22'
  source: BioMed Central
  title: >-
    Comparison of State-of-the-Art Neural Network Survival Models with the
    Pooled Cohort Equations for Cardiovascular Disease Risk Prediction
  type: article-journal
  URL: https://doi.org/10.1186/s12874-022-01829-w
  volume: '23'

- id: deng_imagenet_2009
  author:
    - family: Deng
      given: Jia
    - family: Dong
      given: Wei
    - family: Socher
      given: Richard
    - family: Li
      given: Li-Jia
    - family: Li
      given: Kai
    - family: Fei-Fei
      given: Li
  citation-key: deng_imagenet_2009
  container-title: 2009 IEEE conference on computer vision and pattern recognition
  issued:
    - year: 2009
  page: 248–255
  title: 'Imagenet: A large-scale hierarchical image database'
  type: paper-conference

- id: deng_mnist_2012
  author:
    - family: Deng
      given: L.
  citation-key: deng_mnist_2012
  container-title: IEEE Signal Processing Magazine
  container-title-short: IEEE Signal Processing Magazine
  DOI: 10.1109/MSP.2012.2211477
  ISSN: 1558-0792
  issue: '6'
  issued:
    - year: 2012
      month: 11
  page: 141-142
  title: The MNIST Database of Handwritten Digit Images for Machine Learning Research
  type: article-journal
  volume: '29'

- id: deng_model_2020
  abstract: >-
    We consider a model for logistic regression where only a subset of features
    of size p is used for training a linear classifier over n training samples.
    The classifier is obtained by running gradient-descent (GD) on the
    logistic-loss. For this model, we investigate the dependence of the
    classification error on the overparameterization ratio κ = p/n. First,
    building on known deterministic results on convergence properties of the GD,
    we uncover a phase-transition phenomenon for the case of Gaussian features:
    the classification error of GD is the same as that of the maximum-likelihood
    (ML) solution when κ <; κ\*, and that of the max-margin (SVM) solution when
    κ > κ\*. Next, using the convex Gaussian min-max theorem (CGMT), we sharply
    characterize the performance of both the ML and SVM solutions. Combining
    these results, we obtain curves that explicitly characterize the test error
    of GD for varying values of κ. The numerical results validate the
    theoretical predictions and unveil “double-descent” phenomena that
    complement similar recent observations in linear regression settings.
  author:
    - family: Deng
      given: Zeyu
    - family: Kammoun
      given: Abla
    - family: Thrampoulidis
      given: Christos
  citation-key: deng_model_2020
  container-title: >-
    ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and
    Signal Processing (ICASSP)
  DOI: 10.1109/ICASSP40776.2020.9053524
  event-title: >-
    ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and
    Signal Processing (ICASSP)
  ISSN: 2379-190X
  issued:
    - year: 2020
      month: 5
  page: 4267-4271
  source: IEEE Xplore
  title: A Model of Double Descent for High-Dimensional Logistic Regression
  type: paper-conference

- id: dennis_quasinewton_1977
  author:
    - family: Dennis
      given: John E
      suffix: Jr
    - family: Moré
      given: Jorge J
  citation-key: dennis_quasinewton_1977
  container-title: SIAM review
  DOI: 10.1137/1019005
  issue: '1'
  issued:
    - year: 1977
  page: 46–89
  title: Quasi-Newton methods, motivation and theory
  type: article-journal
  volume: '19'

- id: dennis_two_1979
  author:
    - family: Dennis
      given: John E
    - family: Mei
      given: H. H. W.
  citation-key: dennis_two_1979
  container-title: Journal of Optimization Theory and Applications
  DOI: 10.1007/BF00932218
  issue: '4'
  issued:
    - year: 1979
  page: 453–482
  title: >-
    Two new unconstrained optimization algorithms which use function and
    gradient values
  type: article-journal
  volume: '28'

- id: dennisjr_adaptive_1981
  author:
    - family: Dennis Jr
      given: John E
    - family: Gay
      given: David M
    - family: Walsh
      given: Roy E
  citation-key: dennisjr_adaptive_1981
  container-title: ACM Transactions on Mathematical Software (TOMS)
  issue: '3'
  issued:
    - year: 1981
  page: 348–368
  title: An adaptive nonlinear least-squares algorithm
  type: article-journal
  volume: '7'

- id: dennisjr_adaptive_1981a
  author:
    - family: Dennis Jr
      given: John E
    - family: Gay
      given: David M
    - family: Walsh
      given: Roy E
  citation-key: dennisjr_adaptive_1981a
  container-title: ACM Transactions on Mathematical Software (TOMS)
  issue: '3'
  issued:
    - year: 1981
  note: '00000'
  page: 348-368
  title: An Adaptive Nonlinear Least-Squares Algorithm
  type: article-journal
  volume: '7'

- id: dennisjr_numerical_1996
  author:
    - family: Dennis Jr
      given: John E
    - family: Schnabel
      given: Robert B
  citation-key: dennisjr_numerical_1996
  ISBN: 1-61197-120-9
  issued:
    - year: 1996
  publisher: Siam
  title: Numerical methods for unconstrained optimization and nonlinear equations
  type: book
  volume: '16'

- id: dennisjr_quasinewton_1977
  author:
    - family: Dennis, Jr
      given: John E
    - family: Moré
      given: Jorge J
  citation-key: dennisjr_quasinewton_1977
  container-title: SIAM review
  DOI: 10/cxhdhx
  issue: '1'
  issued:
    - year: 1977
  note: '01684'
  page: 46-89
  title: Quasi-Newton Methods, Motivation and Theory
  type: article-journal
  volume: '19'

- id: derezinski_improved_2020
  author:
    - family: Derezinski
      given: Michal
    - family: Khanna
      given: Rajiv
    - family: Mahoney
      given: Michael W
  citation-key: derezinski_improved_2020
  container-title: Advances in Neural Information Processing Systems
  editor:
    - family: Larochelle
      given: H.
    - family: Ranzato
      given: M.
    - family: Hadsell
      given: R.
    - family: Balcan
      given: M. F.
    - family: Lin
      given: H.
  issued:
    - year: 2020
  page: 4953–4964
  publisher: Curran Associates, Inc.
  title: >-
    Improved guarantees and a multiple-descent curve for Column Subset Selection
    and the Nystrom method
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2020/file/342c472b95d00421be10e9512b532866-Paper.pdf
  volume: '33'

- id: devet_when_2006
  abstract: >-
    Background: Reproducibility concerns the degree to which repeated
    measurements provide similar results. Agreement parameters assess how close
    the results of the repeated measurements are, by estimating the measurement
    error in repeated measurements. Reliability parameters assess whether study
    objects, often persons, can be distinguished from each other, despite
    measurement errors. In that case, the measurement error is related to the
    variability between persons. Consequently, reliability parameters are highly
    dependent on the heterogeneity of the study sample, while the agreement
    parameters, based on measurement error, are more a pure characteristic of
    the measurement instrument.

    Methods and Results: Using an example of an interrater study, in which
    different physical therapists measure the range of motion of the arm in
    patients with shoulder complaints, the differences and relationships between
    reliability and agreement parameters for continuous variables are
    illustrated.

    Conclusion: If the research question concerns the distinction of persons,
    reliability parameters are the most appropriate. But if the aim is to
    measure change in health status, which is often the case in clinical
    practice, parameters of agreement are preferred. Ó 2006 Elsevier Inc. All
    rights reserved.
  accessed:
    - year: 2023
      month: 8
      day: 30
  author:
    - family: De Vet
      given: Henrica C.W.
    - family: Terwee
      given: Caroline B.
    - family: Knol
      given: Dirk L.
    - family: Bouter
      given: Lex M.
  citation-key: devet_when_2006
  container-title: Journal of Clinical Epidemiology
  container-title-short: Journal of Clinical Epidemiology
  DOI: 10.1016/j.jclinepi.2005.10.015
  ISSN: '08954356'
  issue: '10'
  issued:
    - year: 2006
      month: 10
  language: en
  page: 1033-1039
  source: DOI.org (Crossref)
  title: When to use agreement versus reliability measures
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0895435606000291
  volume: '59'

- id: devlin_bert_2018
  abstract: >-
    We introduce a new language representation model called BERT, which stands
    for Bidirectional Encoder Representations from Transformers. Unlike recent
    language representation models, BERT is designed to pre-train deep
    bidirectional representations from unlabeled text by jointly conditioning on
    both left and right context in all layers. As a result, the pre-trained BERT
    model can be fine-tuned with just one additional output layer to create
    state-of-the-art models for a wide range of tasks, such as question
    answering and language inference, without substantial task-specific
    architecture modifications. BERT is conceptually simple and empirically
    powerful. It obtains new state-of-the-art results on eleven natural language
    processing tasks, including pushing the GLUE score to 80.5% (7.7% point
    absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute
    improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point
    absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute
    improvement).
  accessed:
    - year: 2019
      month: 6
      day: 8
  author:
    - family: Devlin
      given: Jacob
    - family: Chang
      given: Ming-Wei
    - family: Lee
      given: Kenton
    - family: Toutanova
      given: Kristina
  citation-key: devlin_bert_2018
  container-title: arXiv:1810.04805 [cs]
  issued:
    - year: 2018
      month: 10
      day: 10
  source: arXiv.org
  title: >-
    BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding
  title-short: BERT
  type: article-journal
  URL: http://arxiv.org/abs/1810.04805

- id: diaconescu_use_2008
  author:
    - family: Diaconescu
      given: Eugen
  citation-key: diaconescu_use_2008
  container-title: WSEAS Transactions on Computer Research
  issue: '3'
  issued:
    - year: 2008
  page: 182–191
  title: The use of NARX neural networks to predict chaotic time series
  type: article-journal
  volume: '3'

- id: diamond_cvxpy_2016
  author:
    - family: Diamond
      given: Steven
    - family: Boyd
      given: Stephen
  citation-key: diamond_cvxpy_2016
  container-title: Journal of Machine Learning Research
  issue: '83'
  issued:
    - year: 2016
  page: 1–5
  title: 'CVXPY: A Python-embedded modeling language for convex optimization'
  type: article-journal
  volume: '17'

- id: diehl_fast_2006
  author:
    - family: Diehl
      given: Moritz
    - family: Bock
      given: Hans Georg
    - family: Diedam
      given: Holger
    - family: Wieber
      given: P-B
  citation-key: diehl_fast_2006
  container-title: Fast motions in biomechanics and robotics
  issued:
    - year: 2006
  page: 65-93
  publisher: Springer
  title: Fast direct multiple shooting algorithms for optimal robot control
  type: chapter

- id: dietterich_approximate_1998
  accessed:
    - year: 2018
      month: 11
      day: 22
  author:
    - family: Dietterich
      given: Thomas G.
  citation-key: dietterich_approximate_1998
  container-title: Neural Computation
  DOI: 10/fqc9w5
  ISSN: 0899-7667, 1530-888X
  issue: '7'
  issued:
    - year: 1998
      month: 10
  language: en
  page: 1895-1923
  source: Crossref
  title: >-
    Approximate Statistical Tests for Comparing Supervised Classification
    Learning Algorithms
  type: article-journal
  URL: http://www.mitpressjournals.org/doi/10.1162/089976698300017197
  volume: '10'

- id: dimopoulos_analog_2012
  author:
    - family: Dimopoulos
      given: Hercules G.
  call-number: TK7872.F5 D56 2012
  citation-key: dimopoulos_analog_2012
  collection-title: Analog circuits and signal processing
  event-place: Dordrecht ; New York
  ISBN: 978-94-007-2189-0 978-94-007-2190-6
  issued:
    - year: 2012
  number-of-pages: '577'
  publisher: Springer
  publisher-place: Dordrecht ; New York
  source: Library of Congress ISBN
  title: 'Analog electronic filters: theory, design and synthesis'
  title-short: Analog electronic filters
  type: book

- id: ding_r1pca_2006
  abstract: >-
    Principal component analysis (PCA) minimizes the sum of squared errors
    (L2-norm) and is sensitive to the presence of outliers. We propose a
    rotational invariant L1-norm PCA (R1-PCA). R1-PCA is similar to PCA in that
    (1) it has a unique global solution, (2) the solution are principal
    eigenvectors of a robust covariance matrix (re-weighted to soften the
    effects of outliers), (3) the solution is rotational invariant. These
    properties are not shared by the L1-norm PCA. A new subspace iteration
    algorithm is given to compute R1-PCA efficiently. Experiments on several
    real-life datasets show R1-PCA can effectively handle outliers. We extend
    R1-norm to K-means clustering and show that L1-norm K-means leads to poor
    results while R1-K-means outperforms standard K-means.
  author:
    - family: Ding
      given: Chris
    - family: Zhou
      given: Ding
    - family: He
      given: Xiaofeng
    - family: Zha
      given: Hongyuan
  citation-key: ding_r1pca_2006
  collection-title: ICML '06
  container-title: Proceedings of the 23rd international conference on machine learning
  DOI: 10.1145/1143844.1143880
  event-place: Pittsburgh, Pennsylvania, USA
  ISBN: 1-59593-383-2
  issued:
    - year: 2006
  number-of-pages: '8'
  page: 281–288
  publisher: Association for Computing Machinery
  publisher-place: Pittsburgh, Pennsylvania, USA
  title: >-
    R1-pca: Rotational invariant l1-norm principal component analysis for robust
    subspace factorization
  type: paper-conference
  URL: https://doi.org/10.1145/1143844.1143880

- id: diochnos_adversarial_2018
  author:
    - family: Diochnos
      given: Dimitrios
    - family: Mahloujifar
      given: Saeed
    - family: Mahmoody
      given: Mohammad
  citation-key: diochnos_adversarial_2018
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2018
  title: >-
    Adversarial risk and robustness: General definitions and implications for
    the uniform distribution
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2018/file/3483e5ec0489e5c394b028ec4e81f3e1-Paper.pdf
  volume: '31'

- id: dobriban_highdimensional_2018
  abstract: >-
    We provide a unified analysis of the predictive risk of ridge regression and
    regularized discriminant analysis in a dense random effects model. We work
    in a high-dimensional asymptotic regime where p,n→∞p,n→∞p,n\to\infty and
    p/n→γ>0p/n→γ>0p/n\to\gamma>0, and allow for arbitrary covariance among the
    features. For both methods, we provide an explicit and efficiently
    computable expression for the limiting predictive risk, which depends only
    on the spectrum of the feature-covariance matrix, the signal strength and
    the aspect ratio γγ\gamma. Especially in the case of regularized
    discriminant analysis, we find that predictive accuracy has a nuanced
    dependence on the eigenvalue distribution of the covariance matrix,
    suggesting that analyses based on the operator norm of the covariance matrix
    may not be sharp. Our results also uncover an exact inverse relation between
    the limiting predictive risk and the limiting estimation risk in
    high-dimensional linear models. The analysis builds on recent advances in
    random matrix theory.
  accessed:
    - year: 2020
      month: 11
      day: 26
  author:
    - family: Dobriban
      given: Edgar
    - family: Wager
      given: Stefan
  citation-key: dobriban_highdimensional_2018
  container-title: Annals of Statistics
  container-title-short: Ann. Statist.
  DOI: 10.1214/17-AOS1549
  ISSN: 0090-5364, 2168-8966
  issue: '1'
  issued:
    - year: 2018
      month: 2
  language: EN
  page: 247-279
  publisher: Institute of Mathematical Statistics
  source: Project Euclid
  title: >-
    High-dimensional asymptotics of prediction: Ridge regression and
    classification
  title-short: High-dimensional asymptotics of prediction
  type: article-journal
  URL: https://projecteuclid.org/euclid.aos/1519268430
  volume: '46'

- id: dobriban_provable_2022
  abstract: >-
    It is well known that machine learning methods can be vulnerable to
    adversarially-chosen perturbations of their inputs. Despite significant
    progress in the area, foundational open problems remain. In this paper, we
    address several key questions. We derive exact and approximate Bayes-optimal
    robust classifiers for the important setting of two- and three-class
    Gaussian classification problems with arbitrary imbalance, for $\ell_2$ and
    $\ell_\infty$ adversaries. In contrast to classical Bayes-optimal
    classifiers, determining the optimal decisions here cannot be made pointwise
    and new theoretical approaches are needed. We develop and leverage new
    tools, including recent breakthroughs from probability theory on robust
    isoperimetry, which, to our knowledge, have not yet been used in the area.
    Our results reveal fundamental tradeoffs between standard and robust
    accuracy that grow when data is imbalanced. We also show further results,
    including an analysis of classification calibration for convex losses in
    certain models, and finite sample rates for the robust risk.
  accessed:
    - year: 2023
      month: 10
      day: 3
  author:
    - family: Dobriban
      given: Edgar
    - family: Hassani
      given: Hamed
    - family: Hong
      given: David
    - family: Robey
      given: Alexander
  citation-key: dobriban_provable_2022
  container-title: arXiv:2006.05161
  DOI: 10.48550/arXiv.2006.05161
  issued:
    - year: 2022
      month: 1
      day: 30
  source: arXiv.org
  title: Provable tradeoffs in adversarially robust classification
  type: article-journal
  URL: http://arxiv.org/abs/2006.05161

- id: dohmatob_generalized_2019
  abstract: >-
    This manuscript presents some new impossibility results on adversarial
    robustness in machine learning, a very important yet largely open problem.
    We show that if conditioned on a class label the data distribution satisfies
    the W₂ Talagrand transportation-cost inequality (for example, this condition
    is satisfied if the conditional distribution has density which is
    log-concave; is the uniform measure on a compact Riemannian manifold with
    positive Ricci curvature, any classifier can be adversarially fooled with
    high probability once the perturbations are slightly greater than the
    natural noise level in the problem. We call this result The Strong "No Free
    Lunch" Theorem as some recent results (Tsipras et al. 2018, Fawzi et al.
    2018, etc.) on the subject can be immediately recovered as very particular
    cases. Our theoretical bounds are demonstrated on both simulated and real
    data (MNIST). We conclude the manuscript with some speculation on possible
    future research directions.
  author:
    - family: Dohmatob
      given: Elvis
  citation-key: dohmatob_generalized_2019
  collection-title: Proceedings of machine learning research
  container-title: Proceedings of the 36th international conference on machine learning
  editor:
    - family: Chaudhuri
      given: Kamalika
    - family: Salakhutdinov
      given: Ruslan
  issued:
    - year: 2019
      month: 6
      day: 9
    - year: 2019
      month: 6
      day: 15
  page: 1646–1654
  publisher: PMLR
  title: Generalized no free lunch theorem for adversarial robustness
  type: paper-conference
  URL: https://proceedings.mlr.press/v97/dohmatob19a.html
  volume: '97'

- id: dolan_benchmarking_2004
  author:
    - family: Dolan
      given: Elizabeth D
    - family: Moré
      given: Jorge J
    - family: Munson
      given: Todd S
  citation-key: dolan_benchmarking_2004
  issued:
    - year: 2004
  publisher: Argonne National Lab., Argonne, IL (US)
  title: Benchmarking optimization software with COPS 3.0.
  type: report

- id: dolan_benchmarking_2004a
  author:
    - family: Dolan
      given: Elizabeth D
    - family: Moré
      given: Jorge J
    - family: Munson
      given: Todd S
  citation-key: dolan_benchmarking_2004a
  issued:
    - year: 2004
  publisher: Argonne National Lab., Argonne, IL (US)
  title: Benchmarking Optimization Software with COPS 3.0.
  type: report

- id: dong_identification_2017
  author:
    - family: Dong
      given: Jianfei
    - family: Zhang
      given: Guoqi
  citation-key: dong_identification_2017
  container-title: IEEE Transactions on Industrial Electronics
  DOI: 10.1109/TIE.2016.2619659
  issue: '3'
  issued:
    - year: 2017
  page: 2215–2225
  title: >-
    Identification and Robust Control of the Nonlinear Photoelectrothermal
    Dynamics of LED Systems
  type: article-journal
  volume: '64'

- id: donoho_compressed_2006
  author:
    - family: Donoho
      given: David L.
  citation-key: donoho_compressed_2006
  container-title: IEEE Transactions on Information Theory
  issue: '4'
  issued:
    - year: 2006
  page: 1289-1306
  title: Compressed sensing
  type: article-journal
  volume: '52'

- id: doucet_tutorial_2009
  author:
    - family: Doucet
      given: Arnaud
    - family: Johansen
      given: Adam M
  citation-key: doucet_tutorial_2009
  container-title: Handbook of nonlinear filtering
  container-title-short: Handbook of nonlinear filtering
  issue: 656-704
  issued:
    - year: 2009
  page: '3'
  title: 'A tutorial on particle filtering and smoothing: Fifteen years later'
  type: article-journal
  volume: '12'

- id: doya_bifurcations_1993
  abstract: >-
    Asymptotic behavior of a recurrent neural network changes qualitatively at
    certain points in the parameter space, which are known as \bifurcation
    points". At bifurcation points, the output of a network can change
    discontinuously with the change of parameters and therefore convergence of
    gradient descent algorithms is not guaranteed. Furthermore, learning
    equations used for error gradient estimation can be unstable. However, some
    kinds of bifurcations are inevitable in training a recurrent network as an
    automaton or an oscillator. Some of the factors underlying successful
    training of recurrent networks are investigated, such as choice of initial
    connections, choice of input patterns, teacher forcing, and truncated
    learning equations.
  author:
    - family: Doya
      given: Kenji
  citation-key: doya_bifurcations_1993
  issued:
    - year: 1993
  source: Semantic Scholar
  title: Bifurcations of Recurrent Neural Networks in Gradient Descent Learning
  type: paper-conference

- id: draper_applied_1966
  author:
    - family: Draper
      given: Norman Richard
    - family: Smith
      given: Harry
    - family: Pownell
      given: Elizabeth
  citation-key: draper_applied_1966
  issued:
    - year: 1966
  publisher: Wiley New York
  title: Applied regression analysis
  type: book
  volume: '3'

- id: dreesen_recovering_2015
  abstract: >-
    This paper considers Wiener-Hammerstein systems consisting of a cascade of a
    linear dynamical system, a static nonlinearity and another linear dynamical
    system. We start from a black-box nonlinear state-space description of the
    system and develop a method to reconstruct the parameters of the underlying
    Wiener-Hammerstein block structure by means of linear algebra operations.
    First, the static nonlinearity is retrieved by decoupling the nonlinear part
    of the state-space equations into a single-branch nonlinear function. From
    there on, a canonical Wiener-Hammerstein nonlinear state-space model is
    recovered by using linear algebraic and geometric tools. The method is
    validated on a simulation example.
  accessed:
    - year: 2018
      month: 11
      day: 26
  author:
    - family: Dreesen
      given: Philippe
    - family: Ishteva
      given: Mariya
    - family: Schoukens
      given: Johan
  citation-key: dreesen_recovering_2015
  collection-title: 17th IFAC Symposium on System Identification SYSID 2015
  container-title: IFAC-PapersOnLine
  container-title-short: IFAC-PapersOnLine
  DOI: 10/gfkd8f
  ISSN: 2405-8963
  issue: '28'
  issued:
    - year: 2015
      month: 1
      day: 1
  page: 951-956
  source: ScienceDirect
  title: >-
    Recovering Wiener-Hammerstein nonlinear state-space models using linear
    algebra
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S2405896315028773
  volume: '48'

- id: drenkow_systematic_2022
  abstract: >-
    Deep neural networks for computer vision are deployed in increasingly
    safety-critical and socially-impactful applications, motivating the need to
    close the gap in model performance under varied, naturally occurring imaging
    conditions. Robustness, ambiguously used in multiple contexts including
    adversarial machine learning, refers here to preserving model performance
    under naturally-induced image corruptions or alterations. We perform a
    systematic review to identify, analyze, and summarize current definitions
    and progress towards non-adversarial robustness in deep learning for
    computer vision. We find this area of research has received
    disproportionately less attention relative to adversarial machine learning,
    yet a significant robustness gap exists that manifests in performance
    degradation similar in magnitude to adversarial conditions. Toward
    developing a more transparent definition of robustness, we provide a
    conceptual framework based on a structural causal model of the data
    generating process and interpret non-adversarial robustness as pertaining to
    a model's behavior on corrupted images corresponding to low-probability
    samples from the unaltered data distribution. We identify key architecture-,
    data augmentation-, and optimization tactics for improving neural network
    robustness. This robustness perspective reveals that common practices in the
    literature correspond to causal concepts. We offer perspectives on how
    future research may mind this evident and significant non-adversarial
    robustness gap.
  accessed:
    - year: 2023
      month: 4
      day: 4
  author:
    - family: Drenkow
      given: Nathan
    - family: Sani
      given: Numair
    - family: Shpitser
      given: Ilya
    - family: Unberath
      given: Mathias
  citation-key: drenkow_systematic_2022
  DOI: 10.48550/arXiv.2112.00639
  issued:
    - year: 2022
      month: 11
      day: 27
  number: arXiv:2112.00639
  publisher: arXiv
  source: arXiv.org
  title: >-
    A Systematic Review of Robustness in Deep Learning for Computer Vision: Mind
    the gap?
  title-short: A Systematic Review of Robustness in Deep Learning for Computer Vision
  type: article
  URL: http://arxiv.org/abs/2112.00639

- id: du_gradient_2018
  abstract: >-
    Gradient descent ﬁnds a global minimum in training deep neural networks
    despite the objective function being non-convex. The current paper proves
    gradient descent achieves zero training loss in polynomial time for a deep
    over-parameterized neural network with residual connections (ResNet). Our
    analysis relies on the particular structure of the Gram matrix induced by
    the neural network architecture. This structure allows us to show the Gram
    matrix is stable throughout the training process and this stability implies
    the global optimality of the gradient descent algorithm. Our bounds also
    shed light on the advantage of using ResNet over the fully connected
    feedforward architecture; our bound requires the number of neurons per layer
    scaling exponentially with depth for feedforward networks whereas for ResNet
    the bound only requires the number of neurons per layer scaling polynomially
    with depth. We further extend our analysis to deep residual convolutional
    neural networks and obtain a similar convergence result.
  accessed:
    - year: 2018
      month: 11
      day: 13
  author:
    - family: Du
      given: Simon S.
    - family: Lee
      given: Jason D.
    - family: Li
      given: Haochuan
    - family: Wang
      given: Liwei
    - family: Zhai
      given: Xiyu
  citation-key: du_gradient_2018
  container-title: arXiv:1811.03804 [cs, math, stat]
  issued:
    - year: 2018
      month: 11
      day: 9
  language: en
  note: '00000'
  source: arXiv.org
  title: Gradient Descent Finds Global Minima of Deep Neural Networks
  type: article-journal
  URL: http://arxiv.org/abs/1811.03804

- id: du_gradient_2019
  abstract: >-
    Gradient descent finds a global minimum in training deep neural networks
    despite the objective function being non-convex. The current paper proves
    gradient descent achieves zero training loss in polynomial time for a deep
    over-parameterized neural network with residual connections (ResNet). Our
    analysis relies on the particular structure of the Gram matrix induced by
    the neural network architecture. This structure allows us to show the Gram
    matrix is stable throughout the training process and this stability implies
    the global optimality of the gradient descent algorithm. We further extend
    our analysis to deep residual convolutional neural networks and obtain a
    similar convergence result.
  author:
    - family: Du
      given: Simon
    - family: Lee
      given: Jason
    - family: Li
      given: Haochuan
    - family: Wang
      given: Liwei
    - family: Zhai
      given: Xiyu
  citation-key: du_gradient_2019
  collection-title: Proceedings of machine learning research
  container-title: Proceedings of the 36th international conference on machine learning
  editor:
    - family: Chaudhuri
      given: Kamalika
    - family: Salakhutdinov
      given: Ruslan
  issued:
    - year: 2019
      month: 6
      day: 9
    - year: 2019
      month: 6
      day: 15
  page: 1675–1685
  publisher: PMLR
  title: Gradient descent finds global minima of deep neural networks
  type: paper-conference
  URL: http://proceedings.mlr.press/v97/du19c.html
  volume: '97'

- id: du_gradient_2019a
  abstract: >-
    One of the mysteries in the success of neural networks is randomly
    initialized first order methods like gradient descent can achieve zero
    training loss even though the objective function is non-convex and
    non-smooth. This paper demystifies this surprising phenomenon for two-layer
    fully connected ReLU activated neural networks. For an $m$ hidden node
    shallow neural network with ReLU activation and $n$ training data, we show
    as long as $m$ is large enough and no two inputs are parallel, randomly
    initialized gradient descent converges to a globally optimal solution at a
    linear convergence rate for the quadratic loss function. Our analysis relies
    on the following observation: over-parameterization and random
    initialization jointly restrict every weight vector to be close to its
    initialization for all iterations, which allows us to exploit a strong
    convexity-like property to show that gradient descent converges at a global
    linear rate to the global optimum. We believe these insights are also useful
    in analyzing deep models and other first order methods.
  accessed:
    - year: 2020
      month: 7
      day: 27
  author:
    - family: Du
      given: Simon S.
    - family: Zhai
      given: Xiyu
    - family: Poczos
      given: Barnabas
    - family: Singh
      given: Aarti
  citation-key: du_gradient_2019a
  container-title: arXiv:1810.02054 [cs, math, stat]
  issued:
    - year: 2019
      month: 2
      day: 4
  source: arXiv.org
  title: Gradient Descent Provably Optimizes Over-parameterized Neural Networks
  type: article-journal
  URL: http://arxiv.org/abs/1810.02054

- id: dubey_fundamentals_2002
  author:
    - family: Dubey
      given: G.K.
  citation-key: dubey_fundamentals_2002
  ISBN: 978-0-8493-2422-2
  issued:
    - year: 2002
  publisher: Alpha Science International
  title: Fundamentals of Electrical Drives
  type: book
  URL: https://books.google.com.br/books?id=2NsGKpLolsQC

- id: dummit_abstract_
  author:
    - family: Dummit
      given: David S.
    - family: Foote
      given: Richard M.
  citation-key: dummit_abstract_
  title: Abstract Algebra
  type: book

- id: dvoretzky_asymptotic_1956
  abstract: >-
    This paper is devoted, in the main, to proving the asymptotic minimax
    character of the sample distribution function (d.f.) for estimating an
    unknown d.f. in FF\mathscr{F} or FcFc\mathscr{F}_c (defined in Section 1)
    for a wide variety of weight functions. Section 1 contains definitions and a
    discussion of measurability considerations. Lemma 2 of Section 2 is an
    essential tool in our proofs and seems to be of interest per se; for
    example, it implies the convergence of the moment generating function of
    GnGnG_n to that of GGG (definitions in (2.1)). In Section 3 the asymptotic
    minimax character is proved for a fundamental class of weight functions
    which are functions of the maximum deviation between estimating and true
    d.f. In Section 4 a device (of more general applicability in decision
    theory) is employed which yields the asymptotic minimax result for a wide
    class of weight functions of this character as a consequence of the results
    of Section 3 for weight functions of the fundamental class. In Section 5 the
    asymptotic minimax character is proved for a class of integrated weight
    functions. A more general class of weight functions for which the asymptotic
    minimax character holds is discussed in Section 6. This includes weight
    functions for which the risk function of the sample d.f. is not a constant
    over Fc.Fc.\mathscr{F}_c. Most weight functions of practical interest are
    included in the considerations of Sections 3 to 6. Section 6 also includes a
    discussion of multinomial estimation problems for which the asymptotic
    minimax character of the classical estimator is contained in our results.
    Finally, Section 7 includes a general discussion of minimization of
    symmetric convex or monotone functionals of symmetric random elements, with
    special consideration of the "tied-down" Wiener process, and with a
    heuristic proof of the results of Sections 3, 4, 5, and much of Section 6.
  accessed:
    - year: 2019
      month: 4
      day: 16
  author:
    - family: Dvoretzky
      given: A.
    - family: Kiefer
      given: J.
    - family: Wolfowitz
      given: J.
  citation-key: dvoretzky_asymptotic_1956
  container-title: The Annals of Mathematical Statistics
  container-title-short: Ann. Math. Statist.
  DOI: 10/csg5cg
  ISSN: 0003-4851, 2168-8990
  issue: '3'
  issued:
    - year: 1956
      month: 9
  language: EN
  page: 642-669
  source: Project Euclid
  title: >-
    Asymptotic Minimax Character of the Sample Distribution Function and of the
    Classical Multinomial Estimator
  type: article-journal
  URL: https://projecteuclid.org/euclid.aoms/1177728174
  volume: '27'

- id: eckhard_cost_2017
  abstract: >-
    Identification of an output error model using the prediction error method
    leads to an optimization problem built on input/output data collected from
    the system to be identified. It is often hard to find the global solution of
    this optimization problem because in most cases both the corresponding
    objective function and the search space are nonconvex. The difficulty in
    solving the optimization problem depends mainly on the experimental
    conditions, more specifically on the spectra of the input/output data
    collected from the system. It is therefore possible to improve the
    convergence of the algorithms by properly choosing the data prefilters; in
    this paper we show how to perform this choice. We present the application of
    the proposed approach to case studies where the standard algorithms tend to
    fail to converge to the global minimum.
  accessed:
    - year: 2019
      month: 4
      day: 14
  author:
    - family: Eckhard
      given: Diego
    - family: Bazanella
      given: Alexandre S.
    - family: Rojas
      given: Cristian R.
    - family: Hjalmarsson
      given: Håkan
  citation-key: eckhard_cost_2017
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10/f9pgk7
  ISSN: 0005-1098
  issued:
    - year: 2017
      month: 2
      day: 1
  page: 53-60
  source: ScienceDirect
  title: Cost function shaping of the output error criterion
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0005109816304198
  volume: '76'

- id: efron_bootstrap_1979
  author:
    - family: Efron
      given: B.
  citation-key: efron_bootstrap_1979
  container-title: The Annals of Statistics
  container-title-short: The Annals of Statistics
  DOI: 10.1214/aos/1176344552
  issue: '1'
  issued:
    - year: 1979
      month: 1
      day: 1
  page: 1-26
  title: 'Bootstrap Methods: Another Look at the Jackknife'
  type: article-journal
  URL: https://doi.org/10.1214/aos/1176344552
  volume: '7'

- id: efron_introduction_1994
  author:
    - family: Efron
      given: Bradley
    - family: Tibshirani
      given: Robert J
  citation-key: efron_introduction_1994
  ISBN: 0-412-04231-2
  issued:
    - year: 1994
  publisher: CRC press
  title: An introduction to the bootstrap
  type: book

- id: efron_least_2004
  abstract: >-
    The purpose of model selection algorithms such as All Subsets, Forward
    Selection and Backward Elimination is to choose a linear model on the basis
    of the same set of data to which the model will be applied. Typically we
    have available a large collection of possible covariates from which we hope
    to select a parsimonious set for the efficient prediction of a response
    variable. Least Angle Regression (LARS), a new model selection algorithm, is
    a useful and less greedy version of traditional forward selection methods.
    Three main properties are derived: (1) A simple modification of the LARS
    algorithm implements the Lasso, an attractive version of ordinary least
    squares that constrains the sum of the absolute regression coefficients; the
    LARS modification calculates all possible Lasso estimates for a given
    problem, using an order of magnitude less computer time than previous
    methods. (2) A different LARS modification efficiently implements Forward
    Stagewise linear regression, another promising new model selection method;
    this connection explains the similar numerical results previously observed
    for the Lasso and Stagewise, and helps us understand the properties of both
    methods, which are seen as constrained versions of the simpler LARS
    algorithm. (3) A simple approximation for the degrees of freedom of a LARS
    estimate is available, from which we derive a Cp estimate of prediction
    error; this allows a principled choice among the range of possible LARS
    estimates. LARS and its variants are computationally efficient: the paper
    describes a publicly available algorithm that requires only the same order
    of magnitude of computational effort as ordinary least squares applied to
    the full set of covariates.
  accessed:
    - year: 2017
      month: 9
      day: 4
  author:
    - family: Efron
      given: Bradley
    - family: Hastie
      given: Trevor
    - family: Johnstone
      given: Iain
    - family: Tibshirani
      given: Robert
  citation-key: efron_least_2004
  container-title: The Annals of Statistics
  container-title-short: Ann. Statist.
  DOI: 10.1214/009053604000000067
  ISSN: 0090-5364, 2168-8966
  issue: '2'
  issued:
    - year: 2004
      month: 4
  page: 407-499
  source: Project Euclid
  title: Least angle regression
  type: article-journal
  URL: https://projecteuclid.org/euclid.aos/1083178935
  volume: '32'

- id: eiben_introduction_2003
  author:
    - family: Eiben
      given: Agoston E
    - family: Smith
      given: James E
  citation-key: eiben_introduction_2003
  issued:
    - year: 2003
  publisher: Springer
  title: Introduction to evolutionary computing
  type: book
  volume: '53'

- id: ek_offpolicy_
  abstract: >-
    We consider the problem of evaluating the performance of a decision policy
    using past observational data. The outcome of a policy is measured in terms
    of a loss (aka. disutility or negative reward) and the main problem is
    making valid inferences about its out-of-sample loss when the past data was
    observed under a different and possibly unknown policy. Using a
    sample-splitting method, we show that it is possible to draw such inferences
    with finitesample coverage guarantees about the entire loss distribution,
    rather than just its mean. Importantly, the method takes into account model
    misspecifications of the past policy –including unmeasured confounding. The
    evaluation method can be used to certify the performance of a policy using
    observational data under a specified range of credible model assumptions.
  author:
    - family: Ek
      given: Sofia
    - family: Zachariah
      given: Dave
    - family: Johansson
      given: Fredrik D
    - family: Stoica
      given: Petre
  citation-key: ek_offpolicy_
  language: en
  source: Zotero
  title: Off-Policy Evaluation with Out-of-Sample Guarantees
  type: article-journal

- id: eldar_asymptotic_2003
  abstract: >-
    We derive the asymptotic signal-to-interference ratio (SIR) of the
    decorrelator in the large system limit, both for the case in which the
    number of users exceeds the spreading gain and for the case in which the
    number of users is less than the spreading gain. We show that, contrary to
    what is claimed in [1], [2], when the number of users exceeds the spreading
    gain and the decorrelator is defined in terms of the Moore–Penrose
    pseudoinverse, the SIR does not converge to zero.
  accessed:
    - year: 2020
      month: 12
      day: 23
  author:
    - family: Eldar
      given: Y.C.
    - family: Chan
      given: A.M.
  citation-key: eldar_asymptotic_2003
  container-title: IEEE Transactions on Information Theory
  container-title-short: IEEE Trans. Inform. Theory
  DOI: 10.1109/TIT.2003.815781
  ISSN: 0018-9448
  issue: '9'
  issued:
    - year: 2003
      month: 9
  language: en
  page: 2309-2313
  source: DOI.org (Crossref)
  title: On the asymptotic performance of the decorrelator
  type: article-journal
  URL: http://ieeexplore.ieee.org/document/1226621/
  volume: '49'

- id: elvira_rethinking_2018
  accessed:
    - year: 2018
      month: 11
      day: 12
  author:
    - family: Elvira
      given: Víctor
    - family: Martino
      given: Luca
    - family: Robert
      given: Christian P.
  citation-key: elvira_rethinking_2018
  issued:
    - year: 2018
      month: 9
      day: 11
  language: en
  note: '00000'
  source: arxiv.org
  title: Rethinking the Effective Sample Size
  type: article-journal
  URL: https://arxiv.org/abs/1809.04129

- id: erhan_why_
  abstract: >-
    Much recent research has been devoted to learning algorithms for deep
    architectures such as Deep Belief Networks and stacks of autoencoder
    variants with impressive results being obtained in several areas, mostly on
    vision and language datasets. The best results obtained on supervised
    learning tasks often involve an unsupervised learning component, usually in
    an unsupervised pre-training phase. The main question investigated here is
    the following: why does unsupervised pre-training work so well? Through
    extensive experimentation, we explore several possible explanations
    discussed in the literature including its action as a regularizer (Erhan et
    al., 2009b) and as an aid to optimization (Bengio et al., 2007). Our results
    build on the work of Erhan et al. (2009b), showing that unsupervised
    pre-training appears to play predominantly a regularization role in
    subsequent supervised training. However our results in an online setting,
    with a virtually unlimited data stream, point to a somewhat more nuanced
    interpretation of the roles of optimization and regularization in the
    unsupervised pre-training effect.
  author:
    - family: Erhan
      given: Dumitru
    - family: Courville
      given: Aaron
    - family: Bengio
      given: Yoshua
    - family: Vincent
      given: Pascal
  citation-key: erhan_why_
  language: en
  page: '8'
  source: Zotero
  title: Why Does Unsupervised Pre-training Help Deep Learning?
  type: article-journal

- id: ernst_chromhmm_2012
  abstract: 'ChromHMM: automating chromatin-state discovery and characterization'
  accessed:
    - year: 2019
      month: 4
      day: 1
  author:
    - family: Ernst
      given: Jason
    - family: Kellis
      given: Manolis
  citation-key: ernst_chromhmm_2012
  container-title: Nature Methods
  DOI: 10/gffkzn
  ISSN: 1548-7105
  issue: '3'
  issued:
    - year: 2012
      month: 3
  language: en
  license: 2012 Nature Publishing Group
  page: 215-216
  source: www.nature.com
  title: 'ChromHMM: automating chromatin-state discovery and characterization'
  title-short: ChromHMM
  type: article-journal
  URL: https://www.nature.com/articles/nmeth.1906
  volume: '9'

- id: ernst_mutual_2008
  author:
    - family: Ernst
      given: Ines
    - family: Hirschmüller
      given: Heiko
  citation-key: ernst_mutual_2008
  container-title: Advances in Visual Computing
  issued:
    - year: 2008
  page: 228–239
  publisher: Springer
  title: Mutual information based semi-global stereo matching on the GPU
  type: chapter

- id: esfahani_polynomial_2017
  author:
    - family: Esfahani
      given: Alireza Fakhrizadeh
    - family: Dreesen
      given: Philippe
    - family: Tiels
      given: Koen
    - family: Noël
      given: Jean-Philippe
    - family: Schoukens
      given: Johan
  citation-key: esfahani_polynomial_2017
  container-title: IFAC-PapersOnLine
  DOI: 10.1016/j.ifacol.2017.08.082
  issue: '1'
  issued:
    - year: 2017
  page: 458–463
  source: Google Scholar
  title: >-
    Polynomial state-space model decoupling for the identification of hysteretic
    systems
  type: article-journal
  volume: '50'

- id: espinoza_kernel_2005
  abstract: >-
    In this note, we propose partially linear models with least squares support
    vector machines (LS-SVMs) for nonlinear ARX models. We illustrate how full
    black-box models can be improved when prior information about model
    structure is available. A real-life example, based on the Silverbox
    benchmark data, shows significant improvements in the generalization ability
    of the structured model with respect to the full black-box model, reflected
    also by a reduction in the effective number of parameters.
  author:
    - family: Espinoza
      given: M.
    - family: Suykens
      given: J. A. K.
    - family: Moor
      given: Bart De
  citation-key: espinoza_kernel_2005
  container-title: IEEE Transactions on Automatic Control
  DOI: 10.1109/TAC.2005.856656
  ISSN: 0018-9286
  issue: '10'
  issued:
    - year: 2005
      month: 10
  page: 1602-1606
  source: IEEE Xplore
  title: Kernel based partially linear models and nonlinear identification
  type: article-journal
  volume: '50'

- id: estes_computerized_2013
  author:
    - family: Estes
      given: N. A. Mark
  citation-key: estes_computerized_2013
  container-title: Circulation. Arrhythmia and Electrophysiology
  container-title-short: Circ Arrhythm Electrophysiol
  DOI: 10.1161/CIRCEP.111.000097
  ISSN: 1941-3084
  issue: '1'
  issued:
    - year: 2013
      month: 2
  language: eng
  page: 2-4
  PMID: '23424219'
  source: PubMed
  title: 'Computerized interpretation of ECGs: supplement not a substitute'
  title-short: Computerized interpretation of ECGs
  type: article-journal
  volume: '6'

- id: eykhoff_system_1974
  author:
    - family: Eykhoff
      given: Pieter
  citation-key: eykhoff_system_1974
  ISBN: 978-0-471-24980-1
  issued:
    - year: 1974
      month: 5
      day: 23
  language: en
  number-of-pages: '588'
  publisher: Wiley-Interscience
  source: Google Books
  title: 'System identification: parameter and state estimation'
  title-short: System identification
  type: book

- id: eykhoff_trends_2014
  abstract: >-
    Trends and Progress in System Identification is a three-part book that
    focuses on model considerations, identification methods, and experimental
    conditions involved in system identification. Organized into 10 chapters,
    this book begins with a discussion of model method in system identification,
    citing four examples differing on the nature of the models involved, the
    nature of the fields, and their goals. Subsequent chapters describe the most
    important aspects of model theory; the ""classical"" methods and time series
    estimation; application of least squares and related techniques for the
    estimation of dynamic system parameters; the maximum likelihood and error
    prediction methods; and the modern development of statistical methods.
    Non-parametric approaches, identification of nonlinear systems by piecewise
    approximation, and the minimax identification are then explained. Other
    chapters explore the Bayesian approach to system identification; choice of
    input signals; and choice and effect of different feedback configurations in
    system identification. This book will be useful for control engineers,
    system scientists, biologists, and members of other disciplines dealing
    withdynamical relations.
  author:
    - family: Eykhoff
      given: Pieter
  citation-key: eykhoff_trends_2014
  ISBN: 978-1-4831-4866-3
  issued:
    - year: 2014
      month: 5
      day: 20
  language: en
  number-of-pages: '419'
  publisher: Elsevier
  source: Google Books
  title: >-
    Trends and Progress in System Identification: Ifac Series for Graduates,
    Research Workers & Practising Engineers
  title-short: Trends and Progress in System Identification
  type: book

- id: fahlman_cascadecorrelation_1989
  abstract: >-
    Cascade-Correlation is a new architecture and supervised learning
    algo(cid:173) rithm for artificial neural networks.  Instead of just
    adjusting the weights  in a network of fixed topology. Cascade-Correlation
    begins with a min(cid:173) imal network,  then automatically trains  and
    adds new hidden  units  one  by  one,  creating a  multi-layer structure. 
    Once  a  new  hidden  unit  has  been added  to the network, its  input-side
    weights are frozen.  This  unit  then becomes a permanent feature-detector
    in the network, available for  producing  outputs  or for  creating other, 
    more complex  feature  detec(cid:173) tors.  The Cascade-Correlation
    architecture has  several advantages over  existing algorithms:  it  learns 
    very quickly,  the network . determines  its  own size and  topology, it
    retains  the structures  it  has  built even  if the  training set changes,
    and it requires no back-propagation of error signals  through  the
    connections of the network.
  accessed:
    - year: 2022
      month: 9
      day: 22
  author:
    - family: Fahlman
      given: Scott
    - family: Lebiere
      given: Christian
  citation-key: fahlman_cascadecorrelation_1989
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 1989
  publisher: Morgan-Kaufmann
  source: Neural Information Processing Systems
  title: The Cascade-Correlation Learning Architecture
  type: paper-conference
  URL: >-
    https://papers.nips.cc/paper/1989/hash/69adc1e107f7f7d035d7baf04342e1ca-Abstract.html
  volume: '2'

- id: falck_kernel_2014
  author:
    - family: Falck
      given: Tillmann
    - family: De Moor
      given: Bart
    - family: Suykens
      given: Johan AK
  citation-key: falck_kernel_2014
  container-title: Regularization, Optimization, Kernels, and Support Vector Machines
  container-title-short: Regularization, Optimization, Kernels, and Support Vector Machines
  ISSN: '1482241390'
  issued:
    - year: 2014
  page: '371'
  title: >-
    Kernel based identification of systems with multiple outputs using nuclear
    norm regularization
  type: article-journal

- id: falck_leastsquares_2012
  abstract: >-
    This paper considers the identification of Wiener–Hammerstein systems using
    Least-Squares Support Vector Machines based models. The power of fully
    black-box NARX-type models is evaluated and compared with models
    incorporating information about the structure of the systems. For the NARX
    models it is shown how to extend the kernel-based estimator to large data
    sets. For the structured model the emphasis is on preserving the convexity
    of the estimation problem through a suitable relaxation of the original
    problem. To develop an empirical understanding of the implications of the
    different model design choices, all considered models are compared on an
    artificial system under a number of different experimental conditions. The
    obtained results are then validated on the Wiener–Hammerstein benchmark data
    set and the final models are presented. It is illustrated that black-box
    models are a suitable technique for the identification of Wiener–Hammerstein
    systems. The incorporation of structural information results in significant
    improvements in modeling performance.
  author:
    - family: Falck
      given: Tillmann
    - family: Dreesen
      given: Philippe
    - family: De Brabanter
      given: Kris
    - family: Pelckmans
      given: Kristiaan
    - family: De Moor
      given: Bart
    - family: Suykens
      given: Johan A. K.
  citation-key: falck_leastsquares_2012
  collection-title: 'Special Section: Wiener-Hammerstein System Identification Benchmark'
  container-title: Control Engineering Practice
  container-title-short: Control Engineering Practice
  DOI: 10.1016/j.conengprac.2012.05.006
  ISSN: 0967-0661
  issue: '11'
  issued:
    - year: 2012
      month: 11
      day: 1
  page: 1165-1174
  source: ScienceDirect
  title: >-
    Least-Squares Support Vector Machines for the identification of
    Wiener–Hammerstein systems
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0967066112001098
  volume: '20'

- id: falck_nuclear_2010
  abstract: >-
    In this paper we study the overparametrization scheme for Hammerstein
    systems in the presence of regularization. The quality of the convex
    approximation is analysed, that is obtained by relaxing the implicit rank
    one constraint. To obtain an improved convex relaxation we propose the use
    of nuclear norms, instead of using ridge regression. On several simple
    examples we illustrate that this yields a solution close to the best
    possible convex approximation. Furthermore the experiments suggest that
    ridge regression in combination with a projection step yield a
    generalization performance close to the one obtained by nuclear norms.
  author:
    - family: Falck
      given: T.
    - family: Suykens
      given: J. A. K.
    - family: Schoukens
      given: J.
    - family: Moor
      given: B. De
  citation-key: falck_nuclear_2010
  container-title: 49th IEEE Conference on Decision and Control (CDC)
  DOI: 10.1109/CDC.2010.5717892
  event-title: 49th IEEE Conference on Decision and Control (CDC)
  issued:
    - year: 2010
      month: 12
  page: 7202-7207
  source: IEEE Xplore
  title: Nuclear norm regularization for overparametrized Hammerstein systems
  type: paper-conference

- id: far_spectra_2006
  abstract: >-
    In a frequency selective slow-fading channel in a MIMO system, the channel
    matrix is of the form of a block matrix. This paper proposes a method to
    calculate the limit of the eigenvalue distribution of block matrices if the
    size of the blocks tends to infinity. While it considers random matrices, it
    takes an operator-valued free probability approach to achieve this goal.
    Using this method, one derives a system of equations, which can be solved
    numerically to compute the desired eigenvalue distribution. The paper
    initially tackles the problem for square block matrices, then extends the
    solution to rectangular block matrices. Finally, it deals with Wishart type
    block matrices. For two special cases, the results of our approach are
    compared with results from simulations. The first scenario investigates the
    limit eigenvalue distribution of block Toeplitz matrices. The second
    scenario deals with the distribution of Wishart type block matrices for a
    frequency selective slow-fading channel in a MIMO system for two different
    cases of $n_R=n_T$ and $n_R=2n_T$. Using this method, one may calculate the
    capacity and the Signal-to-Interference-and-Noise Ratio in large MIMO
    systems.
  accessed:
    - year: 2021
      month: 6
      day: 28
  author:
    - family: Far
      given: Reza Rashidi
    - family: Oraby
      given: Tamer
    - family: Bryc
      given: Wlodzimierz
    - family: Speicher
      given: Roland
  citation-key: far_spectra_2006
  container-title: arXiv:cs/0610045
  issued:
    - year: 2006
      month: 10
      day: 9
  source: arXiv.org
  title: Spectra of large block matrices
  type: article-journal
  URL: http://arxiv.org/abs/cs/0610045

- id: farina_convergence_2008
  abstract: >-
    Multi-step prediction error identification methods are preferred over plain
    one-step ahead prediction error ones in application contexts (e.g.,
    predictive control) where model accuracy is required over a wide horizon.
    For sufficiently high prediction horizons, their properties can be shown to
    be conveniently related to those of output error methods, for which several
    important issues (e.g., uniqueness of estimation, robustness with respect to
    the noise model) have been characterized in the literature. The convergence
    properties of such criteria with respect to the prediction horizon are
    analyzed.
  author:
    - family: Farina
      given: M.
    - family: Piroddi
      given: L.
  citation-key: farina_convergence_2008
  container-title: 2008 47th IEEE Conference on Decision and Control
  DOI: 10/d92tnw
  event-title: 2008 47th IEEE Conference on Decision and Control
  issued:
    - year: 2008
      month: 12
  page: 756-761
  source: IEEE Xplore
  title: >-
    Some convergence properties of multi-step prediction error identification
    criteria
  type: paper-conference

- id: farina_convergence_2008a
  author:
    - family: Farina
      given: Marcello
    - family: Piroddi
      given: Luigi
  citation-key: farina_convergence_2008a
  container-title: Decision and Control, 2008. CDC 2008. 47th IEEE Conference on
  issued:
    - year: 2008
  page: 756–761
  publisher: IEEE
  title: >-
    Some convergence properties of multi-step prediction error identification
    criteria
  type: paper-conference

- id: farina_identification_2012
  author:
    - family: Farina
      given: Marcello
    - family: Piroddi
      given: Luigi
  citation-key: farina_identification_2012
  container-title: International Journal of Systems Science
  DOI: 10.1080/00207721.2010.496055
  issue: '2'
  issued:
    - year: 2012
  page: 319–333
  title: >-
    Identification of polynomial input/output recursive models with simulation
    error minimisation methods
  type: article-journal
  volume: '43'

- id: farina_iterative_2010
  author:
    - family: Farina
      given: Marcello
    - family: Piroddi
      given: Luigi
  citation-key: farina_iterative_2010
  container-title: International Journal of Control
  DOI: 10.1080/00207171003793262
  issue: '7'
  issued:
    - year: 2010
  page: 1442–1456
  title: >-
    An iterative algorithm for simulation error based identification of
    polynomial input–output models using multi-step prediction
  type: article-journal
  volume: '83'

- id: farina_iterative_2010a
  author:
    - family: Farina
      given: Marcello
    - family: Piroddi
      given: Luigi
  citation-key: farina_iterative_2010a
  container-title: International Journal of Control
  DOI: 10/b84h6q
  issue: '7'
  issued:
    - year: 2010
  note: '00011'
  page: 1442-1456
  title: >-
    An Iterative Algorithm for Simulation Error Based Identification of
    Polynomial Input–output Models Using Multi-Step Prediction
  type: article-journal
  volume: '83'

- id: farina_simulation_2011
  author:
    - family: Farina
      given: M
    - family: Piroddi
      given: L
  citation-key: farina_simulation_2011
  container-title: International Journal of Adaptive Control and Signal Processing
  DOI: 10.1002/acs.1203
  issue: '5'
  issued:
    - year: 2011
  page: 389–406
  title: Simulation error minimization identification based on multi-stage prediction
  type: article-journal
  volume: '25'

- id: farina_simulation_2011a
  abstract: >-
    Classical prediction error minimization (PEM) methods are widely used for
    model identification, but they are also known to provide satisfactory
    results only in specific identification conditions, e.g. disturbance model
    matching. If these conditions are not met, the obtained model may have quite
    different dynamical behavior compared with the original system, resulting in
    poor long range prediction or simulation performance, which is a critical
    factor for model analysis, simulation, model-based control design. In the
    mentioned non-ideal conditions a robust and reliable alternative is based on
    the minimization of the simulation error. Unfortunately, direct optimization
    of a simulation error minimization (SEM) criterion is an intrinsically
    complex and computationally intensive task. In this paper a low-complexity
    approximate SEM approach is discussed, based on the iteration of multi-step
    PEM methods. The soundness of the proposed approach is demonstrated by
    showing that, for sufficiently high prediction horizons, the k-steps ahead
    (single- or multi-step) PEM criteria converge to the SEM one.
    Identifiability issues and convergence properties of the algorithm are also
    discussed. Some examples are provided to illustrate the mentioned properties
    of the algorithm. Copyright © 2010 John Wiley & Sons, Ltd.
  accessed:
    - year: 2019
      month: 4
      day: 4
  author:
    - family: Farina
      given: M.
    - family: Piroddi
      given: L.
  citation-key: farina_simulation_2011a
  container-title: International Journal of Adaptive Control and Signal Processing
  DOI: 10/bn2vnd
  ISSN: 1099-1115
  issue: '5'
  issued:
    - year: 2011
  language: en
  license: Copyright © 2010 John Wiley & Sons, Ltd.
  page: 389-406
  source: Wiley Online Library
  title: Simulation error minimization identification based on multi-stage prediction
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/acs.1203
  volume: '25'

- id: faugeras_calibration_1986
  author:
    - family: Faugeras
      given: Olivier D
    - family: Toscani
      given: Giorgio
  citation-key: faugeras_calibration_1986
  container-title: >-
    Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition
  issued:
    - year: 1986
  page: 15–20
  title: The calibration problem for stereo
  type: paper-conference
  volume: '86'

- id: fawzi_analysis_2018
  abstract: >-
    The goal of this paper is to analyze the intriguing instability of
    classifiers to adversarial perturbations (Szegedy et al., in: International
    conference on learning representations (ICLR), 2014). We provide a
    theoretical framework for analyzing the robustness of classifiers to
    adversarial perturbations, and show fundamental upper bounds on the
    robustness of classifiers. Specifically, we establish a general upper bound
    on the robustness of classifiers to adversarial perturbations, and then
    illustrate the obtained upper bound on two practical classes of classifiers,
    namely the linear and quadratic classifiers. In both cases, our upper bound
    depends on a distinguishability measure that captures the notion of
    difficulty of the classification task. Our results for both classes imply
    that in tasks involving small distinguishability, no classifier in the
    considered set will be robust to adversarial perturbations, even if a good
    accuracy is achieved. Our theoretical framework moreover suggests that the
    phenomenon of adversarial instability is due to the low flexibility of
    classifiers, compared to the difficulty of the classification task (captured
    mathematically by the distinguishability measure). We further show the
    existence of a clear distinction between the robustness of a classifier to
    random noise and its robustness to adversarial perturbations. Specifically,
    the former is shown to be larger than the latter by a factor that is
    proportional to $$\sqrt{d}$$(with d being the signal dimension) for linear
    classifiers. This result gives a theoretical explanation for the discrepancy
    between the two robustness properties in high dimensional problems, which
    was empirically observed by Szegedy et al. in the context of neural
    networks. We finally show experimental results on controlled and real-world
    data that confirm the theoretical analysis and extend its spirit to more
    complex classification schemes.
  accessed:
    - year: 2022
      month: 4
      day: 29
  author:
    - family: Fawzi
      given: Alhussein
    - family: Fawzi
      given: Omar
    - family: Frossard
      given: Pascal
  citation-key: fawzi_analysis_2018
  container-title: Machine Learning
  container-title-short: Mach Learn
  DOI: 10.1007/s10994-017-5663-3
  ISSN: 1573-0565
  issue: '3'
  issued:
    - year: 2018
      month: 3
      day: 1
  language: en
  page: 481-508
  source: Springer Link
  title: Analysis of classifiers’ robustness to adversarial perturbations
  type: article-journal
  URL: https://doi.org/10.1007/s10994-017-5663-3
  volume: '107'

- id: fazlyab_efficient_2019
  abstract: >-
    Tight estimation of the Lipschitz constant for deep neural networks (DNNs)
    is useful in many applications ranging from robustness certification of
    classifiers to stability analysis of closed-loop systems with reinforcement
    learning controllers. Existing methods in the literature for estimating the
    Lipschitz constant suffer from either lack of accuracy or poor scalability.
    In this paper, we present a convex optimization framework to compute
    guaranteed upper bounds on the Lipschitz constant of DNNs both accurately
    and efficiently. Our main idea is to interpret activation functions as
    gradients of convex potential functions. Hence, they satisfy certain
    properties that can be described by quadratic constraints. This particular
    description allows us to pose the Lipschitz constant estimation problem as a
    semidefinite program (SDP). The resulting SDP can be adapted to increase
    either the estimation accuracy (by capturing the interaction between
    activation functions of different layers) or scalability (by decomposition
    and parallel implementation). We illustrate the utility of our approach with
    a variety of experiments on randomly generated networks and on classifiers
    trained on the MNIST and Iris datasets. In particular, we experimentally
    demonstrate that our Lipschitz bounds are the most accurate compared to
    those in the literature. We also study the impact of adversarial training
    methods on the Lipschitz bounds of the resulting classifiers and show that
    our bounds can be used to efficiently provide robustness guarantees.
  accessed:
    - year: 2020
      month: 7
      day: 8
  author:
    - family: Fazlyab
      given: Mahyar
    - family: Robey
      given: Alexander
    - family: Hassani
      given: Hamed
    - family: Morari
      given: Manfred
    - family: Pappas
      given: George J.
  citation-key: fazlyab_efficient_2019
  container-title: Advances in Neural Information Processing Systems (NeurIPS)
  issued:
    - year: 2019
      month: 6
      day: 11
  source: arXiv.org
  title: >-
    Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural
    Networks
  type: article-journal
  URL: http://arxiv.org/abs/1906.04893

- id: fialho_automacao_2004
  author:
    - family: Fialho
      given: Arivelto Bustamante
  citation-key: fialho_automacao_2004
  edition: '2'
  issued:
    - year: 2004
  title: Automação pneumática projetos, dimensionamento e análise de circuitos
  type: book
  URL: http://gen.lib.rus.ec/book/index.php?md5=9d2d9fd28255227c59bdbedd1ff75834

- id: finlay_improved_2019
  abstract: >-
    We augment adversarial training (AT) with worst case adversarial training
    (WCAT) which improves adversarial robustness by 11% over the current
    state-of-the-art result in the $\ell_2$ norm on CIFAR-10. We obtain
    verifiable average case and worst case robustness guarantees, based on the
    expected and maximum values of the norm of the gradient of the loss. We
    interpret adversarial training as Total Variation Regularization, which is a
    fundamental tool in mathematical image processing, and WCAT as Lipschitz
    regularization.
  accessed:
    - year: 2020
      month: 7
      day: 7
  author:
    - family: Finlay
      given: Chris
    - family: Oberman
      given: Adam
    - family: Abbasi
      given: Bilal
  citation-key: finlay_improved_2019
  container-title: arXiv:1810.00953 [cs, stat]
  issued:
    - year: 2019
      month: 9
      day: 13
  source: arXiv.org
  title: >-
    Improved robustness to adversarial examples using Lipschitz regularization
    of the loss
  type: article-journal
  URL: http://arxiv.org/abs/1810.00953

- id: finlayson_generative_2018
  accessed:
    - year: 2018
      month: 12
      day: 13
  author:
    - family: Finlayson
      given: Samuel G.
    - family: Lee
      given: Hyunkwang
    - family: Kohane
      given: Isaac S.
    - family: Oakden-Rayner
      given: Luke
  citation-key: finlayson_generative_2018
  issued:
    - year: 2018
      month: 12
      day: 4
  language: en
  source: arxiv.org
  title: >-
    Towards generative adversarial networks as a new paradigm for radiology
    education
  type: article-journal
  URL: https://arxiv.org/abs/1812.01547v1

- id: fisher_use_1936
  abstract: >-
    The articles published by the Annals of Eugenics (1925?1954) have been made
    available online as an historical archive intended for scholarly use. The
    work of eugenicists was often pervaded by prejudice against racial, ethnic
    and disabled groups. The online publication of this material for scholarly
    research purposes is not an endorsement of those views nor a promotion of
    eugenics in any way.
  accessed:
    - year: 2024
      month: 5
      day: 21
  author:
    - family: Fisher
      given: R.A.
  citation-key: fisher_use_1936
  container-title: Annals of Eugenics
  container-title-short: Annals of Eugenics
  DOI: 10.1111/j.1469-1809.1936.tb02137.x
  ISSN: 2050-1420
  issue: '2'
  issued:
    - year: 1936
      month: 9
      day: 1
  page: 179-188
  publisher: John Wiley & Sons, Ltd
  title: The use of multiple measurements in taxonomic problems
  type: article-journal
  URL: https://doi.org/10.1111/j.1469-1809.1936.tb02137.x
  volume: '7'

- id: fletcher_function_1964
  author:
    - family: Fletcher
      given: Reeves
    - family: Reeves
      given: Colin M
  citation-key: fletcher_function_1964
  container-title: The computer journal
  DOI: 10.1093/comjnl/7.2.149
  issue: '2'
  issued:
    - year: 1964
  page: 149–154
  title: Function minimization by conjugate gradients
  type: article-journal
  volume: '7'

- id: fletcher_modified_1971
  author:
    - family: Fletcher
      given: R.
    - family: Authority
      given: United Kingdom Atomic Energy
    - literal: H.M.S.O.
  citation-key: fletcher_modified_1971
  collection-title: AERE report
  issued:
    - year: 1971
  publisher: Theoretical Physics Division, Atomic Energy Research Establishment
  title: A Modified Marquardt Subroutine for Non-linear Least Squares
  type: book

- id: fletcher_modified_1971a
  author:
    - family: Fletcher
      given: R.
    - family: Authority
      given: United Kingdom Atomic Energy
    - literal: H.M.S.O.
  citation-key: fletcher_modified_1971a
  collection-title: AERE report
  issued:
    - year: 1971
  note: '00003'
  publisher: Theoretical Physics Division, Atomic Energy Research Establishment
  title: A Modified Marquardt Subroutine for Non-Linear Least Squares
  type: book

- id: fletcher_practical_2013
  author:
    - family: Fletcher
      given: Roger
  citation-key: fletcher_practical_2013
  ISBN: 1-118-72318-X
  issued:
    - year: 2013
  publisher: John Wiley & Sons
  title: Practical methods of optimization
  type: book

- id: fletcher_user_1998
  author:
    - family: Fletcher
      given: Roger
    - family: Leyffer
      given: Sven
  citation-key: fletcher_user_1998
  container-title: >-
    Numerical Analysis Report NA/181, Department of Mathematics, University of
    Dundee, Dundee, Scotland
  issued:
    - year: 1998
  title: User manual for filterSQP
  type: article-journal

- id: forsyth_computer_2012
  author:
    - family: Forsyth
      given: D.
    - family: Ponce
      given: J.
  citation-key: forsyth_computer_2012
  collection-title: Always learning
  ISBN: 978-0-13-608592-8
  issued:
    - year: 2012
  publisher: Pearson
  title: 'Computer Vision: A Modern Approach'
  type: book
  URL: https://books.google.com.br/books?id=gM63QQAACAAJ

- id: forsyth_modern_2003
  author:
    - family: Forsyth
      given: David A
    - family: Ponce
      given: Jean
  citation-key: forsyth_modern_2003
  container-title: 'Computer Vision: A Modern Approach'
  issued:
    - year: 2003
  title: A Modern Approach
  type: article-journal

- id: forsythe_computer_1977
  author:
    - family: Forsythe
      given: George Elmer
    - family: Moler
      given: Cleve B
    - family: Malcolm
      given: Michael A
  citation-key: forsythe_computer_1977
  issued:
    - year: 1977
  title: Computer methods for mathematical computations
  type: book

- id: fort_deep_2020
  abstract: >-
    Deep ensembles have been empirically shown to be a promising approach for
    improving accuracy, uncertainty and out-of-distribution robustness of deep
    learning models. While deep ensembles were theoretically motivated by the
    bootstrap, non-bootstrap ensembles trained with just random initialization
    also perform well in practice, which suggests that there could be other
    explanations for why deep ensembles work well. Bayesian neural networks,
    which learn distributions over the parameters of the network, are
    theoretically well-motivated by Bayesian principles, but do not perform as
    well as deep ensembles in practice, particularly under dataset shift. One
    possible explanation for this gap between theory and practice is that
    popular scalable variational Bayesian methods tend to focus on a single
    mode, whereas deep ensembles tend to explore diverse modes in function
    space. We investigate this hypothesis by building on recent work on
    understanding the loss landscape of neural networks and adding our own
    exploration to measure the similarity of functions in the space of
    predictions. Our results show that random initializations explore entirely
    different modes, while functions along an optimization trajectory or sampled
    from the subspace thereof cluster within a single mode predictions-wise,
    while often deviating significantly in the weight space. Developing the
    concept of the diversity--accuracy plane, we show that the decorrelation
    power of random initializations is unmatched by popular subspace sampling
    methods. Finally, we evaluate the relative effects of ensembling, subspace
    based methods and ensembles of subspace based methods, and the experimental
    results validate our hypothesis.
  accessed:
    - year: 2020
      month: 7
      day: 13
  author:
    - family: Fort
      given: Stanislav
    - family: Hu
      given: Huiyi
    - family: Lakshminarayanan
      given: Balaji
  citation-key: fort_deep_2020
  container-title: arXiv:1912.02757 [cs, stat]
  issued:
    - year: 2020
      month: 6
      day: 24
  source: arXiv.org
  title: 'Deep Ensembles: A Loss Landscape Perspective'
  title-short: Deep Ensembles
  type: article-journal
  URL: http://arxiv.org/abs/1912.02757

- id: fox_tutorial_2012
  abstract: >-
    This tutorial describes the mean-ﬁeld variational Bayesian approximation to
    inference in graphical models, using modern machine learning terminology
    rather than statistical physics concepts. It begins by seeking to ﬁnd an
    approximate mean-ﬁeld distribution close to the target joint in the
    KL-divergence sense. It then derives local node updates and reviews the
    recent Variational Message Passing framework.
  accessed:
    - year: 2018
      month: 12
      day: 5
  author:
    - family: Fox
      given: Charles W.
    - family: Roberts
      given: Stephen J.
  citation-key: fox_tutorial_2012
  container-title: Artificial Intelligence Review
  DOI: 10/fd6kdm
  ISSN: 0269-2821, 1573-7462
  issue: '2'
  issued:
    - year: 2012
      month: 8
  language: en
  page: 85-95
  source: Crossref
  title: A tutorial on variational Bayesian inference
  type: article-journal
  URL: http://link.springer.com/10.1007/s10462-011-9236-8
  volume: '38'

- id: franco_design_2002
  author:
    - family: Franco
      given: S.
  citation-key: franco_design_2002
  collection-title: McGraw-Hill series in electrical and computer engineering
  ISBN: 978-0-07-232084-8
  issued:
    - year: 2002
  publisher: McGraw-Hill
  title: Design with operational amplifiers and analog integrated circuits
  type: book
  URL: https://books.google.com.br/books?id=LXMeAQAAIAAJ

- id: frank_statistical_1993
  accessed:
    - year: 2017
      month: 9
      day: 8
  author:
    - family: Frank
      given: Ildiko E.
    - family: Friedman
      given: Jerome H.
  citation-key: frank_statistical_1993
  container-title: Technometrics
  DOI: 10.2307/1269656
  ISSN: '00401706'
  issue: '2'
  issued:
    - year: 1993
      month: 5
  page: '109'
  source: CrossRef
  title: A Statistical View of Some Chemometrics Regression Tools
  type: article-journal
  URL: http://www.jstor.org/stable/1269656?origin=crossref
  volume: '35'

- id: frankle_early_2020
  author:
    - family: Frankle
      given: Jonathan
    - family: Schwab
      given: David J.
    - family: Morcos
      given: Ari S.
  citation-key: frankle_early_2020
  container-title: International conference on learning representations
  issued:
    - year: 2020
  title: The early phase of neural network training
  type: paper-conference
  URL: https://openreview.net/forum?id=Hkl1iRNFwS

- id: frankle_linear_2020
  abstract: >-
    We introduce "instability analysis," which assesses whether a neural network
    optimizes to the same, linearly connected minimum under different samples of
    SGD noise. We find that standard vision models become "stable" in this way
    early in training. From then on, the outcome of optimization is determined
    to within a linearly connected region. We use instability to study
    "iterative magnitude pruning" (IMP), the procedure used by work on the
    lottery ticket hypothesis to identify subnetworks that could have trained to
    full accuracy from initialization. We find that these subnetworks only reach
    full accuracy when they are stable, which either occurs at initialization
    for small-scale settings (MNIST) or early in training for large-scale
    settings (Resnet-50 and Inception-v3 on ImageNet). This submission subsumes
    1903.01611 ("Stabilizing the Lottery Ticket Hypothesis" and "The Lottery
    Ticket Hypothesis at Scale")
  accessed:
    - year: 2020
      month: 7
      day: 4
  author:
    - family: Frankle
      given: Jonathan
    - family: Dziugaite
      given: Gintare Karolina
    - family: Roy
      given: Daniel M.
    - family: Carbin
      given: Michael
  citation-key: frankle_linear_2020
  container-title: arXiv:1912.05671 [cs, stat]
  issued:
    - year: 2020
      month: 2
      day: 20
  source: arXiv.org
  title: Linear Mode Connectivity and the Lottery Ticket Hypothesis
  type: article-journal
  URL: http://arxiv.org/abs/1912.05671

- id: frankle_lottery_2018
  abstract: >-
    Neural network compression techniques are able to reduce the parameter
    counts of trained networks by over 90%--decreasing storage requirements and
    improving inference performance--without compromising accuracy. However,
    contemporary experience is that it is difficult to train small architectures
    from scratch, which would similarly improve training performance. We
    articulate a new conjecture to explain why it is easier to train large
    networks: the "lottery ticket hypothesis." It states that large networks
    that train successfully contain subnetworks that--when trained in
    isolation--converge in a comparable number of iterations to comparable
    accuracy. These subnetworks, which we term "winning tickets," have won the
    initialization lottery: their connections have initial weights that make
    training particularly effective. We find that a standard technique for
    pruning unnecessary network weights naturally uncovers a subnetwork which,
    at the start of training, comprised a winning ticket. We present an
    algorithm to identify winning tickets and a series of experiments that
    support the lottery ticket hypothesis. We consistently find winning tickets
    that are less than 20% of the size of several fully-connected,
    convolutional, and residual architectures for MNIST and CIFAR10.
    Furthermore, winning tickets at moderate levels of pruning (20-50% of the
    original network size) converge up to 6.7x faster than the original network
    and exhibit higher test accuracy.
  accessed:
    - year: 2018
      month: 11
      day: 6
  author:
    - family: Frankle
      given: Jonathan
    - family: Carbin
      given: Michael
  citation-key: frankle_lottery_2018
  container-title: arXiv:1803.03635 [cs]
  issued:
    - year: 2018
      month: 3
      day: 9
  note: '00000'
  source: arXiv.org
  title: 'The Lottery Ticket Hypothesis: Finding Small, Trainable Neural Networks'
  title-short: The Lottery Ticket Hypothesis
  type: article-journal
  URL: http://arxiv.org/abs/1803.03635

- id: frankle_stabilizing_2019
  abstract: >-
    Pruning is a well-established technique for removing unnecessary structure
    from neural networks after training to improve the performance of inference.
    Several recent results have explored the possibility of pruning at
    initialization time to provide similar benefits during training. In
    particular, the "lottery ticket hypothesis" conjectures that typical neural
    networks contain small subnetworks that can train to similar accuracy in a
    commensurate number of steps. The evidence for this claim is that a
    procedure based on iterative magnitude pruning (IMP) reliably finds such
    subnetworks retroactively on small vision tasks. However, IMP fails on
    deeper networks, and proposed methods to prune before training or train
    pruned networks encounter similar scaling limitations. In this paper, we
    argue that these efforts have struggled on deeper networks because they have
    focused on pruning precisely at initialization. We modify IMP to search for
    subnetworks that could have been obtained by pruning early in training (0.1%
    to 7% through) rather than at iteration 0. With this change, it finds small
    subnetworks of deeper networks (e.g., 80% sparsity on Resnet-50) that can
    complete the training process to match the accuracy of the original network
    on more challenging tasks (e.g., ImageNet). In situations where IMP fails at
    iteration 0, the accuracy benefits of delaying pruning accrue rapidly over
    the earliest iterations of training. To explain these behaviors, we study
    subnetwork "stability," finding that - as accuracy improves in this fashion
    - IMP subnetworks train to parameters closer to those of the full network
    and do so with improved consistency in the face of gradient noise. These
    results offer new insights into the opportunity to prune large-scale
    networks early in training and the behaviors underlying the lottery ticket
    hypothesis.
  accessed:
    - year: 2020
      month: 6
      day: 29
  author:
    - family: Frankle
      given: Jonathan
    - family: Dziugaite
      given: Gintare Karolina
    - family: Roy
      given: Daniel M.
    - family: Carbin
      given: Michael
  citation-key: frankle_stabilizing_2019
  container-title: arXiv:1903.01611 [cs, stat]
  issued:
    - year: 2019
      month: 6
      day: 7
  source: arXiv.org
  title: Stabilizing the Lottery Ticket Hypothesis
  type: article-journal
  URL: http://arxiv.org/abs/1903.01611

- id: friedman_elements_2001
  author:
    - family: Friedman
      given: Jerome
    - family: Hastie
      given: Trevor
    - family: Tibshirani
      given: Robert
  citation-key: friedman_elements_2001
  issued:
    - year: 2001
  publisher: Springer series in statistics New York
  source: Google Scholar
  title: The elements of statistical learning
  type: book
  volume: '1'

- id: friedman_fast_2012
  abstract: >-
    Many present day applications of statistical learning involve large numbers
    of predictor variables. Often, that number is much larger than the number of
    cases or observations available for training the learning algorithm. In such
    situations, traditional methods fail. Recently, new techniques have been
    developed, based on regularization, which can often produce accurate models
    in these settings. This paper describes the basic principles underlying the
    method of regularization, then focuses on those methods which exploit the
    sparsity of the predicting model. The potential merits of these methods are
    then explored by example.
  author:
    - family: Friedman
      given: Jerome H.
  citation-key: friedman_fast_2012
  container-title: International Journal of Forecasting
  container-title-short: International Journal of Forecasting
  DOI: 10.1016/j.ijforecast.2012.05.001
  ISSN: 0169-2070
  issue: '3'
  issued:
    - year: 2012
      month: 7
      day: 1
  page: 722-738
  source: ScienceDirect
  title: Fast sparse regression and classification
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0169207012000490
  volume: '28'

- id: friedman_glmnet_2009
  author:
    - family: Friedman
      given: Jerome
    - family: Hastie
      given: Trevor
    - family: Tibshirani
      given: Rob
  citation-key: friedman_glmnet_2009
  container-title: R package version
  container-title-short: R package version
  issue: '4'
  issued:
    - year: 2009
  title: 'glmnet: Lasso and elastic-net regularized generalized linear models'
  type: article-journal
  volume: '1'

- id: friedman_multivariate_1991
  author:
    - family: Friedman
      given: Jerome H
  citation-key: friedman_multivariate_1991
  container-title: The annals of statistics
  container-title-short: The annals of statistics
  DOI: 10.1214/aos/1176347963
  ISSN: 0090-5364
  issued:
    - year: 1991
  page: 1-67
  title: Multivariate adaptive regression splines
  type: article-journal

- id: friedman_pathwise_2007
  accessed:
    - year: 2017
      month: 9
      day: 13
  author:
    - family: Friedman
      given: Jerome
    - family: Hastie
      given: Trevor
    - family: Höfling
      given: Holger
    - family: Tibshirani
      given: Robert
  citation-key: friedman_pathwise_2007
  container-title: The Annals of Applied Statistics
  DOI: 10.1214/07-AOAS131
  ISSN: 1932-6157
  issue: '2'
  issued:
    - year: 2007
      month: 12
  language: en
  page: 302-332
  source: CrossRef
  title: Pathwise coordinate optimization
  type: article-journal
  URL: http://projecteuclid.org/euclid.aoas/1196438020
  volume: '1'

- id: friedman_regularization_2010
  accessed:
    - year: 2017
      month: 9
      day: 14
  author:
    - family: Friedman
      given: Jerome
    - family: Hastie
      given: Trevor
    - family: Tibshirani
      given: Rob
  citation-key: friedman_regularization_2010
  container-title: Journal of statistical software
  DOI: 10.18637/jss.v033.i01
  issue: '1'
  issued:
    - year: 2010
  page: '1'
  source: Google Scholar
  title: Regularization paths for generalized linear models via coordinate descent
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/pmc2929880/
  volume: '33'

- id: friedman_sparse_2008
  accessed:
    - year: 2017
      month: 9
      day: 18
  author:
    - family: Friedman
      given: J.
    - family: Hastie
      given: T.
    - family: Tibshirani
      given: R.
  citation-key: friedman_sparse_2008
  container-title: Biostatistics
  DOI: 10.1093/biostatistics/kxm045
  ISSN: 1465-4644, 1468-4357
  issue: '3'
  issued:
    - year: 2008
      month: 7
      day: 1
  language: en
  page: 432-441
  source: CrossRef
  title: Sparse inverse covariance estimation with the graphical lasso
  type: article-journal
  URL: >-
    https://academic.oup.com/biostatistics/article-lookup/doi/10.1093/biostatistics/kxm045
  volume: '9'

- id: fu_penalized_1998
  abstract: >-
    Bridge regression, a special family of penalized regressions of a penalty
    function ∑|β ~j~|^γ^ with γ ≥ 1, is considered. A general approach to solve
    for the bridge estimator is developed. A new algorithm for the lasso (γ = 1)
    is obtained by studying the structure of the bridge estimators. The
    shrinkage parameter γ and the tuning parameter λ are selected via
    generalized cross-validation (GCV). Comparison between the bridge model (γ ≥
    1) and several other shrinkage models, namely the ordinary least squares
    regression (λ = 0), the lasso (γ = 1) and ridge regression (γ = 2), is made
    through a simulation study. It is shown that the bridge regression performs
    well compared to the lasso and ridge regression. These methods are
    demonstrated through an analysis of a prostate cancer data. Some
    computational advantages and limitations are discussed.
  author:
    - family: Fu
      given: Wenjiang J.
  citation-key: fu_penalized_1998
  container-title: Journal of Computational and Graphical Statistics
  DOI: 10.2307/1390712
  ISSN: 1061-8600
  issue: '3'
  issued:
    - year: 1998
  page: 397-416
  source: JSTOR
  title: 'Penalized Regressions: The Bridge versus the Lasso'
  title-short: Penalized Regressions
  type: article-journal
  URL: http://www.jstor.org/stable/1390712
  volume: '7'

- id: fujieda_wavelet_2018
  abstract: >-
    Spatial and spectral approaches are two major approaches for image
    processing tasks such as image classification and object recognition. Among
    many such algorithms, convolutional neural networks (CNNs) have recently
    achieved significant performance improvement in many challenging tasks.
    Since CNNs process images directly in the spatial domain, they are
    essentially spatial approaches. Given that spatial and spectral approaches
    are known to have different characteristics, it will be interesting to
    incorporate a spectral approach into CNNs. We propose a novel CNN
    architecture, wavelet CNNs, which combines a multiresolution analysis and
    CNNs into one model. Our insight is that a CNN can be viewed as a limited
    form of a multiresolution analysis. Based on this insight, we supplement
    missing parts of the multiresolution analysis via wavelet transform and
    integrate them as additional components in the entire architecture. Wavelet
    CNNs allow us to utilize spectral information which is mostly lost in
    conventional CNNs but useful in most image processing tasks. We evaluate the
    practical performance of wavelet CNNs on texture classification and image
    annotation. The experiments show that wavelet CNNs can achieve better
    accuracy in both tasks than existing models while having significantly fewer
    parameters than conventional CNNs.
  accessed:
    - year: 2020
      month: 7
      day: 7
  author:
    - family: Fujieda
      given: Shin
    - family: Takayama
      given: Kohei
    - family: Hachisuka
      given: Toshiya
  citation-key: fujieda_wavelet_2018
  container-title: arXiv:1805.08620 [cs]
  issued:
    - year: 2018
      month: 5
      day: 20
  source: arXiv.org
  title: Wavelet Convolutional Neural Networks
  type: article-journal
  URL: http://arxiv.org/abs/1805.08620

- id: furnival_regressions_1974
  author:
    - family: Furnival
      given: George M.
    - family: Wilson
      given: Robert W.
  citation-key: furnival_regressions_1974
  container-title: Technometrics
  container-title-short: Technometrics
  DOI: 10.1080/00401706.1974.10489231
  ISSN: 0040-1706
  issue: '4'
  issued:
    - year: 1974
      month: 11
      day: 1
  page: 499-511
  title: Regressions by Leaps and Bounds
  type: article-journal
  URL: http://www.tandfonline.com/doi/abs/10.1080/00401706.1974.10489231
  volume: '16'

- id: gander_analysis_2007
  author:
    - family: Gander
      given: Martin J
    - family: Vandewalle
      given: Stefan
  citation-key: gander_analysis_2007
  container-title: SIAM Journal on Scientific Computing
  DOI: 10.1137/05064607X
  issue: '2'
  issued:
    - year: 2007
  page: 556–578
  title: Analysis of the parareal time-parallel time-integration method
  type: article-journal
  volume: '29'

- id: garcia_comprehensive_2015
  author:
    - family: Garcıa
      given: Javier
    - family: Fernández
      given: Fernando
  citation-key: garcia_comprehensive_2015
  container-title: Journal of Machine Learning Research
  issue: '1'
  issued:
    - year: 2015
  page: 1437–1480
  source: Google Scholar
  title: A comprehensive survey on safe reinforcement learning
  type: article-journal
  volume: '16'

- id: garnelo_conditional_
  abstract: >-
    Deep neural networks excel at function approximation, yet they are typically
    trained from scratch for each new function. On the other hand, Bayesian
    methods, such as Gaussian Processes (GPs), exploit prior knowledge to
    quickly infer the shape of a new function at test time. Yet GPs are
    computationally expensive, and it can be hard to design appropriate priors.
    In this paper we propose a family of neural models, Conditional Neural
    Processes (CNPs), that combine the beneﬁts of both. CNPs are inspired by the
    ﬂexibility of stochastic processes such as GPs, but are structured as neural
    networks and trained via gradient descent. CNPs make accurate predictions
    after observing only a handful of training data points, yet scale to complex
    functions and large datasets. We demonstrate the performance and versatility
    of the approach on a range of canonical machine learning tasks, including
    regression, classiﬁcation and image completion.
  author:
    - family: Garnelo
      given: Marta
    - family: Rosenbaum
      given: Dan
    - family: Maddison
      given: Chris J
    - family: Ramalho
      given: Tiago
    - family: Saxton
      given: David
    - family: Shanahan
      given: Murray
    - family: Teh
      given: Yee Whye
    - family: Rezende
      given: Danilo J
    - family: Eslami
      given: S M Ali
  citation-key: garnelo_conditional_
  language: en
  page: '10'
  source: Zotero
  title: Conditional Neural Processes
  type: article-journal

- id: garnelo_neural_2018
  abstract: >-
    A neural network (NN) is a parameterised function that can be tuned via
    gradient descent to approximate a labelled collection of data with high
    precision. A Gaussian process (GP), on the other hand, is a probabilistic
    model that defines a distribution over possible functions, and is updated in
    light of data via the rules of probabilistic inference. GPs are
    probabilistic, data-efficient and flexible, however they are also
    computationally intensive and thus limited in their applicability. We
    introduce a class of neural latent variable models which we call Neural
    Processes (NPs), combining the best of both worlds. Like GPs, NPs define
    distributions over functions, are capable of rapid adaptation to new
    observations, and can estimate the uncertainty in their predictions. Like
    NNs, NPs are computationally efficient during training and evaluation but
    also learn to adapt their priors to data. We demonstrate the performance of
    NPs on a range of learning tasks, including regression and optimisation, and
    compare and contrast with related models in the literature.
  accessed:
    - year: 2019
      month: 1
      day: 7
  author:
    - family: Garnelo
      given: Marta
    - family: Schwarz
      given: Jonathan
    - family: Rosenbaum
      given: Dan
    - family: Viola
      given: Fabio
    - family: Rezende
      given: Danilo J.
    - family: Eslami
      given: S. M. Ali
    - family: Teh
      given: Yee Whye
  citation-key: garnelo_neural_2018
  container-title: arXiv:1807.01622 [cs, stat]
  issued:
    - year: 2018
      month: 7
      day: 4
  source: arXiv.org
  title: Neural Processes
  type: article-journal
  URL: http://arxiv.org/abs/1807.01622

- id: garrels_bash_2010
  author:
    - family: Garrels
      given: Machtelt
  citation-key: garrels_bash_2010
  issued:
    - year: 2010
  publisher: Fultus Corporation
  source: Google Scholar
  title: Bash Guide for Beginners
  type: book

- id: gastal_highorder_2015
  abstract: >-
    We present a discrete-time mathematical formulation for applying recursive
    digital ﬁlters to non-uniformly sampled signals. Our solution presents
    several desirable features: it preserves the stability of the original
    ﬁlters; is well-conditioned for low-pass, high-pass, and band-pass ﬁlters
    alike; its cost is linear in the number of samples and is not affected by
    the size of the ﬁlter support. Our method is general and works with any
    non-uniformly sampled signal and any recursive digital ﬁlter deﬁned by a
    difference equation. Since our formulation directly uses the ﬁlter
    coefﬁcients, it works out-of-the-box with existing methodologies for digital
    ﬁlter design. We demonstrate the effectiveness of our approach by ﬁltering
    non-uniformly sampled signals in various image and video processing tasks
    including edge-preserving color ﬁltering, noise reduction, stylization, and
    detail enhancement. Our formulation enables, for the ﬁrst time, edge-aware
    evaluation of any recursive inﬁnite impulse response digital ﬁlter (not only
    low-pass), producing high-quality ﬁltering results in real time.
  accessed:
    - year: 2020
      month: 7
      day: 16
  author:
    - family: Gastal
      given: Eduardo S. L.
    - family: Oliveira
      given: Manuel M.
  citation-key: gastal_highorder_2015
  container-title: Computer Graphics Forum
  container-title-short: Computer Graphics Forum
  DOI: 10.1111/cgf.12543
  ISSN: '01677055'
  issue: '2'
  issued:
    - year: 2015
      month: 5
  language: en
  page: 81-93
  source: DOI.org (Crossref)
  title: >-
    High-Order Recursive Filtering of Non-Uniformly Sampled Signals for Image
    and Video Processing
  type: article-journal
  URL: http://doi.wiley.com/10.1111/cgf.12543
  volume: '34'

- id: gatys_neural_2015
  abstract: >-
    In fine art, especially painting, humans have mastered the skill to create
    unique visual experiences through composing a complex interplay between the
    content and style of an image. Thus far the algorithmic basis of this
    process is unknown and there exists no artificial system with similar
    capabilities. However, in other key areas of visual perception such as
    object and face recognition near-human performance was recently demonstrated
    by a class of biologically inspired vision models called Deep Neural
    Networks. Here we introduce an artificial system based on a Deep Neural
    Network that creates artistic images of high perceptual quality. The system
    uses neural representations to separate and recombine content and style of
    arbitrary images, providing a neural algorithm for the creation of artistic
    images. Moreover, in light of the striking similarities between
    performance-optimised artificial neural networks and biological vision, our
    work offers a path forward to an algorithmic understanding of how humans
    create and perceive artistic imagery.
  author:
    - family: Gatys
      given: Leon A.
    - family: Ecker
      given: Alexander S.
    - family: Bethge
      given: Matthias
  citation-key: gatys_neural_2015
  container-title: arXiv:1508.06576 [cs, q-bio]
  issued:
    - year: 2015
      month: 8
      day: 26
  source: arXiv.org
  title: A Neural Algorithm of Artistic Style
  type: article-journal
  URL: http://arxiv.org/abs/1508.06576

- id: gbd2016causesofdeathcollaborators_global_2017
  abstract: >-
    BACKGROUND: Monitoring levels and trends in premature mortality is crucial
    to understanding how societies can address prominent sources of early death.
    The Global Burden of Disease 2016 Study (GBD 2016) provides a comprehensive
    assessment of cause-specific mortality for 264 causes in 195 locations from
    1980 to 2016. This assessment includes evaluation of the expected
    epidemiological transition with changes in development and where local
    patterns deviate from these trends.

    METHODS: We estimated cause-specific deaths and years of life lost (YLLs) by
    age, sex, geography, and year. YLLs were calculated from the sum of each
    death multiplied by the standard life expectancy at each age. We used the
    GBD cause of death database composed of: vital registration (VR) data
    corrected for under-registration and garbage coding; national and
    subnational verbal autopsy (VA) studies corrected for garbage coding; and
    other sources including surveys and surveillance systems for specific causes
    such as maternal mortality. To facilitate assessment of quality, we reported
    on the fraction of deaths assigned to GBD Level 1 or Level 2 causes that
    cannot be underlying causes of death (major garbage codes) by location and
    year. Based on completeness, garbage coding, cause list detail, and time
    periods covered, we provided an overall data quality rating for each
    location with scores ranging from 0 stars (worst) to 5 stars (best). We used
    robust statistical methods including the Cause of Death Ensemble model
    (CODEm) to generate estimates for each location, year, age, and sex. We
    assessed observed and expected levels and trends of cause-specific deaths in
    relation to the Socio-demographic Index (SDI), a summary indicator derived
    from measures of average income per capita, educational attainment, and
    total fertility, with locations grouped into quintiles by SDI. Relative to
    GBD 2015, we expanded the GBD cause hierarchy by 18 causes of death for GBD
    2016.

    FINDINGS: The quality of available data varied by location. Data quality in
    25 countries rated in the highest category (5 stars), while 48, 30, 21, and
    44 countries were rated at each of the succeeding data quality levels. Vital
    registration or verbal autopsy data were not available in 27 countries,
    resulting in the assignment of a zero value for data quality. Deaths from
    non-communicable diseases (NCDs) represented 72·3% (95% uncertainty interval
    [UI] 71·2-73·2) of deaths in 2016 with 19·3% (18·5-20·4) of deaths in that
    year occurring from communicable, maternal, neonatal, and nutritional (CMNN)
    diseases and a further 8·43% (8·00-8·67) from injuries. Although
    age-standardised rates of death from NCDs decreased globally between 2006
    and 2016, total numbers of these deaths increased; both numbers and
    age-standardised rates of death from CMNN causes decreased in the decade
    2006-16-age-standardised rates of deaths from injuries decreased but total
    numbers varied little. In 2016, the three leading global causes of death in
    children under-5 were lower respiratory infections, neonatal preterm birth
    complications, and neonatal encephalopathy due to birth asphyxia and trauma,
    combined resulting in 1·80 million deaths (95% UI 1·59 million to 1·89
    million). Between 1990 and 2016, a profound shift toward deaths at older
    ages occurred with a 178% (95% UI 176-181) increase in deaths in ages 90-94
    years and a 210% (208-212) increase in deaths older than age 95 years. The
    ten leading causes by rates of age-standardised YLL significantly decreased
    from 2006 to 2016 (median annualised rate of change was a decrease of
    2·89%); the median annualised rate of change for all other causes was lower
    (a decrease of 1·59%) during the same interval. Globally, the five leading
    causes of total YLLs in 2016 were cardiovascular diseases; diarrhoea, lower
    respiratory infections, and other common infectious diseases; neoplasms;
    neonatal disorders; and HIV/AIDS and tuberculosis. At a finer level of
    disaggregation within cause groupings, the ten leading causes of total YLLs
    in 2016 were ischaemic heart disease, cerebrovascular disease, lower
    respiratory infections, diarrhoeal diseases, road injuries, malaria,
    neonatal preterm birth complications, HIV/AIDS, chronic obstructive
    pulmonary disease, and neonatal encephalopathy due to birth asphyxia and
    trauma. Ischaemic heart disease was the leading cause of total YLLs in 113
    countries for men and 97 countries for women. Comparisons of observed levels
    of YLLs by countries, relative to the level of YLLs expected on the basis of
    SDI alone, highlighted distinct regional patterns including the greater than
    expected level of YLLs from malaria and from HIV/AIDS across sub-Saharan
    Africa; diabetes mellitus, especially in Oceania; interpersonal violence,
    notably within Latin America and the Caribbean; and cardiomyopathy and
    myocarditis, particularly in eastern and central Europe. The level of YLLs
    from ischaemic heart disease was less than expected in 117 of 195 locations.
    Other leading causes of YLLs for which YLLs were notably lower than expected
    included neonatal preterm birth complications in many locations in both
    south Asia and southeast Asia, and cerebrovascular disease in western
    Europe.

    INTERPRETATION: The past 37 years have featured declining rates of
    communicable, maternal, neonatal, and nutritional diseases across all
    quintiles of SDI, with faster than expected gains for many locations
    relative to their SDI. A global shift towards deaths at older ages suggests
    success in reducing many causes of early death. YLLs have increased globally
    for causes such as diabetes mellitus or some neoplasms, and in some
    locations for causes such as drug use disorders, and conflict and terrorism.
    Increasing levels of YLLs might reflect outcomes from conditions that
    required high levels of care but for which effective treatments remain
    elusive, potentially increasing costs to health systems.

    FUNDING: Bill & Melinda Gates Foundation.
  author:
    - literal: GBD 2016 Causes of Death Collaborators
  citation-key: gbd2016causesofdeathcollaborators_global_2017
  container-title: Lancet (London, England)
  container-title-short: Lancet
  DOI: 10.1016/S0140-6736(17)32152-9
  ISSN: 1474-547X
  issue: '10100'
  issued:
    - year: 2017
      month: 9
      day: 16
  language: eng
  page: 1151-1210
  PMCID: PMC5605883
  PMID: '28919116'
  source: PubMed
  title: >-
    Global, regional, and national age-sex specific mortality for 264 causes of
    death, 1980-2016: a systematic analysis for the Global Burden of Disease
    Study 2016
  title-short: >-
    Global, regional, and national age-sex specific mortality for 264 causes of
    death, 1980-2016
  type: article-journal
  volume: '390'

- id: gedon_deep_2020
  abstract: >-
    An actively evolving model class for generative temporal models developed in
    the deep learning community are deep state space models (SSMs) which have a
    close connection to classic SSMs. In this work six new deep SSMs are
    implemented and evaluated for the identification of established nonlinear
    dynamic system benchmarks. The models and their parameter learning
    algorithms are elaborated rigorously. The usage of deep SSMs as a black-box
    identification model can describe a wide range of dynamics due to the
    flexibility of deep neural networks. Additionally, the uncertainty of the
    system is modelled and therefore one obtains a much richer representation
    and a whole class of systems to describe the underlying dynamics.
  author:
    - family: Gedon
      given: Daniel
    - family: Wahlström
      given: Niklas
    - family: Schön
      given: Thomas B.
    - family: Ljung
      given: Lennart
  citation-key: gedon_deep_2020
  container-title: Proceedings of the 2020 IEEE  59th Conference on Decision and Control
  issued:
    - year: 2020
  title: Deep State Space Models for Nonlinear System Identification
  type: article-journal

- id: gedon_first_2021
  abstract: >-
    Self-supervised learning is a paradigm that extracts general features which
    describe the input space by artificially generating labels from the input
    without the need for explicit annotations. The learned features can then be
    used by transfer learning to boost the performance on a downstream task.
    Such methods have recently produced state of the art results in natural
    language processing and computer vision. Here, we propose a self-supervised
    learning method for 12-lead electrocardiograms (ECGs). For pretraining the
    model we design a task to mask out subsegements of all channels of the input
    signals and try to predict the actual values. As the model architecture, we
    use a U-ResNet containing an encoder-decoder structure. We test our method
    by self-supervised pretraining on the CODE dataset and then transfer the
    learnt features by finetuning on the PTB-XL and CPSC benchmarks to evaluate
    the effect of our method in the classification of 12-leads ECGs. The method
    does provide modest improvements in performance when compared to not using
    pretraining. In future work we will make use of these ideas in smaller
    dataset, where we believe it can lead to larger performance gains.
  author:
    - family: Gedon
      given: Daniel
    - family: Ribeiro
      given: Antônio H.
    - family: Wahlström
      given: Niklas
    - family: Schön
      given: Thomas B.
  citation-key: gedon_first_2021
  container-title: Computing in Cardiology (CinC)
  DOI: 10.23919/CinC53138.2021.9662748
  event-title: Computing in Cardiology (CinC)
  ISSN: 2325-887X
  issued:
    - year: 2021
      month: 9
  license: All rights reserved
  page: 1-4
  source: IEEE Xplore
  title: First Steps Towards Self-Supervised Pretraining of the 12-Lead ECG
  type: paper-conference
  volume: '48'

- id: gedon_invertible_2023
  abstract: >-
    Kernel principal component analysis (kPCA) is a widely studied method to
    construct a low-dimensional data representation after a nonlinear
    transformation. The prevailing method to reconstruct the original input
    signal from kPCA—an important task for denoising—requires us to solve a
    supervised learning problem. In this paper, we present an alternative method
    where the reconstruction follows naturally from the compression step. We
    first approximate the kernel with random Fourier features. Then, we exploit
    the fact that the nonlinear transformation is invertible in a certain
    subdomain. Hence, the name invertible kernel PCA (ikPCA) . We experiment
    with different data modalities and show that ikPCA performs similarly to
    kPCA with supervised reconstruction on denoising tasks, making it a strong
    alternative.
  author:
    - family: Gedon
      given: Daniel
    - family: Ribeiro
      given: Antônio H.
    - family: Wahlström
      given: Niklas
    - family: Schön
      given: Thomas B.
  citation-key: gedon_invertible_2023
  container-title: IEEE Signal Processing Letters
  DOI: 10.1109/LSP.2023.3275499
  issued:
    - year: 2023
      month: 5
      day: 11
  source: arXiv.org
  title: Invertible Kernel PCA with Random Fourier Features
  type: article-journal

- id: gedon_no_2024
  author:
    - family: Gedon
      given: Daniel
    - family: Ribeiro
      given: Antonio H.
    - family: Schön
      given: Thomas B.
  citation-key: gedon_no_2024
  container-title: International Conference on Machine Learning (ICML)
  issued:
    - year: 2024
  license: All rights reserved
  title: >-
    No Double Descent in Principal Component Regression: A High-Dimensional
    Analysis
  type: article-journal

- id: gedon_resnetbased_2021
  author:
    - family: Gedon
      given: Daniel
    - family: Gustafsson
      given: Stefan
    - family: Lampa
      given: Erik
    - family: Ribeiro
      given: Antônio H.
    - family: Holzmann
      given: Martin J.
    - family: Schön
      given: Thomas B.
    - family: Sundström
      given: Johan
  citation-key: gedon_resnetbased_2021
  container-title: >-
    Machine learning from ground truth: New medical imaging datasets for
    unsolved medical problems Workshop at NeurIPS
  event-title: NeurIPS
  issued:
    - year: 2021
  license: All rights reserved
  title: >-
    ResNet-based ECG Diagnosis of Myocardial Infarction in the Emergency
    Department
  type: paper-conference

- id: gehring_convolutional_2017
  author:
    - family: Gehring
      given: Jonas
    - family: Auli
      given: Michael
    - family: Grangier
      given: David
    - family: Dauphin
      given: Yann
  citation-key: gehring_convolutional_2017
  event-title: >-
    Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)
  issued:
    - year: 2017
  page: 123-135
  title: A Convolutional Encoder Model for Neural Machine Translation
  type: paper-conference
  volume: '1'

- id: geiger_are_2012
  accessed:
    - year: 2019
      month: 9
      day: 6
  author:
    - family: Geiger
      given: A.
    - family: Lenz
      given: P.
    - family: Urtasun
      given: R.
  citation-key: geiger_are_2012
  container-title: 2012 IEEE Conference on Computer Vision and Pattern Recognition
  DOI: 10/gf7nxj
  event-place: Providence, RI
  event-title: 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  ISBN: 978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1
  issued:
    - year: 2012
      month: 6
  language: en
  page: 3354-3361
  publisher: IEEE
  publisher-place: Providence, RI
  source: DOI.org (Crossref)
  title: Are we ready for autonomous driving? The KITTI vision benchmark suite
  title-short: Are we ready for autonomous driving?
  type: paper-conference
  URL: http://ieeexplore.ieee.org/document/6248074/

- id: geiger_jamming_2019
  abstract: >-
    Deep learning has been immensely successful at a variety of tasks, ranging
    from classification to artificial intelligence. Learning corresponds to
    fitting training data, which is implemented by descending a very
    high-dimensional loss function. Understanding under which conditions neural
    networks do not get stuck in poor minima of the loss, and how the landscape
    of that loss evolves as depth is increased, remains a challenge. Here we
    predict, and test empirically, an analogy between this landscape and the
    energy landscape of repulsive ellipses. We argue that in fully connected
    deep networks a phase transition delimits the over- and underparametrized
    regimes where fitting can or cannot be achieved. In the vicinity of this
    transition, properties of the curvature of the minima of the loss (the
    spectrum of the Hessian) are critical. This transition shares direct
    similarities with the jamming transition by which particles form a
    disordered solid as the density is increased, which also occurs in certain
    classes of computational optimization and learning problems such as the
    perceptron. Our analysis gives a simple explanation as to why poor minima of
    the loss cannot be encountered in the overparametrized regime.
    Interestingly, we observe that the ability of fully connected networks to
    fit random data is independent of their depth, an independence that appears
    to also hold for real data. We also study a quantity Δ which characterizes
    how well (Δ<0) or badly (Δ>0) a datum is learned. At the critical point it
    is power-law distributed on several decades, P+(Δ)∼Δθ for Δ>0 and
    P−(Δ)∼(−Δ)−γ for Δ<0, with exponents that depend on the choice of activation
    function. This observation suggests that near the transition the loss
    landscape has a hierarchical structure and that the learning dynamics is
    prone to avalanche-like dynamics, with abrupt changes in the set of patterns
    that are learned.
  accessed:
    - year: 2021
      month: 5
      day: 23
  author:
    - family: Geiger
      given: Mario
    - family: Spigler
      given: Stefano
    - family: Ascoli
      given: Stéphane
      non-dropping-particle: d'
    - family: Sagun
      given: Levent
    - family: Baity-Jesi
      given: Marco
    - family: Biroli
      given: Giulio
    - family: Wyart
      given: Matthieu
  citation-key: geiger_jamming_2019
  container-title: Physical Review E
  container-title-short: Phys. Rev. E
  DOI: 10.1103/PhysRevE.100.012115
  issue: '1'
  issued:
    - year: 2019
      month: 7
      day: 11
  page: '012115'
  publisher: American Physical Society
  source: APS
  title: >-
    Jamming transition as a paradigm to understand the loss landscape of deep
    neural networks
  type: article-journal
  URL: https://link.aps.org/doi/10.1103/PhysRevE.100.012115
  volume: '100'

- id: geiger_scaling_2020
  abstract: >-
    Supervised deep learning involves the training of neural networks with a
    large number $N$ of parameters. For large enough $N$, in the so-called
    over-parametrized regime, one can essentially fit the training data points.
    Sparsity-based arguments would suggest that the generalization error
    increases as $N$ grows past a certain threshold $N^{*}$. Instead, empirical
    studies have shown that in the over-parametrized regime, generalization
    error keeps decreasing with $N$. We resolve this paradox through a new
    framework. We rely on the so-called Neural Tangent Kernel, which connects
    large neural nets to kernel methods, to show that the initialization causes
    finite-size random fluctuations $\|f_{N}-\bar{f}_{N}\|\sim N^{-1/4}$ of the
    neural net output function $f_{N}$ around its expectation $\bar{f}_{N}$.
    These affect the generalization error $\epsilon_{N}$ for classification:
    under natural assumptions, it decays to a plateau value $\epsilon_{\infty}$
    in a power-law fashion $\sim N^{-1/2}$. This description breaks down at a
    so-called jamming transition $N=N^{*}$. At this threshold, we argue that
    $\|f_{N}\|$ diverges. This result leads to a plausible explanation for the
    cusp in test error known to occur at $N^{*}$. Our results are confirmed by
    extensive empirical observations on the MNIST and CIFAR image datasets. Our
    analysis finally suggests that, given a computational envelope, the smallest
    generalization error is obtained using several networks of intermediate
    sizes, just beyond $N^{*}$, and averaging their outputs.
  accessed:
    - year: 2021
      month: 5
      day: 23
  author:
    - family: Geiger
      given: Mario
    - family: Jacot
      given: Arthur
    - family: Spigler
      given: Stefano
    - family: Gabriel
      given: Franck
    - family: Sagun
      given: Levent
    - family: Ascoli
      given: Stéphane
      non-dropping-particle: d'
    - family: Biroli
      given: Giulio
    - family: Hongler
      given: Clément
    - family: Wyart
      given: Matthieu
  citation-key: geiger_scaling_2020
  container-title: 'Journal of Statistical Mechanics: Theory and Experiment'
  container-title-short: J. Stat. Mech.
  DOI: 10.1088/1742-5468/ab633c
  ISSN: 1742-5468
  issue: '2'
  issued:
    - year: 2020
      month: 2
      day: 5
  page: '023401'
  source: arXiv.org
  title: >-
    Scaling description of generalization with number of parameters in deep
    learning
  type: article-journal
  URL: http://arxiv.org/abs/1901.01608
  volume: '2020'

- id: geisert_trajectory_2016
  author:
    - family: Geisert
      given: Mathieu
    - family: Mansard
      given: Nicolas
  citation-key: geisert_trajectory_2016
  event-title: Robotics and Automation (ICRA), 2016 IEEE International Conference on
  ISBN: 1-4673-8026-1
  issued:
    - year: 2016
  page: 2958-2964
  publisher: IEEE
  title: >-
    Trajectory generation for quadrotor based systems using numerical optimal
    control
  type: paper-conference

- id: geisert_trajectory_2016a
  author:
    - family: Geisert
      given: Mathieu
    - family: Mansard
      given: Nicolas
  citation-key: geisert_trajectory_2016a
  container-title: 2016 IEEE international conference on robotics and automation (ICRA)
  ISBN: 1-4673-8026-1
  issued:
    - year: 2016
  page: 2958-2964
  publisher: IEEE
  title: >-
    Trajectory Generation for Quadrotor Based Systems Using Numerical Optimal
    Control
  type: paper-conference

- id: genton_classes_2001
  abstract: >-
    In this paper, we present classes of kernels for machine learning from a
    statistics perspective. Indeed, kernels are positive definite functions and
    thus also covariances. After discussing key properties of kernels, as well
    as a new formula to construct kernels, we present several important classes
    of kernels: anisotropic stationary kernels, isotropic stationary kernels,
    compactly supported kernels, locally stationary kernels, nonstationary
    kernels, and separable nonstationary kernels. Compactly supported kernels
    and separable nonstationary kernels are of prime interest because they
    provide a computational reduction for kernel-based methods. We describe the
    spectral representation of the various classes of kernels and conclude with
    a discussion on the characterization of nonlinear maps that reduce
    nonstationary kernels to either stationarity or local stationarity.
  accessed:
    - year: 2021
      month: 11
      day: 23
  author:
    - family: Genton
      given: Marc G.
  citation-key: genton_classes_2001
  container-title: Journal of Machine Learning Research
  ISSN: ISSN 1533-7928
  issue: Dec
  issued:
    - year: 2001
  page: 299-312
  source: www.jmlr.org
  title: 'Classes of Kernels for Machine Learning: A Statistics Perspective'
  title-short: Classes of Kernels for Machine Learning
  type: article-journal
  URL: https://www.jmlr.org/papers/v2/genton01a
  volume: '2'

- id: gevers_identification_2005
  author:
    - family: Gevers
      given: Michel
  citation-key: gevers_identification_2005
  container-title: European Journal of Control
  issue: 4-5
  issued:
    - year: 2005
  page: 335–352
  publisher: Elsevier
  source: Google Scholar
  title: >-
    Identification for control: From the early achievements to the revival of
    experiment design
  title-short: Identification for control
  type: article-journal
  volume: '11'

- id: ghorbani_linearized_2020
  abstract: >-
    We consider the problem of learning an unknown function $f_{\star}$ on the
    $d$-dimensional sphere with respect to the square loss, given i.i.d. samples
    $\{(y_i,{\boldsymbol x}_i)\}_{i\le n}$ where ${\boldsymbol x}_i$ is a
    feature vector uniformly distributed on the sphere and
    $y_i=f_{\star}({\boldsymbol x}_i)+\varepsilon_i$. We study two popular
    classes of models that can be regarded as linearizations of two-layers
    neural networks around a random initialization: the random features model of
    Rahimi-Recht (RF); the neural tangent kernel model of Jacot-Gabriel-Hongler
    (NT). Both these approaches can also be regarded as randomized
    approximations of kernel ridge regression (with respect to different
    kernels), and enjoy universal approximation properties when the number of
    neurons $N$ diverges, for a fixed dimension $d$. We consider two specific
    regimes: the approximation-limited regime, in which $n=\infty$ while $d$ and
    $N$ are large but finite; and the sample size-limited regime in which
    $N=\infty$ while $d$ and $n$ are large but finite. In the first regime we
    prove that if $d^{\ell + \delta} \le N\le d^{\ell+1-\delta}$ for small
    $\delta > 0$, then \RF\, effectively fits a degree-$\ell$ polynomial in the
    raw features, and \NT\, fits a degree-$(\ell+1)$ polynomial. In the second
    regime, both RF and NT reduce to kernel methods with rotationally invariant
    kernels. We prove that, if the number of samples is $d^{\ell + \delta} \le n
    \le d^{\ell +1-\delta}$, then kernel methods can fit at most a a
    degree-$\ell$ polynomial in the raw features. This lower bound is achieved
    by kernel ridge regression. Optimal prediction error is achieved for
    vanishing ridge regularization.
  accessed:
    - year: 2021
      month: 4
      day: 23
  author:
    - family: Ghorbani
      given: Behrooz
    - family: Mei
      given: Song
    - family: Misiakiewicz
      given: Theodor
    - family: Montanari
      given: Andrea
  citation-key: ghorbani_linearized_2020
  container-title: arXiv:1904.12191 [cs, math, stat]
  issued:
    - year: 2020
      month: 2
      day: 16
  source: arXiv.org
  title: Linearized two-layers neural networks in high dimension
  type: article-journal
  URL: http://arxiv.org/abs/1904.12191

- id: ghosh_objective_2011
  abstract: >-
    Bayesian methods are increasingly applied in these days in the theory and
    practice of statistics. Any Bayesian inference depends on a likelihood and a
    prior. Ideally one would like to elicit a prior from related sources of
    information or past data. However, in its absence, Bayesian methods need to
    rely on some "objective" or "default" priors, and the resulting posterior
    inference can still be quite valuable. Not surprisingly, over the years, the
    catalog of objective priors also has become prohibitively large, and one has
    to set some specific criteria for the selection of such priors. Our aim is
    to review some of these criteria, compare their performance, and illustrate
    them with some simple examples. While for very large sample sizes, it does
    not possibly matter what objective prior one uses, the selection of such a
    prior does influence inference for small or moderate samples. For regular
    models where asymptotic normality holds, Jeffreys' general rule prior, the
    positive square root of the determinant of the Fisher information matrix,
    enjoys many optimality properties in the absence of nuisance parameters. In
    the presence of nuisance parameters, however, there are many other priors
    which emerge as optimal depending on the criterion selected. One new feature
    in this article is that a prior different from Jeffreys' is shown to be
    optimal under the chi-square divergence criterion even in the absence of
    nuisance parameters. The latter is also invariant under one-to-one
    reparameterization.
  accessed:
    - year: 2018
      month: 10
      day: 7
  author:
    - family: Ghosh
      given: Malay
  citation-key: ghosh_objective_2011
  container-title: Statistical Science
  DOI: 10.1214/10-STS338
  ISSN: 0883-4237
  issue: '2'
  issued:
    - year: 2011
      month: 5
  page: 187-202
  source: arXiv.org
  title: 'Objective Priors: An Introduction for Frequentists'
  title-short: Objective Priors
  type: article-journal
  URL: http://arxiv.org/abs/1108.2120
  volume: '26'

- id: gilmer_adversarial_2018
  abstract: >-
    Machine learning models with very low test error have been shown to be
    consistently vulnerable to small, adversarially chosen perturbations of the
    input. We hypothesize that this counterintuitive behavior is a result of the
    high-dimensional geometry of the data manifold, and explore this hypothesis
    on a simple highdimensional dataset. For this dataset we show a fundamental
    bound relating the classiﬁcation error rate to the average distance to the
    nearest misclassiﬁcation, which is independent of the model. We train
    different neural network architectures on this dataset and show their error
    sets approach this theoretical bound. As a result of the theory, the
    vulnerability of machine learning models to small adversarial perturbations
    is a logical consequence of the amount of test error observed. We hope that
    our theoretical analysis of this foundational synthetic case will point a
    way forward to explore how the geometry of complex real-world data sets
    leads to adversarial examples.
  accessed:
    - year: 2020
      month: 3
      day: 20
  author:
    - family: Gilmer
      given: Justin
    - family: Metz
      given: Luke
    - family: Faghri
      given: Fartash
    - family: Schoenholz
      given: Samuel S.
    - family: Raghu
      given: Maithra
    - family: Wattenberg
      given: Martin
    - family: Goodfellow
      given: Ian
  citation-key: gilmer_adversarial_2018
  container-title: arXiv:1801.02774
  issued:
    - year: 2018
      month: 9
      day: 10
  language: en
  source: arXiv.org
  title: Adversarial Spheres
  type: article-journal
  URL: http://arxiv.org/abs/1801.02774

- id: girshick_fast_2015
  author:
    - family: Girshick
      given: Ross
  citation-key: girshick_fast_2015
  container-title: Proceedings of the IEEE international conference on computer vision
  issued:
    - year: 2015
  page: 1440–1448
  source: Google Scholar
  title: Fast r-cnn
  type: paper-conference

- id: girshick_rich_2014
  author:
    - family: Girshick
      given: Ross
    - family: Donahue
      given: Jeff
    - family: Darrell
      given: Trevor
    - family: Malik
      given: Jitendra
  citation-key: girshick_rich_2014
  container-title: >-
    Proceedings of the IEEE conference on computer vision and pattern
    recognition
  issued:
    - year: 2014
  page: 580–587
  source: Google Scholar
  title: >-
    Rich feature hierarchies for accurate object detection and semantic
    segmentation
  type: paper-conference

- id: globerson_nightmare_2006
  abstract: >-
    When constructing a classiﬁer from labeled data, it is important not to
    assign too much weight to any single input feature, in order to increase the
    robustness of the classiﬁer. This is particularly important in domains with
    nonstationary feature distributions or with input sensor failures. A common
    approach to achieving such robustness is to introduce regularization which
    spreads the weight more evenly between the features. However, this strategy
    is very generic, and cannot induce robustness speciﬁcally tailored to the
    classiﬁcation task at hand. In this work, we introduce a new algorithm for
    avoiding single feature over-weighting by analyzing robustness using a game
    theoretic formalization. We develop classiﬁers which are optimally resilient
    to deletion of features in a minimax sense, and show how to construct such
    classiﬁers using quadratic programming. We illustrate the applicability of
    our methods on spam ﬁltering and handwritten digit recognition tasks, where
    feature deletion is indeed a realistic noise model.
  author:
    - family: Globerson
      given: Amir
    - family: Roweis
      given: Sam
  citation-key: globerson_nightmare_2006
  container-title: Proceedings of the 23rd international conference on Machine learning  (ICML)
  DOI: 10.1145/1143844.1143889
  ISBN: 978-1-59593-383-6
  issued:
    - year: 2006
  page: 353-360
  title: 'Nightmare at test time: robust learning by feature deletion'
  type: paper-conference
  URL: http://portal.acm.org/citation.cfm?doid=1143844.1143889

- id: glorot_apprentissage_2015
  author:
    - family: Glorot
      given: Xavier
  citation-key: glorot_apprentissage_2015
  issued:
    - year: 2015
  source: Google Scholar
  title: >-
    Apprentissage des réseaux de neurones profonds et applications en traitement
    automatique de la langue naturelle
  type: article-journal

- id: glorot_understanding_2010
  author:
    - family: Glorot
      given: Xavier
    - family: Bengio
      given: Yoshua
  citation-key: glorot_understanding_2010
  event-title: >-
    Proceedings of the Thirteenth International Conference on Artificial
    Intelligence and Statistics
  issued:
    - year: 2010
  page: 249-256
  title: Understanding the difficulty of training deep feedforward neural networks
  type: paper-conference

- id: gluch_constructing_
  author:
    - family: Głuch
      given: Grzegorz
    - family: Urbanke
      given: Rüdiger
  citation-key: gluch_constructing_
  language: en
  page: '10'
  source: Zotero
  title: >-
    Constructing a provably adversarially-robust classiﬁer from a high accuracy
    one
  type: article-journal

- id: goethals_identification_2005
  abstract: >-
    This paper studies a method for the identification of Hammerstein models
    based on least squares support vector machines (LS-SVMs). The technique
    allows for the determination of the memoryless static nonlinearity as well
    as the estimation of the model parameters of the dynamic ARX part. This is
    done by applying the equivalent of Bai's overparameterization method for
    identification of Hammerstein systems in an LS-SVM context. The SISO as well
    as the MIMO identification cases are elaborated. The technique can lead to
    significant improvements with respect to classical overparameterization
    methods as illustrated in a number of examples. Another important advantage
    is that no stringent assumptions on the nature of the nonlinearity need to
    be imposed except for a certain degree of smoothness.
  author:
    - family: Goethals
      given: Ivan
    - family: Pelckmans
      given: Kristiaan
    - family: Suykens
      given: Johan A. K.
    - family: De Moor
      given: Bart
  citation-key: goethals_identification_2005
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10.1016/j.automatica.2005.02.002
  ISSN: 0005-1098
  issue: '7'
  issued:
    - year: 2005
      month: 7
      day: 1
  page: 1263-1272
  source: ScienceDirect
  title: >-
    Identification of MIMO Hammerstein models using least squares support vector
    machines
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0005109805000737
  volume: '41'

- id: goethals_subspace_2005
  abstract: >-
    This paper presents a method for the identification of
    multiple-input-multiple-output (MIMO) Hammerstein systems for the goal of
    prediction. The method extends the numerical algorithms for subspace state
    space system identification (N4SID), mainly by rewriting the oblique
    projection in the N4SID algorithm as a set of componentwise least squares
    support vector machines (LS-SVMs) regression problems. The linear model and
    static nonlinearities follow from a low-rank approximation of a matrix
    obtained from this regression problem.
  author:
    - family: Goethals
      given: I.
    - family: Pelckmans
      given: K.
    - family: Suykens
      given: J. A. K.
    - family: Moor
      given: Bart De
  citation-key: goethals_subspace_2005
  container-title: IEEE Transactions on Automatic Control
  DOI: 10.1109/TAC.2005.856647
  ISSN: 0018-9286
  issue: '10'
  issued:
    - year: 2005
      month: 10
  page: 1509-1519
  source: IEEE Xplore
  title: >-
    Subspace identification of Hammerstein systems using least squares support
    vector machines
  type: article-journal
  volume: '50'

- id: goldberg_genetic_2000
  author:
    - family: Goldberg
      given: David E
  citation-key: goldberg_genetic_2000
  issued:
    - year: 2000
  publisher: Addison Wesley, New Delhi
  title: Genetic Algorithms–In Search, Optimization & Machine Learning, Revised ed
  type: book

- id: goldberger_physiobank_2000
  accessed:
    - year: 2018
      month: 10
      day: 21
  author:
    - family: Goldberger
      given: Ary L.
    - family: Amaral
      given: Luis A. N.
    - family: Glass
      given: Leon
    - family: Hausdorff
      given: Jeffrey M.
    - family: Ivanov
      given: Plamen Ch.
    - family: Mark
      given: Roger G.
    - family: Mietus
      given: Joseph E.
    - family: Moody
      given: George B.
    - family: Peng
      given: Chung-Kang
    - family: Stanley
      given: H. Eugene
  citation-key: goldberger_physiobank_2000
  container-title: Circulation
  DOI: 10.1161/01.CIR.101.23.e215
  ISSN: 0009-7322, 1524-4539
  issue: '23'
  issued:
    - year: 2000
      month: 6
      day: 13
  language: en
  source: Crossref
  title: >-
    PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research
    Resource for Complex Physiologic Signals
  title-short: PhysioBank, PhysioToolkit, and PhysioNet
  type: article-journal
  URL: https://www.ahajournals.org/doi/10.1161/01.CIR.101.23.e215
  volume: '101'

- id: golub_matrix_2012
  author:
    - family: Golub
      given: Gene H
    - family: Van Loan
      given: Charles F
  citation-key: golub_matrix_2012
  issued:
    - year: 2012
  publisher: JHU Press
  title: Matrix Computations
  type: book
  volume: '3'

- id: gong_impact_2018
  abstract: >-
    A recent trend in audio and speech processing is to learn target labels
    directly from raw waveforms rather than hand-crafted acoustic features.
    Previous work has shown that deep convolutional neural networks (CNNs) as
    front-end can learn effective representations from the raw waveform.
    However, due to the large dimension of raw audio waveforms, pooling layers
    are usually used aggressively between temporal convolutional layers. In
    essence, these pooling layers perform operations that are similar to signal
    downsampling, which may lead to temporal aliasing according to the
    Nyquist-Shannon sampling theorem. This paper explores, using a series of
    experiments, if and how this aliasing effect impacts modern deep CNN-based
    models.
  accessed:
    - year: 2020
      month: 3
      day: 23
  author:
    - family: Gong
      given: Yuan
    - family: Poellabauer
      given: Christian
  citation-key: gong_impact_2018
  container-title: Interspeech
  DOI: 10.21437/Interspeech.2018-1371
  event-title: Interspeech
  issued:
    - year: 2018
  language: en
  page: 2698-2702
  source: DOI.org (Crossref)
  title: Impact of Aliasing on Deep CNN-Based End-to-End Acoustic Models
  type: paper-conference
  URL: http://www.isca-speech.org/archive/Interspeech_2018/abstracts/1371.html

- id: gonzalez_nonlinear_2018
  abstract: >-
    Long-Short Term Memory (LSTM) is a type of Recurrent Neural Networks (RNN).
    It takes sequences of information and uses recurrent mechanisms and gate
    techniques. LSTM has many advantages over other feedforward and recurrent
    NNs in modeling of time series, such as audio and video. However, in
    non-linear system modeling normal LSTM does not work well(Wang, 2017). In
    this paper, we combine LSTM with NN, and use the advantages. The novel
    neural model consists of hierarchical recurrent networks and one multilayer
    perceptron. We design a special learning algorithm which uses
    backpropagation, and backpropagation through time methods. We use two
    non-linear system examples to compare our neural modeling with other well
    known methods. The results show that for the simulation model (only the test
    input is used and the past test output is not used), the modified LSTM model
    proposed in this paper is much better than the other existing neural models.
  accessed:
    - year: 2022
      month: 6
      day: 9
  author:
    - family: Gonzalez
      given: Jesús
    - family: Yu
      given: Wen
  citation-key: gonzalez_nonlinear_2018
  collection-title: >-
    2nd IFAC Conference on Modelling, Identification and Control of Nonlinear
    Systems MICNON 2018
  container-title: IFAC-PapersOnLine
  container-title-short: IFAC-PapersOnLine
  DOI: 10.1016/j.ifacol.2018.07.326
  ISSN: 2405-8963
  issue: '13'
  issued:
    - year: 2018
      month: 1
      day: 1
  language: en
  page: 485-489
  source: ScienceDirect
  title: Non-linear system modeling using LSTM neural networks
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S2405896318310814
  volume: '51'

- id: goodfellow_deep_2016
  author:
    - family: Goodfellow
      given: Ian
    - family: Bengio
      given: Yoshua
    - family: Courville
      given: Aaron
  citation-key: goodfellow_deep_2016
  issued:
    - year: 2016
  publisher: MIT Press
  title: Deep Learning
  type: book

- id: goodfellow_explaining_2015
  abstract: >-
    Several machine learning models, including neural networks, consistently
    misclassify adversarial examples—inputs formed by applying small but
    intentionally worst-case perturbations to examples from the dataset, such
    that the perturbed input results in the model outputting an incorrect answer
    with high conﬁdence. Early attempts at explaining this phenomenon focused on
    nonlinearity and overﬁtting. We argue instead that the primary cause of
    neural networks’ vulnerability to adversarial perturbation is their linear
    nature. This explanation is supported by new quantitative results while
    giving the ﬁrst explanation of the most intriguing fact about them: their
    generalization across architectures and training sets. Moreover, this view
    yields a simple and fast method of generating adversarial examples. Using
    this approach to provide examples for adversarial training, we reduce the
    test set error of a maxout network on the MNIST dataset.
  author:
    - family: Goodfellow
      given: Ian J.
    - family: Shlens
      given: Jonathon
    - family: Szegedy
      given: Christian
  citation-key: goodfellow_explaining_2015
  container-title: International Conference on Learning Representations (ICLR)
  event-title: International Conference on Learning Representations (ICLR)
  issued:
    - year: 2015
  title: Explaining and Harnessing Adversarial Examples
  type: paper-conference

- id: goodfellow_generative_2014
  accessed:
    - year: 2018
      month: 10
      day: 4
  author:
    - family: Goodfellow
      given: Ian
    - family: Pouget-Abadie
      given: Jean
    - family: Mirza
      given: Mehdi
    - family: Xu
      given: Bing
    - family: Warde-Farley
      given: David
    - family: Ozair
      given: Sherjil
    - family: Courville
      given: Aaron
    - family: Bengio
      given: Yoshua
  citation-key: goodfellow_generative_2014
  container-title: Advances in Neural Information Processing Systems 27
  editor:
    - family: Ghahramani
      given: Z.
    - family: Welling
      given: M.
    - family: Cortes
      given: C.
    - family: Lawrence
      given: N. D.
    - family: Weinberger
      given: K. Q.
  issued:
    - year: 2014
  page: 2672–2680
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Generative Adversarial Nets
  type: chapter
  URL: http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf

- id: gopalakrishnan_combating_2018
  abstract: >-
    It is by now well-known that small adversarial perturbations can induce
    classification errors in deep neural networks (DNNs). In this paper, we make
    the case that sparse representations of the input data are a crucial tool
    for combating such attacks. For linear classifiers, we show that a
    sparsifying front end is provably effective against $\ell_{\infty}$-bounded
    attacks, reducing output distortion due to the attack by a factor of roughly
    $K / N$ where $N$ is the data dimension and $K$ is the sparsity level. We
    then extend this concept to DNNs, showing that a "locally linear" model can
    be used to develop a theoretical foundation for crafting attacks and
    defenses. Experimental results for the MNIST dataset show the efficacy of
    the proposed sparsifying front end.
  accessed:
    - year: 2022
      month: 2
      day: 3
  author:
    - family: Gopalakrishnan
      given: Soorya
    - family: Marzi
      given: Zhinus
    - family: Madhow
      given: Upamanyu
    - family: Pedarsani
      given: Ramtin
  citation-key: gopalakrishnan_combating_2018
  container-title: arXiv:1803.03880
  issued:
    - year: 2018
      month: 7
      day: 13
  source: arXiv.org
  title: Combating Adversarial Attacks Using Sparse Representations
  type: article-journal
  URL: http://arxiv.org/abs/1803.03880

- id: gorelick_fast_2013
  author:
    - family: Gorelick
      given: Lena
    - family: Schmidt
      given: Frank R
    - family: Boykov
      given: Yuri
  citation-key: gorelick_fast_2013
  container-title: >-
    Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition
  issued:
    - year: 2013
  page: 1714–1721
  title: Fast trust region for segmentation
  type: paper-conference

- id: goto_artificial_2019
  abstract: >-
    Background Patient with acute coronary syndrome benefits from early
    revascularization. However, methods for the selection of patients who
    require urgent revascularization from a variety of patients visiting the
    emergency room with chest symptoms is not fully established.
    Electrocardiogram is an easy and rapid procedure, but may contain crucial
    information not recognized even by well-trained physicians. Objective To
    make a prediction model for the needs for urgent revascularization from
    12-lead electrocardiogram recorded in the emergency room. Method We
    developed an artificial intelligence model enabling the detection of hidden
    information from a 12-lead electrocardiogram recorded in the emergency room.
    Electrocardiograms obtained from consecutive patients visiting the emergency
    room at Keio University Hospital from January 2012 to April 2018 with chest
    discomfort was collected. These data were splitted into validation and
    derivation dataset with no duplication in each dataset. The artificial
    intelligence model was constructed to select patients who require urgent
    revascularization within 48 hours. The model was trained with the derivation
    dataset and tested using the validation dataset. Results Of the consecutive
    39,619 patients visiting the emergency room with chest discomfort, 362
    underwent urgent revascularization. Of them, 249 were included in the
    derivation dataset and the remaining 113 were included in validation
    dataset. For the control, 300 were randomly selected as derivation dataset
    and another 130 patients were randomly selected for validation dataset from
    the 39,317 who did not undergo urgent revascularization. On validation, our
    artificial intelligence model had predictive value of the c-statistics 0.88
    (95% CI 0.84–0.93) for detecting patients who required urgent
    revascularization. Conclusions Our artificial intelligence model provides
    information to select patients who need urgent revascularization from only
    12-leads electrocardiogram in those visiting the emergency room with chest
    discomfort.
  accessed:
    - year: 2019
      month: 5
      day: 29
  author:
    - family: Goto
      given: Shinichi
    - family: Kimura
      given: Mai
    - family: Katsumata
      given: Yoshinori
    - family: Goto
      given: Shinya
    - family: Kamatani
      given: Takashi
    - family: Ichihara
      given: Genki
    - family: Ko
      given: Seien
    - family: Sasaki
      given: Junichi
    - family: Fukuda
      given: Keiichi
    - family: Sano
      given: Motoaki
  citation-key: goto_artificial_2019
  container-title: PLOS ONE
  container-title-short: PLOS ONE
  DOI: 10/gf286h
  ISSN: 1932-6203
  issue: '1'
  issued:
    - year: 2019
      month: 9
      day: 1
  language: en
  page: e0210103
  source: PLoS Journals
  title: >-
    Artificial intelligence to predict needs for urgent revascularization from
    12-leads electrocardiography in emergency patients
  type: article-journal
  URL: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0210103
  volume: '14'

- id: goudis_chargeaf_2022
  abstract: >-
    Atrial fibrillation (AF) is the commonest arrhythmia in clinical practice
    and is associated

    with increased morbidity and mortality. Various predictive scores for
    new-onset AF have been

    proposed, but so far, none have been widely used in clinical practice.
    CHARGE-AF score was developed

    from a pooled diverse population from three large cohorts (Atherosclerosis
    Risk in Communities

    study, Cardiovascular Health Study and Framingham Heart Study). A simple
    5-year predictive

    model includes the variables of age, race, height, weight, systolic and
    diastolic blood pressure,

    current smoking, use of antihypertensive medication, diabetes mellitus,
    history of myocardial

    infarction and heart failure. Recent studies report that the CHARGE-AF score
    has good discrimination

    for incident AF and seems to be a promising prediction model for this
    arrhythmia. New

    screening tools (smartphone apps, smartwatches) are rapidly developing for
    AF detection. Therefore,

    the wide application of the CHARGE-AF score in clinical practice and the
    upcoming usage of

    mobile health technologies and smartwatches may result in better AF
    prediction and adequate

    stroke prevention, especially in high-risk patients.
  accessed:
    - year: 2024
      month: 6
      day: 10
  author:
    - family: Goudis
      given: Christos
    - family: Daios
      given: Stylianos
    - family: Dimitriadis
      given: Fotios
    - family: Liu
      given: Tong
  citation-key: goudis_chargeaf_2022
  container-title: Current Cardiology Reviews
  DOI: 10.2174/1573403X18666220901102557
  issue: '2'
  issued:
    - year: 2022
  language: en
  page: 5-10
  source: www.eurekaselect.com
  title: 'CHARGE-AF: A Useful Score For Atrial Fibrillation Prediction?'
  title-short: CHARGE-AF
  type: article-journal
  URL: https://www.eurekaselect.com/article/126070
  volume: '19'

- id: gouk_regularisation_2018
  abstract: >-
    We investigate the effect of explicitly enforcing the Lipschitz continuity
    of neural networks with respect to their inputs. To this end, we provide a
    simple technique for computing an upper bound to the Lipschitz constant of a
    feed forward neural network composed of commonly used layer types and
    demonstrate inaccuracies in previous work on this topic. Our technique is
    then used to formulate training a neural network with a bounded Lipschitz
    constant as a constrained optimisation problem that can be solved using
    projected stochastic gradient methods. Our evaluation study shows that, in
    isolation, our method performs comparatively to state-of-the-art
    regularisation techniques. Moreover, when combined with existing approaches
    to regularising neural networks the performance gains are cumulative. We
    also provide evidence that the hyperparameters are intuitive to tune and
    demonstrate how the choice of norm for computing the Lipschitz constant
    impacts the resulting model.
  accessed:
    - year: 2020
      month: 6
      day: 5
  author:
    - family: Gouk
      given: Henry
    - family: Frank
      given: Eibe
    - family: Pfahringer
      given: Bernhard
    - family: Cree
      given: Michael
  citation-key: gouk_regularisation_2018
  container-title: arXiv:1804.04368 [cs, stat]
  issued:
    - year: 2018
      month: 9
      day: 14
  source: arXiv.org
  title: Regularisation of Neural Networks by Enforcing Lipschitz Continuity
  type: article-journal
  URL: http://arxiv.org/abs/1804.04368

- id: gould_cutest_2015
  author:
    - family: Gould
      given: Nicholas IM
    - family: Orban
      given: Dominique
    - family: Toint
      given: Philippe L
  citation-key: gould_cutest_2015
  container-title: Computational Optimization and Applications
  container-title-short: Computational Optimization and Applications
  DOI: 10.1007/s10589-014-9687-3
  ISSN: 0926-6003
  issue: '3'
  issued:
    - year: 2015
  page: 545-557
  title: >-
    CUTEst: a constrained and unconstrained testing environment with safe
    threads for mathematical optimization
  type: article-journal
  volume: '60'

- id: gould_cutest_2015a
  author:
    - family: Gould
      given: Nicholas IM
    - family: Orban
      given: Dominique
    - family: Toint
      given: Philippe L
  citation-key: gould_cutest_2015a
  container-title: Computational Optimization and Applications
  DOI: 10/gfjwmh
  ISSN: 0926-6003
  issue: '3'
  issued:
    - year: 2015
  page: 545-557
  title: >-
    CUTEst: A Constrained and Unconstrained Testing Environment with Safe
    Threads for Mathematical Optimization
  type: article-journal
  volume: '60'

- id: gould_solution_2001
  author:
    - family: Gould
      given: Nicholas IM
    - family: Hribar
      given: Mary E
    - family: Nocedal
      given: Jorge
  citation-key: gould_solution_2001
  container-title: SIAM Journal on Scientific Computing
  container-title-short: SIAM Journal on Scientific Computing
  DOI: 10.1137/S1064827598345667
  ISSN: 1064-8275
  issue: '4'
  issued:
    - year: 2001
  page: 1376-1395
  title: >-
    On the solution of equality constrained quadratic programming problems
    arising in optimization
  type: article-journal
  volume: '23'

- id: gould_solving_1999
  author:
    - family: Gould
      given: Nicholas I. M.
    - family: Lucidi
      given: Stefano
    - family: Roma
      given: Massimo
    - family: Toint
      given: Philippe L
  citation-key: gould_solving_1999
  container-title: SIAM Journal on Optimization
  DOI: 10.1137/S1052623497322735
  issue: '2'
  issued:
    - year: 1999
  page: 504–525
  title: Solving the trust-region subproblem using the Lanczos method
  type: article-journal
  volume: '9'

- id: gower_variancereduced_2020
  author:
    - family: Gower
      given: R. M.
    - family: Schmidt
      given: M.
    - family: Bach
      given: F.
    - family: Richtárik
      given: P.
  citation-key: gower_variancereduced_2020
  container-title: Proceedings of the IEEE
  container-title-short: Proceedings of the IEEE
  DOI: 10.1109/JPROC.2020.3028013
  ISSN: 1558-2256
  issue: '11'
  issued:
    - year: 2020
      month: 11
  page: 1968-1983
  title: Variance-Reduced Methods for Machine Learning
  type: article-journal
  volume: '108'

- id: graeme_operational_1971
  author:
    - family: Graeme
      given: J.G.
    - family: Tobey
      given: G.E.
    - family: Huelsman
      given: L.P.
    - literal: Burr-Brown Research Corporation
  citation-key: graeme_operational_1971
  collection-title: McGraw-Hill Electrical and Electronic Engineering Series
  issued:
    - year: 1971
  publisher: McGraw-Hill
  title: 'Operational amplifiers: design and applications'
  type: book
  URL: https://books.google.com.br/books?id=rbvPswEACAAJ

- id: graham_concrete_1994
  author:
    - family: Graham
      given: Ronald L.
    - family: Knuth
      given: Donald Ervin
    - family: Patashnik
      given: Oren
  call-number: QA39.2 .G733 1994
  citation-key: graham_concrete_1994
  edition: 2nd ed
  event-place: Reading, Mass
  ISBN: 978-0-201-55802-9
  issued:
    - year: 1994
  number-of-pages: '657'
  publisher: Addison-Wesley
  publisher-place: Reading, Mass
  source: Library of Congress ISBN
  title: 'Concrete mathematics: a foundation for computer science'
  title-short: Concrete mathematics
  type: book

- id: graves_connectionist_2006
  author:
    - family: Graves
      given: Alex
    - family: Fernández
      given: Santiago
    - family: Gomez
      given: Faustino
    - family: Schmidhuber
      given: Jürgen
  citation-key: graves_connectionist_2006
  container-title: Proceedings of the 23rd international conference on Machine learning
  issued:
    - year: 2006
  page: 369–376
  publisher: ACM
  source: Google Scholar
  title: >-
    Connectionist temporal classification: labelling unsegmented sequence data
    with recurrent neural networks
  title-short: Connectionist temporal classification
  type: paper-conference

- id: graves_hybrid_2013
  abstract: >-
    Deep Bidirectional LSTM (DBLSTM) recurrent neural networks have recently
    been shown to give state-of-the-art performance on the TIMIT speech
    database. However, the results in that work relied on
    recurrent-neural-network-speciﬁc objective functions, which are difﬁcult to
    integrate with existing large vocabulary speech recognition systems. This
    paper investigates the use of DBLSTM as an acoustic model in a standard
    neural network-HMM hybrid system. We ﬁnd that a DBLSTM-HMM hybrid gives
    equally good results on TIMIT as the previous work. It also outperforms both
    GMM and deep network benchmarks on a subset of the Wall Street Journal
    corpus. However the improvement in word error rate over the deep network is
    modest, despite a great increase in framelevel accuracy. We conclude that
    the hybrid approach with DBLSTM appears to be well suited for tasks where
    acoustic modelling predominates. Further investigation needs to be conducted
    to understand how to better leverage the improvements in frame-level
    accuracy towards better word error rates.
  accessed:
    - year: 2019
      month: 11
      day: 22
  author:
    - family: Graves
      given: Alex
    - family: Jaitly
      given: Navdeep
    - family: Mohamed
      given: Abdel-rahman
  citation-key: graves_hybrid_2013
  container-title: 2013 IEEE Workshop on Automatic Speech Recognition and Understanding
  DOI: 10.1109/ASRU.2013.6707742
  event-place: Olomouc, Czech Republic
  event-title: 2013 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU)
  ISBN: 978-1-4799-2756-2
  issued:
    - year: 2013
      month: 12
  language: en
  page: 273-278
  publisher: IEEE
  publisher-place: Olomouc, Czech Republic
  source: DOI.org (Crossref)
  title: Hybrid speech recognition with Deep Bidirectional LSTM
  type: paper-conference
  URL: http://ieeexplore.ieee.org/document/6707742/

- id: greblicki_hammerstein_2017
  abstract: >-
    The nonlinear characteristic in a Hammerstein system, i.e., a system in
    which a nonlinear memoryless subsystem and a linear dynamic are connected in
    a cascade, is recovered with the nonparametric nearest neighbor regression
    estimate. The a priori information is nonparametric, both the nonlinear
    characteristic and the impulse response are completely unknown and can be of
    any form. Local and global properties of the estimate are examined. Whatever
    the probability density of the input signal, the estimate converges at every
    continuity point of the characteristic as well as in the global sense. We
    derive the asymptotic bias and variance of the proposed estimate. As a
    result, the optimal rate of convergence is established that additionally is
    independent of the shape of the input density. Results of numerical
    simulations are also presented.
  author:
    - family: Greblicki
      given: W.
    - family: Pawlak
      given: M.
  citation-key: greblicki_hammerstein_2017
  container-title: IEEE Transactions on Information Theory
  DOI: 10.1109/TIT.2017.2694013
  ISSN: 0018-9448
  issue: '8'
  issued:
    - year: 2017
      month: 8
  page: 4746-4757
  source: IEEE Xplore
  title: Hammerstein System Identification With the Nearest Neighbor Algorithm
  type: article-journal
  volume: '63'

- id: greene_econometric_2003
  author:
    - family: Greene
      given: William H.
  call-number: HB139 .G74 2003
  citation-key: greene_econometric_2003
  edition: 5th ed
  event-place: Upper Saddle River, N.J
  ISBN: 978-0-13-066189-0
  issued:
    - year: 2003
  number-of-pages: '1026'
  publisher: Prentice Hall
  publisher-place: Upper Saddle River, N.J
  source: Library of Congress ISBN
  title: Econometric analysis
  type: book

- id: groover_automation_2000
  author:
    - family: Groover
      given: Mikell P.
  citation-key: groover_automation_2000
  edition: '2'
  ISBN: 0-13-088978-4 978-0-13-088978-2
  issued:
    - year: 2000
  publisher: Prentice Hall
  title: >-
    Automation, production systems, and computer-integrated manufacturing (2nd
    edition)
  type: book
  URL: http://gen.lib.rus.ec/book/index.php?md5=eaf2497c97c9ed94f2dd6cba7cfc33bb

- id: grosse_statistical_2017
  abstract: >-
    Machine Learning (ML) models are applied in a variety of tasks such as
    network intrusion detection or malware classification. Yet, these models are
    vulnerable to a class of malicious inputs known as adversarial examples.
    These are slightly perturbed inputs that are classified incorrectly by the
    ML model. The mitigation of these adversarial inputs remains an open
    problem.
  accessed:
    - year: 2021
      month: 11
      day: 16
  author:
    - family: Grosse
      given: Kathrin
    - family: Manoharan
      given: Praveen
    - family: Papernot
      given: Nicolas
    - family: Backes
      given: Michael
    - family: McDaniel
      given: Patrick
  citation-key: grosse_statistical_2017
  container-title: arXiv:1702.06280 [cs, stat]
  issued:
    - year: 2017
      month: 10
      day: 17
  language: en
  source: arXiv.org
  title: On the (Statistical) Detection of Adversarial Examples
  type: article-journal
  URL: http://arxiv.org/abs/1702.06280

- id: grosse_statistical_2017a
  abstract: >-
    Machine Learning (ML) models are applied in a variety of tasks such as
    network intrusion detection or malware classification. Yet, these models are
    vulnerable to a class of malicious inputs known as adversarial examples.
    These are slightly perturbed inputs that are classified incorrectly by the
    ML model. The mitigation of these adversarial inputs remains an open
    problem.
  accessed:
    - year: 2021
      month: 11
      day: 16
  author:
    - family: Grosse
      given: Kathrin
    - family: Manoharan
      given: Praveen
    - family: Papernot
      given: Nicolas
    - family: Backes
      given: Michael
    - family: McDaniel
      given: Patrick
  citation-key: grosse_statistical_2017a
  container-title: arXiv:1702.06280 [cs, stat]
  issued:
    - year: 2017
      month: 10
      day: 17
  language: en
  source: arXiv.org
  title: On the (Statistical) Detection of Adversarial Examples
  type: article-journal
  URL: http://arxiv.org/abs/1702.06280

- id: grune_nonlinear_2017
  accessed:
    - year: 2018
      month: 4
      day: 27
  author:
    - family: Grüne
      given: Lars
    - family: Pannek
      given: Jürgen
  citation-key: grune_nonlinear_2017
  collection-title: Communications and Control Engineering
  DOI: 10.1007/978-3-319-46024-6
  event-place: Cham
  ISBN: 978-3-319-46023-9 978-3-319-46024-6
  issued:
    - year: 2017
  publisher: Springer International Publishing
  publisher-place: Cham
  source: Crossref
  title: Nonlinear Model Predictive Control
  type: book
  URL: http://link.springer.com/10.1007/978-3-319-46024-6

- id: guarner_chagas_2019
  abstract: >-
    Trypanosoma cruzi, the protozoan that causes Chagas disease, is primarily
    transmitted by three main Triatomine vectors in endemic areas. However, the
    infection has become a potential emerging disease because the vector is
    found in non-endemic areas, there is migration of infected asymptomatic
    people that can infect the vector, become blood donors, or pass the disease
    vertically (congenital infections). Lastly, the disease can be acquired
    through contaminated food (oral transmission). This review will present the
    different transmission pathways, clinical manifestations, diagnostic
    modalities and treatment considerations of Chagas disease.
  accessed:
    - year: 2021
      month: 11
      day: 25
  author:
    - family: Guarner
      given: Jeannette
  citation-key: guarner_chagas_2019
  collection-title: Emerging infections issue
  container-title: Seminars in Diagnostic Pathology
  container-title-short: Seminars in Diagnostic Pathology
  DOI: 10.1053/j.semdp.2019.04.008
  ISSN: 0740-2570
  issue: '3'
  issued:
    - year: 2019
      month: 5
      day: 1
  language: en
  page: 164-169
  source: ScienceDirect
  title: Chagas disease as example of a reemerging parasite
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0740257019300401
  volume: '36'

- id: guimaraes_super_
  abstract: Página que contém a partitura da música Forró no Escuro v.2 (Luiz Gonzaga).
  accessed:
    - year: 2023
      month: 11
      day: 28
  author:
    - family: Guimaraes
      given: Jonatas
  citation-key: guimaraes_super_
  container-title: SuperPartituras
  language: pt-br
  title: Super Partituras - Partitura da música Forró no Escuro v.2 (Luiz Gonzaga).
  type: webpage
  URL: https://www.superpartituras.com.br/luiz-gonzaga/forro-no-escuro-v-2

- id: gunasekar_implicit_2017
  abstract: >-
    We study implicit regularization when optimizing an underdetermined
    quadratic objective over a matrix $X$ with gradient descent on a
    factorization of $X$. We conjecture and provide empirical and theoretical
    evidence that with small enough step sizes and initialization close enough
    to the origin, gradient descent on a full dimensional factorization
    converges to the minimum nuclear norm solution.
  accessed:
    - year: 2020
      month: 8
      day: 9
  author:
    - family: Gunasekar
      given: Suriya
    - family: Woodworth
      given: Blake
    - family: Bhojanapalli
      given: Srinadh
    - family: Neyshabur
      given: Behnam
    - family: Srebro
      given: Nathan
  citation-key: gunasekar_implicit_2017
  container-title: arXiv:1705.09280 [cs, stat]
  issued:
    - year: 2017
      month: 5
      day: 25
  source: arXiv.org
  title: Implicit Regularization in Matrix Factorization
  type: article-journal
  URL: http://arxiv.org/abs/1705.09280

- id: guo_calibration_2017
  abstract: >-
    Conﬁdence calibration – the problem of predicting probability estimates
    representative of the true correctness likelihood – is important for
    classiﬁcation models in many applications. We discover that modern neural
    networks, unlike those from a decade ago, are poorly calibrated. Through
    extensive experiments, we observe that depth, width, weight decay, and Batch
    Normalization are important factors inﬂuencing calibration. We evaluate the
    performance of various post-processing calibration methods on
    state-ofthe-art architectures with image and document classiﬁcation
    datasets. Our analysis and experiments not only offer insights into neural
    network learning, but also provide a simple and straightforward recipe for
    practical settings: on most datasets, temperature scaling – a
    singleparameter variant of Platt Scaling – is surprisingly effective at
    calibrating predictions.
  accessed:
    - year: 2020
      month: 11
      day: 10
  author:
    - family: Guo
      given: Chuan
    - family: Pleiss
      given: Geoff
    - family: Sun
      given: Yu
    - family: Weinberger
      given: Kilian Q.
  citation-key: guo_calibration_2017
  container-title: arXiv:1706.04599 [cs]
  issued:
    - year: 2017
      month: 8
      day: 3
  language: en
  source: arXiv.org
  title: On Calibration of Modern Neural Networks
  type: article-journal
  URL: http://arxiv.org/abs/1706.04599

- id: guo_sparse_2018
  accessed:
    - year: 2022
      month: 2
      day: 3
  author:
    - family: Guo
      given: Yiwen
    - family: Zhang
      given: Chao
    - family: Zhang
      given: Changshui
    - family: Chen
      given: Yurong
  citation-key: guo_sparse_2018
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2018
  source: Neural Information Processing Systems
  title: Sparse DNNs with Improved Adversarial Robustness
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2018/hash/4c5bde74a8f110656874902f07378009-Abstract.html
  volume: '31'

- id: gupta_feedback_2018
  abstract: >-
    Generative Adversarial Networks (GANs) represent an attractive and novel
    approach to generate realistic data, such as genes, proteins, or drugs, in
    synthetic biology. Here, we apply GANs to generate synthetic DNA sequences
    encoding for proteins of variable length. We propose a novel feedback-loop
    architecture, called Feedback GAN (FBGAN), to optimize the synthetic gene
    sequences for desired properties using an external function analyzer. The
    proposed architecture also has the advantage that the analyzer need not be
    differentiable. We apply the feedback-loop mechanism to two examples: 1)
    generating synthetic genes coding for antimicrobial peptides, and 2)
    optimizing synthetic genes for the secondary structure of their resulting
    peptides. A suite of metrics demonstrate that the GAN generated proteins
    have desirable biophysical properties. The FBGAN architecture can also be
    used to optimize GAN-generated datapoints for useful properties in domains
    beyond genomics.
  accessed:
    - year: 2019
      month: 4
      day: 1
  author:
    - family: Gupta
      given: Anvita
    - family: Zou
      given: James
  citation-key: gupta_feedback_2018
  container-title: arXiv:1804.01694 [cs, q-bio]
  issued:
    - year: 2018
      month: 4
      day: 5
  source: arXiv.org
  title: >-
    Feedback GAN (FBGAN) for DNA: a Novel Feedback-Loop Architecture for
    Optimizing Protein Functions
  title-short: Feedback GAN (FBGAN) for DNA
  type: article-journal
  URL: http://arxiv.org/abs/1804.01694

- id: gupta_note_2012
  author:
    - family: Gupta
      given: Shuva
  citation-key: gupta_note_2012
  container-title: Sankhya A
  container-title-short: Sankhya A
  DOI: 10.1007/s13171-012-0006-8
  ISSN: 0976-836X
  issue: '1'
  issued:
    - year: 2012
  page: 10-28
  title: A note on the asymptotic distribution of LASSO estimator for correlated data
  type: article-journal
  volume: '74'

- id: gurbuzbalaban_heavytail_2021
  abstract: >-
    In recent years, various notions of capacity and complexity have been
    proposed for characterizing the generalization properties of stochastic
    gradient descent (SGD) in deep learning. Some of the popular notions that
    correlate well with the performance on unseen data are (i) the `flatness' of
    the local minimum found by SGD, which is related to the eigenvalues of the
    Hessian, (ii) the ratio of the stepsize $\eta$ to the batch-size $b$, which
    essentially controls the magnitude of the stochastic gradient noise, and
    (iii) the `tail-index', which measures the heaviness of the tails of the
    network weights at convergence. In this paper, we argue that these three
    seemingly unrelated perspectives for generalization are deeply linked to
    each other. We claim that depending on the structure of the Hessian of the
    loss at the minimum, and the choices of the algorithm parameters $\eta$ and
    $b$, the SGD iterates will converge to a \emph{heavy-tailed} stationary
    distribution. We rigorously prove this claim in the setting of quadratic
    optimization: we show that even in a simple linear regression problem with
    independent and identically distributed data whose distribution has finite
    moments of all order, the iterates can be heavy-tailed with infinite
    variance. We further characterize the behavior of the tails with respect to
    algorithm parameters, the dimension, and the curvature. We then translate
    our results into insights about the behavior of SGD in deep learning. We
    support our theory with experiments conducted on synthetic data, fully
    connected, and convolutional neural networks.
  accessed:
    - year: 2023
      month: 11
      day: 6
  author:
    - family: Gurbuzbalaban
      given: Mert
    - family: Şimşekli
      given: Umut
    - family: Zhu
      given: Lingjiong
  citation-key: gurbuzbalaban_heavytail_2021
  DOI: 10.48550/arXiv.2006.04740
  issued:
    - year: 2021
      month: 6
      day: 14
  number: arXiv:2006.04740
  publisher: arXiv
  source: arXiv.org
  title: The Heavy-Tail Phenomenon in SGD
  type: article
  URL: http://arxiv.org/abs/2006.04740

- id: gurushewal_robust_2012
  author:
    - family: Gurushewal
      given: Singh
  citation-key: gurushewal_robust_2012
  container-title: International Journal of Advances in Computing and Information Technology
  issued:
    - year: 2012
  source: Google Scholar
  title: >-
    Robust Stability of LTI (SISO) System with System Gain (A) under uncertainty
    set using PID controller
  type: article-journal

- id: gustafsson_development_2022
  abstract: >-
    Myocardial infarction diagnosis is a common challenge in the emergency
    department. In managed settings, deep learning-based models and especially
    convolutional deep models have shown promise in electrocardiogram (ECG)
    classification, but there is a lack of high-performing models for the
    diagnosis of myocardial infarction in real-world scenarios. We aimed to
    train and validate a deep learning model using ECGs to predict myocardial
    infarction in real-world emergency department patients. We studied emergency
    department patients in the Stockholm region between 2007 and 2016 that had
    an ECG obtained because of their presenting complaint. We developed a deep
    neural network based on convolutional layers similar to a residual network.
    Inputs to the model were ECG tracing, age, and sex; and outputs were the
    probabilities of three mutually exclusive classes: non-ST-elevation
    myocardial infarction (NSTEMI), ST-elevation myocardial infarction (STEMI),
    and control status, as registered in the SWEDEHEART and other registries. We
    used an ensemble of five models. Among 492,226 ECGs in 214,250 patients,
    5,416 were recorded with an NSTEMI, 1,818 a STEMI, and 485,207 without a
    myocardial infarction. In a random test set, our model could discriminate
    STEMIs/NSTEMIs from controls with a C-statistic of 0.991/0.832 and had a
    Brier score of 0.001/0.008. The model obtained a similar performance in a
    temporally separated test set of the study sample, and achieved a
    C-statistic of 0.985 and a Brier score of 0.002 in discriminating STEMIs
    from controls in an external test set. We developed and validated a deep
    learning model with excellent performance in discriminating between control,
    STEMI, and NSTEMI on the presenting ECG of a real-world sample of the
    important population of all-comers to the emergency department. Hence, deep
    learning models for ECG decision support could be valuable in the emergency
    department.
  accessed:
    - year: 2022
      month: 11
      day: 17
  author:
    - family: Gustafsson
      given: Stefan
    - family: Gedon
      given: Daniel
    - family: Lampa
      given: Erik
    - family: Ribeiro
      given: Antônio H.
    - family: Holzmann
      given: Martin J.
    - family: Schön
      given: Thomas B.
    - family: Sundström
      given: Johan
  citation-key: gustafsson_development_2022
  container-title: Scientific Reports
  container-title-short: Sci Rep
  DOI: 10.1038/s41598-022-24254-x
  ISSN: 2045-2322
  issue: '1'
  issued:
    - year: 2022
      month: 11
      day: 15
  license: 2022 The Author(s)
  number: '1'
  page: '19615'
  publisher: Nature Publishing Group
  source: www.nature.com
  title: >-
    Development and validation of deep learning ECG-based prediction of
    myocardial infarction in emergency department patients
  type: article-journal
  URL: https://www.nature.com/articles/s41598-022-24254-x
  volume: '12'

- id: guzman_use_2017
  author:
    - family: Guzman
      given: Sandra M
    - family: Paz
      given: Joel O
    - family: Tagert
      given: Mary Love M
  citation-key: guzman_use_2017
  container-title: Water Resources Management
  DOI: 10.1007/s11269-017-1598-5
  issue: '5'
  issued:
    - year: 2017
  page: 1591–1603
  title: The Use of NARX Neural Networks to Forecast Daily Groundwater Levels
  type: article-journal
  volume: '31'

- id: guzman_use_2017a
  author:
    - family: Guzman
      given: Sandra M
    - family: Paz
      given: Joel O
    - family: Tagert
      given: Mary Love M
  citation-key: guzman_use_2017a
  container-title: Water Resources Management
  DOI: 10/f92hfc
  issue: '5'
  issued:
    - year: 2017
  page: 1591-1603
  title: The Use of NARX Neural Networks to Forecast Daily Groundwater Levels
  type: article-journal
  volume: '31'

- id: ha_model_2015
  abstract: >-
    The aim of this paper is to propose a new method to select the model order
    in continuous time system identification, instrumental variable methods. The
    idea is to over-parameterize the model and utilize regularization based on
    the l1 norm to obtain a sparse estimate. The model order of the identified
    system is then determined by the rank of the Hankel matrix of the estimated
    parameter. Simulation results show that the proposed method works very
    effectively. For low signal to noise ratio (SNR), it offers a significant
    improvement to existing model order selection methods with the performance
    at high SNR comparable to the existing methods.
  author:
    - family: Ha
      given: H.
    - family: Welsh
      given: J. S.
  citation-key: ha_model_2015
  container-title: 2015 54th IEEE Conference on Decision and Control (CDC)
  DOI: 10.1109/CDC.2015.7402323
  event-title: 2015 54th IEEE Conference on Decision and Control (CDC)
  issued:
    - year: 2015
      month: 12
  page: 771-776
  source: IEEE Xplore
  title: >-
    Model order selection for continuous time instrumental variable methods
    using regularization
  type: paper-conference

- id: haagerup_new_2005
  accessed:
    - year: 2020
      month: 12
      day: 21
  author:
    - family: Haagerup
      given: Uffe
    - family: Thorbjørnsen
      given: Steen
  citation-key: haagerup_new_2005
  container-title: Annals of Mathematics
  container-title-short: Ann. Math.
  DOI: 10.4007/annals.2005.162.711
  ISSN: 0003-486X
  issue: '2'
  issued:
    - year: 2005
      month: 9
      day: 1
  language: en
  page: 711-775
  source: DOI.org (Crossref)
  title: >-
    A new application of random matrices: Ext(C ~red~ ^∗^ (F ~2~ )) is not a
    group
  title-short: A new application of random matrices
  type: article-journal
  URL: http://annals.math.princeton.edu/2005/162-2/p03
  volume: '162'

- id: habineza_endtoend_2023
  author:
    - family: Habineza
      given: Theogene
    - family: Ribeiro
      given: Antônio H.
    - family: Gedon
      given: Daniel
    - family: Behar
      given: Joachim A.
    - family: Ribeiro
      given: Antonio Luiz P.
    - family: Schön
      given: Thomas B.
  citation-key: habineza_endtoend_2023
  container-title: Journal of Electrocardiology
  DOI: 10.1016/j.jelectrocard.2023.09.011
  issued:
    - year: 2023
  license: All rights reserved
  title: >-
    End-to-end Risk Prediction of Atrial Fibrillation from the 12-Lead ECG by
    Deep Neural Networks
  type: article-journal

- id: hagan_training_1994
  author:
    - family: Hagan
      given: Martin T
    - family: Menhaj
      given: Mohammad B
  citation-key: hagan_training_1994
  container-title: Neural Networks, IEEE Transactions on
  DOI: 10.1109/72.329697
  issue: '6'
  issued:
    - year: 1994
  page: 989–993
  title: Training feedforward networks with the Marquardt algorithm
  type: article-journal
  volume: '5'

- id: hager_new_2005
  author:
    - family: Hager
      given: William W
    - family: Zhang
      given: Hongchao
  citation-key: hager_new_2005
  container-title: SIAM Journal on Optimization
  DOI: 10.1137/030601880
  issue: '1'
  issued:
    - year: 2005
  page: 170–192
  title: >-
    A new conjugate gradient method with guaranteed descent and an efficient
    line search
  type: article-journal
  volume: '16'

- id: han_deep_2020
  abstract: >-
    Electrocardiogram (ECG) acquisition is increasingly widespread in medical
    and commercial devices, necessitating the development of automated
    interpretation strategies. Recently, deep neural networks have been used to
    automatically analyze ECG tracings and outperform physicians in detecting
    certain rhythm irregularities1. However, deep learning classifiers are
    susceptible to adversarial examples, which are created from raw data to fool
    the classifier such that it assigns the example to the wrong class, but
    which are undetectable to the human eye2,3. Adversarial examples have also
    been created for medical-related tasks4,5. However, traditional attack
    methods to create adversarial examples do not extend directly to ECG
    signals, as such methods introduce square-wave artefacts that are not
    physiologically plausible. Here we develop a method to construct smoothed
    adversarial examples for ECG tracings that are invisible to human expert
    evaluation and show that a deep learning model for arrhythmia detection from
    single-lead ECG6 is vulnerable to this type of attack. Moreover, we provide
    a general technique for collating and perturbing known adversarial examples
    to create multiple new ones. The susceptibility of deep learning ECG
    algorithms to adversarial misclassification implies that care should be
    taken when evaluating these models on ECGs that may have been altered,
    particularly when incentives for causing misclassification exist.
  accessed:
    - year: 2020
      month: 10
      day: 28
  author:
    - family: Han
      given: Xintian
    - family: Hu
      given: Yuxuan
    - family: Foschini
      given: Luca
    - family: Chinitz
      given: Larry
    - family: Jankelson
      given: Lior
    - family: Ranganath
      given: Rajesh
  citation-key: han_deep_2020
  container-title: Nature Medicine
  DOI: 10.1038/s41591-020-0791-x
  ISSN: 1546-170X
  issue: '3'
  issued:
    - year: 2020
      month: 3
  language: en
  license: 2020 The Author(s), under exclusive licence to Springer Nature America, Inc.
  number: '3'
  page: 360-363
  publisher: Nature Publishing Group
  source: www.nature.com
  title: >-
    Deep learning models for electrocardiograms are susceptible to adversarial
    attack
  type: article-journal
  URL: https://www.nature.com/articles/s41591-020-0791-x
  volume: '26'

- id: hanin_which_2018
  accessed:
    - year: 2018
      month: 12
      day: 10
  author:
    - family: Hanin
      given: Boris
  citation-key: hanin_which_2018
  issued:
    - year: 2018
      month: 1
      day: 11
  language: en
  source: arxiv.org
  title: >-
    Which Neural Net Architectures Give Rise To Exploding and Vanishing
    Gradients?
  type: article-journal
  URL: https://arxiv.org/abs/1801.03744

- id: hannun_cardiologistlevel_2019
  abstract: >-
    Computerized electrocardiogram (ECG) interpretation plays a critical role in
    the clinical ECG workflow1. Widely available digital ECG data and the
    algorithmic paradigm of deep learning2 present an opportunity to
    substantially improve the accuracy and scalability of automated ECG
    analysis. However, a comprehensive evaluation of an end-to-end deep learning
    approach for ECG analysis across a wide variety of diagnostic classes has
    not been previously reported. Here, we develop a deep neural network (DNN)
    to classify 12 rhythm classes using 91,232 single-lead ECGs from 53,549
    patients who used a single-lead ambulatory ECG monitoring device. When
    validated against an independent test dataset annotated by a consensus
    committee of board-certified practicing cardiologists, the DNN achieved an
    average area under the receiver operating characteristic curve (ROC) of
    0.97. The average F1 score, which is the harmonic mean of the positive
    predictive value and sensitivity, for the DNN (0.837) exceeded that of
    average cardiologists (0.780). With specificity fixed at the average
    specificity achieved by cardiologists, the sensitivity of the DNN exceeded
    the average cardiologist sensitivity for all rhythm classes. These findings
    demonstrate that an end-to-end deep learning approach can classify a broad
    range of distinct arrhythmias from single-lead ECGs with high diagnostic
    performance similar to that of cardiologists. If confirmed in clinical
    settings, this approach could reduce the rate of misdiagnosed computerized
    ECG interpretations and improve the efficiency of expert human ECG
    interpretation by accurately triaging or prioritizing the most urgent
    conditions.
  author:
    - family: Hannun
      given: Awni Y.
    - family: Rajpurkar
      given: Pranav
    - family: Haghpanahi
      given: Masoumeh
    - family: Tison
      given: Geoffrey H.
    - family: Bourn
      given: Codie
    - family: Turakhia
      given: Mintu P.
    - family: Ng
      given: Andrew Y.
  citation-key: hannun_cardiologistlevel_2019
  container-title: Nature Medicine
  container-title-short: Nature Medicine
  DOI: 10/gftc8p
  ISSN: 1546-170X
  issue: '1'
  issued:
    - year: 2019
      month: 1
      day: 1
  page: 65-69
  title: >-
    Cardiologist-level arrhythmia detection and classification in ambulatory
    electrocardiograms using a deep neural network
  type: article-journal
  URL: https://doi.org/10.1038/s41591-018-0268-3
  volume: '25'

- id: hara_can_2017
  abstract: >-
    The purpose of this study is to determine whether current video datasets
    have sufficient data for training very deep convolutional neural networks
    (CNNs) with spatio-temporal three-dimensional (3D) kernels. Recently, the
    performance levels of 3D CNNs in the field of action recognition have
    improved significantly. However, to date, conventional research has only
    explored relatively shallow 3D architectures. We examine the architectures
    of various 3D CNNs from relatively shallow to very deep ones on current
    video datasets. Based on the results of those experiments, the following
    conclusions could be obtained: (i) ResNet-18 training resulted in
    significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for
    Kinetics. (ii) The Kinetics dataset has sufficient data for training of deep
    3D CNNs, and enables training of up to 152 ResNets layers, interestingly
    similar to 2D ResNets on ImageNet. ResNeXt-101 achieved 78.4% average
    accuracy on the Kinetics test set. (iii) Kinetics pretrained simple 3D
    architectures outperforms complex 2D architectures, and the pretrained
    ResNeXt-101 achieved 94.5% and 70.2% on UCF-101 and HMDB-51, respectively.
    The use of 2D CNNs trained on ImageNet has produced significant progress in
    various tasks in image. We believe that using deep 3D CNNs together with
    Kinetics will retrace the successful history of 2D CNNs and ImageNet, and
    stimulate advances in computer vision for videos. The codes and pretrained
    models used in this study are publicly available.
    https://github.com/kenshohara/3D-ResNets-PyTorch
  author:
    - family: Hara
      given: Kensho
    - family: Kataoka
      given: Hirokatsu
    - family: Satoh
      given: Yutaka
  citation-key: hara_can_2017
  container-title: arXiv:1711.09577 [cs]
  issued:
    - year: 2017
      month: 11
      day: 27
  source: arXiv.org
  title: Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?
  type: article-journal
  URL: http://arxiv.org/abs/1711.09577

- id: hardt_identity_2018
  abstract: >-
    An emerging design principle in deep learning is that each layer of a deep
    artiﬁcial neural network should be able to easily express the identity
    transformation. This idea not only motivated various normalization
    techniques, such as batch normalization, but was also key to the immense
    success of residual networks.
  accessed:
    - year: 2020
      month: 8
      day: 27
  author:
    - family: Hardt
      given: Moritz
    - family: Ma
      given: Tengyu
  citation-key: hardt_identity_2018
  container-title: arXiv:1611.04231 [cs, stat]
  issued:
    - year: 2018
      month: 7
      day: 20
  language: en
  source: arXiv.org
  title: Identity Matters in Deep Learning
  type: article-journal
  URL: http://arxiv.org/abs/1611.04231

- id: hassan_statistical_2015
  author:
    - family: Hassan
      given: Abdel-Karim SO
    - family: Abdel-Malek
      given: Hany L
    - family: Mohamed
      given: Ahmed SA
    - family: Abuelfadl
      given: Tamer M
    - family: Elqenawy
      given: Ahmed E
  citation-key: hassan_statistical_2015
  container-title: >-
    Numerical Electromagnetic and Multiphysics Modeling and Optimization (NEMO),
    2015 IEEE MTT-S International Conference on
  issued:
    - year: 2015
  page: 1–3
  publisher: IEEE
  title: >-
    Statistical design centering of RF cavity linear accelerator via
    non-derivative trust region optimization
  type: paper-conference

- id: hassani_curse_2022
  abstract: >-
    Successful deep learning models often involve training neural network
    architectures that contain more parameters than the number of training
    samples. Such overparametrized models have been extensively studied in
    recent years, and the virtues of overparametrization have been established
    from both the statistical perspective, via the double-descent phenomenon,
    and the computational perspective via the structural properties of the
    optimization landscape. Despite the remarkable success of deep learning
    architectures in the overparametrized regime, it is also well known that
    these models are highly vulnerable to small adversarial perturbations in
    their inputs. Even when adversarially trained, their performance on
    perturbed inputs (robust generalization) is considerably worse than their
    best attainable performance on benign inputs (standard generalization). It
    is thus imperative to understand how overparametrization fundamentally
    affects robustness. In this paper, we will provide a precise
    characterization of the role of overparametrization on robustness by
    focusing on random features regression models (two-layer neural networks
    with random first layer weights). We consider a regime where the sample
    size, the input dimension and the number of parameters grow in proportion to
    each other, and derive an asymptotically exact formula for the robust
    generalization error when the model is adversarially trained. Our developed
    theory reveals the nontrivial effect of overparametrization on robustness
    and indicates that for adversarially trained random features models, high
    overparametrization can hurt robust generalization.
  accessed:
    - year: 2022
      month: 2
      day: 25
  author:
    - family: Hassani
      given: Hamed
    - family: Javanmard
      given: Adel
  citation-key: hassani_curse_2022
  container-title: arXiv:2201.05149
  issued:
    - year: 2022
      month: 1
      day: 13
  source: arXiv.org
  title: >-
    The curse of overparametrization in adversarial training: Precise analysis
    of robust generalization for random features regression
  title-short: The curse of overparametrization in adversarial training
  type: article-journal
  URL: http://arxiv.org/abs/2201.05149

- id: hassani_survey_2014
  author:
    - family: Hassani
      given: Vahid
    - family: Tjahjowidodo
      given: Tegoeh
    - family: Do
      given: Thanh Nho
  citation-key: hassani_survey_2014
  container-title: Mechanical Systems and Signal Processing
  container-title-short: Mechanical Systems and Signal Processing
  DOI: 10.1016/j.ymssp.2014.04.012
  ISSN: 0888-3270
  issue: '1'
  issued:
    - year: 2014
      month: 12
      day: 20
  page: 209-233
  title: A survey on hysteresis modeling, identification and control
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0888327014001186
  volume: '49'

- id: hassibi_optimal_1993
  author:
    - family: Hassibi
      given: Babak
    - family: Stork
      given: David G
    - family: Wolff
      given: Gregory J
  citation-key: hassibi_optimal_1993
  container-title: Neural Networks, 1993., IEEE International Conference on
  issued:
    - year: 1993
  page: 293–299
  publisher: IEEE
  title: Optimal brain surgeon and general network pruning
  type: paper-conference

- id: hastie_elements_2009
  author:
    - family: Hastie
      given: Trevor
    - family: Tibshirani
      given: Robert
    - family: Friedman
      given: Jerome
  citation-key: hastie_elements_2009
  edition: Second
  issued:
    - year: 2009
  publisher: Springer Science & Business Media
  title: Elements of Statistical Learning
  type: book

- id: hastie_forward_2007
  accessed:
    - year: 2017
      month: 9
      day: 12
  author:
    - family: Hastie
      given: Trevor
    - family: Taylor
      given: Jonathan
    - family: Tibshirani
      given: Robert
    - family: Walther
      given: Guenther
  citation-key: hastie_forward_2007
  container-title: Electronic Journal of Statistics
  DOI: 10.1214/07-EJS004
  ISSN: 1935-7524
  issue: '0'
  issued:
    - year: 2007
  language: en
  page: 1-29
  source: CrossRef
  title: Forward stagewise regression and the monotone lasso
  type: article-journal
  URL: http://projecteuclid.org/euclid.ejs/1177687773
  volume: '1'

- id: hastie_statistical_2015
  accessed:
    - year: 2017
      month: 9
      day: 20
  author:
    - family: Hastie
      given: Trevor
    - family: Tibshirani
      given: Robert
    - family: Wainwright
      given: Martin
  citation-key: hastie_statistical_2015
  issued:
    - year: 2015
  publisher: CRC press
  source: Google Scholar
  title: 'Statistical learning with sparsity: the lasso and generalizations'
  title-short: Statistical learning with sparsity
  type: book
  URL: >-
    http://books.google.com/books?hl=en&lr=&id=f-A_CQAAQBAJ&oi=fnd&pg=PP1&dq=%22Example:+Distribution%22+%22Computation+for+the+Group%22+%22The+Fused%22+%22Nondi%EF%AC%80erentiable+Functions+and%22+%22A+Simulation%22+%22Fixed-%CE%BB+Inference+for+the%22+%22Cox+Proportional+Hazards%22+%22The+Overlap+Group%22+%22A+Dual+Path%22+&ots=G4ROH6i_SZ&sig=E1_E5N4-_lnR4S_DAyoMex6Xz2g

- id: hastie_surprises_2019
  abstract: >-
    Interpolators---estimators that achieve zero training error---have attracted
    growing attention in machine learning, mainly because state-of-the art
    neural networks appear to be models of this type. In this paper, we study
    minimum $\ell_2$ norm ("ridgeless") interpolation in high-dimensional least
    squares regression. We consider two different models for the feature
    distribution: a linear model, where the feature vectors $x_i \in
    \mathbb{R}^p$ are obtained by applying a linear transform to a vector of
    i.i.d. entries, $x_i = \Sigma^{1/2} z_i$ (with $z_i \in \mathbb{R}^p$); and
    a nonlinear model, where the feature vectors are obtained by passing the
    input through a random one-layer neural network, $x_i = \varphi(W z_i)$
    (with $z_i \in \mathbb{R}^d$, $W \in \mathbb{R}^{p \times d}$ a matrix of
    i.i.d. entries, and $\varphi$ an activation function acting componentwise on
    $W z_i$). We recover---in a precise quantitative way---several phenomena
    that have been observed in large-scale neural networks and kernel machines,
    including the "double descent" behavior of the prediction risk, and the
    potential benefits of overparametrization.
  accessed:
    - year: 2020
      month: 7
      day: 23
  author:
    - family: Hastie
      given: Trevor
    - family: Montanari
      given: Andrea
    - family: Rosset
      given: Saharon
    - family: Tibshirani
      given: Ryan J.
  citation-key: hastie_surprises_2019
  container-title: arXiv:1903.08560
  issued:
    - year: 2019
      month: 11
      day: 4
  source: arXiv.org
  title: Surprises in High-Dimensional Ridgeless Least Squares Interpolation
  type: article-journal
  URL: http://arxiv.org/abs/1903.08560

- id: hastie_surprises_2022
  author:
    - family: Hastie
      given: Trevor
    - family: Montanari
      given: Andrea
    - family: Rosset
      given: Saharon
    - family: Tibshirani
      given: Ryan J.
  citation-key: hastie_surprises_2022
  container-title: The Annals of Statistics
  DOI: 10.1214/21-AOS2133
  issue: '2'
  issued:
    - year: 2022
  page: 949 – 986
  publisher: Institute of Mathematical Statistics
  title: Surprises in high-dimensional ridgeless least squares interpolation
  type: article-journal
  URL: https://doi.org/10.1214/21-AOS2133
  volume: '50'

- id: he_control_2017
  abstract: >-
    In this brief, the control problem for flexible wings of a robotic aircraft
    is addressed by using boundary control schemes. Inspired by birds and bats,
    the wing with flexibility and articulation is modeled as a distributed
    parameter system described by hybrid partial differential equations and
    ordinary differential equations. Boundary control for both wing twist and
    bending is proposed on the original coupled dynamics, and bounded stability
    is proved by introducing a proper Lyapunov function. The effectiveness of
    the proposed control is verified by simulations.
  author:
    - family: He
      given: Wei
    - family: Zhang
      given: Shuang
  citation-key: he_control_2017
  container-title: IEEE Transactions on Control Systems Technology
  DOI: 10.1109/TCST.2016.2536708
  ISSN: 2374-0159
  issue: '1'
  issued:
    - year: 2017
      month: 1
  page: 351-357
  source: IEEE Xplore
  title: Control Design for Nonlinear Flexible Wings of a Robotic Aircraft
  type: article-journal
  volume: '25'

- id: he_deep_2016
  abstract: >-
    Deeper neural networks are more difficult to train. We present a residual
    learning framework to ease the training of networks that are substantially
    deeper than those used previously. We explicitly reformulate the layers as
    learning residual functions with reference to the layer inputs, instead of
    learning unreferenced functions. We provide comprehensive empirical evidence
    showing that these residual networks are easier to optimize, and can gain
    accuracy from considerably increased depth. On the ImageNet dataset we
    evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG
    nets but still having lower complexity. An ensemble of these residual nets
    achieves 3.57% error on the ImageNet test set. This result won the 1st place
    on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10
    with 100 and 1000 layers. The depth of representations is of central
    importance for many visual recognition tasks. Solely due to our extremely
    deep representations, we obtain a 28% relative improvement on the COCO
    object detection dataset. Deep residual nets are foundations of our
    submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st
    places on the tasks of ImageNet detection, ImageNet localization, COCO
    detection, and COCO segmentation.
  author:
    - family: He
      given: Kaiming
    - family: Zhang
      given: Xiangyu
    - family: Ren
      given: Shaoqing
    - family: Sun
      given: Jian
  citation-key: he_deep_2016
  container-title: >-
    Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)
  issued:
    - year: 2016
  page: 770-778
  source: arXiv.org
  title: Deep Residual Learning for Image Recognition
  type: paper-conference
  URL: http://arxiv.org/abs/1512.03385

- id: he_delving_2015
  author:
    - family: He
      given: Kaiming
    - family: Zhang
      given: Xiangyu
    - family: Ren
      given: Shaoqing
    - family: Sun
      given: Jian
  citation-key: he_delving_2015
  container-title: Proceedings of the IEEE International Conference on Computer Vision
  issued:
    - year: 2015
  page: 1026–1034
  source: Google Scholar
  title: >-
    Delving deep into rectifiers: Surpassing human-level performance on imagenet
    classification
  title-short: Delving deep into rectifiers
  type: paper-conference

- id: he_delving_2015a
  abstract: >-
    Rectified activation units (rectifiers) are essential for state-of-the-art
    neural networks. In this work, we study rectifier neural networks for image
    classification from two aspects. First, we propose a Parametric Rectified
    Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU
    improves model fitting with nearly zero extra computational cost and little
    overfitting risk. Second, we derive a robust initialization method that
    particularly considers the rectifier nonlinearities. This method enables us
    to train extremely deep rectified models directly from scratch and to
    investigate deeper or wider network architectures. Based on our PReLU
    networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet
    2012 classification dataset. This is a 26% relative improvement over the
    ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the
    first to surpass human-level performance (5.1%, Russakovsky et al.) on this
    visual recognition challenge.
  author:
    - family: He
      given: Kaiming
    - family: Zhang
      given: Xiangyu
    - family: Ren
      given: Shaoqing
    - family: Sun
      given: Jian
  citation-key: he_delving_2015a
  container-title: arXiv:1502.01852 [cs]
  issued:
    - year: 2015
      month: 2
      day: 6
  source: arXiv.org
  title: >-
    Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification
  title-short: Delving Deep into Rectifiers
  type: article-journal
  URL: http://arxiv.org/abs/1502.01852

- id: he_identity_2016
  abstract: "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\_% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers."
  author:
    - family: He
      given: Kaiming
    - family: Zhang
      given: Xiangyu
    - family: Ren
      given: Shaoqing
    - family: Sun
      given: Jian
  citation-key: he_identity_2016
  container-title: Computer Vision – ECCV 2016
  editor:
    - family: Leibe
      given: Bastian
    - family: Matas
      given: Jiri
    - family: Sebe
      given: Nicu
    - family: Welling
      given: Max
  ISBN: 978-3-319-46493-0
  issued:
    - year: 2016
  page: 630-645
  publisher: Springer International Publishing
  title: Identity Mappings in Deep Residual Networks
  type: paper-conference

- id: he_model_2017
  author:
    - family: He
      given: Wei
    - family: Ge
      given: Weiliang
    - family: Li
      given: Yunchuan
    - family: Liu
      given: Yan-Jun
    - family: Yang
      given: Chenguang
    - family: Sun
      given: Changyin
  citation-key: he_model_2017
  container-title: 'IEEE Transactions on Systems, Man, and Cybernetics: Systems'
  DOI: 10.1109/TSMC.2016.2557227
  issue: '1'
  issued:
    - year: 2017
  page: 45–57
  title: Model identification and control design for a humanoid robot
  type: article-journal
  volume: '47'

- id: he_model_2017a
  author:
    - family: He
      given: Wei
    - family: Ge
      given: Weiliang
    - family: Li
      given: Yunchuan
    - family: Liu
      given: Yan-Jun
    - family: Yang
      given: Chenguang
    - family: Sun
      given: Changyin
  citation-key: he_model_2017a
  container-title: 'IEEE Transactions on Systems, Man, and Cybernetics: Systems'
  DOI: 10/f9kc4j
  issue: '1'
  issued:
    - year: 2017
  note: '00054'
  page: 45-57
  title: Model Identification and Control Design for a Humanoid Robot
  type: article-journal
  volume: '47'

- id: he_new_1993
  author:
    - family: He
      given: Xiangdong
    - family: Asada
      given: Haruhiko
  citation-key: he_new_1993
  container-title: American Control Conference, 1993
  issued:
    - year: 1993
  page: 2520–2523
  publisher: IEEE
  title: >-
    A new method for identifying orders of input-output models for nonlinear
    dynamic systems
  type: paper-conference

- id: he_twosided_2016
  abstract: >-
    In this paper, we investigate and analyze in detail the structure and
    properties of a simultaneous decomposition for fifteen matrices: Ai∈Cpi×ti,
    Bi∈Csi×qi, Ci∈Cpi×ti+1, Di∈Csi+1×qi, and Ei∈Cpi×qi (i=1,2,3). We show that
    from this simultaneous decomposition we can derive some necessary and
    sufficient conditions for the existence of a solution to the system of
    two-sided coupled generalized Sylvester matrix equations with four unknowns
    AiXiBi+CiXi+1Di=Ei (i=1,2,3). Apart from proving an expression for the
    general solutions to this system, we derive the range of ranks of these
    solutions using the ranks of the given matrices Ai, Bi, Ci, Di, and Ei. We
    provide some numerical examples to illustrate our results. Moreover, we
    present a similar approach to consider the simultaneous decomposition for 5k
    matrices and the system of k two-sided coupled generalized Sylvester matrix
    equations with k+1 unknowns AiXiBi+CiXi+1Di=Ei (i=1,…,k, k≥4). The main
    results are also valid over the real number field and the real quaternion
    algebra.
  author:
    - family: He
      given: Zhuo-Heng
    - family: Agudelo
      given: Oscar Mauricio
    - family: Wang
      given: Qing-Wen
    - family: De Moor
      given: Bart
  citation-key: he_twosided_2016
  container-title: Linear Algebra and its Applications
  container-title-short: Linear Algebra and its Applications
  DOI: 10.1016/j.laa.2016.02.013
  ISSN: 0024-3795
  issued:
    - year: 2016
      month: 5
      day: 1
  page: 549-593
  source: ScienceDirect
  title: >-
    Two-sided coupled generalized Sylvester matrix equations solving using a
    simultaneous decomposition for fifteen matrices
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0024379516001142
  volume: '496'

- id: heikkila_fourstep_1997
  author:
    - family: Heikkila
      given: Janne
    - family: Silvén
      given: Olli
  citation-key: heikkila_fourstep_1997
  container-title: >-
    Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE
    Computer Society Conference on
  issued:
    - year: 1997
  page: 1106–1112
  publisher: IEEE
  title: A four-step camera calibration procedure with implicit image correction
  type: paper-conference

- id: helfrich_orthogonal_2018
  abstract: >-
    Recurrent Neural Networks (RNNs) are designed to handle sequential data but
    suffer from vanishing or exploding gradients. Recent work on Unitary
    Recurrent Neural Networks (uRNNs) have been used to address this issue and
    in some cases, exceed the capabilities of Long Short-Term Memory networks
    (LSTMs). We propose a simpler and novel update scheme to maintain orthogonal
    recurrent weight matrices without using complex valued matrices. This is
    done by parametrizing with a skew-symmetric matrix using the Cayley
    transform; such a parametrization is unable to represent matrices with
    negative one eigenvalues, but this limitation is overcome by scaling the
    recurrent weight matrix by a diagonal matrix consisting of ones and negative
    ones. The proposed training scheme involves a straightforward gradient
    calculation and update step. In several experiments, the proposed scaled
    Cayley orthogonal recurrent neural network (scoRNN) achieves superior
    results with fewer trainable parameters than other unitary RNNs.
  author:
    - family: Helfrich
      given: Kyle
    - family: Willmott
      given: Devin
    - family: Ye
      given: Qiang
  citation-key: helfrich_orthogonal_2018
  collection-title: Proceedings of machine learning research
  container-title: Proceedings of the 35th International Conference on Machine Learning
  editor:
    - family: Dy
      given: Jennifer
    - family: Krause
      given: Andreas
  event-place: Stockholmsmässan, Stockholm Sweden
  issued:
    - year: 2018
  page: 1969-1978
  publisher: PMLR
  publisher-place: Stockholmsmässan, Stockholm Sweden
  title: Orthogonal recurrent neural networks with scaled Cayley transform
  type: paper-conference
  URL: http://proceedings.mlr.press/v80/helfrich18a.html
  volume: '80'

- id: henaff_geodesics_2016
  abstract: >-
    We develop a new method for visualizing and refining the invariances of
    learned representations. Specifically, we test for a general form of
    invariance, linearization, in which the action of a transformation is
    confined to a low-dimensional subspace. Given two reference images
    (typically, differing by some transformation), we synthesize a sequence of
    images lying on a path between them that is of minimal length in the space
    of the representation (a "representational geodesic"). If the transformation
    relating the two reference images is linearized by the representation, this
    sequence should follow the gradual evolution of this transformation. We use
    this method to assess the invariance properties of a state-of-the-art image
    classification network and find that geodesics generated for image pairs
    differing by translation, rotation, and dilation do not evolve according to
    their associated transformations. Our method also suggests a remedy for
    these failures, and following this prescription, we show that the modified
    representation is able to linearize a variety of geometric image
    transformations.
  accessed:
    - year: 2020
      month: 3
      day: 23
  author:
    - family: Hénaff
      given: Olivier J.
    - family: Simoncelli
      given: Eero P.
  citation-key: henaff_geodesics_2016
  container-title: >-
    Proceedings of the International Conference for Learning Representations
    (ICLR)
  issued:
    - year: 2016
      month: 2
      day: 22
  source: arXiv.org
  title: Geodesics of learned representations
  type: paper-conference
  URL: http://arxiv.org/abs/1511.06394

- id: hendriks_deep_2021
  abstract: >-
    This paper is directed towards the problem of learning nonlinear ARX models
    based on system input--output data. In particular, our interest is in
    learning a conditional distribution of the current output based on a finite
    window of past inputs and outputs. To achieve this, we consider the use of
    so-called energy-based models, which have been developed in allied fields
    for learning unknown distributions based on data. This energy-based model
    relies on a general function to describe the distribution, and here we
    consider a deep neural network for this purpose. The primary benefit of this
    approach is that it is capable of learning both simple and highly complex
    noise models, which we demonstrate on simulated and experimental data.
  author:
    - family: Hendriks
      given: Johannes N.
    - family: Gustafsson
      given: Fredrik K.
    - family: Ribeiro
      given: Antônio H.
    - family: Wills
      given: Adrian G.
    - family: Schön
      given: Thomas B.
  citation-key: hendriks_deep_2021
  container-title: IFAC Symposium on System Identification (SYSID)
  DOI: 10.1016/j.ifacol.2021.08.410
  issue: '7'
  issued:
    - year: 2021
  license: All rights reserved
  page: 505-510
  source: arXiv.org
  title: Deep Energy-Based NARX Models
  type: article-journal
  volume: '54'

- id: hendriks_deep_2021a
  abstract: >-
    This paper is directed towards the problem of learning nonlinear ARX models
    based on system input–output data. In particular, our interest is in
    learning a conditional distribution of the current output based on a finite
    window of past inputs and outputs. To achieve this, we consider the use of
    so-called energy-based models, which have been developed in allied fields
    for learning unknown distributions based on data. This energy-based model
    relies on a general function to describe the distribution, and here we
    consider a deep neural network for this purpose. The primary benefit of this
    approach is that it is capable of learning both simple and highly complex
    noise models, which we demonstrate on simulated and experimental data.
  author:
    - family: Hendriks
      given: Johannes N.
    - family: Gustafsson
      given: Fredrik K.
    - family: Ribeiro
      given: Antônio H.
    - family: Wills
      given: Adrian G.
    - family: Schön
      given: Thomas B.
  citation-key: hendriks_deep_2021a
  container-title: Workshop on Nonlinear System Identification
  issued:
    - year: 2021
  title: Deep Energy-Based NARX Models
  type: article-journal

- id: hendrycks_benchmarking_2019
  author:
    - family: Hendrycks
      given: Dan
    - family: Dietterich
      given: Thomas
  citation-key: hendrycks_benchmarking_2019
  container-title: Proceedings of the International Conference on Learning Representations
  issued:
    - year: 2019
  title: >-
    Benchmarking neural network robustness to common corruptions and
    perturbations
  type: article-journal

- id: hendrycks_unsolved_2022
  abstract: >-
    Machine learning (ML) systems are rapidly increasing in size, are acquiring
    new capabilities, and are increasingly deployed in high-stakes settings. As
    with other powerful technologies, safety for ML should be a leading research
    priority. In response to emerging safety challenges in ML, such as those
    introduced by recent large-scale models, we provide a new roadmap for ML
    Safety and refine the technical problems that the field needs to address. We
    present four problems ready for research, namely withstanding hazards
    ("Robustness"), identifying hazards ("Monitoring"), reducing inherent model
    hazards ("Alignment"), and reducing systemic hazards ("Systemic Safety").
    Throughout, we clarify each problem's motivation and provide concrete
    research directions.
  accessed:
    - year: 2022
      month: 10
      day: 24
  author:
    - family: Hendrycks
      given: Dan
    - family: Carlini
      given: Nicholas
    - family: Schulman
      given: John
    - family: Steinhardt
      given: Jacob
  citation-key: hendrycks_unsolved_2022
  DOI: 10.48550/arXiv.2109.13916
  issued:
    - year: 2022
      month: 6
      day: 16
  number: arXiv:2109.13916
  publisher: arXiv
  source: arXiv.org
  title: Unsolved Problems in ML Safety
  type: article
  URL: http://arxiv.org/abs/2109.13916

- id: hendrycks_using_2019
  abstract: >-
    He et al. (2018) have called into question the utility of pre-training by
    showing that training from scratch can often yield similar performance to
    pre-training. We show that although pre-training may not improve performance
    on traditional classification metrics, it improves model robustness and
    uncertainty estimates. Through extensive experiments on adversarial
    examples, label corruption, class imbalance, out-of-distribution detection,
    and confidence calibration, we demonstrate large gains from pre-training and
    complementary effects with task-specific methods. We introduce adversarial
    pre-training and show approximately a 10% absolute improvement over the
    previous state-of-the-art in adversarial robustness. In some cases, using
    pre-training without task-specific methods also surpasses the
    state-of-the-art, highlighting the need for pre-training when evaluating
    future methods on robustness and uncertainty tasks.
  accessed:
    - year: 2020
      month: 3
      day: 24
  author:
    - family: Hendrycks
      given: Dan
    - family: Lee
      given: Kimin
    - family: Mazeika
      given: Mantas
  citation-key: hendrycks_using_2019
  issued:
    - year: 2019
  source: arXiv.org
  title: Using Pre-Training Can Improve Model Robustness and Uncertainty
  type: article-journal
  URL: http://arxiv.org/abs/1901.09960

- id: hennessy_computer_2012
  author:
    - family: Hennessy
      given: John L.
    - family: Patterson
      given: David A.
    - family: Asanović
      given: Krste
  call-number: QA76.9.A73 P377 2012
  citation-key: hennessy_computer_2012
  edition: 5th ed
  event-place: Waltham, MA
  ISBN: 978-0-12-383872-8
  issued:
    - year: 2012
  note: 'OCLC: ocn755102367'
  number-of-pages: '493'
  publisher: Morgan Kaufmann/Elsevier
  publisher-place: Waltham, MA
  source: Library of Congress ISBN
  title: 'Computer architecture: a quantitative approach'
  title-short: Computer architecture
  type: book

- id: hereid_hybrid_2015
  abstract: >-
    Hybrid zero dynamics (HZD) has emerged as a popular framework for the stable
    control of bipedal robotic gaits, but typically designing a gait's virtual
    constraints is a slow and undependable optimization process. To expedite and
    boost the reliability of HZD gait generation, we borrow methods from
    trajectory optimization to formulate a smoother and more linear optimization
    problem. We present a multiple-shooting formulation for the optimization of
    virtual constraints, combining the stability-friendly properties of HZD with
    an optimization-conducive problem formulation. To showcase the implications
    of this recipe for improving gait generation, we use the same process to
    generate periodic planar walking gaits on two different robot models, and in
    one case, demonstrate stable walking on the hardware prototype, DURUS-R.
  author:
    - family: Hereid
      given: A.
    - family: Hubicki
      given: C. M.
    - family: Cousineau
      given: E. A.
    - family: Hurst
      given: J. W.
    - family: Ames
      given: A. D.
  citation-key: hereid_hybrid_2015
  container-title: 2015 IEEE International Conference on Robotics and Automation (ICRA)
  DOI: 10.1109/ICRA.2015.7140002
  event-title: 2015 IEEE International Conference on Robotics and Automation (ICRA)
  issued:
    - year: 2015
      month: 5
  page: 5734-5740
  source: IEEE Xplore
  title: >-
    Hybrid zero dynamics based multiple shooting optimization with applications
    to robotic walking
  type: paper-conference

- id: hernan_causal_2020
  author:
    - family: Hernan
      given: Miguel A
    - family: Robins
      given: James M
  citation-key: hernan_causal_2020
  issued:
    - year: 2020
  language: en
  source: Zotero
  title: 'Causal Inference: What If'
  type: article-journal

- id: hernan_data_2018
  abstract: >-
    Causal inference from observational data is the goal of many data analyses
    in the health and social sciences. However, academic statistics has often
    frowned upon data analyses with a causal objective. The introduction of the
    term "data science" provides a historic opportunity to redefine data
    analysis in such a way that it naturally accommodates causal inference from
    observational data. Like others before, we organize the scientific
    contributions of data science into three classes of tasks: description,
    prediction, and causal inference. An explicit classification of data science
    tasks is necessary to discuss the data, assumptions, and analytics required
    to successfully accomplish each task. We argue that a failure to adequately
    describe the role of subject-matter expert knowledge in data analysis is a
    source of widespread misunderstandings about data science. Specifically,
    causal analyses typically require not only good data and algorithms, but
    also domain expert knowledge. We discuss the implications for the use of
    data science to guide decision-making in the real world and to train data
    scientists.
  accessed:
    - year: 2018
      month: 12
      day: 13
  author:
    - family: Hernán
      given: Miguel A.
    - family: Hsu
      given: John
    - family: Healy
      given: Brian
  citation-key: hernan_data_2018
  container-title: arXiv:1804.10846 [cs, stat]
  issued:
    - year: 2018
      month: 4
      day: 28
  source: arXiv.org
  title: >-
    Data science is science's second chance to get causal inference right: A
    classification of data science tasks
  title-short: Data science is science's second chance to get causal inference right
  type: article-journal
  URL: http://arxiv.org/abs/1804.10846

- id: hestenes_methods_1952
  author:
    - family: Hestenes
      given: Magnus Rudolph
    - family: Stiefel
      given: Eduard
  citation-key: hestenes_methods_1952
  issued:
    - year: 1952
  publisher: NBS
  title: Methods of conjugate gradients for solving linear systems
  type: book
  volume: '49'

- id: hinton_deep_2012
  abstract: >-
    Most current speech recognition systems use hidden Markov models (HMMs) to
    deal with the temporal variability of speech and Gaussian mixture models
    (GMMs) to determine how well each state of each HMM fits a frame or a short
    window of frames of coefficients that represents the acoustic input. An
    alternative way to evaluate the fit is to use a feed-forward neural network
    that takes several frames of coefficients as input and produces posterior
    probabilities over HMM states as output. Deep neural networks (DNNs) that
    have many hidden layers and are trained using new methods have been shown to
    outperform GMMs on a variety of speech recognition benchmarks, sometimes by
    a large margin. This article provides an overview of this progress and
    represents the shared views of four research groups that have had recent
    successes in using DNNs for acoustic modeling in speech recognition.
  author:
    - family: Hinton
      given: G.
    - family: Deng
      given: L.
    - family: Yu
      given: D.
    - family: Dahl
      given: G. E.
    - family: Mohamed
      given: A.
    - family: Jaitly
      given: N.
    - family: Senior
      given: A.
    - family: Vanhoucke
      given: V.
    - family: Nguyen
      given: P.
    - family: Sainath
      given: T. N.
    - family: Kingsbury
      given: B.
  citation-key: hinton_deep_2012
  container-title: IEEE Signal Processing Magazine
  DOI: 10/gc8z3r
  ISSN: 1053-5888
  issue: '6'
  issued:
    - year: 2012
      month: 11
  page: 82-97
  source: IEEE Xplore
  title: >-
    Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared
    Views of Four Research Groups
  title-short: Deep Neural Networks for Acoustic Modeling in Speech Recognition
  type: article-journal
  volume: '29'

- id: hinton_deep_2018
  abstract: >-
    Widespread application of artificial intelligence in health care has been
    anticipated for half a century. For most of that time, the dominant approach
    to artificial intelligence was inspired by logic: researchers assumed that
    the essence of intelligence was manipulating symbolic expressions, using
    rules of inference. This approach produced expert systems and graphical
    models that attempted to automate the reasoning processes of experts. In the
    last decade, however, a radically different approach to artificial
    intelligence, called deep learning, has produced major breakthroughs and is
    now used on billions of digital devices for complex tasks such as speech
    recognition, image interpretation, and language translation. The purpose of
    this Viewpoint is to give health care professionals an intuitive
    understanding of the technology underlying deep learning. In an accompanying
    Viewpoint, Naylor1 outlines some of the factors propelling adoption of this
    technology in medicine and health care.
  author:
    - family: Hinton
      given: Geoffrey
  citation-key: hinton_deep_2018
  container-title: JAMA
  container-title-short: JAMA
  DOI: 10/gfkhr6
  ISSN: 0098-7484
  issue: '11'
  issued:
    - year: 2018
      month: 9
      day: 18
  page: 1101-1102
  title: Deep learning—a technology with the potential to transform health care
  type: article-journal
  URL: http://dx.doi.org/10.1001/jama.2018.11100
  volume: '320'

- id: hinton_fast_2006
  abstract: >-
    We show how to use “complementary priors” to eliminate the explaining away
    effects that make inference difﬁcult in densely-connected belief nets that
    have many hidden layers. Using complementary priors, we derive a fast,
    greedy algorithm that can learn deep, directed belief networks one layer at
    a time, provided the top two layers form an undirected associative memory.
    The fast, greedy algorithm is used to initialize a slower learning procedure
    that ﬁne-tunes the weights using a contrastive version of the wake-sleep
    algorithm. After ﬁne-tuning, a network with three hidden layers forms a very
    good generative model of the joint distribution of handwritten digit images
    and their labels. This generative model gives better digit classiﬁcation
    than the best discriminative learning algorithms. The low-dimensional
    manifolds on which the digits lie are modelled by long ravines in the
    free-energy landscape of the top-level associative memory and it is easy to
    explore these ravines by using the directed connections to display what the
    associative memory has in mind.
  accessed:
    - year: 2019
      month: 2
      day: 27
  author:
    - family: Hinton
      given: Geoffrey E.
    - family: Osindero
      given: Simon
    - family: Teh
      given: Yee-Whye
  citation-key: hinton_fast_2006
  container-title: Neural Computation
  DOI: 10/cjnhxz
  ISSN: 0899-7667, 1530-888X
  issue: '7'
  issued:
    - year: 2006
      month: 7
  language: en
  page: 1527-1554
  source: Crossref
  title: A Fast Learning Algorithm for Deep Belief Nets
  type: article-journal
  URL: http://www.mitpressjournals.org/doi/10.1162/neco.2006.18.7.1527
  volume: '18'

- id: hinton_learning_1986
  author:
    - family: Hinton
      given: Geoffrey E
  citation-key: hinton_learning_1986
  event-title: Proceedings of the eighth annual conference of the cognitive science society
  issued:
    - year: 1986
  page: '12'
  publisher: Amherst, MA
  title: Learning distributed representations of concepts
  type: paper-conference
  volume: '1'

- id: hinton_potential_2018
  author:
    - family: Hinton
      given: Geoffrey
  citation-key: hinton_potential_2018
  issued:
    - year: 2018
  language: en
  page: '2'
  source: Zotero
  title: With the Potential to Transform Health Care
  type: article-journal

- id: hirschmuller_accurate_2005
  author:
    - family: Hirschmüller
      given: Heiko
  citation-key: hirschmuller_accurate_2005
  container-title: >-
    Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer
    Society Conference on
  issued:
    - year: 2005
  page: 807–814
  publisher: IEEE
  title: >-
    Accurate and efficient stereo processing by semi-global matching and mutual
    information
  type: paper-conference
  volume: '2'

- id: hirschmuller_stereo_2005
  accessed:
    - year: 2017
      month: 8
      day: 20
  author:
    - family: Hirschmüller
      given: Heiko
    - family: Scholten
      given: Frank
    - family: Hirzinger
      given: Gerd
  citation-key: hirschmuller_stereo_2005
  container-title: Lecture notes in computer science
  DOI: 10.1007/11550518_8
  issued:
    - year: 2005
  page: '58'
  source: Google Scholar
  title: >-
    Stereo vision based reconstruction of huge urban areas from an airborne
    pushbroom camera (HRSC)
  type: article-journal
  URL: http://link.springer.com/content/pdf/10.1007/11550518.pdf#page=72
  volume: '3663'

- id: hirschmuller_stereo_2008
  author:
    - family: Hirschmüller
      given: Heiko
  citation-key: hirschmuller_stereo_2008
  container-title: Pattern Analysis and Machine Intelligence, IEEE Transactions on
  DOI: 10.1109/TPAMI.2007.1166
  issue: '2'
  issued:
    - year: 2008
  page: 328–341
  title: Stereo processing by semiglobal matching and mutual information
  type: article-journal
  volume: '30'

- id: hlatky_criteria_2009
  abstract: >-
    There is increasing interest in utilizing novel markers of cardiovascular
    disease risk, and consequently, there is a need to assess the value of their
    use. This scientific statement reviews current concepts of risk evaluation
    and proposes standards for the critical appraisal of risk assessment
    methods. An adequate evaluation of a novel risk marker requires a sound
    research design, a representative at-risk population, and an adequate number
    of outcome events. Studies of a novel marker should report the degree to
    which it adds to the prognostic information provided by standard risk
    markers. No single statistical measure provides all the information needed
    to assess a novel marker, so measures of both discrimination and accuracy
    should be reported. The clinical value of a marker should be assessed by its
    effect on patient management and outcomes. In general, a novel risk marker
    should be evaluated in several phases, including initial proof of concept,
    prospective validation in independent populations, documentation of
    incremental information when added to standard risk markers, assessment of
    effects on patient management and outcomes, and ultimately,
    cost-effectiveness.
  accessed:
    - year: 2023
      month: 8
      day: 30
  author:
    - family: Hlatky
      given: Mark A.
    - family: Greenland
      given: Philip
    - family: Arnett
      given: Donna K.
    - family: Ballantyne
      given: Christie M.
    - family: Criqui
      given: Michael H.
    - family: Elkind
      given: Mitchell S.V.
    - family: Go
      given: Alan S.
    - family: Harrell
      given: Frank E.
    - family: Hong
      given: Yuling
    - family: Howard
      given: Barbara V.
    - family: Howard
      given: Virginia J.
    - family: Hsue
      given: Priscilla Y.
    - family: Kramer
      given: Christopher M.
    - family: McConnell
      given: Joseph P.
    - family: Normand
      given: Sharon-Lise T.
    - family: O'Donnell
      given: Christopher J.
    - family: Smith
      given: Sidney C.
    - family: Wilson
      given: Peter W.F.
  citation-key: hlatky_criteria_2009
  container-title: Circulation
  container-title-short: Circulation
  DOI: 10.1161/CIRCULATIONAHA.109.192278
  ISSN: 0009-7322, 1524-4539
  issue: '17'
  issued:
    - year: 2009
      month: 5
      day: 5
  language: en
  page: 2408-2416
  source: DOI.org (Crossref)
  title: >-
    Criteria for Evaluation of Novel Markers of Cardiovascular Risk: A
    Scientific Statement From the American Heart Association
  title-short: Criteria for Evaluation of Novel Markers of Cardiovascular Risk
  type: article-journal
  URL: https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.109.192278
  volume: '119'

- id: hochreiter_long_1997
  author:
    - family: Hochreiter
      given: Sepp
    - family: Schmidhuber
      given: Jürgen
  citation-key: hochreiter_long_1997
  container-title: Neural computation
  container-title-short: Neural computation
  DOI: 10.1162/neco.1997.9.8.1735
  ISSN: 0899-7667
  issue: '8'
  issued:
    - year: 1997
  page: 1735-1780
  title: Long short-term memory
  type: article-journal
  volume: '9'

- id: hochreiter_vanishing_1998
  author:
    - family: Hochreiter
      given: Sepp
  citation-key: hochreiter_vanishing_1998
  container-title: International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems
  DOI: 10.1142/S0218488598000094
  ISSN: 0218-4885, 1793-6411
  issue: '02'
  issued:
    - year: 1998
      month: 4
  language: en
  page: 107-116
  source: CrossRef
  title: >-
    The Vanishing Gradient Problem During Learning Recurrent Neural Nets and
    Problem Solutions
  type: article-journal
  URL: http://www.worldscientific.com/doi/abs/10.1142/S0218488598000094
  volume: '06'

- id: hodgkin_quantitative_1952
  accessed:
    - year: 2022
      month: 1
      day: 12
  author:
    - family: Hodgkin
      given: A. L.
    - family: Huxley
      given: A. F.
  citation-key: hodgkin_quantitative_1952
  container-title: The Journal of Physiology
  DOI: 10.1113/jphysiol.1952.sp004764
  ISSN: 1469-7793
  issue: '4'
  issued:
    - year: 1952
  language: en
  page: 500-544
  source: Wiley Online Library
  title: >-
    A quantitative description of membrane current and its application to
    conduction and excitation in nerve
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1952.sp004764
  volume: '117'

- id: hoffer_modern_2011
  author:
    - family: Hoffer
      given: Jeffrey A.
    - family: Ramesh
      given: V.
    - family: Topi
      given: Heikki
  call-number: QA76.9.D3 M395 2011
  citation-key: hoffer_modern_2011
  edition: 10th ed
  event-place: Upper Saddle River, N.J
  ISBN: 978-0-13-608839-4
  issued:
    - year: 2011
  note: 'OCLC: ocn613293263'
  number-of-pages: '581'
  publisher: Prentice Hall
  publisher-place: Upper Saddle River, N.J
  source: Library of Congress ISBN
  title: Modern database management
  type: book

- id: hoffman_nouturn_2011
  abstract: >-
    Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm
    that avoids the random walk behavior and sensitivity to correlated
    parameters that plague many MCMC methods by taking a series of steps
    informed by first-order gradient information. These features allow it to
    converge to high-dimensional target distributions much more quickly than
    simpler methods such as random walk Metropolis or Gibbs sampling. However,
    HMC's performance is highly sensitive to two user-specified parameters: a
    step size {\epsilon} and a desired number of steps L. In particular, if L is
    too small then the algorithm exhibits undesirable random walk behavior,
    while if L is too large the algorithm wastes computation. We introduce the
    No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to
    set a number of steps L. NUTS uses a recursive algorithm to build a set of
    likely candidate points that spans a wide swath of the target distribution,
    stopping automatically when it starts to double back and retrace its steps.
    Empirically, NUTS perform at least as efficiently as and sometimes more
    efficiently than a well tuned standard HMC method, without requiring user
    intervention or costly tuning runs. We also derive a method for adapting the
    step size parameter {\epsilon} on the fly based on primal-dual averaging.
    NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for
    applications such as BUGS-style automatic inference engines that require
    efficient "turnkey" sampling algorithms.
  author:
    - family: Hoffman
      given: Matthew D.
    - family: Gelman
      given: Andrew
  citation-key: hoffman_nouturn_2011
  container-title: arXiv:1111.4246 [cs, stat]
  issued:
    - year: 2011
      month: 11
      day: 17
  source: arXiv.org
  title: >-
    The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte
    Carlo
  title-short: The No-U-Turn Sampler
  type: article-journal
  URL: http://arxiv.org/abs/1111.4246

- id: hoffman_unsupervised_2012
  abstract: >-
    We trained Segway, a dynamic Bayesian network method, simultaneously on
    chromatin data from multiple experiments, including positions of histone
    modifications, transcription-factor binding and open chromatin, all derived
    from a human chronic myeloid leukemia cell line. In an unsupervised fashion,
    we identified patterns associated with transcription start sites, gene ends,
    enhancers, transcriptional regulator CTCF-binding regions and repressed
    regions. Software and genome browser tracks are at
    http://noble.gs.washington.edu/proj/segway/.
  accessed:
    - year: 2019
      month: 4
      day: 1
  author:
    - family: Hoffman
      given: Michael M.
    - family: Buske
      given: Orion J.
    - family: Wang
      given: Jie
    - family: Weng
      given: Zhiping
    - family: Bilmes
      given: Jeff A.
    - family: Noble
      given: William Stafford
  citation-key: hoffman_unsupervised_2012
  container-title: Nature Methods
  DOI: 10/gfxrbn
  ISSN: 1548-7105
  issue: '5'
  issued:
    - year: 2012
      month: 5
  language: en
  license: 2012 Nature Publishing Group
  page: 473-476
  source: www.nature.com
  title: >-
    Unsupervised pattern discovery in human chromatin structure through genomic
    segmentation
  type: article-journal
  URL: https://www.nature.com/articles/nmeth.1937
  volume: '9'

- id: hong_encase_2017
  abstract: >-
    We propose ENCASE to combine expert features and DNNs (Deep Neural Networks)
    together for ECG classiﬁcation. We ﬁrst explore and implement expert
    features from statistical area, signal processing area and medical area.
    Then, we build DNNs to automatically extract deep features. Besides, we
    propose a new algorithm to ﬁnd the most representative wave (called
    centerwave) among long ECG record, and extract features from centerwave.
    Finally, we combine these features together and put them into ensemble
    classiﬁers. Experiment on 4-class ECG data classiﬁcation reports 0.84 F1
    score, which is much better than any of the single model.
  accessed:
    - year: 2019
      month: 1
      day: 24
  author:
    - family: Hong
      given: Shenda
    - family: Wu
      given: Meng
    - family: Zhou
      given: Yuxi
    - family: Wang
      given: Qingyun
    - family: Shang
      given: Junyuan
    - family: Li
      given: Hongyan
    - family: Xie
      given: Junqing
  citation-key: hong_encase_2017
  DOI: 10/gftsxw
  event-title: 2017 Computing in Cardiology Conference
  issued:
    - year: 2017
      month: 9
      day: 14
  language: en
  source: Crossref
  title: >-
    ENCASE: an ENsemble ClASsifiEr for ECG Classification Using Expert Features
    and Deep Neural Networks
  title-short: ENCASE
  type: paper-conference
  URL: http://www.cinc.org/archives/2017/pdf/178-245.pdf

- id: hooke_direct_1961
  author:
    - family: Hooke
      given: Robert
    - family: Jeeves
      given: Terry A
  citation-key: hooke_direct_1961
  container-title: Journal of the ACM (JACM)
  DOI: 10.1145/321062.321069
  issue: '2'
  issued:
    - year: 1961
  page: 212–229
  title: “Direct Search” Solution of Numerical and Statistical Problems
  type: article-journal
  volume: '8'

- id: horbelt_identifying_2001
  author:
    - family: Horbelt
      given: W
    - family: Timmer
      given: J
    - family: Bünner
      given: MJ
    - family: Meucci
      given: R
    - family: Ciofini
      given: M
  citation-key: horbelt_identifying_2001
  container-title: Physical Review E
  DOI: 10.1103/PhysRevE.64.016222
  issue: '1'
  issued:
    - year: 2001
  page: '016222'
  title: >-
    Identifying physical properties of a CO2 LASER by dynamical modeling of
    measured time series
  type: article-journal
  volume: '64'

- id: hornik_multilayer_1989
  author:
    - family: Hornik
      given: Kurt
    - family: Stinchcombe
      given: Maxwell
    - family: White
      given: Halbert
  citation-key: hornik_multilayer_1989
  container-title: Neural Networks
  issue: '5'
  issued:
    - year: 1989
  page: 359–366
  title: Multilayer feedforward networks are universal approximators
  type: article-journal
  volume: '2'

- id: howard_universal_2018
  abstract: >-
    Inductive transfer learning has greatly impacted computer vision, but
    existing approaches in NLP still require task-specific modifications and
    training from scratch. We propose Universal Language Model Fine-tuning
    (ULMFiT), an effective transfer learning method that can be applied to any
    task in NLP, and introduce techniques that are key for fine-tuning a
    language model. Our method significantly outperforms the state-of-the-art on
    six text classification tasks, reducing the error by 18-24% on the majority
    of datasets. Furthermore, with only 100 labeled examples, it matches the
    performance of training from scratch on 100x more data. We open-source our
    pretrained models and code.
  accessed:
    - year: 2019
      month: 6
      day: 10
  author:
    - family: Howard
      given: Jeremy
    - family: Ruder
      given: Sebastian
  citation-key: howard_universal_2018
  container-title: arXiv:1801.06146 [cs, stat]
  issued:
    - year: 2018
      month: 1
      day: 18
  source: arXiv.org
  title: Universal Language Model Fine-tuning for Text Classification
  type: article-journal
  URL: http://arxiv.org/abs/1801.06146

- id: hsu_proliferation_2021
  abstract: >-
    The support vector machine (SVM) is a well-established classification method
    whose name refers to the particular training examples, called support
    vectors, that determine the maximum margin separating hyperplane. The SVM
    classifier is known to enjoy good generalization properties when the number
    of support vectors is small compared to the number of training examples.
    However, recent research has shown that in sufficiently high-dimensional
    linear classification problems, the SVM can generalize well despite a
    proliferation of support vectors where all training examples are support
    vectors. In this paper, we identify new deterministic equivalences for this
    phenomenon of support vector proliferation, and use them to (1)
    substantially broaden the conditions under which the phenomenon occurs in
    high-dimensional settings, and (2) prove a nearly matching converse result.
  accessed:
    - year: 2022
      month: 11
      day: 16
  author:
    - family: Hsu
      given: Daniel
    - family: Muthukumar
      given: Vidya
    - family: Xu
      given: Ji
  citation-key: hsu_proliferation_2021
  container-title: >-
    Proceedings of The 24th International Conference on Artificial Intelligence
    and Statistics
  event-title: International Conference on Artificial Intelligence and Statistics
  ISSN: 2640-3498
  issued:
    - year: 2021
      month: 3
      day: 18
  language: en
  page: 91-99
  publisher: PMLR
  source: proceedings.mlr.press
  title: On the proliferation of support vectors in high dimensions
  type: paper-conference
  URL: https://proceedings.mlr.press/v130/hsu21a.html

- id: hsu_subset_2008
  abstract: >-
    A subset selection method is proposed for vector autoregressive (VAR)
    processes using the Lasso [Tibshirani, R. (1996). Regression shrinkage and
    selection via the Lasso. Journal of the Royal Statistical Society, Series B
    58, 267–288] technique. Simply speaking, Lasso is a shrinkage method in a
    regression setup which selects the model and estimates the parameters
    simultaneously. Compared to the conventional information-based methods such
    as AIC and BIC, the Lasso approach avoids computationally intensive and
    exhaustive search. On the other hand, compared to the existing subset
    selection methods with parameter constraints such as the top-down and
    bottom-up strategies, the Lasso method is computationally efficient and its
    result is robust to the order of series included in the autoregressive
    model. We derive the asymptotic theorem for the Lasso estimator under VAR
    processes. Simulation results demonstrate that the Lasso method performs
    better than several conventional subset selection methods for small samples
    in terms of prediction mean squared errors and estimation errors under
    various settings. The methodology is applied to modeling U.S. macroeconomic
    data for illustration.
  author:
    - family: Hsu
      given: Nan-Jung
    - family: Hung
      given: Hung-Lin
    - family: Chang
      given: Ya-Mei
  citation-key: hsu_subset_2008
  container-title: Computational Statistics & Data Analysis
  container-title-short: Computational Statistics & Data Analysis
  DOI: 10.1016/j.csda.2007.12.004
  ISSN: 0167-9473
  issue: '7'
  issued:
    - year: 2008
      month: 3
      day: 15
  page: 3645-3657
  source: ScienceDirect
  title: Subset selection for vector autoregressive processes using Lasso
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0167947307004549
  volume: '52'

- id: hu_infinitely_2020
  abstract: >-
    Graph convolutional neural networks~(GCNs) have recently demonstrated
    promising results on graph-based semi-supervised classification, but little
    work has been done to explore their theoretical properties. Recently,
    several deep neural networks, e.g., fully connected and convolutional neural
    networks, with infinite hidden units have been proved to be equivalent to
    Gaussian processes~(GPs). To exploit both the powerful representational
    capacity of GCNs and the great expressive power of GPs, we investigate
    similar properties of infinitely wide GCNs. More specifically, we propose a
    GP regression model via GCNs~(GPGC) for graph-based semi-supervised
    learning. In the process, we formulate the kernel matrix computation of GPGC
    in an iterative analytical form. Finally, we derive a conditional
    distribution for the labels of unobserved nodes based on the graph
    structure, labels for the observed nodes, and the feature matrix of all the
    nodes. We conduct extensive experiments to evaluate the semi-supervised
    classification performance of GPGC and demonstrate that it outperforms other
    state-of-the-art methods by a clear margin on all the datasets while being
    efficient.
  accessed:
    - year: 2022
      month: 7
      day: 26
  author:
    - family: Hu
      given: Jilin
    - family: Shen
      given: Jianbing
    - family: Yang
      given: Bin
    - family: Shao
      given: Ling
  citation-key: hu_infinitely_2020
  DOI: 10.48550/arXiv.2002.12168
  issued:
    - year: 2020
      month: 2
      day: 26
  number: arXiv:2002.12168
  publisher: arXiv
  source: arXiv.org
  title: >-
    Infinitely Wide Graph Convolutional Networks: Semi-supervised Learning via
    Gaussian Processes
  title-short: Infinitely Wide Graph Convolutional Networks
  type: article
  URL: http://arxiv.org/abs/2002.12168

- id: hua_causal_2022
  abstract: >-
    The information bottleneck (IB) method is a feasible defense solution
    against adversarial attacks in deep learning. However, this method suffers
    from the spurious correlation, which leads to the limitation of its further
    improvement of adversarial robustness. In this paper, we incorporate the
    causal inference into the IB framework to alleviate such a problem.
    Specifically, we divide the features obtained by the IB method into robust
    features (content information) and non-robust features (style information)
    via the instrumental variables to estimate the causal effects. With the
    utilization of such a framework, the influence of non-robust features could
    be mitigated to strengthen the adversarial robustness. We make an analysis
    of the effectiveness of our proposed method. The extensive experiments in
    MNIST, FashionMNIST, and CIFAR-10 show that our method exhibits the
    considerable robustness against multiple adversarial attacks. Our code would
    be released.
  accessed:
    - year: 2023
      month: 4
      day: 14
  author:
    - family: Hua
      given: Huan
    - family: Yan
      given: Jun
    - family: Fang
      given: Xi
    - family: Huang
      given: Weiquan
    - family: Yin
      given: Huilin
    - family: Ge
      given: Wancheng
  citation-key: hua_causal_2022
  DOI: 10.48550/arXiv.2210.14229
  issued:
    - year: 2022
      month: 10
      day: 25
  number: arXiv:2210.14229
  publisher: arXiv
  source: arXiv.org
  title: >-
    Causal Information Bottleneck Boosts Adversarial Robustness of Deep Neural
    Network
  type: article
  URL: http://arxiv.org/abs/2210.14229

- id: huang_densely_2016
  abstract: >-
    Recent work has shown that convolutional networks can be substantially
    deeper, more accurate, and efficient to train if they contain shorter
    connections between layers close to the input and those close to the output.
    In this paper, we embrace this observation and introduce the Dense
    Convolutional Network (DenseNet), which connects each layer to every other
    layer in a feed-forward fashion. Whereas traditional convolutional networks
    with L layers have L connections - one between each layer and its subsequent
    layer - our network has L(L+1)/2 direct connections. For each layer, the
    feature-maps of all preceding layers are used as inputs, and its own
    feature-maps are used as inputs into all subsequent layers. DenseNets have
    several compelling advantages: they alleviate the vanishing-gradient
    problem, strengthen feature propagation, encourage feature reuse, and
    substantially reduce the number of parameters. We evaluate our proposed
    architecture on four highly competitive object recognition benchmark tasks
    (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant
    improvements over the state-of-the-art on most of them, whilst requiring
    less computation to achieve high performance. Code and pre-trained models
    are available at https://github.com/liuzhuang13/DenseNet .
  accessed:
    - year: 2019
      month: 6
      day: 13
  author:
    - family: Huang
      given: Gao
    - family: Liu
      given: Zhuang
    - family: Maaten
      given: Laurens
      non-dropping-particle: van der
    - family: Weinberger
      given: Kilian Q.
  citation-key: huang_densely_2016
  container-title: arXiv:1608.06993 [cs]
  issued:
    - year: 2016
      month: 8
      day: 24
  source: arXiv.org
  title: Densely Connected Convolutional Networks
  type: article-journal
  URL: http://arxiv.org/abs/1608.06993

- id: huang_generalization_2024
  abstract: "Aim: Deep learning’s widespread use prompts heightened scrutiny, particularly in the biomedical fields, with a specific focus on model generalizability. This study delves into the influence of training data characteristics on the generalization performance of models, specifically in cardiac abnormality detection. Materials & methods: Leveraging diverse electrocardiogram datasets, models are trained on subsets with varying characteristics and subsequently compared for performance. Additionally, the introduction of the attention mechanism aims to improve generalizability. Results: Experiments reveal that using a balanced dataset, just 1% of a large dataset, leads to equal performance in generalization tasks, notably in detecting cardiology abnormalities. Conclusion: This balanced training data notably enhances model generalizability, while the integration of the attention mechanism further refines the model’s ability to generalize effectively. This study tackles a common problem for deep learning models: they often struggle when faced with new, unfamiliar data that they have not\_been trained on. This phenomenon is also known as performance drop in out-of-distribution generalization. This reduced performance on out-of-distribution generalization is a key focus of the research, aiming to improve the models’\_ability to handle diverse data sets beyond their training data. The study examines how the characteristics of the dataset used to train deep learning models affect their ability to detect abnormal heart activities when applied to new, unseen data. Researchers trained these models using various sets of electrocardiogram\_(ECG)\_data and then evaluated their performance in identifying abnormalities. They also introduced an attention mechanism to enhance the models’\_learning capabilities. The attention mechanism in deep learning is like a spotlight that helps the model focus on important information while ignoring less relevant details. The findings were particularly noteworthy. Despite being trained on a small, well-balanced subset of a larger dataset, the models excelled in detecting heart abnormalities in new, unfamiliar data. This training method significantly improved the models’\_generalization and performance with unseen data. Furthermore, integrating the attention mechanism substantially enhanced the models’\_ability to generalize effectively on new information. Investigate the impact of training data characteristics and attention mechanism on deep learning model generalizability in cardiac abnormality detection. Balanced dataset (1% of the total) improves model performance in generalization tasks, especially in detecting cardiology abnormalities. The attention mechanism further enhances the model’s capacity to comprehend and utilize out-of-distribution data effectively. Utilized multiple electrocardiogram datasets for the study. Trained models on subsets with varying characteristics and evaluated performance. Added attention mechanism to enhance learning capabilities. Balanced training data significantly enhances model generalizability. Attention mechanism improves the model’s ability to generalize on out-of-distribution data. Lack of clinical user information in datasets due to privacy and ethical considerations. Future research may consider patient-specific models for improved generalization in biomedical machine learning. Balanced and curated datasets are crucial for training high-performing models in cardiac abnormality detection using deep learning. Attention mechanisms show promise in enhancing model accuracy and generalization."
  accessed:
    - year: 2024
      month: 6
      day: 12
  author:
    - family: Huang
      given: Zhaojing
    - family: MacLachlan
      given: Sarisha
    - family: Yu
      given: Leping
    - family: Herbozo Contreras
      given: Luis Fernando
    - family: Truong
      given: Nhan Duy
    - family: Ribeiro
      given: Antonio Horta
    - family: Kavehei
      given: Omid
  citation-key: huang_generalization_2024
  container-title: Future Cardiology
  DOI: 10.1080/14796678.2024.2354082
  ISSN: 1479-6678
  issue: '0'
  issued:
    - year: 2024
  license: All rights reserved
  note: https://www.medrxiv.org/content/10.1101/2023.07.05.23292238v2
  page: 1–12
  source: Taylor and Francis+NEJM
  title: >-
    Generalization challenges in electrocardiogram deep learning: insights from
    dataset characteristics and attention mechanism
  title-short: Generalization challenges in electrocardiogram deep learning
  type: article-journal
  URL: https://doi.org/10.1080/14796678.2024.2354082
  volume: '0'

- id: huang_importance_2021
  abstract: >-
    Detecting out-of-distribution (OOD) data has become a critical component in
    ensuring the safe deployment of machine learning models in the real world.
    Existing OOD detection approaches primarily rely on the output or feature
    space for deriving OOD scores, while largely overlooking information from
    the gradient space. In this paper, we present GradNorm, a simple and
    effective approach for detecting OOD inputs by utilizing information
    extracted from the gradient space. GradNorm directly employs the vector norm
    of gradients, backpropagated from the KL divergence between the softmax
    output and a uniform probability distribution. Our key idea is that the
    magnitude of gradients is higher for in-distribution (ID) data than that for
    OOD data, making it informative for OOD detection. GradNorm demonstrates
    superior performance, reducing the average FPR95 by up to 16.33% compared to
    the previous best method.
  accessed:
    - year: 2021
      month: 11
      day: 24
  author:
    - family: Huang
      given: Rui
    - family: Geng
      given: Andrew
    - family: Li
      given: Yixuan
  citation-key: huang_importance_2021
  container-title: arXiv:2110.00218 [cs]
  issued:
    - year: 2021
      month: 10
      day: 9
  source: arXiv.org
  title: >-
    On the Importance of Gradients for Detecting Distributional Shifts in the
    Wild
  type: article-journal
  URL: http://arxiv.org/abs/2110.00218

- id: huang_learning_2016
  abstract: >-
    The robustness of neural networks to intended perturbations has recently
    attracted significant attention. In this paper, we propose a new method,
    \emph{learning with a strong adversary}, that learns robust classifiers from
    supervised data. The proposed method takes finding adversarial examples as
    an intermediate step. A new and simple way of finding adversarial examples
    is presented and experimentally shown to be efficient. Experimental results
    demonstrate that resulting learning method greatly improves the robustness
    of the classification models produced.
  accessed:
    - year: 2022
      month: 4
      day: 29
  author:
    - family: Huang
      given: Ruitong
    - family: Xu
      given: Bing
    - family: Schuurmans
      given: Dale
    - family: Szepesvari
      given: Csaba
  citation-key: huang_learning_2016
  container-title: arXiv:1511.03034
  issued:
    - year: 2016
      month: 1
      day: 15
  source: arXiv.org
  title: Learning with a Strong Adversary
  type: article-journal
  URL: http://arxiv.org/abs/1511.03034

- id: huber_robust_1981
  author:
    - family: Huber
      given: Peter J.
  call-number: QA276 .H785
  citation-key: huber_robust_1981
  collection-title: Wiley series in probability and mathematical statistics
  event-place: New York
  ISBN: 978-0-471-41805-4
  issued:
    - year: 1981
  language: en
  number-of-pages: '308'
  publisher: Wiley
  publisher-place: New York
  source: Library of Congress ISBN
  title: Robust statistics
  type: book

- id: hughes_using_2018
  abstract: >-
    We develop a multi-task convolutional neural network (CNN) to classify
    multiple diagnoses from 12-lead electrocardiograms (ECGs) using a dataset
    comprised of over 40,000 ECGs, with labels derived from cardiologist
    clinical interpretations. Since many clinically important classes can occur
    in low frequencies, approaches are needed to improve performance on rare
    classes. We compare the performance of several single-class classifiers on
    rare classes to a multi-headed classifier across all available classes. We
    demonstrate that the addition of common classes can significantly improve
    CNN performance on rarer classes when compared to a model trained on the
    rarer class in isolation. Using this method, we develop a model with high
    performance as measured by F1 score on multiple clinically relevant classes
    compared against the gold-standard cardiologist interpretation.
  accessed:
    - year: 2018
      month: 12
      day: 4
  author:
    - family: Hughes
      given: J. Weston
    - family: MD
      given: Taylor Sittler
    - family: Joseph
      given: Anthony D.
    - family: MD
      given: Jeffrey E. Olgin
    - family: Gonzalez
      given: Joseph E.
    - family: MD
      given: Geoffrey H. Tison
  citation-key: hughes_using_2018
  container-title: arXiv:1812.00497 [cs, stat]
  issued:
    - year: 2018
      month: 12
      day: 2
  source: arXiv.org
  title: Using Multitask Learning to Improve 12-Lead Electrocardiogram Classification
  type: article-journal
  URL: http://arxiv.org/abs/1812.00497

- id: huijben_deep_2019
  abstract: >-
    The field of deep learning is commonly concerned with optimizing predictive
    models using large pre-acquired datasets of densely sampled datapoints or
    signals. In this work, we demonstrate that the...
  accessed:
    - year: 2020
      month: 7
      day: 20
  author:
    - family: Huijben
      given: Iris A. M.
    - family: Veeling
      given: Bastiaan S.
    - family: Sloun
      given: Ruud J. G.
      dropping-particle: van
  citation-key: huijben_deep_2019
  event-title: International Conference on Learning Representations
  issued:
    - year: 2019
      month: 9
      day: 25
  source: openreview.net
  title: Deep probabilistic subsampling for task-adaptive compressed sensing
  type: paper-conference
  URL: https://openreview.net/forum?id=SJeq9JBFvH

- id: hurvich_impact_1990
  author:
    - family: Hurvich
      given: Clifford M.
    - family: Tsai
      given: Chih—Ling
  citation-key: hurvich_impact_1990
  container-title: The American Statistician
  container-title-short: The American Statistician
  DOI: 10.1080/00031305.1990.10475722
  ISSN: 0003-1305
  issue: '3'
  issued:
    - year: 1990
      month: 8
      day: 1
  page: 214-217
  title: The Impact of Model Selection on Inference in Linear Regression
  type: article-journal
  URL: http://amstat.tandfonline.com/doi/abs/10.1080/00031305.1990.10475722
  volume: '44'

- id: hussain_autonomous_2019
  abstract: >-
    Throughout the last century, the automobile industry achieved remarkable
    milestones in manufacturing reliable, safe, and affordable vehicles. Because
    of significant recent advances in computation and communication
    technologies, autonomous cars are becoming a reality. Already autonomous car
    prototype models have covered millions of miles in test driving. Leading
    technical companies and car manufacturers have invested a staggering amount
    of resources in autonomous car technology, as they prepare for autonomous
    cars' full commercialization in the coming years. However, to achieve this
    goal, several technical and nontechnical issues remain: software complexity,
    real-time data analytics, and testing and verification are among the greater
    technical challenges; and consumer stimulation, insurance management, and
    ethical/moral concerns rank high among the nontechnical issues. Tackling
    these challenges requires thoughtful solutions that satisfy consumers,
    industry, and governmental requirements, regulations, and policies. Thus,
    here we present a comprehensive review of state-of-the-art results for
    autonomous car technology. We discuss current issues that hinder autonomous
    cars' development and deployment on a large scale. We also highlight
    autonomous car applications that will benefit consumers and many other
    sectors. Finally, to enable cost-effective, safe, and efficient autonomous
    cars, we discuss several challenges that must be addressed (and provide
    helpful suggestions for adoption) by designers, implementers, policymakers,
    regulatory organizations, and car manufacturers.
  author:
    - family: Hussain
      given: Rasheed
    - family: Zeadally
      given: Sherali
  citation-key: hussain_autonomous_2019
  container-title: IEEE Communications Surveys & Tutorials
  DOI: 10.1109/COMST.2018.2869360
  ISSN: 1553-877X
  issue: '2'
  issued:
    - year: 2019
  page: 1275-1313
  source: IEEE Xplore
  title: 'Autonomous Cars: Research Results, Issues, and Future Challenges'
  title-short: Autonomous Cars
  type: article-journal
  volume: '21'

- id: huster_limitations_2019
  abstract: >-
    Several recent papers have discussed utilizing Lipschitz constants to limit
    the susceptibility of neural networks to adversarial examples. We analyze
    recently proposed methods for computing the Lipschitz constant. We show that
    the Lipschitz constant may indeed enable adversarially robust neural
    networks. However, the methods currently employed for computing it suffer
    from theoretical and practical limitations. We argue that addressing this
    shortcoming is a promising direction for future research into certified
    adversarial defenses.
  author:
    - family: Huster
      given: Todd
    - family: Chiang
      given: Cho-Yu Jason
    - family: Chadha
      given: Ritu
  citation-key: huster_limitations_2019
  collection-title: Lecture Notes in Computer Science
  container-title: ECML PKDD 2018 Workshops
  DOI: 10.1007/978-3-030-13453-2_2
  editor:
    - family: Alzate
      given: Carlos
    - family: Monreale
      given: Anna
    - family: Assem
      given: Haytham
    - family: Bifet
      given: Albert
    - family: Buda
      given: Teodora Sandra
    - family: Caglayan
      given: Bora
    - family: Drury
      given: Brett
    - family: García-Martín
      given: Eva
    - family: Gavaldà
      given: Ricard
    - family: Koprinska
      given: Irena
    - family: Kramer
      given: Stefan
    - family: Lavesson
      given: Niklas
    - family: Madden
      given: Michael
    - family: Molloy
      given: Ian
    - family: Nicolae
      given: Maria-Irina
    - family: Sinn
      given: Mathieu
  event-place: Cham
  ISBN: 978-3-030-13453-2
  issued:
    - year: 2019
  language: en
  page: 16-29
  publisher: Springer International Publishing
  publisher-place: Cham
  source: Springer Link
  title: >-
    Limitations of the Lipschitz Constant as a Defense Against Adversarial
    Examples
  type: paper-conference

- id: hutchison_evaluation_2010
  abstract: >-
    A common practice to gain invariant features in object recognition models is
    to aggregate multiple low-level features over a small neighborhood. However,
    the diﬀerences between those models makes a comparison of the properties of
    diﬀerent aggregation functions hard. Our aim is to gain insight into
    diﬀerent functions by directly comparing them on a ﬁxed architecture for
    several common object recognition tasks. Empirical results show that a
    maximum pooling operation signiﬁcantly outperforms subsampling operations.
    Despite their shift-invariant properties, overlapping pooling windows are no
    signiﬁcant improvement over non-overlapping pooling windows. By applying
    this knowledge, we achieve state-of-the-art error rates of 4.57% on the NORB
    normalized-uniform dataset and 5.6% on the NORB jittered-cluttered dataset.
  accessed:
    - year: 2019
      month: 2
      day: 13
  author:
    - family: Hutchison
      given: David
    - family: Kanade
      given: Takeo
    - family: Kittler
      given: Josef
    - family: Kleinberg
      given: Jon M.
    - family: Mattern
      given: Friedemann
    - family: Mitchell
      given: John C.
    - family: Naor
      given: Moni
    - family: Nierstrasz
      given: Oscar
    - family: Pandu Rangan
      given: C.
    - family: Steffen
      given: Bernhard
    - family: Sudan
      given: Madhu
    - family: Terzopoulos
      given: Demetri
    - family: Tygar
      given: Doug
    - family: Vardi
      given: Moshe Y.
    - family: Weikum
      given: Gerhard
    - family: Scherer
      given: Dominik
    - family: Müller
      given: Andreas
    - family: Behnke
      given: Sven
  citation-key: hutchison_evaluation_2010
  container-title: Artificial Neural Networks – ICANN 2010
  DOI: 10.1007/978-3-642-15825-4_10
  editor:
    - family: Diamantaras
      given: Konstantinos
    - family: Duch
      given: Wlodek
    - family: Iliadis
      given: Lazaros S.
  ISBN: 978-3-642-15824-7 978-3-642-15825-4
  issued:
    - year: 2010
  page: 92-101
  source: Crossref
  title: >-
    Evaluation of Pooling Operations in Convolutional Architectures for Object
    Recognition
  type: paper-conference
  URL: http://link.springer.com/10.1007/978-3-642-15825-4_10
  volume: '6354'

- id: huval_empirical_2015
  author:
    - family: Huval
      given: Brody
    - family: Wang
      given: Tao
    - family: Tandon
      given: Sameep
    - family: Kiske
      given: Jeff
    - family: Song
      given: Will
    - family: Pazhayampallil
      given: Joel
    - family: Andriluka
      given: Mykhaylo
    - family: Rajpurkar
      given: Pranav
    - family: Migimatsu
      given: Toki
    - family: Cheng-Yue
      given: Royce
    - family: Mujica
      given: Fernando A.
    - family: Coates
      given: Adam
    - family: Ng
      given: Andrew Y.
  citation-key: huval_empirical_2015
  container-title: CoRR
  issued:
    - year: 2015
  title: An empirical evaluation of deep learning on highway driving
  type: article-journal
  URL: http://arxiv.org/abs/1504.01716
  volume: abs/1504.01716

- id: huyck_identification_2011
  abstract: >-
    This paper describes the identification of a binary distillation column with
    Least-Squares Support Vector Machines (LS-SVM). It is our intention to
    investigate whether a kernel based model, particularly an LS-SVM, can be
    used for the simulation of the top and bottom temperature of a binary
    distillation column. Furthermore, we compare the latter model with standard
    linear models by means of mean-squared error (MSE). It will be demonstrated
    that this nonlinear model class achieves higher performances in MSE than
    linear models in the presence of nonlinear distortions. When the system is
    close to linear, the performance of the LS-SVM is only slightly better than
    the linear models.
  author:
    - family: Huyck
      given: B.
    - family: De Brabanter
      given: K.
    - family: Logist
      given: F.
    - family: De Brabanter
      given: J.
    - family: Van Impe
      given: J.
    - family: De Moor
      given: B.
  citation-key: huyck_identification_2011
  collection-title: 18th IFAC World Congress
  container-title: IFAC Proceedings Volumes
  container-title-short: IFAC Proceedings Volumes
  DOI: 10.3182/20110828-6-IT-1002.01512
  ISSN: 1474-6670
  issue: '1'
  issued:
    - year: 2011
      month: 1
      day: 1
  page: 471-476
  source: ScienceDirect
  title: 'Identification of a Pilot Scale Distillation Column: A Kernel Based Approach'
  title-short: Identification of a Pilot Scale Distillation Column
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S1474667016436542
  volume: '44'

- id: hwang_simulated_1988
  author:
    - family: Hwang
      given: Chii-Ruey
  citation-key: hwang_simulated_1988
  container-title: Acta Applicandae Mathematicae
  issue: '1'
  issued:
    - year: 1988
  page: 108–111
  title: 'Simulated annealing: theory and applications'
  type: article-journal
  volume: '12'

- id: iangoodfellow_maxout_2013
  abstract: >-
    We consider the problem of designing models to leverage a recently
    introduced approximate model averaging technique called dropout. We define a
    simple new model called maxout (so named because its output is the max of a
    set of inputs, and because it is a natural companion to dropout) designed to
    both facilitate optimization by dropout and improve the accuracy of
    dropout's fast approximate model averaging technique. We empirically verify
    that the model successfully accomplishes both of these tasks. We use maxout
    and dropout to demonstrate state of the art classification performance on
    four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.
  author:
    - literal: Ian Goodfellow
    - literal: David Warde-Farley
    - literal: Mehdi Mirza
    - literal: Aaron Courville
    - literal: Yoshua Bengio
  citation-key: iangoodfellow_maxout_2013
  container-title: Proceedings of the 30th International Conference on Machine Learning
  editor:
    - literal: Sanjoy Dasgupta
    - literal: David McAllester
  issued:
    - year: 2013
      month: 2
      day: 13
  page: 1319-1327
  publisher: PMLR
  source: PMLR
  title: Maxout Networks
  type: paper-conference
  URL: http://proceedings.mlr.press/v28/goodfellow13.html

- id: ifju_flexiblewingbased_2002
  author:
    - family: Ifju
      given: P
    - family: Jenkins
      given: D
    - family: Ettinger
      given: Scott
    - family: Lian
      given: Yongsheng
    - family: Shyy
      given: Wei
    - family: Waszak
      given: M
  citation-key: ifju_flexiblewingbased_2002
  event-title: 40th AIAA aerospace sciences meeting & exhibit
  issued:
    - year: 2002
  page: '705'
  title: Flexible-wing-based micro air vehicles
  type: paper-conference

- id: ilyas_adversarial_2019
  abstract: >-
    Adversarial examples have attracted significant attention in machine
    learning, but the reasons for their existence and pervasiveness remain
    unclear. We demonstrate that adversarial examples can be directly attributed
    to the presence of non-robust features: features derived from patterns in
    the data distribution that are highly predictive, yet brittle and
    incomprehensible to humans. After capturing these features within a
    theoretical framework, we establish their widespread existence in standard
    datasets. Finally, we present a simple setting where we can rigorously tie
    the phenomena we observe in practice to a misalignment between the
    (human-specified) notion of robustness and the inherent geometry of the
    data.
  accessed:
    - year: 2020
      month: 5
      day: 18
  author:
    - family: Ilyas
      given: Andrew
    - family: Santurkar
      given: Shibani
    - family: Tsipras
      given: Dimitris
    - family: Engstrom
      given: Logan
    - family: Tran
      given: Brandon
    - family: Madry
      given: Aleksander
  citation-key: ilyas_adversarial_2019
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2019
  source: arXiv.org
  title: Adversarial Examples Are Not Bugs, They Are Features
  type: article-journal
  URL: http://arxiv.org/abs/1905.02175
  volume: '32'

- id: instruments_ni_2015
  author:
    - family: Instruments
      given: National
  citation-key: instruments_ni_2015
  issued:
    - year: 2015
  publisher: National Instruments
  title: NI USB-6008/6009 User Guide
  type: report

- id: ioffe_batch_2015
  abstract: >-
    Training Deep Neural Networks is complicated by the fact that the
    distribution of each layer’s inputs changes during training, as the
    parameters of the previous layers change. This slows down the training by
    requiring lower learning rates and careful parameter initialization, and
    makes it notoriously hard to train models with saturating nonlinearities. We
    refer to this phenomenon as internal covariate shift, and address the
    problem by normalizing layer inputs. Our method draws its strength from
    making normalization a part of the model architecture and performing the
    normalization for each training mini-batch. Batch Normalization allows us to
    use much higher learning rates and be less careful about initialization, and
    in some cases eliminates the need for Dropout. Applied to a stateof-the-art
    image classification model, Batch Normalization achieves the same accuracy
    with 14 times fewer training steps, and beats the original model by a
    significant margin. Using an ensemble of batch-normalized networks, we
    improve upon the best published result on ImageNet classification: reaching
    4.82% top-5 test error, exceeding the accuracy of human raters.
  author:
    - family: Ioffe
      given: Sergey
    - family: Szegedy
      given: Christian
  citation-key: ioffe_batch_2015
  container-title: Proceedings of the 32nd International Conference on Machine Learning
  issued:
    - year: 2015
      month: 6
      day: 1
  page: 448-456
  publisher: PMLR
  source: PMLR
  title: >-
    Batch Normalization: Accelerating Deep Network Training by Reducing Internal
    Covariate Shift
  type: paper-conference
  URL: http://proceedings.mlr.press/v37/ioffe15.html

- id: iravanian_novel_2002
  abstract: >-
    In this paper, a new algorithm is presented for the filtering (de-noising)
    of cardiac bioelectrical signals. The primary target of this algorithm is
    the class of cardiac action potentials recorded using voltage-sensitive
    dyes, although the method is also applied to electrocardiographic signals
    High periodicity is one of the main features of cardiac biosignals. The
    proposed algorithm exploits this feature in filtering signals with a minimum
    amount of distortion. The basic idea is to use signal averaging in time to
    rind the stationary portion of the signal. The residue is found by
    subtracting the signal average from the corresponding points of the input.
    After passing through a low-pass filter, the filtered residue (FR) is added
    back to the signal average to reconstruct the output. The practical
    implementation of the filter residue algorithm is discussed. Stretching and
    shrinking operations are the basis for the conversion of quasi-periodic
    signals into periodic signals, which can then be subjected to the FR
    algorithm. Various examples are presented, and error estimation is performed
    to guide the selection of optimal parameters for the algorithm. The ability
    of the algorithm to reconstruct the variation among beats is demonstrated,
    and its limitations are discussed.
  author:
    - family: Iravanian
      given: S.
    - family: Tung
      given: L.
  citation-key: iravanian_novel_2002
  container-title: IEEE Transactions on Biomedical Engineering
  DOI: 10.1109/TBME.2002.804589
  ISSN: 0018-9294
  issue: '11'
  issued:
    - year: 2002
      month: 11
  page: 1310-1317
  source: IEEE Xplore
  title: >-
    A novel algorithm for cardiac biosignal filtering based on filtered residue
    method
  type: article-journal
  volume: '49'

- id: irigoyen_narx_2013
  author:
    - family: Irigoyen
      given: Eloy
    - family: Miñano
      given: Gorka
  citation-key: irigoyen_narx_2013
  container-title: Neurocomputing
  DOI: 10.1016/j.neucom.2012.07.031
  issued:
    - year: 2013
  page: 9–15
  title: >-
    A NARX neural network model for enhancing cardiovascular rehabilitation
    therapies
  type: article-journal
  volume: '109'

- id: isaksson_using_2015
  author:
    - family: Isaksson
      given: Alf J
    - family: Sjöberg
      given: Johan
    - family: Törnqvist
      given: David
    - family: Ljung
      given: Lennart
    - family: Kok
      given: Manon
  citation-key: isaksson_using_2015
  container-title: Journal of Process Control
  container-title-short: Journal of Process Control
  DOI: 10.1016/j.jprocont.2014.12.008
  ISSN: 0959-1524
  issued:
    - year: 2015
  page: 69-79
  title: >-
    Using horizon estimation and nonlinear optimization for grey-box
    identification
  type: article-journal
  volume: '30'

- id: jacobs_sparse_2018
  abstract: >-
    Bayesian nonlinear system identification for one of the major classes of
    dynamic model, the nonlinear autoregressive with exogenous input (NARX)
    model, has not been widely studied to date. Markov chain Monte Carlo (MCMC)
    methods have been developed, which tend to be accurate but can also be slow
    to converge. In this contribution, we present a novel, computationally
    efficient solution to sparse Bayesian identification of the NARX model using
    variational inference, which is orders of magnitude faster than MCMC
    methods. A sparsity-inducing hyper-prior is used to solve the structure
    detection problem. Key results include: 1. successful demonstration of the
    method on low signal-to-noise ratio signals (down to 2dB); 2. successful
    benchmarking in terms of speed and accuracy against a number of other
    algorithms: Bayesian LASSO, reversible jump MCMC, forward regression
    orthogonalisation, LASSO and simulation error minimisation with pruning; 3.
    accurate identification of a real world system, an electroactive polymer;
    and 4. demonstration for the first time of numerically propagating the
    estimated nonlinear time-domain model parameter uncertainty into the
    frequency-domain.
  author:
    - family: Jacobs
      given: W. R.
    - family: Baldacchino
      given: T.
    - family: Dodd
      given: T. J.
    - family: Anderson
      given: S. R.
  citation-key: jacobs_sparse_2018
  container-title: IEEE Transactions on Automatic Control
  DOI: 10.1109/TAC.2018.2813004
  ISSN: 0018-9286
  issued:
    - year: 2018
  page: 1-1
  source: IEEE Xplore
  title: Sparse Bayesian Nonlinear System Identification using Variational Inference
  type: article-journal

- id: jacot_implicit_
  abstract: >-
    Random Feature (RF) models are used as efﬁcient parametric approximations of
    kernel methods. We investigate, by means of random matrix theory, the
    connection between Gaussian RF models and Kernel Ridge Regression (KRR). For
    a Gaussian RF model with P features, N data points, and a ridge λ, we show
    that the average (i.e. expected) RF predictor is close to a KRR predictor
    with an effective ridge λ˜. We show that λ˜ > λ and λ˜ λ monotonically as P
    grows, thus revealing the implicit regularization effect of ﬁnite RF
    sampling. We then compare the risk (i.e. test error) of the λ˜KRR predictor
    with the average risk of the λ-RF predictor and obtain a precise and
    explicit bound on their difference. Finally, we empirically ﬁnd an extremely
    good agreement between the test errors of the average λ-RF predictor and
    λ˜-KRR predictor.
  author:
    - family: Jacot
      given: Arthur
    - family: Simsek
      given: Berfin
    - family: Spadaro
      given: Francesco
    - family: Hongler
      given: Clément
    - family: Gabriel
      given: Franck
  citation-key: jacot_implicit_
  language: en
  page: '10'
  source: Zotero
  title: Implicit Regularization of Random Feature Models
  type: article-journal

- id: jacot_neural_2018
  abstract: >-
    At initialization, artificial neural networks (ANNs) are equivalent to
    Gaussian processes in the infinite-width limit, thus connecting them to
    kernel methods. We prove that the evolution of an ANN during training can
    also be described by a kernel: during gradient descent on the parameters of
    an ANN, the network function $f_\theta$ (which maps input vectors to output
    vectors) follows the kernel gradient of the functional cost (which is
    convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural
    Tangent Kernel (NTK). This kernel is central to describe the generalization
    features of ANNs. While the NTK is random at initialization and varies
    during training, in the infinite-width limit it converges to an explicit
    limiting kernel and it stays constant during training. This makes it
    possible to study the training of ANNs in function space instead of
    parameter space. Convergence of the training can then be related to the
    positive-definiteness of the limiting NTK. We prove the
    positive-definiteness of the limiting NTK when the data is supported on the
    sphere and the non-linearity is non-polynomial. We then focus on the setting
    of least-squares regression and show that in the infinite-width limit, the
    network function $f_\theta$ follows a linear differential equation during
    training. The convergence is fastest along the largest kernel principal
    components of the input data with respect to the NTK, hence suggesting a
    theoretical motivation for early stopping. Finally we study the NTK
    numerically, observe its behavior for wide networks, and compare it to the
    infinite-width limit.
  accessed:
    - year: 2020
      month: 7
      day: 27
  author:
    - family: Jacot
      given: Arthur
    - family: Gabriel
      given: Franck
    - family: Hongler
      given: Clément
  citation-key: jacot_neural_2018
  container-title: Advances in Neural Information Processing Systems 31
  issued:
    - year: 2018
  source: arXiv.org
  title: 'Neural Tangent Kernel: Convergence and Generalization in Neural Networks'
  title-short: Neural Tangent Kernel
  type: article-journal
  URL: http://arxiv.org/abs/1806.07572

- id: jaeger_echo_
  author:
    - family: Jaeger
      given: Herbert
  citation-key: jaeger_echo_
  language: en
  page: '48'
  source: Zotero
  title: >-
    The “echo state” approach to analysing and training recurrent neural
    networks – with an Erratum note
  type: article-journal

- id: jaeger_harnessing_2008
  author:
    - family: Jaeger
      given: Herbert
    - family: Haas
      given: Harald
  citation-key: jaeger_harnessing_2008
  container-title: Science
  issued:
    - year: 2008
  page: 78--80
  source: Semantic Scholar
  title: >-
    Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in
    Wireless Communication
  type: article-journal
  volume: '304'

- id: jaeger_optimization_2007
  accessed:
    - year: 2019
      month: 10
      day: 28
  author:
    - family: Jaeger
      given: Herbert
    - family: Lukoševičius
      given: Mantas
    - family: Popovici
      given: Dan
    - family: Siewert
      given: Udo
  citation-key: jaeger_optimization_2007
  container-title: Neural Networks
  container-title-short: Neural Networks
  DOI: 10.1016/j.neunet.2007.04.016
  ISSN: '08936080'
  issue: '3'
  issued:
    - year: 2007
      month: 4
  language: en
  page: 335-352
  source: DOI.org (Crossref)
  title: >-
    Optimization and applications of echo state networks with leaky- integrator
    neurons
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S089360800700041X
  volume: '20'

- id: jaeger_reservoir_2009
  abstract: >-
    Echo State Networks and Liquid State Machines introduced a new paradigm in
    artificial recurrent neural network (RNN) training, where an RNN (the
    reservoir) is generated randomly and only a readout is trained. The
    paradigm, becoming known as reservoir computing, greatly facilitated the
    practical application of RNNs and outperformed classical fully trained RNNs
    in many tasks. It has lately become a vivid research field with numerous
    extensions of the basic idea, including reservoir adaptation, thus
    broadening the initial paradigm to using different methods for training the
    reservoir and the readout. This review systematically surveys both: current
    ways of generating/adapting the reservoirs and training different types of
    readouts. It offers a natural conceptual classification of the techniques,
    which transcends boundaries of the current “brand-names” of reservoir
    methods, and thus aims to help unifying the field and providing the reader
    with a detailed “map” of it.
  author:
    - family: Jaeger
      given: Herbert
  citation-key: jaeger_reservoir_2009
  issued:
    - year: 2009
  source: CiteSeer
  title: Reservoir computing approaches to recurrent neural network training
  type: book

- id: jakubovitz_improving_2018
  author:
    - family: Jakubovitz
      given: Daniel
    - family: Giryes
      given: Raja
  citation-key: jakubovitz_improving_2018
  container-title: European conference on computer vision
  issued:
    - year: 2018
  title: >-
    Improving DNN robustness to adversarial attacks using jacobian
    regularization
  type: paper-conference
  URL: https://api.semanticscholar.org/CorpusID:4326223

- id: jambukia_classification_2015
  abstract: >-
    Classification of electrocardiogram (ECG) signals plays an important role in
    diagnoses of heart diseases. An accurate ECG classification is a challenging
    problem. A survey of ECG classification into arrhythmia types is presented
    in this paper. Early and accurate detection of arrhythmia types is important
    in detecting heart diseases and finding treatment of a patient. Different
    classifiers are available for ECG classification. Amongst all classifiers,
    artificial neural networks have become very popular and most widely used for
    ECG classification. In this paper a detailed survey of preprocessing
    techniques, ECG databases, feature extraction techniques, classifiers and
    performance measures are presented. This paper also discusses issues in ECG
    classification, analysis of input beat selection, and output of classifiers.
  accessed:
    - year: 2018
      month: 10
      day: 21
  author:
    - family: Jambukia
      given: Shweta H.
    - family: Dabhi
      given: Vipul K.
    - family: Prajapati
      given: Harshadkumar B.
  citation-key: jambukia_classification_2015
  container-title: >-
    Proceedings of the International Conference on Advances in Computer
    Engineering and Applications (ICACEA)
  DOI: 10.1109/ICACEA.2015.7164783
  event-title: >-
    2015 International Conference on Advances in Computer Engineering and
    Applications (ICACEA)
  ISBN: 978-1-4673-6911-4
  issued:
    - year: 2015
      month: 3
  language: en
  page: 714-721
  publisher: IEEE
  source: Crossref
  title: 'Classification of ECG signals using machine learning techniques: A survey'
  title-short: Classification of ECG signals using machine learning techniques
  type: paper-conference
  URL: http://ieeexplore.ieee.org/document/7164783/

- id: james_global_2018
  abstract: >-
    Background

    The Global Burden of Diseases, Injuries, and Risk Factors Study 2017 (GBD
    2017) includes a comprehensive assessment of incidence, prevalence, and
    years lived with disability (YLDs) for 354 causes in 195 countries and
    territories from 1990 to 2017. Previous GBD studies have shown how the
    decline of mortality rates from 1990 to 2016 has led to an increase in life
    expectancy, an ageing global population, and an expansion of the non-fatal
    burden of disease and injury. These studies have also shown how a
    substantial portion of the world's population experiences non-fatal health
    loss with considerable heterogeneity among different causes, locations,
    ages, and sexes. Ongoing objectives of the GBD study include increasing the
    level of estimation detail, improving analytical strategies, and increasing
    the amount of high-quality data.

    Methods

    We estimated incidence and prevalence for 354 diseases and injuries and 3484
    sequelae. We used an updated and extensive body of literature studies,
    survey data, surveillance data, inpatient admission records, outpatient
    visit records, and health insurance claims, and additionally used results
    from cause of death models to inform estimates using a total of 68 781 data
    sources. Newly available clinical data from India, Iran, Japan, Jordan,
    Nepal, China, Brazil, Norway, and Italy were incorporated, as well as
    updated claims data from the USA and new claims data from Taiwan (province
    of China) and Singapore. We used DisMod-MR 2.1, a Bayesian meta-regression
    tool, as the main method of estimation, ensuring consistency between rates
    of incidence, prevalence, remission, and cause of death for each condition.
    YLDs were estimated as the product of a prevalence estimate and a disability
    weight for health states of each mutually exclusive sequela, adjusted for
    comorbidity. We updated the Socio-demographic Index (SDI), a summary
    development indicator of income per capita, years of schooling, and total
    fertility rate. Additionally, we calculated differences between male and
    female YLDs to identify divergent trends across sexes. GBD 2017 complies
    with the Guidelines for Accurate and Transparent Health Estimates Reporting.

    Findings

    Globally, for females, the causes with the greatest age-standardised
    prevalence were oral disorders, headache disorders, and haemoglobinopathies
    and haemolytic anaemias in both 1990 and 2017. For males, the causes with
    the greatest age-standardised prevalence were oral disorders, headache
    disorders, and tuberculosis including latent tuberculosis infection in both
    1990 and 2017. In terms of YLDs, low back pain, headache disorders, and
    dietary iron deficiency were the leading Level 3 causes of YLD counts in
    1990, whereas low back pain, headache disorders, and depressive disorders
    were the leading causes in 2017 for both sexes combined. All-cause
    age-standardised YLD rates decreased by 3·9% (95% uncertainty interval [UI]
    3·1–4·6) from 1990 to 2017; however, the all-age YLD rate increased by 7·2%
    (6·0–8·4) while the total sum of global YLDs increased from 562 million
    (421–723) to 853 million (642–1100). The increases for males and females
    were similar, with increases in all-age YLD rates of 7·9% (6·6–9·2) for
    males and 6·5% (5·4–7·7) for females. We found significant differences
    between males and females in terms of age-standardised prevalence estimates
    for multiple causes. The causes with the greatest relative differences
    between sexes in 2017 included substance use disorders (3018 cases [95% UI
    2782–3252] per 100 000 in males vs s1400 [1279–1524] per 100 000 in
    females), transport injuries (3322 [3082–3583] vs 2336 [2154–2535]), and
    self-harm and interpersonal violence (3265 [2943–3630] vs 5643 [5057–6302]).

    Interpretation

    Global all-cause age-standardised YLD rates have improved only slightly over
    a period spanning nearly three decades. However, the magnitude of the
    non-fatal disease burden has expanded globally, with increasing numbers of
    people who have a wide spectrum of conditions. A subset of conditions has
    remained globally pervasive since 1990, whereas other conditions have
    displayed more dynamic trends, with different ages, sexes, and geographies
    across the globe experiencing varying burdens and trends of health loss.
    This study emphasises how global improvements in premature mortality for
    select conditions have led to older populations with complex and potentially
    expensive diseases, yet also highlights global achievements in certain
    domains of disease and injury.

    Funding

    Bill & Melinda Gates Foundation.
  accessed:
    - year: 2021
      month: 11
      day: 25
  author:
    - family: James
      given: Spencer L
    - family: Abate
      given: Degu
    - family: Abate
      given: Kalkidan Hassen
    - family: Abay
      given: Solomon M
    - family: Abbafati
      given: Cristiana
    - family: Abbasi
      given: Nooshin
    - family: Abbastabar
      given: Hedayat
    - family: Abd-Allah
      given: Foad
    - family: Abdela
      given: Jemal
    - family: Abdelalim
      given: Ahmed
    - family: Abdollahpour
      given: Ibrahim
    - family: Abdulkader
      given: Rizwan Suliankatchi
    - family: Abebe
      given: Zegeye
    - family: Abera
      given: Semaw F
    - family: Abil
      given: Olifan Zewdie
    - family: Abraha
      given: Haftom Niguse
    - family: Abu-Raddad
      given: Laith Jamal
    - family: Abu-Rmeileh
      given: Niveen M E
    - family: Accrombessi
      given: Manfred Mario Kokou
    - family: Acharya
      given: Dilaram
    - family: Acharya
      given: Pawan
    - family: Ackerman
      given: Ilana N
    - family: Adamu
      given: Abdu A
    - family: Adebayo
      given: Oladimeji M
    - family: Adekanmbi
      given: Victor
    - family: Adetokunboh
      given: Olatunji O
    - family: Adib
      given: Mina G
    - family: Adsuar
      given: Jose C
    - family: Afanvi
      given: Kossivi Agbelenko
    - family: Afarideh
      given: Mohsen
    - family: Afshin
      given: Ashkan
    - family: Agarwal
      given: Gina
    - family: Agesa
      given: Kareha M
    - family: Aggarwal
      given: Rakesh
    - family: Aghayan
      given: Sargis Aghasi
    - family: Agrawal
      given: Sutapa
    - family: Ahmadi
      given: Alireza
    - family: Ahmadi
      given: Mehdi
    - family: Ahmadieh
      given: Hamid
    - family: Ahmed
      given: Muktar Beshir
    - family: Aichour
      given: Amani Nidhal
    - family: Aichour
      given: Ibtihel
    - family: Aichour
      given: Miloud Taki Eddine
    - family: Akinyemiju
      given: Tomi
    - family: Akseer
      given: Nadia
    - family: Al-Aly
      given: Ziyad
    - family: Al-Eyadhy
      given: Ayman
    - family: Al-Mekhlafi
      given: Hesham M
    - family: Al-Raddadi
      given: Rajaa M
    - family: Alahdab
      given: Fares
    - family: Alam
      given: Khurshid
    - family: Alam
      given: Tahiya
    - family: Alashi
      given: Alaa
    - family: Alavian
      given: Seyed Moayed
    - family: Alene
      given: Kefyalew Addis
    - family: Alijanzadeh
      given: Mehran
    - family: Alizadeh-Navaei
      given: Reza
    - family: Aljunid
      given: Syed Mohamed
    - family: Alkerwi
      given: Ala'a
    - family: Alla
      given: François
    - family: Allebeck
      given: Peter
    - family: Alouani
      given: Mohamed M L
    - family: Altirkawi
      given: Khalid
    - family: Alvis-Guzman
      given: Nelson
    - family: Amare
      given: Azmeraw T
    - family: Aminde
      given: Leopold N
    - family: Ammar
      given: Walid
    - family: Amoako
      given: Yaw Ampem
    - family: Anber
      given: Nahla Hamed
    - family: Andrei
      given: Catalina Liliana
    - family: Androudi
      given: Sofia
    - family: Animut
      given: Megbaru Debalkie
    - family: Anjomshoa
      given: Mina
    - family: Ansha
      given: Mustafa Geleto
    - family: Antonio
      given: Carl Abelardo T
    - family: Anwari
      given: Palwasha
    - family: Arabloo
      given: Jalal
    - family: Arauz
      given: Antonio
    - family: Aremu
      given: Olatunde
    - family: Ariani
      given: Filippo
    - family: Armoon
      given: Bahroom
    - family: Ärnlöv
      given: Johan
    - family: Arora
      given: Amit
    - family: Artaman
      given: Al
    - family: Aryal
      given: Krishna K
    - family: Asayesh
      given: Hamid
    - family: Asghar
      given: Rana Jawad
    - family: Ataro
      given: Zerihun
    - family: Atre
      given: Sachin R
    - family: Ausloos
      given: Marcel
    - family: Avila-Burgos
      given: Leticia
    - family: Avokpaho
      given: Euripide F G A
    - family: Awasthi
      given: Ashish
    - family: Ayala Quintanilla
      given: Beatriz Paulina
    - family: Ayer
      given: Rakesh
    - family: Azzopardi
      given: Peter S
    - family: Babazadeh
      given: Arefeh
    - family: Badali
      given: Hamid
    - family: Badawi
      given: Alaa
    - family: Bali
      given: Ayele Geleto
    - family: Ballesteros
      given: Katherine E
    - family: Ballew
      given: Shoshana H
    - family: Banach
      given: Maciej
    - family: Banoub
      given: Joseph Adel Mattar
    - family: Banstola
      given: Amrit
    - family: Barac
      given: Aleksandra
    - family: Barboza
      given: Miguel A
    - family: Barker-Collo
      given: Suzanne Lyn
    - family: Bärnighausen
      given: Till Winfried
    - family: Barrero
      given: Lope H
    - family: Baune
      given: Bernhard T
    - family: Bazargan-Hejazi
      given: Shahrzad
    - family: Bedi
      given: Neeraj
    - family: Beghi
      given: Ettore
    - family: Behzadifar
      given: Masoud
    - family: Behzadifar
      given: Meysam
    - family: Béjot
      given: Yannick
    - family: Belachew
      given: Abate Bekele
    - family: Belay
      given: Yihalem Abebe
    - family: Bell
      given: Michelle L
    - family: Bello
      given: Aminu K
    - family: Bensenor
      given: Isabela M
    - family: Bernabe
      given: Eduardo
    - family: Bernstein
      given: Robert S
    - family: Beuran
      given: Mircea
    - family: Beyranvand
      given: Tina
    - family: Bhala
      given: Neeraj
    - family: Bhattarai
      given: Suraj
    - family: Bhaumik
      given: Soumyadeep
    - family: Bhutta
      given: Zulfiqar A
    - family: Biadgo
      given: Belete
    - family: Bijani
      given: Ali
    - family: Bikbov
      given: Boris
    - family: Bilano
      given: Ver
    - family: Bililign
      given: Nigus
    - family: Bin Sayeed
      given: Muhammad Shahdaat
    - family: Bisanzio
      given: Donal
    - family: Blacker
      given: Brigette F
    - family: Blyth
      given: Fiona M
    - family: Bou-Orm
      given: Ibrahim R
    - family: Boufous
      given: Soufiane
    - family: Bourne
      given: Rupert
    - family: Brady
      given: Oliver J
    - family: Brainin
      given: Michael
    - family: Brant
      given: Luisa C
    - family: Brazinova
      given: Alexandra
    - family: Breitborde
      given: Nicholas J K
    - family: Brenner
      given: Hermann
    - family: Briant
      given: Paul Svitil
    - family: Briggs
      given: Andrew M
    - family: Briko
      given: Andrey Nikolaevich
    - family: Britton
      given: Gabrielle
    - family: Brugha
      given: Traolach
    - family: Buchbinder
      given: Rachelle
    - family: Busse
      given: Reinhard
    - family: Butt
      given: Zahid A
    - family: Cahuana-Hurtado
      given: Lucero
    - family: Cano
      given: Jorge
    - family: Cárdenas
      given: Rosario
    - family: Carrero
      given: Juan J
    - family: Carter
      given: Austin
    - family: Carvalho
      given: Félix
    - family: Castañeda-Orjuela
      given: Carlos A
    - family: Castillo Rivas
      given: Jacqueline
    - family: Castro
      given: Franz
    - family: Catalá-López
      given: Ferrán
    - family: Cercy
      given: Kelly M
    - family: Cerin
      given: Ester
    - family: Chaiah
      given: Yazan
    - family: Chang
      given: Alex R
    - family: Chang
      given: Hsing-Yi
    - family: Chang
      given: Jung-Chen
    - family: Charlson
      given: Fiona J
    - family: Chattopadhyay
      given: Aparajita
    - family: Chattu
      given: Vijay Kumar
    - family: Chaturvedi
      given: Pankaj
    - family: Chiang
      given: Peggy Pei-Chia
    - family: Chin
      given: Ken Lee
    - family: Chitheer
      given: Abdulaal
    - family: Choi
      given: Jee-Young J
    - family: Chowdhury
      given: Rajiv
    - family: Christensen
      given: Hanne
    - family: Christopher
      given: Devasahayam J
    - family: Cicuttini
      given: Flavia M
    - family: Ciobanu
      given: Liliana G
    - family: Cirillo
      given: Massimo
    - family: Claro
      given: Rafael M
    - family: Collado-Mateo
      given: Daniel
    - family: Cooper
      given: Cyrus
    - family: Coresh
      given: Josef
    - family: Cortesi
      given: Paolo Angelo
    - family: Cortinovis
      given: Monica
    - family: Costa
      given: Megan
    - family: Cousin
      given: Ewerton
    - family: Criqui
      given: Michael H
    - family: Cromwell
      given: Elizabeth A
    - family: Cross
      given: Marita
    - family: Crump
      given: John A
    - family: Dadi
      given: Abel Fekadu
    - family: Dandona
      given: Lalit
    - family: Dandona
      given: Rakhi
    - family: Dargan
      given: Paul I
    - family: Daryani
      given: Ahmad
    - family: Das Gupta
      given: Rajat
    - family: Das Neves
      given: José
    - family: Dasa
      given: Tamirat Tesfaye
    - family: Davey
      given: Gail
    - family: Davis
      given: Adrian C
    - family: Davitoiu
      given: Dragos Virgil
    - family: De Courten
      given: Barbora
    - family: De La Hoz
      given: Fernando Pio
    - family: De Leo
      given: Diego
    - family: De Neve
      given: Jan-Walter
    - family: Degefa
      given: Meaza Girma
    - family: Degenhardt
      given: Louisa
    - family: Deiparine
      given: Selina
    - family: Dellavalle
      given: Robert P
    - family: Demoz
      given: Gebre Teklemariam
    - family: Deribe
      given: Kebede
    - family: Dervenis
      given: Nikolaos
    - family: Des Jarlais
      given: Don C
    - family: Dessie
      given: Getenet Ayalew
    - family: Dey
      given: Subhojit
    - family: Dharmaratne
      given: Samath Dhamminda
    - family: Dinberu
      given: Mesfin Tadese
    - family: Dirac
      given: M Ashworth
    - family: Djalalinia
      given: Shirin
    - family: Doan
      given: Linh
    - family: Dokova
      given: Klara
    - family: Doku
      given: David Teye
    - family: Dorsey
      given: E Ray
    - family: Doyle
      given: Kerrie E
    - family: Driscoll
      given: Tim Robert
    - family: Dubey
      given: Manisha
    - family: Dubljanin
      given: Eleonora
    - family: Duken
      given: Eyasu Ejeta
    - family: Duncan
      given: Bruce B
    - family: Duraes
      given: Andre R
    - family: Ebrahimi
      given: Hedyeh
    - family: Ebrahimpour
      given: Soheil
    - family: Echko
      given: Michelle Marie
    - family: Edvardsson
      given: David
    - family: Effiong
      given: Andem
    - family: Ehrlich
      given: Joshua R
    - family: El Bcheraoui
      given: Charbel
    - family: El Sayed Zaki
      given: Maysaa
    - family: El-Khatib
      given: Ziad
    - family: Elkout
      given: Hajer
    - family: Elyazar
      given: Iqbal R F
    - family: Enayati
      given: Ahmadali
    - family: Endries
      given: Aman Yesuf
    - family: Er
      given: Benjamin
    - family: Erskine
      given: Holly E
    - family: Eshrati
      given: Babak
    - family: Eskandarieh
      given: Sharareh
    - family: Esteghamati
      given: Alireza
    - family: Esteghamati
      given: Sadaf
    - family: Fakhim
      given: Hamed
    - family: Fallah Omrani
      given: Vahid
    - family: Faramarzi
      given: Mahbobeh
    - family: Fareed
      given: Mohammad
    - family: Farhadi
      given: Farzaneh
    - family: Farid
      given: Talha A
    - family: Farinha
      given: Carla Sofia E
      dropping-particle: sá
    - family: Farioli
      given: Andrea
    - family: Faro
      given: Andre
    - family: Farvid
      given: Maryam S
    - family: Farzadfar
      given: Farshad
    - family: Feigin
      given: Valery L
    - family: Fentahun
      given: Netsanet
    - family: Fereshtehnejad
      given: Seyed-Mohammad
    - family: Fernandes
      given: Eduarda
    - family: Fernandes
      given: Joao C
    - family: Ferrari
      given: Alize J
    - family: Feyissa
      given: Garumma Tolu
    - family: Filip
      given: Irina
    - family: Fischer
      given: Florian
    - family: Fitzmaurice
      given: Christina
    - family: Foigt
      given: Nataliya A
    - family: Foreman
      given: Kyle J
    - family: Fox
      given: Jack
    - family: Frank
      given: Tahvi D
    - family: Fukumoto
      given: Takeshi
    - family: Fullman
      given: Nancy
    - family: Fürst
      given: Thomas
    - family: Furtado
      given: João M
    - family: Futran
      given: Neal D
    - family: Gall
      given: Seana
    - family: Ganji
      given: Morsaleh
    - family: Gankpe
      given: Fortune Gbetoho
    - family: Garcia-Basteiro
      given: Alberto L
    - family: Gardner
      given: William M
    - family: Gebre
      given: Abadi Kahsu
    - family: Gebremedhin
      given: Amanuel Tesfay
    - family: Gebremichael
      given: Teklu Gebrehiwo
    - family: Gelano
      given: Tilayie Feto
    - family: Geleijnse
      given: Johanna M
    - family: Genova-Maleras
      given: Ricard
    - family: Geramo
      given: Yilma Chisha Dea
    - family: Gething
      given: Peter W
    - family: Gezae
      given: Kebede Embaye
    - family: Ghadiri
      given: Keyghobad
    - family: Ghasemi Falavarjani
      given: Khalil
    - family: Ghasemi-Kasman
      given: Maryam
    - family: Ghimire
      given: Mamata
    - family: Ghosh
      given: Rakesh
    - family: Ghoshal
      given: Aloke Gopal
    - family: Giampaoli
      given: Simona
    - family: Gill
      given: Paramjit Singh
    - family: Gill
      given: Tiffany K
    - family: Ginawi
      given: Ibrahim Abdelmageed
    - family: Giussani
      given: Giorgia
    - family: Gnedovskaya
      given: Elena V
    - family: Goldberg
      given: Ellen M
    - family: Goli
      given: Srinivas
    - family: Gómez-Dantés
      given: Hector
    - family: Gona
      given: Philimon N
    - family: Gopalani
      given: Sameer Vali
    - family: Gorman
      given: Taren M
    - family: Goulart
      given: Alessandra C
    - family: Goulart
      given: Bárbara Niegia Garcia
    - family: Grada
      given: Ayman
    - family: Grams
      given: Morgan E
    - family: Grosso
      given: Giuseppe
    - family: Gugnani
      given: Harish Chander
    - family: Guo
      given: Yuming
    - family: Gupta
      given: Prakash C
    - family: Gupta
      given: Rahul
    - family: Gupta
      given: Rajeev
    - family: Gupta
      given: Tanush
    - family: Gyawali
      given: Bishal
    - family: Haagsma
      given: Juanita A
    - family: Hachinski
      given: Vladimir
    - family: Hafezi-Nejad
      given: Nima
    - family: Haghparast Bidgoli
      given: Hassan
    - family: Hagos
      given: Tekleberhan B
    - family: Hailu
      given: Gessessew Bugssa
    - family: Haj-Mirzaian
      given: Arvin
    - family: Haj-Mirzaian
      given: Arya
    - family: Hamadeh
      given: Randah R
    - family: Hamidi
      given: Samer
    - family: Handal
      given: Alexis J
    - family: Hankey
      given: Graeme J
    - family: Hao
      given: Yuantao
    - family: Harb
      given: Hilda L
    - family: Harikrishnan
      given: Sivadasanpillai
    - family: Haro
      given: Josep Maria
    - family: Hasan
      given: Mehedi
    - family: Hassankhani
      given: Hadi
    - family: Hassen
      given: Hamid Yimam
    - family: Havmoeller
      given: Rasmus
    - family: Hawley
      given: Caitlin N
    - family: Hay
      given: Roderick J
    - family: Hay
      given: Simon I
    - family: Hedayatizadeh-Omran
      given: Akbar
    - family: Heibati
      given: Behzad
    - family: Hendrie
      given: Delia
    - family: Henok
      given: Andualem
    - family: Herteliu
      given: Claudiu
    - family: Heydarpour
      given: Sousan
    - family: Hibstu
      given: Desalegn Tsegaw
    - family: Hoang
      given: Huong Thanh
    - family: Hoek
      given: Hans W
    - family: Hoffman
      given: Howard J
    - family: Hole
      given: Michael K
    - family: Homaie Rad
      given: Enayatollah
    - family: Hoogar
      given: Praveen
    - family: Hosgood
      given: H Dean
    - family: Hosseini
      given: Seyed Mostafa
    - family: Hosseinzadeh
      given: Mehdi
    - family: Hostiuc
      given: Mihaela
    - family: Hostiuc
      given: Sorin
    - family: Hotez
      given: Peter J
    - family: Hoy
      given: Damian G
    - family: Hsairi
      given: Mohamed
    - family: Htet
      given: Aung Soe
    - family: Hu
      given: Guoqing
    - family: Huang
      given: John J
    - family: Huynh
      given: Chantal K
    - family: Iburg
      given: Kim Moesgaard
    - family: Ikeda
      given: Chad Thomas
    - family: Ileanu
      given: Bogdan
    - family: Ilesanmi
      given: Olayinka Stephen
    - family: Iqbal
      given: Usman
    - family: Irvani
      given: Seyed Sina Naghibi
    - family: Irvine
      given: Caleb Mackay Salpeter
    - family: Islam
      given: Sheikh Mohammed Shariful
    - family: Islami
      given: Farhad
    - family: Jacobsen
      given: Kathryn H
    - family: Jahangiry
      given: Leila
    - family: Jahanmehr
      given: Nader
    - family: Jain
      given: Sudhir Kumar
    - family: Jakovljevic
      given: Mihajlo
    - family: Javanbakht
      given: Mehdi
    - family: Jayatilleke
      given: Achala Upendra
    - family: Jeemon
      given: Panniyammakal
    - family: Jha
      given: Ravi Prakash
    - family: Jha
      given: Vivekanand
    - family: Ji
      given: John S
    - family: Johnson
      given: Catherine O
    - family: Jonas
      given: Jost B
    - family: Jozwiak
      given: Jacek Jerzy
    - family: Jungari
      given: Suresh Banayya
    - family: Jürisson
      given: Mikk
    - family: Kabir
      given: Zubair
    - family: Kadel
      given: Rajendra
    - family: Kahsay
      given: Amaha
    - family: Kalani
      given: Rizwan
    - family: Kanchan
      given: Tanuj
    - family: Karami
      given: Manoochehr
    - family: Karami Matin
      given: Behzad
    - family: Karch
      given: André
    - family: Karema
      given: Corine
    - family: Karimi
      given: Narges
    - family: Karimi
      given: Seyed M
    - family: Kasaeian
      given: Amir
    - family: Kassa
      given: Dessalegn H
    - family: Kassa
      given: Getachew Mullu
    - family: Kassa
      given: Tesfaye Dessale
    - family: Kassebaum
      given: Nicholas J
    - family: Katikireddi
      given: Srinivasa Vittal
    - family: Kawakami
      given: Norito
    - family: Karyani
      given: Ali Kazemi
    - family: Keighobadi
      given: Masoud Masoud
    - family: Keiyoro
      given: Peter Njenga
    - family: Kemmer
      given: Laura
    - family: Kemp
      given: Grant Rodgers
    - family: Kengne
      given: Andre Pascal
    - family: Keren
      given: Andre
    - family: Khader
      given: Yousef Saleh
    - family: Khafaei
      given: Behzad
    - family: Khafaie
      given: Morteza Abdullatif
    - family: Khajavi
      given: Alireza
    - family: Khalil
      given: Ibrahim A
    - family: Khan
      given: Ejaz Ahmad
    - family: Khan
      given: Muhammad Shahzeb
    - family: Khan
      given: Muhammad Ali
    - family: Khang
      given: Young-Ho
    - family: Khazaei
      given: Mohammad
    - family: Khoja
      given: Abdullah T
    - family: Khosravi
      given: Ardeshir
    - family: Khosravi
      given: Mohammad Hossein
    - family: Kiadaliri
      given: Aliasghar A
    - family: Kiirithio
      given: Daniel N
    - family: Kim
      given: Cho-Il
    - family: Kim
      given: Daniel
    - family: Kim
      given: Pauline
    - family: Kim
      given: Young-Eun
    - family: Kim
      given: Yun Jin
    - family: Kimokoti
      given: Ruth W
    - family: Kinfu
      given: Yohannes
    - family: Kisa
      given: Adnan
    - family: Kissimova-Skarbek
      given: Katarzyna
    - family: Kivimäki
      given: Mika
    - family: Knudsen
      given: Ann Kristin Skrindo
    - family: Kocarnik
      given: Jonathan M
    - family: Kochhar
      given: Sonali
    - family: Kokubo
      given: Yoshihiro
    - family: Kolola
      given: Tufa
    - family: Kopec
      given: Jacek A
    - family: Kosen
      given: Soewarta
    - family: Kotsakis
      given: Georgios A
    - family: Koul
      given: Parvaiz A
    - family: Koyanagi
      given: Ai
    - family: Kravchenko
      given: Michael A
    - family: Krishan
      given: Kewal
    - family: Krohn
      given: Kristopher J
    - family: Kuate Defo
      given: Barthelemy
    - family: Kucuk Bicer
      given: Burcu
    - family: Kumar
      given: G Anil
    - family: Kumar
      given: Manasi
    - family: Kyu
      given: Hmwe Hmwe
    - family: Lad
      given: Deepesh P
    - family: Lad
      given: Sheetal D
    - family: Lafranconi
      given: Alessandra
    - family: Lalloo
      given: Ratilal
    - family: Lallukka
      given: Tea
    - family: Lami
      given: Faris Hasan
    - family: Lansingh
      given: Van C
    - family: Latifi
      given: Arman
    - family: Lau
      given: Kathryn Mei-Ming
    - family: Lazarus
      given: Jeffrey V
    - family: Leasher
      given: Janet L
    - family: Ledesma
      given: Jorge R
    - family: Lee
      given: Paul H
    - family: Leigh
      given: James
    - family: Leung
      given: Janni
    - family: Levi
      given: Miriam
    - family: Lewycka
      given: Sonia
    - family: Li
      given: Shanshan
    - family: Li
      given: Yichong
    - family: Liao
      given: Yu
    - family: Liben
      given: Misgan Legesse
    - family: Lim
      given: Lee-Ling
    - family: Lim
      given: Stephen S
    - family: Liu
      given: Shiwei
    - family: Lodha
      given: Rakesh
    - family: Looker
      given: Katharine J
    - family: Lopez
      given: Alan D
    - family: Lorkowski
      given: Stefan
    - family: Lotufo
      given: Paulo A
    - family: Low
      given: Nicola
    - family: Lozano
      given: Rafael
    - family: Lucas
      given: Tim C D
    - family: Lucchesi
      given: Lydia R
    - family: Lunevicius
      given: Raimundas
    - family: Lyons
      given: Ronan A
    - family: Ma
      given: Stefan
    - family: Macarayan
      given: Erlyn Rachelle King
    - family: Mackay
      given: Mark T
    - family: Madotto
      given: Fabiana
    - family: Magdy Abd El Razek
      given: Hassan
    - family: Magdy Abd El Razek
      given: Muhammed
    - family: Maghavani
      given: Dhaval P
    - family: Mahotra
      given: Narayan Bahadur
    - family: Mai
      given: Hue Thi
    - family: Majdan
      given: Marek
    - family: Majdzadeh
      given: Reza
    - family: Majeed
      given: Azeem
    - family: Malekzadeh
      given: Reza
    - family: Malta
      given: Deborah Carvalho
    - family: Mamun
      given: Abdullah A
    - family: Manda
      given: Ana-Laura
    - family: Manguerra
      given: Helena
    - family: Manhertz
      given: Treh
    - family: Mansournia
      given: Mohammad Ali
    - family: Mantovani
      given: Lorenzo Giovanni
    - family: Mapoma
      given: Chabila Christopher
    - family: Maravilla
      given: Joemer C
    - family: Marcenes
      given: Wagner
    - family: Marks
      given: Ashley
    - family: Martins-Melo
      given: Francisco Rogerlândio
    - family: Martopullo
      given: Ira
    - family: März
      given: Winfried
    - family: Marzan
      given: Melvin B
    - family: Mashamba-Thompson
      given: Tivani Phosa
    - family: Massenburg
      given: Benjamin Ballard
    - family: Mathur
      given: Manu Raj
    - family: Matsushita
      given: Kunihiro
    - family: Maulik
      given: Pallab K
    - family: Mazidi
      given: Mohsen
    - family: McAlinden
      given: Colm
    - family: McGrath
      given: John J
    - family: McKee
      given: Martin
    - family: Mehndiratta
      given: Man Mohan
    - family: Mehrotra
      given: Ravi
    - family: Mehta
      given: Kala M
    - family: Mehta
      given: Varshil
    - family: Mejia-Rodriguez
      given: Fabiola
    - family: Mekonen
      given: Tesfa
    - family: Melese
      given: Addisu
    - family: Melku
      given: Mulugeta
    - family: Meltzer
      given: Michele
    - family: Memiah
      given: Peter T N
    - family: Memish
      given: Ziad A
    - family: Mendoza
      given: Walter
    - family: Mengistu
      given: Desalegn Tadese
    - family: Mengistu
      given: Getnet
    - family: Mensah
      given: George A
    - family: Mereta
      given: Seid Tiku
    - family: Meretoja
      given: Atte
    - family: Meretoja
      given: Tuomo J
    - family: Mestrovic
      given: Tomislav
    - family: Mezerji
      given: Naser Mohammad Gholi
    - family: Miazgowski
      given: Bartosz
    - family: Miazgowski
      given: Tomasz
    - family: Millear
      given: Anoushka I
    - family: Miller
      given: Ted R
    - family: Miltz
      given: Benjamin
    - family: Mini
      given: G K
    - family: Mirarefin
      given: Mojde
    - family: Mirrakhimov
      given: Erkin M
    - family: Misganaw
      given: Awoke Temesgen
    - family: Mitchell
      given: Philip B
    - family: Mitiku
      given: Habtamu
    - family: Moazen
      given: Babak
    - family: Mohajer
      given: Bahram
    - family: Mohammad
      given: Karzan Abdulmuhsin
    - family: Mohammadifard
      given: Noushin
    - family: Mohammadnia-Afrouzi
      given: Mousa
    - family: Mohammed
      given: Mohammed A
    - family: Mohammed
      given: Shafiu
    - family: Mohebi
      given: Farnam
    - family: Moitra
      given: Modhurima
    - family: Mokdad
      given: Ali H
    - family: Molokhia
      given: Mariam
    - family: Monasta
      given: Lorenzo
    - family: Moodley
      given: Yoshan
    - family: Moosazadeh
      given: Mahmood
    - family: Moradi
      given: Ghobad
    - family: Moradi-Lakeh
      given: Maziar
    - family: Moradinazar
      given: Mehdi
    - family: Moraga
      given: Paula
    - family: Morawska
      given: Lidia
    - family: Moreno Velásquez
      given: Ilais
    - family: Morgado-Da-Costa
      given: Joana
    - family: Morrison
      given: Shane Douglas
    - family: Moschos
      given: Marilita M
    - family: Mountjoy-Venning
      given: W Cliff
    - family: Mousavi
      given: Seyyed Meysam
    - family: Mruts
      given: Kalayu Brhane
    - family: Muche
      given: Achenef Asmamaw
    - family: Muchie
      given: Kindie Fentahun
    - family: Mueller
      given: Ulrich Otto
    - family: Muhammed
      given: Oumer Sada
    - family: Mukhopadhyay
      given: Satinath
    - family: Muller
      given: Kate
    - family: Mumford
      given: John Everett
    - family: Murhekar
      given: Manoj
    - family: Musa
      given: Jonah
    - family: Musa
      given: Kamarul Imran
    - family: Mustafa
      given: Ghulam
    - family: Nabhan
      given: Ashraf F
    - family: Nagata
      given: Chie
    - family: Naghavi
      given: Mohsen
    - family: Naheed
      given: Aliya
    - family: Nahvijou
      given: Azin
    - family: Naik
      given: Gurudatta
    - family: Naik
      given: Nitish
    - family: Najafi
      given: Farid
    - family: Naldi
      given: Luigi
    - family: Nam
      given: Hae Sung
    - family: Nangia
      given: Vinay
    - family: Nansseu
      given: Jobert Richie
    - family: Nascimento
      given: Bruno Ramos
    - family: Natarajan
      given: Gopalakrishnan
    - family: Neamati
      given: Nahid
    - family: Negoi
      given: Ionut
    - family: Negoi
      given: Ruxandra Irina
    - family: Neupane
      given: Subas
    - family: Newton
      given: Charles Richard James
    - family: Ngunjiri
      given: Josephine W
    - family: Nguyen
      given: Anh Quynh
    - family: Nguyen
      given: Ha Thu
    - family: Nguyen
      given: Huong Lan Thi
    - family: Nguyen
      given: Huong Thanh
    - family: Nguyen
      given: Long Hoang
    - family: Nguyen
      given: Minh
    - family: Nguyen
      given: Nam Ba
    - family: Nguyen
      given: Son Hoang
    - family: Nichols
      given: Emma
    - family: Ningrum
      given: Dina Nur Anggraini
    - family: Nixon
      given: Molly R
    - family: Nolutshungu
      given: Nomonde
    - family: Nomura
      given: Shuhei
    - family: Norheim
      given: Ole F
    - family: Noroozi
      given: Mehdi
    - family: Norrving
      given: Bo
    - family: Noubiap
      given: Jean Jacques
    - family: Nouri
      given: Hamid Reza
    - family: Nourollahpour Shiadeh
      given: Malihe
    - family: Nowroozi
      given: Mohammad Reza
    - family: Nsoesie
      given: Elaine O
    - family: Nyasulu
      given: Peter S
    - family: Odell
      given: Christopher M
    - family: Ofori-Asenso
      given: Richard
    - family: Ogbo
      given: Felix Akpojene
    - family: Oh
      given: In-Hwan
    - family: Oladimeji
      given: Olanrewaju
    - family: Olagunju
      given: Andrew T
    - family: Olagunju
      given: Tinuke O
    - family: Olivares
      given: Pedro R
    - family: Olsen
      given: Helen Elizabeth
    - family: Olusanya
      given: Bolajoko Olubukunola
    - family: Ong
      given: Kanyin L
    - family: Ong
      given: Sok King
    - family: Oren
      given: Eyal
    - family: Ortiz
      given: Alberto
    - family: Ota
      given: Erika
    - family: Otstavnov
      given: Stanislav S
    - family: Øverland
      given: Simon
    - family: Owolabi
      given: Mayowa Ojo
    - family: P a
      given: Mahesh
    - family: Pacella
      given: Rosana
    - family: Pakpour
      given: Amir H
    - family: Pana
      given: Adrian
    - family: Panda-Jonas
      given: Songhomitra
    - family: Parisi
      given: Andrea
    - family: Park
      given: Eun-Kee
    - family: Parry
      given: Charles D H
    - family: Patel
      given: Shanti
    - family: Pati
      given: Sanghamitra
    - family: Patil
      given: Snehal T
    - family: Patle
      given: Ajay
    - family: Patton
      given: George C
    - family: Paturi
      given: Vishnupriya Rao
    - family: Paulson
      given: Katherine R
    - family: Pearce
      given: Neil
    - family: Pereira
      given: David M
    - family: Perico
      given: Norberto
    - family: Pesudovs
      given: Konrad
    - family: Pham
      given: Hai Quang
    - family: Phillips
      given: Michael R
    - family: Pigott
      given: David M
    - family: Pillay
      given: Julian David
    - family: Piradov
      given: Michael A
    - family: Pirsaheb
      given: Meghdad
    - family: Pishgar
      given: Farhad
    - family: Plana-Ripoll
      given: Oleguer
    - family: Plass
      given: Dietrich
    - family: Polinder
      given: Suzanne
    - family: Popova
      given: Svetlana
    - family: Postma
      given: Maarten J
    - family: Pourshams
      given: Akram
    - family: Poustchi
      given: Hossein
    - family: Prabhakaran
      given: Dorairaj
    - family: Prakash
      given: Swayam
    - family: Prakash
      given: V
    - family: Purcell
      given: Caroline A
    - family: Purwar
      given: Manorama B
    - family: Qorbani
      given: Mostafa
    - family: Quistberg
      given: D Alex
    - family: Radfar
      given: Amir
    - family: Rafay
      given: Anwar
    - family: Rafiei
      given: Alireza
    - family: Rahim
      given: Fakher
    - family: Rahimi
      given: Kazem
    - family: Rahimi-Movaghar
      given: Afarin
    - family: Rahimi-Movaghar
      given: Vafa
    - family: Rahman
      given: Mahfuzar
    - family: Rahman
      given: Mohammad Hifz
      dropping-particle: ur
    - family: Rahman
      given: Muhammad Aziz
    - family: Rahman
      given: Sajjad Ur
    - family: Rai
      given: Rajesh Kumar
    - family: Rajati
      given: Fatemeh
    - family: Ram
      given: Usha
    - family: Ranjan
      given: Prabhat
    - family: Ranta
      given: Anna
    - family: Rao
      given: Puja C
    - family: Rawaf
      given: David Laith
    - family: Rawaf
      given: Salman
    - family: Reddy
      given: K Srinath
    - family: Reiner
      given: Robert C
    - family: Reinig
      given: Nickolas
    - family: Reitsma
      given: Marissa Bettay
    - family: Remuzzi
      given: Giuseppe
    - family: Renzaho
      given: Andre M N
    - family: Resnikoff
      given: Serge
    - family: Rezaei
      given: Satar
    - family: Rezai
      given: Mohammad Sadegh
    - family: Ribeiro
      given: Antonio Luiz P
    - family: Roberts
      given: Nicholas L S
    - family: Robinson
      given: Stephen R
    - family: Roever
      given: Leonardo
    - family: Ronfani
      given: Luca
    - family: Roshandel
      given: Gholamreza
    - family: Rostami
      given: Ali
    - family: Roth
      given: Gregory A
    - family: Roy
      given: Ambuj
    - family: Rubagotti
      given: Enrico
    - family: Sachdev
      given: Perminder S
    - family: Sadat
      given: Nafis
    - family: Saddik
      given: Basema
    - family: Sadeghi
      given: Ehsan
    - family: Saeedi Moghaddam
      given: Sahar
    - family: Safari
      given: Hosein
    - family: Safari
      given: Yahya
    - family: Safari-Faramani
      given: Roya
    - family: Safdarian
      given: Mahdi
    - family: Safi
      given: Sare
    - family: Safiri
      given: Saeid
    - family: Sagar
      given: Rajesh
    - family: Sahebkar
      given: Amirhossein
    - family: Sahraian
      given: Mohammad Ali
    - family: Sajadi
      given: Haniye Sadat
    - family: Salam
      given: Nasir
    - family: Salama
      given: Joseph S
    - family: Salamati
      given: Payman
    - family: Saleem
      given: Komal
    - family: Saleem
      given: Zikria
    - family: Salimi
      given: Yahya
    - family: Salomon
      given: Joshua A
    - family: Salvi
      given: Sundeep Santosh
    - family: Salz
      given: Inbal
    - family: Samy
      given: Abdallah M
    - family: Sanabria
      given: Juan
    - family: Sang
      given: Yingying
    - family: Santomauro
      given: Damian Francesco
    - family: Santos
      given: Itamar S
    - family: Santos
      given: João Vasco
    - family: Santric Milicevic
      given: Milena M
    - family: Sao Jose
      given: Bruno Piassi
    - family: Sardana
      given: Mayank
    - family: Sarker
      given: Abdur Razzaque
    - family: Sarrafzadegan
      given: Nizal
    - family: Sartorius
      given: Benn
    - family: Sarvi
      given: Shahabeddin
    - family: Sathian
      given: Brijesh
    - family: Satpathy
      given: Maheswar
    - family: Sawant
      given: Arundhati R
    - family: Sawhney
      given: Monika
    - family: Saxena
      given: Sonia
    - family: Saylan
      given: Mete
    - family: Schaeffner
      given: Elke
    - family: Schmidt
      given: Maria Inês
    - family: Schneider
      given: Ione J C
    - family: Schöttker
      given: Ben
    - family: Schwebel
      given: David C
    - family: Schwendicke
      given: Falk
    - family: Scott
      given: James G
    - family: Sekerija
      given: Mario
    - family: Sepanlou
      given: Sadaf G
    - family: Serván-Mori
      given: Edson
    - family: Seyedmousavi
      given: Seyedmojtaba
    - family: Shabaninejad
      given: Hosein
    - family: Shafieesabet
      given: Azadeh
    - family: Shahbazi
      given: Mehdi
    - family: Shaheen
      given: Amira A
    - family: Shaikh
      given: Masood Ali
    - family: Shams-Beyranvand
      given: Mehran
    - family: Shamsi
      given: Mohammadbagher
    - family: Shamsizadeh
      given: Morteza
    - family: Sharafi
      given: Heidar
    - family: Sharafi
      given: Kiomars
    - family: Sharif
      given: Mehdi
    - family: Sharif-Alhoseini
      given: Mahdi
    - family: Sharma
      given: Meenakshi
    - family: Sharma
      given: Rajesh
    - family: She
      given: Jun
    - family: Sheikh
      given: Aziz
    - family: Shi
      given: Peilin
    - family: Shibuya
      given: Kenji
    - family: Shigematsu
      given: Mika
    - family: Shiri
      given: Rahman
    - family: Shirkoohi
      given: Reza
    - family: Shishani
      given: Kawkab
    - family: Shiue
      given: Ivy
    - family: Shokraneh
      given: Farhad
    - family: Shoman
      given: Haitham
    - family: Shrime
      given: Mark G
    - family: Si
      given: Si
    - family: Siabani
      given: Soraya
    - family: Siddiqi
      given: Tariq J
    - family: Sigfusdottir
      given: Inga Dora
    - family: Sigurvinsdottir
      given: Rannveig
    - family: Silva
      given: João Pedro
    - family: Silveira
      given: Dayane Gabriele Alves
    - family: Singam
      given: Narayana Sarma Venkata
    - family: Singh
      given: Jasvinder A
    - family: Singh
      given: Narinder Pal
    - family: Singh
      given: Virendra
    - family: Sinha
      given: Dhirendra Narain
    - family: Skiadaresi
      given: Eirini
    - family: Slepak
      given: Erica Leigh N
    - family: Sliwa
      given: Karen
    - family: Smith
      given: David L
    - family: Smith
      given: Mari
    - family: Soares Filho
      given: Adauto Martins
    - family: Sobaih
      given: Badr Hasan
    - family: Sobhani
      given: Soheila
    - family: Sobngwi
      given: Eugène
    - family: Soneji
      given: Samir S
    - family: Soofi
      given: Moslem
    - family: Soosaraei
      given: Masoud
    - family: Sorensen
      given: Reed J D
    - family: Soriano
      given: Joan B
    - family: Soyiri
      given: Ireneous N
    - family: Sposato
      given: Luciano A
    - family: Sreeramareddy
      given: Chandrashekhar T
    - family: Srinivasan
      given: Vinay
    - family: Stanaway
      given: Jeffrey D
    - family: Stein
      given: Dan J
    - family: Steiner
      given: Caitlyn
    - family: Steiner
      given: Timothy J
    - family: Stokes
      given: Mark A
    - family: Stovner
      given: Lars Jacob
    - family: Subart
      given: Michelle L
    - family: Sudaryanto
      given: Agus
    - family: Sufiyan
      given: Mu'awiyyah Babale
    - family: Sunguya
      given: Bruno F
    - family: Sur
      given: Patrick John
    - family: Sutradhar
      given: Ipsita
    - family: Sykes
      given: Bryan L
    - family: Sylte
      given: Dillon O
    - family: Tabarés-Seisdedos
      given: Rafael
    - family: Tadakamadla
      given: Santosh Kumar
    - family: Tadesse
      given: Birkneh Tilahun
    - family: Tandon
      given: Nikhil
    - family: Tassew
      given: Segen Gebremeskel
    - family: Tavakkoli
      given: Mohammad
    - family: Taveira
      given: Nuno
    - family: Taylor
      given: Hugh R
    - family: Tehrani-Banihashemi
      given: Arash
    - family: Tekalign
      given: Tigist Gashaw
    - family: Tekelemedhin
      given: Shishay Wahdey
    - family: Tekle
      given: Merhawi Gebremedhin
    - family: Temesgen
      given: Habtamu
    - family: Temsah
      given: Mohamad-Hani
    - family: Temsah
      given: Omar
    - family: Terkawi
      given: Abdullah Sulieman
    - family: Teweldemedhin
      given: Mebrahtu
    - family: Thankappan
      given: Kavumpurathu Raman
    - family: Thomas
      given: Nihal
    - family: Tilahun
      given: Binyam
    - family: To
      given: Quyen G
    - family: Tonelli
      given: Marcello
    - family: Topor-Madry
      given: Roman
    - family: Topouzis
      given: Fotis
    - family: Torre
      given: Anna E
    - family: Tortajada-Girbés
      given: Miguel
    - family: Touvier
      given: Mathilde
    - family: Tovani-Palone
      given: Marcos Roberto
    - family: Towbin
      given: Jeffrey A
    - family: Tran
      given: Bach Xuan
    - family: Tran
      given: Khanh Bao
    - family: Troeger
      given: Christopher E
    - family: Truelsen
      given: Thomas Clement
    - family: Tsilimbaris
      given: Miltiadis K
    - family: Tsoi
      given: Derrick
    - family: Tudor Car
      given: Lorainne
    - family: Tuzcu
      given: E Murat
    - family: Ukwaja
      given: Kingsley N
    - family: Ullah
      given: Irfan
    - family: Undurraga
      given: Eduardo A
    - family: Unutzer
      given: Jurgen
    - family: Updike
      given: Rachel L
    - family: Usman
      given: Muhammad Shariq
    - family: Uthman
      given: Olalekan A
    - family: Vaduganathan
      given: Muthiah
    - family: Vaezi
      given: Afsane
    - family: Valdez
      given: Pascual R
    - family: Varughese
      given: Santosh
    - family: Vasankari
      given: Tommi Juhani
    - family: Venketasubramanian
      given: Narayanaswamy
    - family: Villafaina
      given: Santos
    - family: Violante
      given: Francesco S
    - family: Vladimirov
      given: Sergey Konstantinovitch
    - family: Vlassov
      given: Vasily
    - family: Vollset
      given: Stein Emil
    - family: Vosoughi
      given: Kia
    - family: Vujcic
      given: Isidora S
    - family: Wagnew
      given: Fasil Shiferaw
    - family: Waheed
      given: Yasir
    - family: Waller
      given: Stephen G
    - family: Wang
      given: Yafeng
    - family: Wang
      given: Yuan-Pang
    - family: Weiderpass
      given: Elisabete
    - family: Weintraub
      given: Robert G
    - family: Weiss
      given: Daniel J
    - family: Weldegebreal
      given: Fitsum
    - family: Weldegwergs
      given: Kidu Gidey
    - family: Werdecker
      given: Andrea
    - family: West
      given: T Eoin
    - family: Whiteford
      given: Harvey A
    - family: Widecka
      given: Justyna
    - family: Wijeratne
      given: Tissa
    - family: Wilner
      given: Lauren B
    - family: Wilson
      given: Shadrach
    - family: Winkler
      given: Andrea Sylvia
    - family: Wiyeh
      given: Alison B
    - family: Wiysonge
      given: Charles Shey
    - family: Wolfe
      given: Charles D A
    - family: Woolf
      given: Anthony D
    - family: Wu
      given: Shouling
    - family: Wu
      given: Yun-Chun
    - family: Wyper
      given: Grant M A
    - family: Xavier
      given: Denis
    - family: Xu
      given: Gelin
    - family: Yadgir
      given: Simon
    - family: Yadollahpour
      given: Ali
    - family: Yahyazadeh Jabbari
      given: Seyed Hossein
    - family: Yamada
      given: Tomohide
    - family: Yan
      given: Lijing L
    - family: Yano
      given: Yuichiro
    - family: Yaseri
      given: Mehdi
    - family: Yasin
      given: Yasin Jemal
    - family: Yeshaneh
      given: Alex
    - family: Yimer
      given: Ebrahim M
    - family: Yip
      given: Paul
    - family: Yisma
      given: Engida
    - family: Yonemoto
      given: Naohiro
    - family: Yoon
      given: Seok-Jun
    - family: Yotebieng
      given: Marcel
    - family: Younis
      given: Mustafa Z
    - family: Yousefifard
      given: Mahmoud
    - family: Yu
      given: Chuanhua
    - family: Zadnik
      given: Vesna
    - family: Zaidi
      given: Zoubida
    - family: Zaman
      given: Sojib Bin
    - family: Zamani
      given: Mohammad
    - family: Zare
      given: Zohreh
    - family: Zeleke
      given: Ayalew Jejaw
    - family: Zenebe
      given: Zerihun Menlkalew
    - family: Zhang
      given: Kai
    - family: Zhao
      given: Zheng
    - family: Zhou
      given: Maigeng
    - family: Zodpey
      given: Sanjay
    - family: Zucker
      given: Inbar
    - family: Vos
      given: Theo
    - family: Murray
      given: Christopher J L
  citation-key: james_global_2018
  container-title: The Lancet
  container-title-short: The Lancet
  DOI: 10.1016/S0140-6736(18)32279-7
  ISSN: 0140-6736
  issue: '10159'
  issued:
    - year: 2018
      month: 11
      day: 10
  language: en
  page: 1789-1858
  source: ScienceDirect
  title: >-
    Global, regional, and national incidence, prevalence, and years lived with
    disability for 354 diseases and injuries for 195 countries and territories,
    1990–2017: a systematic analysis for the Global Burden of Disease Study 2017
  title-short: >-
    Global, regional, and national incidence, prevalence, and years lived with
    disability for 354 diseases and injuries for 195 countries and territories,
    1990–2017
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0140673618322797
  volume: '392'

- id: jansson_input_2005
  abstract: >-
    A framework for reformulating input design problems in prediction error
    identification as convex optimization problems is presented. For linear
    time-invariant single input/single output systems, this framework unifies
    and extends existing results on open-loop input design that are based on the
    finite dimensional asymptotic covariance matrix of the parameter estimates.
    Basic methods for parametrizing the input spectrum are provided and
    conditions on these parametrizations that guarantee that all possible
    covariance matrices for the asymptotic distribution of the parameter
    estimates can be generated are provided. A wide range of model quality
    constraints can be handled. In particular, different frequency-by-frequency
    constraints can be used. This opens up new applications of input design in
    areas such as robust control. Furthermore, quality specifications can be
    imposed on all models in a confidence region. Thus, allowing for statements
    such as "with at least 99% probability the model quality specifications will
    be satisfied".
  author:
    - family: Jansson
      given: H.
    - family: Hjalmarsson
      given: H.
  citation-key: jansson_input_2005
  container-title: IEEE Transactions on Automatic Control
  DOI: 10.1109/TAC.2005.856652
  ISSN: 0018-9286
  issue: '10'
  issued:
    - year: 2005
      month: 10
  page: 1534-1549
  source: IEEE Xplore
  title: >-
    Input design via LMIs admitting frequency-wise model specifications in
    confidence regions
  type: article-journal
  volume: '50'

- id: javanmard_precise_2020
  abstract: >-
    Despite breakthrough performance, modern learning models are known to be
    highly vulnerable to small adversarial perturbations in their inputs. While
    a wide variety of recent *adversarial training* methods have been effective
    at improving robustness to perturbed inputs (robust accuracy), often this
    benefit is accompanied by a decrease in accuracy on benign inputs (standard
    accuracy), leading to a tradeoff between often competing objectives.
    Complicating matters further, recent empirical evidence suggest that a
    variety of other factors (size and quality of training data, model size,
    etc.) affect this tradeoff in somewhat surprising ways. In this paper we
    provide a precise and comprehensive understanding of the role of adversarial
    training in the context of linear regression with Gaussian features. In
    particular, we characterize the fundamental tradeoff between the accuracies
    achievable by any algorithm regardless of computational power or size of the
    training data. Furthermore, we precisely characterize the standard/robust
    accuracy and the corresponding tradeoff achieved by a contemporary mini-max
    adversarial training approach in a high-dimensional regime where the number
    of data points and the parameters of the model grow in proportion to each
    other. Our theory for adversarial training algorithms also facilitates the
    rigorous study of how a variety of factors (size and quality of training
    data, model overparametrization etc.) affect the tradeoff between these two
    competing accuracies.
  author:
    - family: Javanmard
      given: Adel
    - family: Soltanolkotabi
      given: Mahdi
    - family: Hassani
      given: Hamed
  citation-key: javanmard_precise_2020
  container-title: Proceedings of the Conference on Learning Theory
  issued:
    - year: 2020
      month: 7
      day: 9
    - year: 2020
      month: 7
      day: 12
  page: 2034–2078
  title: Precise tradeoffs in adversarial training for linear regression
  type: paper-conference
  URL: http://proceedings.mlr.press/v125/javanmard20a.html
  volume: '125'

- id: javanmard_precise_2022
  author:
    - family: Javanmard
      given: Adel
    - family: Soltanolkotabi
      given: Mahdi
  citation-key: javanmard_precise_2022
  container-title: The Annals of Statistics
  DOI: 10.1214/22-AOS2180
  issue: '4'
  issued:
    - year: 2022
  page: 2127 – 2156
  publisher: Institute of Mathematical Statistics
  title: >-
    Precise statistical analysis of classification accuracies for adversarial
    training
  type: article-journal
  URL: https://doi.org/10.1214/22-AOS2180
  volume: '50'

- id: jeffreys_invariant_1946
  accessed:
    - year: 2018
      month: 10
      day: 7
  author:
    - family: Jeffreys
      given: H.
  citation-key: jeffreys_invariant_1946
  container-title: >-
    Proceedings of the Royal Society A: Mathematical, Physical and Engineering
    Sciences
  DOI: 10.1098/rspa.1946.0056
  ISSN: 1364-5021, 1471-2946
  issue: '1007'
  issued:
    - year: 1946
      month: 9
      day: 24
  language: en
  page: 453-461
  source: Crossref
  title: An Invariant Form for the Prior Probability in Estimation Problems
  type: article-journal
  URL: http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1946.0056
  volume: '186'

- id: jidling_screening_2023
  author:
    - family: Jidling
      given: Carl
    - family: Gedon
      given: Daniel
    - family: Schön
      given: Thomas B.
    - family: Oliveira
      given: Claudia Di Lorenzo
    - family: Cardos
      given: Clareci Silva
    - family: Ferreira
      given: Ariela Mota
    - family: Giatti
      given: Luana
    - family: Barreto
      given: Sandhi Maria
    - family: Sabino
      given: Ester C.
    - family: Ribeiro
      given: Antônio L. P.
    - family: Ribeiro
      given: Antônio H.
  citation-key: jidling_screening_2023
  container-title: Plos Neglected Tropical Diseases
  DOI: 10.1371/journal.pntd.0011118
  issue: '7'
  issued:
    - year: 2023
  title: >-
    Screening for Chagas disease from the electrocardiogram using a deep neural
    network
  type: article-journal
  volume: '17'

- id: jin_fpga_2010
  author:
    - family: Jin
      given: Seunghun
    - family: Cho
      given: Junguk
    - family: Pham
      given: Xuan Dai
    - family: Lee
      given: Kyoung Mu
    - family: Park
      given: S-K
    - family: Kim
      given: Munsang
    - family: Jeon
      given: Jae Wook
  citation-key: jin_fpga_2010
  container-title: Circuits and Systems for Video Technology, IEEE Transactions on
  issue: '1'
  issued:
    - year: 2010
  page: 15–26
  title: FPGA design and implementation of a real-time stereo vision system
  type: article-journal
  volume: '20'

- id: jing_tunable_2017
  abstract: >-
    Using unitary (instead of general) matrices in artiﬁcial neural networks
    (ANNs) is a promising way to solve the gradient explosion/vanishing problem,
    as well as to enable ANNs to learn long-term correlations in the data. This
    approach appears particularly promising for Recurrent Neural Networks
    (RNNs). In this work, we present a new architecture for implementing an
    Efﬁcient Unitary Neural Network (EUNNs); its main advantages can be
    summarized as follows. Firstly, the representation capacity of the unitary
    space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the
    entire unitary space. Secondly, the computational complexity for training an
    EUNN is merely O(1) per parameter. Finally, we test the performance of EUNNs
    on the standard copying task, the pixelpermuted MNIST digit recognition
    benchmark as well as the Speech Prediction Test (TIMIT). We ﬁnd that our
    architecture signiﬁcantly outperforms both other state-of-the-art unitary
    RNNs and the LSTM architecture, in terms of the ﬁnal performance and/or the
    wall-clock training speed. EUNNs are thus promising alternatives to RNNs and
    LSTMs for a wide variety of applications.
  author:
    - family: Jing
      given: Li
    - family: Shen
      given: Yichen
    - family: Dubcek
      given: Tena
    - family: Peurifoy
      given: John
    - family: Skirlo
      given: Scott
    - family: LeCun
      given: Yann
    - family: Tegmark
      given: Max
    - family: Soljacic
      given: Marin
  citation-key: jing_tunable_2017
  container-title: Proceedings of the 34 th International Conference on Machine Learning
  issued:
    - year: 2017
  language: en
  page: '9'
  source: Zotero
  title: >-
    Tunable Efficient Unitary Neural Networks (EUNN) and their application to
    RNNs
  type: article-journal

- id: johnstone_pca_2018
  abstract: >-
    When the data are high dimensional, widely used multivariate statistical
    methods such as principal component analysis can behave in unexpected ways.
    In settings where the dimension of the observations is comparable to the
    sample size, upward bias in sample eigenvalues and inconsistency of sample
    eigenvectors are among the most notable phenomena that appear. These
    phenomena, and the limiting behavior of the rescaled extreme sample
    eigenvalues, have recently been investigated in detail under the spiked
    covariance model. The behavior of the bulk of the sample eigenvalues under
    weak distributional assumptions on the observations has been described.
    These results have been exploited to develop new estimation and hypothesis
    testing methods for the population covariance matrix. Furthermore, partly in
    response to these phenomena, alternative classes of estimation procedures
    have been developed by exploiting sparsity of the eigenvectors or the
    covariance matrix. This paper gives an orientation to these areas.
  author:
    - family: Johnstone
      given: Iain M.
    - family: Paul
      given: Debashis
  citation-key: johnstone_pca_2018
  container-title: Proceedings of the IEEE
  DOI: 10.1109/JPROC.2018.2846730
  ISSN: 1558-2256
  issue: '8'
  issued:
    - year: 2018
      month: 8
  page: 1277-1292
  source: IEEE Xplore
  title: 'PCA in High Dimensions: An Orientation'
  title-short: PCA in High Dimensions
  type: article-journal
  volume: '106'

- id: jones_scipy_2001
  author:
    - family: Jones
      given: Eric
    - family: Oliphant
      given: Travis
    - family: Peterson
      given: Pearu
    - literal: others
  citation-key: jones_scipy_2001
  issued:
    - year: 2001
    - year: 0
  title: 'SciPy: Open source scientific tools for Python'
  type: book
  URL: http://www.scipy.org/

- id: jung_op_2006
  call-number: TK7871.58.O6 O62 2006
  citation-key: jung_op_2006
  collection-title: Analog Devices series
  editor:
    - family: Jung
      given: Walter G.
  event-place: Burlington, MA
  ISBN: 978-0-7506-7844-5
  issued:
    - year: 2006
  note: 'OCLC: ocm55633774'
  number-of-pages: '878'
  publisher: Newnes
  publisher-place: Burlington, MA
  source: Library of Congress ISBN
  title: Op Amp applications handbook
  type: book

- id: kadlec_datadriven_2009
  accessed:
    - year: 2017
      month: 12
      day: 24
  author:
    - family: Kadlec
      given: Petr
    - family: Gabrys
      given: Bogdan
    - family: Strandt
      given: Sibylle
  citation-key: kadlec_datadriven_2009
  container-title: Computers & Chemical Engineering
  DOI: 10.1016/j.compchemeng.2008.12.012
  ISSN: '00981354'
  issue: '4'
  issued:
    - year: 2009
      month: 4
  language: en
  page: 795-814
  source: CrossRef
  title: Data-driven Soft Sensors in the process industry
  type: article-journal
  URL: http://linkinghub.elsevier.com/retrieve/pii/S0098135409000076
  volume: '33'

- id: kalarot_comparison_2010
  author:
    - family: Kalarot
      given: Ratheesh
    - family: Morris
      given: John
  citation-key: kalarot_comparison_2010
  container-title: >-
    Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE
    Computer Society Conference on
  issued:
    - year: 2010
  page: 9–15
  publisher: IEEE
  title: Comparison of FPGA and GPU implementations of real-time stereo vision
  type: paper-conference

- id: kalchbrenner_neural_2016
  abstract: >-
    We present a novel neural network for processing sequences. The ByteNet is a
    one-dimensional convolutional neural network that is composed of two parts,
    one to encode the source sequence and the other to decode the target
    sequence. The two network parts are connected by stacking the decoder on top
    of the encoder and preserving the temporal resolution of the sequences. To
    address the differing lengths of the source and the target, we introduce an
    efﬁcient mechanism by which the decoder is dynamically unfolded over the
    representation of the encoder. The ByteNet uses dilation in the
    convolutional layers to increase its receptive ﬁeld. The resulting network
    has two core properties: it runs in time that is linear in the length of the
    sequences and it sidesteps the need for excessive memorization. The ByteNet
    decoder attains state-of-the-art performance on character-level language
    modelling and outperforms the previous best results obtained with recurrent
    networks. The ByteNet also achieves state-of-the-art performance on
    character-to-character machine translation on the English-to-German WMT
    translation task, surpassing comparable neural translation models that are
    based on recurrent networks with attentional pooling and run in quadratic
    time. We ﬁnd that the latent alignment structure contained in the
    representations reﬂects the expected alignment between the tokens.
  accessed:
    - year: 2019
      month: 3
      day: 13
  author:
    - family: Kalchbrenner
      given: Nal
    - family: Espeholt
      given: Lasse
    - family: Simonyan
      given: Karen
    - family: Oord
      given: Aaron
      dropping-particle: van den
    - family: Graves
      given: Alex
    - family: Kavukcuoglu
      given: Koray
  citation-key: kalchbrenner_neural_2016
  container-title: arXiv:1610.10099 [cs]
  issued:
    - year: 2016
      month: 10
      day: 31
  language: en
  source: arXiv.org
  title: Neural Machine Translation in Linear Time
  type: article-journal
  URL: http://arxiv.org/abs/1610.10099

- id: kamaleswaran_robust_2018
  abstract: >-
    OBJECTIVE: Atrial fibrillation (AF) is a major cause of hospitalization and
    death in the United States. Moreover, as the average age of individuals
    increases around the world, early detection and diagnosis of AF become even
    more pressing. In this paper, we introduce a novel deep learning
    architecture for the detection of normal sinus rhythm, AF, other abnormal
    rhythms, and noise.

    APPROACH: We have demonstrated through a systematic approach many
    hyperparameters, input sets, and optimization methods that yielded influence
    in both training time and performance accuracy. We have focused on these
    properties to identify an optimal 13-layer convolutional neural network
    (CNN) model which was trained on 8528 short single-lead ECG recordings and
    evaluated on a test dataset of 3658 recordings.

    MAIN RESULTS: The proposed CNN architecture achieved a state-of-the-art
    performance in identifying normal, AF and other rhythms with an average F
    1-score of 0.83.

    SIGNIFICANCE: We have presented a robust deep learning-based architecture
    that can identify abnormal cardiac rhythms using short single-lead ECG
    recordings. The proposed architecture is computationally fast and can also
    be used in real-time cardiac arrhythmia detection applications.
  author:
    - family: Kamaleswaran
      given: Rishikesan
    - family: Mahajan
      given: Ruhi
    - family: Akbilgic
      given: Oguz
  citation-key: kamaleswaran_robust_2018
  container-title: Physiological Measurement
  container-title-short: Physiol Meas
  DOI: 10/gf2594
  ISSN: 1361-6579
  issue: '3'
  issued:
    - year: 2018
      month: 3
      day: 27
  language: eng
  page: '035006'
  PMID: '29369044'
  source: PubMed
  title: >-
    A robust deep convolutional neural network for the classification of
    abnormal cardiac rhythm using single lead electrocardiograms of variable
    length
  type: article-journal
  volume: '39'

- id: kaminnski_genetic_1996
  author:
    - family: Kamińnski
      given: W
    - family: Strumitto
      given: P
    - family: Tomczak
      given: E
  citation-key: kaminnski_genetic_1996
  container-title: Drying Technology
  DOI: 10.1080/07373939608917198
  issue: '9'
  issued:
    - year: 1996
  page: 2117–2133
  title: >-
    Genetic algorithms and artificial neural networks for description of thermal
    deterioration processes
  type: article-journal
  volume: '14'

- id: kanuparthi_hdetach_2019
  abstract: >-
    Recurrent neural networks are known for their notorious exploding and
    vanishing gradient problem (EVGP). This problem becomes more evident in
    tasks where the information needed to correctly solve them exist over long
    time scales, because EVGP prevents important gradient components from being
    back-propagated adequately over a large number of steps. We introduce a
    simple stochastic algorithm (h-detach) that is speciﬁc to LSTM optimization
    and targeted towards addressing this problem. Speciﬁcally, we show that when
    the LSTM weights are large, the gradient components through the linear path
    (cell state) in the LSTM computational graph get suppressed. Based on the
    hypothesis that these components carry information about long term
    dependencies (which we show empirically), their suppression can prevent
    LSTMs from capturing them. Our algorithm1 prevents gradients ﬂowing through
    this path from getting suppressed, thus allowing the LSTM to capture such
    dependencies better. We show signiﬁcant improvements over vanilla LSTM
    gradient based training in terms of convergence speed, robustness to seed
    and learning rate, and generalization using our modiﬁcation of LSTM gradient
    on various benchmark datasets.
  author:
    - family: Kanuparthi
      given: Bhargav
    - family: Arpit
      given: Devansh
    - family: Kerg
      given: Giancarlo
    - family: Ke
      given: Nan Rosemary
    - family: Mitliagkas
      given: Ioannis
    - family: Bengio
      given: Yoshua
  citation-key: kanuparthi_hdetach_2019
  container-title: >-
    Proceedings of the International Conference for Learning Representations
    (ICLR)
  issued:
    - year: 2019
  language: en
  page: '19'
  source: Zotero
  title: 'h-DETACH: Modifying the LSTM Gradient Towards Better Optimization'
  type: article-journal

- id: karikov_construction_2013
  author:
    - family: Karikov
      given: Evgeny Borisovich
    - family: Rubanov
      given: Vasily Grigorievich
    - family: Klassen
      given: Victor Korneevich
  citation-key: karikov_construction_2013
  container-title: World Applied Sciences Journal
  issue: '2'
  issued:
    - year: 2013
  page: 227–232
  title: >-
    Construction of a Dynamic Neural Network Model as a Stage of Grate Cooler
    Automation
  type: article-journal
  volume: '25'

- id: karki_understanding_1998
  author:
    - family: Karki
      given: Jim
  citation-key: karki_understanding_1998
  container-title: >-
    Mixed Signal and Analog Operational Amplifiers. Digital Signal Processing
    Solutions, no. White Paper: SLOA011
  container-title-short: >-
    Mixed Signal and Analog Operational Amplifiers. Digital Signal Processing
    Solutions, no. White Paper: SLOA011
  issued:
    - year: 1998
  title: Understanding operational amplifier specifications
  type: article-journal

- id: karpathy_unreasonable_2015
  accessed:
    - year: 2020
      month: 1
      day: 27
  author:
    - family: Karpathy
      given: Andrej
  citation-key: karpathy_unreasonable_2015
  container-title: Andrej Karpathy blog
  issued:
    - year: 2015
      month: 5
      day: 21
  title: The Unreasonable Effectiveness of Recurrent Neural Networks
  type: post-weblog
  URL: http://karpathy.github.io/2015/05/21/rnn-effectiveness/

- id: kassam_robust_1985
  author:
    - family: Kassam
      given: Saleem A
    - family: Poor
      given: H Vincent
  citation-key: kassam_robust_1985
  container-title: Proceedings of the IEEE
  issue: '3'
  issued:
    - year: 1985
  page: 433–481
  publisher: IEEE
  title: 'Robust techniques for signal processing: A survey'
  type: article-journal
  volume: '73'

- id: katharopoulos_transformers_2020
  abstract: >-
    Transformers achieve remarkable performance in several tasks but due to
    their quadratic complexity, with respect to the input's length, they are
    prohibitively slow for very long sequences. To address this limitation, we
    express the self-attention as a linear dot-product of kernel feature maps
    and make use of the associativity property of matrix products to reduce the
    complexity from $\mathcal{O}\left(N^2\right)$ to
    $\mathcal{O}\left(N\right)$, where $N$ is the sequence length. We show that
    this formulation permits an iterative implementation that dramatically
    accelerates autoregressive transformers and reveals their relationship to
    recurrent neural networks. Our linear transformers achieve similar
    performance to vanilla transformers and they are up to 4000x faster on
    autoregressive prediction of very long sequences.
  accessed:
    - year: 2020
      month: 7
      day: 4
  author:
    - family: Katharopoulos
      given: Angelos
    - family: Vyas
      given: Apoorv
    - family: Pappas
      given: Nikolaos
    - family: Fleuret
      given: François
  citation-key: katharopoulos_transformers_2020
  container-title: arXiv:2006.16236 [cs, stat]
  issued:
    - year: 2020
      month: 6
      day: 30
  source: arXiv.org
  title: >-
    Transformers are RNNs: Fast Autoregressive Transformers with Linear
    Attention
  title-short: Transformers are RNNs
  type: article-journal
  URL: http://arxiv.org/abs/2006.16236

- id: katzman_deepsurv_2018
  abstract: >-
    Medical practitioners use survival models to explore and understand the
    relationships between patients' covariates (e.g. clinical and genetic
    features) and the effectiveness of various treatment options. Standard
    survival models like the linear Cox proportional hazards model require
    extensive feature engineering or prior medical knowledge to model treatment
    interaction at an individual level. While nonlinear survival methods, such
    as neural networks and survival forests, can inherently model these
    high-level interaction terms, they have yet to be shown as effective
    treatment recommender systems. We introduce DeepSurv, a Cox proportional
    hazards deep neural network and state-of-the-art survival method for
    modeling interactions between a patient's covariates and treatment
    effectiveness in order to provide personalized treatment recommendations. We
    perform a number of experiments training DeepSurv on simulated and real
    survival data. We demonstrate that DeepSurv performs as well as or better
    than other state-of-the-art survival models and validate that DeepSurv
    successfully models increasingly complex relationships between a patient's
    covariates and their risk of failure. We then show how DeepSurv models the
    relationship between a patient's features and effectiveness of different
    treatment options to show how DeepSurv can be used to provide individual
    treatment recommendations. Finally, we train DeepSurv on real clinical
    studies to demonstrate how it's personalized treatment recommendations would
    increase the survival time of a set of patients. The predictive and modeling
    capabilities of DeepSurv will enable medical researchers to use deep neural
    networks as a tool in their exploration, understanding, and prediction of
    the effects of a patient's characteristics on their risk of failure.
  accessed:
    - year: 2023
      month: 7
      day: 6
  author:
    - family: Katzman
      given: Jared
    - family: Shaham
      given: Uri
    - family: Bates
      given: Jonathan
    - family: Cloninger
      given: Alexander
    - family: Jiang
      given: Tingting
    - family: Kluger
      given: Yuval
  citation-key: katzman_deepsurv_2018
  container-title: BMC Medical Research Methodology
  container-title-short: BMC Med Res Methodol
  DOI: 10.1186/s12874-018-0482-1
  ISSN: 1471-2288
  issue: '1'
  issued:
    - year: 2018
      month: 12
  page: '24'
  source: arXiv.org
  title: >-
    DeepSurv: Personalized Treatment Recommender System Using A Cox Proportional
    Hazards Deep Neural Network
  title-short: DeepSurv
  type: article-journal
  URL: http://arxiv.org/abs/1606.00931
  volume: '18'

- id: katzman_deepsurv_2018a
  abstract: >-
    Medical practitioners use survival models to explore and understand the
    relationships between patients’ covariates (e.g. clinical and genetic
    features) and the effectiveness of various treatment options. Standard
    survival models like the linear Cox proportional hazards model require
    extensive feature engineering or prior medical knowledge to model treatment
    interaction at an individual level. While nonlinear survival methods, such
    as neural networks and survival forests, can inherently model these
    high-level interaction terms, they have yet to be shown as effective
    treatment recommender systems.
  accessed:
    - year: 2023
      month: 7
      day: 6
  author:
    - family: Katzman
      given: Jared L.
    - family: Shaham
      given: Uri
    - family: Cloninger
      given: Alexander
    - family: Bates
      given: Jonathan
    - family: Jiang
      given: Tingting
    - family: Kluger
      given: Yuval
  citation-key: katzman_deepsurv_2018a
  container-title: BMC Medical Research Methodology
  container-title-short: BMC Medical Research Methodology
  DOI: 10.1186/s12874-018-0482-1
  ISSN: 1471-2288
  issue: '1'
  issued:
    - year: 2018
      month: 2
      day: 26
  page: '24'
  source: BioMed Central
  title: >-
    DeepSurv: personalized treatment recommender system using a Cox proportional
    hazards deep neural network
  title-short: DeepSurv
  type: article-journal
  URL: https://doi.org/10.1186/s12874-018-0482-1
  volume: '18'

- id: kawaguchi_deep_2016
  accessed:
    - year: 2020
      month: 8
      day: 27
  author:
    - family: Kawaguchi
      given: Kenji
  citation-key: kawaguchi_deep_2016
  container-title: Advances in Neural Information Processing Systems 29
  editor:
    - family: Lee
      given: D. D.
    - family: Sugiyama
      given: M.
    - family: Luxburg
      given: U. V.
    - family: Guyon
      given: I.
    - family: Garnett
      given: R.
  issued:
    - year: 2016
  page: 586–594
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Deep Learning without Poor Local Minima
  type: chapter
  URL: http://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima.pdf

- id: kawaguchi_elimination_2020
  abstract: >-
    In this paper, we theoretically prove that adding one special neuron per
    output unit eliminates all suboptimal local minima of any deep neural
    network, for multi-class classification, binary classi...
  accessed:
    - year: 2020
      month: 8
      day: 27
  author:
    - family: Kawaguchi
      given: Kenji
    - family: Kaelbling
      given: Leslie
  citation-key: kawaguchi_elimination_2020
  container-title: International Conference on Artificial Intelligence and Statistics
  event-title: International Conference on Artificial Intelligence and Statistics
  ISSN: 2640-3498
  issued:
    - year: 2020
      month: 6
      day: 3
  language: en
  page: 853-863
  publisher: PMLR
  source: proceedings.mlr.press
  title: Elimination of All Bad Local Minima in Deep Learning
  type: paper-conference
  URL: http://proceedings.mlr.press/v108/kawaguchi20b.html

- id: kay_intuitive_2006
  author:
    - family: Kay
      given: Steven M.
  call-number: QA273 .K326 2006
  citation-key: kay_intuitive_2006
  event-place: New York
  ISBN: 978-0-387-24157-9 978-0-387-24158-6
  issued:
    - year: 2006
  number-of-pages: '833'
  publisher: Springer
  publisher-place: New York
  source: Library of Congress ISBN
  title: Intuitive probability and random processes using MATLAB
  type: book

- id: kayacan_identification_2015
  author:
    - family: Kayacan
      given: Erkan
    - family: Kayacan
      given: Erdal
    - family: Khanesar
      given: Mojtaba Ahmadieh
  citation-key: kayacan_identification_2015
  container-title: IEEE Transactions on Industrial Electronics
  container-title-short: IEEE Transactions on Industrial Electronics
  DOI: 10.1109/TIE.2014.2345353
  ISSN: 0278-0046
  issue: '3'
  issued:
    - year: 2015
  page: 1716-1724
  title: >-
    Identification of nonlinear dynamic systems using type-2 fuzzy neural
    networks—A novel learning algorithm and a comparative study
  type: article-journal
  volume: '62'

- id: kennedy_particle_2011
  author:
    - family: Kennedy
      given: James
  citation-key: kennedy_particle_2011
  container-title: Encyclopedia of machine learning
  issued:
    - year: 2011
  page: 760–766
  publisher: Springer
  title: Particle swarm optimization
  type: chapter

- id: kerg_nonnormal_2019
  abstract: >-
    A recent strategy to circumvent the exploding and vanishing gradient problem
    in RNNs, and to allow the stable propagation of signals over long time
    scales, is to constrain recurrent connectivity matrices to be orthogonal or
    unitary. This ensures eigenvalues with unit norm and thus stable dynamics
    and training. However this comes at the cost of reduced expressivity due to
    the limited variety of orthogonal transformations. We propose a novel
    connectivity structure based on the Schur decomposition and a splitting of
    the Schur form into normal and non-normal parts. This allows to parametrize
    matrices with unit-norm eigenspectra without orthogonality constraints on
    eigenbases. The resulting architecture ensures access to a larger space of
    spectrally constrained matrices, of which orthogonal matrices are a subset.
    This crucial difference retains the stability advantages and training speed
    of orthogonal RNNs while enhancing expressivity, especially on tasks that
    require computations over ongoing input sequences.
  accessed:
    - year: 2019
      month: 6
      day: 6
  author:
    - family: Kerg
      given: Giancarlo
    - family: Goyette
      given: Kyle
    - family: Touzel
      given: Maximilian Puelma
    - family: Gidel
      given: Gauthier
    - family: Vorontsov
      given: Eugene
    - family: Bengio
      given: Yoshua
    - family: Lajoie
      given: Guillaume
  citation-key: kerg_nonnormal_2019
  container-title: arXiv:1905.12080 [cs, stat]
  issued:
    - year: 2019
      month: 5
      day: 28
  source: arXiv.org
  title: >-
    Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies
    while improving expressivity with transient dynamics
  title-short: Non-normal Recurrent Neural Network (nnRNN)
  type: article-journal
  URL: http://arxiv.org/abs/1905.12080

- id: keskar_largebatch_2016
  abstract: >-
    The stochastic gradient descent (SGD) method and its variants are algorithms
    of choice for many Deep Learning tasks. These methods operate in a
    small-batch regime wherein a fraction of the training data, say $32$-$512$
    data points, is sampled to compute an approximation to the gradient. It has
    been observed in practice that when using a larger batch there is a
    degradation in the quality of the model, as measured by its ability to
    generalize. We investigate the cause for this generalization drop in the
    large-batch regime and present numerical evidence that supports the view
    that large-batch methods tend to converge to sharp minimizers of the
    training and testing functions - and as is well known, sharp minima lead to
    poorer generalization. In contrast, small-batch methods consistently
    converge to flat minimizers, and our experiments support a commonly held
    view that this is due to the inherent noise in the gradient estimation. We
    discuss several strategies to attempt to help large-batch methods eliminate
    this generalization gap.
  author:
    - family: Keskar
      given: Nitish Shirish
    - family: Mudigere
      given: Dheevatsa
    - family: Nocedal
      given: Jorge
    - family: Smelyanskiy
      given: Mikhail
    - family: Tang
      given: Ping Tak Peter
  citation-key: keskar_largebatch_2016
  container-title: arXiv:1609.04836 [cs, math]
  issued:
    - year: 2016
      month: 9
      day: 15
  source: arXiv.org
  title: >-
    On Large-Batch Training for Deep Learning: Generalization Gap and Sharp
    Minima
  title-short: On Large-Batch Training for Deep Learning
  type: article-journal
  URL: http://arxiv.org/abs/1609.04836

- id: keskar_secondorder_2017
  author:
    - family: Keskar
      given: Nitish Shirish
  citation-key: keskar_secondorder_2017
  genre: PhD Thesis
  issued:
    - year: 2017
  publisher: Northwestern University
  source: Google Scholar
  title: Second-Order Methods for Stochastic and Nonsmooth Optimization
  type: thesis

- id: khalil_nonlinear_2002
  author:
    - family: Khalil
      given: Hassan K
  citation-key: khalil_nonlinear_2002
  edition: Third
  issued:
    - year: 2002
  publisher: Upper Saddle River
  title: Nonlinear systems
  type: book

- id: khan_forecasting_2015
  author:
    - family: Khan
      given: Esam A
    - family: Elgamal
      given: Mahmoud A
    - family: Shaarawy
      given: Sameer M
  citation-key: khan_forecasting_2015
  container-title: British Journal of Mathematics & Computer Science
  DOI: 10.9734/BJMCS/2015/14563
  issue: '5'
  issued:
    - year: 2015
  page: '394'
  title: >-
    Forecasting the Number of Muslim Pilgrims Using NARX Neural Networks with a
    Comparison Study with Other Modern Methods
  type: article-journal
  volume: '6'

- id: khandelwal_sharp_2018
  abstract: >-
    We know very little about how neural language models (LM) use prior
    linguistic context. In this paper, we investigate the role of context in an
    LSTM LM, through ablation studies. Specifically, we analyze the increase in
    perplexity when prior context words are shuffled, replaced, or dropped. On
    two standard datasets, Penn Treebank and WikiText-2, we find that the model
    is capable of using about 200 tokens of context on average, but sharply
    distinguishes nearby context (recent 50 tokens) from the distant history.
    The model is highly sensitive to the order of words within the most recent
    sentence, but ignores word order in the long-range context (beyond 50
    tokens), suggesting the distant past is modeled only as a rough semantic
    field or topic. We further find that the neural caching model (Grave et al.,
    2017b) especially helps the LSTM to copy words from within this distant
    context. Overall, our analysis not only provides a better understanding of
    how neural LMs use their context, but also sheds light on recent success
    from cache-based models.
  accessed:
    - year: 2019
      month: 6
      day: 14
  author:
    - family: Khandelwal
      given: Urvashi
    - family: He
      given: He
    - family: Qi
      given: Peng
    - family: Jurafsky
      given: Dan
  citation-key: khandelwal_sharp_2018
  container-title: arXiv:1805.04623 [cs]
  issued:
    - year: 2018
      month: 5
      day: 11
  source: arXiv.org
  title: 'Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context'
  title-short: Sharp Nearby, Fuzzy Far Away
  type: article-journal
  URL: http://arxiv.org/abs/1805.04623

- id: kiefer_sequential_1953
  author:
    - family: Kiefer
      given: Jack
  citation-key: kiefer_sequential_1953
  container-title: Proceedings of the American Mathematical Society
  DOI: 10.1090/S0002-9939-1953-0055639-3
  issue: '3'
  issued:
    - year: 1953
  page: 502–506
  title: Sequential minimax search for a maximum
  type: article-journal
  volume: '4'

- id: kilani_cognitive_2016
  author:
    - family: Kilani
      given: Moez Ben
    - family: Nijsure
      given: Yogesh
    - family: Gagnon
      given: Ghyslain
    - family: Kaddoum
      given: Georges
    - family: Gagnon
      given: François
  citation-key: kilani_cognitive_2016
  container-title: IET Radar, Sonar & Navigation
  issue: '2'
  issued:
    - year: 2016
  page: 417–425
  title: Cognitive waveform and receiver selection mechanism for multistatic radar
  type: article-journal
  volume: '10'

- id: kilts_advanced_2007
  author:
    - family: Kilts
      given: Steve
  call-number: TK7895.G36 K55 2007
  citation-key: kilts_advanced_2007
  event-place: Hoboken, N.J
  ISBN: 978-0-470-05437-6
  issued:
    - year: 2007
  note: 'OCLC: ocm72799161'
  number-of-pages: '336'
  publisher: 'Wiley : IEEE'
  publisher-place: Hoboken, N.J
  source: Library of Congress ISBN
  title: 'Advanced FPGA design: architecture, implementation, and optimization'
  title-short: Advanced FPGA design
  type: book

- id: kim_interpretability_
  abstract: >-
    The interpretation of deep learning models is a challenge due to their size,
    complexity, and often opaque internal state. In addition, many systems, such
    as image classiﬁers, operate on low-level features rather than high-level
    concepts. To address these challenges, we introduce Concept Activation
    Vectors (CAVs), which provide an interpretation of a neural net’s internal
    state in terms of human-friendly concepts. The key idea is to view the
    high-dimensional internal state of a neural net as an aid, not an obstacle.
    We show how to use CAVs as part of a technique, Testing with CAVs (TCAV),
    that uses directional derivatives to quantify the degree to which a
    user-deﬁned concept is important to a classiﬁcation result–for example, how
    sensitive a prediction of zebra is to the presence of stripes. Using the
    domain of image classiﬁcation as a testing ground, we describe how CAVs may
    be used to explore hypotheses and generate insights for a standard image
    classiﬁcation network as well as a medical application.
  author:
    - family: Kim
      given: Been
    - family: Wattenberg
      given: Martin
    - family: Gilmer
      given: Justin
    - family: Cai
      given: Carrie
    - family: Wexler
      given: James
    - family: Viegas
      given: Fernanda
    - family: Sayres
      given: Rory
  citation-key: kim_interpretability_
  language: en
  note: '00006'
  page: '10'
  source: Zotero
  title: >-
    Interpretability Beyond Feature Attribution:  Quantitative Testing with
    Concept Activation Vectors (TCAV)
  type: article-journal

- id: kim_interpretability_2018
  abstract: >-
    The interpretation of deep learning models is a challenge due to their size,
    complexity, and often opaque internal state. In addition, many systems, such
    as image classifiers, operate on low-level ...
  accessed:
    - year: 2018
      month: 11
      day: 15
  author:
    - family: Kim
      given: Been
    - family: Wattenberg
      given: Martin
    - family: Gilmer
      given: Justin
    - family: Cai
      given: Carrie
    - family: Wexler
      given: James
    - family: Viegas
      given: Fernanda
    - family: Sayres
      given: Rory
  citation-key: kim_interpretability_2018
  container-title: International Conference on Machine Learning
  event-title: International Conference on Machine Learning
  issued:
    - year: 2018
      month: 7
      day: 3
  language: en
  page: 2668-2677
  source: proceedings.mlr.press
  title: >-
    Interpretability Beyond Feature Attribution: Quantitative Testing with
    Concept Activation Vectors (TCAV)
  title-short: Interpretability Beyond Feature Attribution
  type: paper-conference
  URL: http://proceedings.mlr.press/v80/kim18d.html

- id: kim_languageuniversal_2017
  abstract: >-
    Building speech recognizers in multiple languages typically involves
    replicating a monolingual training recipe for each language, or utilizing a
    multi-task learning approach where models for different languages have
    separate output labels but share some internal parameters. In this work, we
    exploit recent progress in end-to-end speech recognition to create a single
    multilingual speech recognition system capable of recognizing any of the
    languages seen in training. To do so, we propose the use of a universal
    character set that is shared among all languages. We also create a
    language-specific gating mechanism within the network that can modulate the
    network's internal representations in a language-specific way. We evaluate
    our proposed approach on the Microsoft Cortana task across three languages
    and show that our system outperforms both the individual monolingual systems
    and systems built with a multi-task learning approach. We also show that
    this model can be used to initialize a monolingual speech recognizer, and
    can be used to create a bilingual model for use in code-switching scenarios.
  author:
    - family: Kim
      given: Suyoun
    - family: Seltzer
      given: Michael L.
  citation-key: kim_languageuniversal_2017
  container-title: arXiv:1711.02207 [cs]
  issued:
    - year: 2017
      month: 11
      day: 6
  source: arXiv.org
  title: Towards Language-Universal End-to-End Speech Recognition
  type: article-journal
  URL: http://arxiv.org/abs/1711.02207

- id: kingma_adam_2014
  abstract: >-
    We introduce Adam, an algorithm for first-order gradient-based optimization
    of stochastic objective functions, based on adaptive estimates of
    lower-order moments. The method is straightforward to implement, is
    computationally efficient, has little memory requirements, is invariant to
    diagonal rescaling of the gradients, and is well suited for problems that
    are large in terms of data and/or parameters. The method is also appropriate
    for non-stationary objectives and problems with very noisy and/or sparse
    gradients. The hyper-parameters have intuitive interpretations and typically
    require little tuning. Some connections to related algorithms, on which Adam
    was inspired, are discussed. We also analyze the theoretical convergence
    properties of the algorithm and provide a regret bound on the convergence
    rate that is comparable to the best known results under the online convex
    optimization framework. Empirical results demonstrate that Adam works well
    in practice and compares favorably to other stochastic optimization methods.
    Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.
  author:
    - family: Kingma
      given: Diederik P.
    - family: Ba
      given: Jimmy
  citation-key: kingma_adam_2014
  container-title: >-
    Proceedings of the 3rd International Conference for Learning Representations
    (ICLR)
  issued:
    - year: 2014
      month: 12
      day: 22
  source: arXiv.org
  title: 'Adam: A Method for Stochastic Optimization'
  title-short: Adam
  type: paper-conference
  URL: http://arxiv.org/abs/1412.6980

- id: kingma_autoencoding_2014
  abstract: >-
    How can we perform efficient inference and learning in directed
    probabilistic models, in the presence of continuous latent variables with
    intractable posterior distributions, and large datasets? We introduce a
    stochastic variational inference and learning algorithm that scales to large
    datasets and, under some mild differentiability conditions, even works in
    the intractable case. Our contributions is two-fold. First, we show that a
    reparameterization of the variational lower bound yields a lower bound
    estimator that can be straightforwardly optimized using standard stochastic
    gradient methods. Second, we show that for i.i.d. datasets with continuous
    latent variables per datapoint, posterior inference can be made especially
    efficient by fitting an approximate inference model (also called a
    recognition model) to the intractable posterior using the proposed lower
    bound estimator. Theoretical advantages are reflected in experimental
    results.
  archive: arxiv.org/abs/1312.6114
  author:
    - family: Kingma
      given: Diederik P.
    - family: Welling
      given: Max
  citation-key: kingma_autoencoding_2014
  container-title: ICLR
  issued:
    - year: 2014
  source: arXiv.org
  title: Auto-Encoding Variational Bayes
  type: article-journal

- id: kirk_optimal_2012
  author:
    - family: Kirk
      given: Donald E.
  citation-key: kirk_optimal_2012
  issued:
    - year: 2012
  publisher: Courier Corporation
  source: Google Scholar
  title: 'Optimal control theory: an introduction'
  title-short: Optimal control theory
  type: book

- id: kirkpatrick_optimization_1983
  author:
    - family: Kirkpatrick
      given: Scott
    - family: Gelatt
      given: C Daniel
    - family: Vecchi
      given: Mario P
    - literal: others
  citation-key: kirkpatrick_optimization_1983
  container-title: science
  DOI: 10.1126/science.220.4598.671
  issue: '4598'
  issued:
    - year: 1983
  page: 671–680
  title: Optimization by simulated annealing
  type: article-journal
  volume: '220'

- id: kleinberg_algorithm_2006
  author:
    - family: Kleinberg
      given: Jon
    - family: Tardos
      given: Éva
  call-number: QA76.9.A43 K54 2006
  citation-key: kleinberg_algorithm_2006
  event-place: Boston
  ISBN: 978-0-321-29535-4
  issued:
    - year: 2006
  number-of-pages: '838'
  publisher: Pearson/Addison-Wesley
  publisher-place: Boston
  source: Library of Congress ISBN
  title: Algorithm design
  type: book

- id: kligfield_recommendations_2007
  abstract: >-
    This statement examines the relation of the resting ECG to its technology.
    Its purpose is to foster understanding of how the modern ECG is derived and
    displayed and to establish standards that will improve the accuracy and
    usefulness of the ECG in practice. Derivation of representative waveforms
    and measurements based on global intervals are described. Special emphasis
    is placed on digital signal acquisition and computer-based signal
    processing, which provide automated measurements that lead to
    computer-generated diagnostic statements. Lead placement, recording methods,
    and waveform presentation are reviewed. Throughout the statement,
    recommendations for ECG standards are placed in context of the clinical
    implications of evolving ECG technology.
  author:
    - family: Kligfield
      given: Paul
    - family: Gettes
      given: Leonard S.
    - family: Bailey
      given: James J.
    - family: Childers
      given: Rory
    - family: Deal
      given: Barbara J.
    - family: Hancock
      given: E. William
    - family: Herpen
      given: Gerard
      non-dropping-particle: van
    - family: Kors
      given: Jan A.
    - family: Macfarlane
      given: Peter
    - family: Mirvis
      given: David M.
    - family: Pahlm
      given: Olle
    - family: Rautaharju
      given: Pentti
    - family: Wagner
      given: Galen S.
  citation-key: kligfield_recommendations_2007
  container-title: Journal of the American College of Cardiology
  DOI: 10/bwp48j
  issue: '10'
  issued:
    - year: 2007
      month: 3
      day: 13
  page: '1109'
  title: >-
    Recommendations for the Standardization and Interpretation of the
    Electrocardiogram
  type: article-journal
  URL: http://www.onlinejacc.org/content/49/10/1109.abstract
  volume: '49'

- id: kobak_optimal_2020
  abstract: >-
    A conventional wisdom in statistical learning is that large models require
    strong regularization to prevent overfitting. Here we show that this rule
    can be violated by linear regression in the underdetermined $n\ll p$
    situation under realistic conditions. Using simulations and real-life
    high-dimensional data sets, we demonstrate that an explicit positive ridge
    penalty can fail to provide any improvement over the minimum-norm least
    squares estimator. Moreover, the optimal value of ridge penalty in this
    situation can be negative. This happens when the high-variance directions in
    the predictor space can predict the response variable, which is often the
    case in the real-world high-dimensional data. In this regime, low-variance
    directions provide an implicit ridge regularization and can make any further
    positive ridge penalty detrimental. We prove that augmenting any linear
    model with random covariates and using minimum-norm estimator is
    asymptotically equivalent to adding the ridge penalty. We use a spiked
    covariance model as an analytically tractable example and prove that the
    optimal ridge penalty in this case is negative when $n\ll p$.
  accessed:
    - year: 2020
      month: 11
      day: 12
  author:
    - family: Kobak
      given: Dmitry
    - family: Lomond
      given: Jonathan
    - family: Sanchez
      given: Benoit
  citation-key: kobak_optimal_2020
  container-title: arXiv:1805.10939 [math, stat]
  issued:
    - year: 2020
      month: 4
      day: 9
  source: arXiv.org
  title: >-
    Optimal ridge penalty for real-world high-dimensional data can be zero or
    negative due to the implicit ridge regularization
  type: article-journal
  URL: http://arxiv.org/abs/1805.10939
  version: '4'

- id: koehler_uniform_2021
  abstract: >-
    Uniform convergence of interpolating predictors can explain consistency for
    high-dimensional linear regression.
  accessed:
    - year: 2022
      month: 8
      day: 4
  author:
    - family: Koehler
      given: Frederic
    - family: Zhou
      given: Lijia
    - family: Sutherland
      given: Danica J.
    - family: Srebro
      given: Nathan
  citation-key: koehler_uniform_2021
  container-title: NeurIPS
  event-title: NeurIPS
  issued:
    - year: 2021
      month: 10
      day: 26
  language: en
  source: openreview.net
  title: >-
    Uniform Convergence of Interpolators: Gaussian Width, Norm Bounds and Benign
    Overfitting
  title-short: Uniform Convergence of Interpolators
  type: paper-conference
  URL: https://openreview.net/forum?id=FyOhThdDBM

- id: kohl_probabilistic_2018
  abstract: >-
    Many real-world vision problems suffer from inherent ambiguities. In
    clinical applications for example, it might not be clear from a CT scan
    alone which particular region is cancer tissue. Therefore a group of graders
    typically produces a set of diverse but plausible segmentations. We consider
    the task of learning a distribution over segmentations given an input. To
    this end we propose a generative segmentation model based on a combination
    of a U-Net with a conditional variational autoencoder that is capable of
    efficiently producing an unlimited number of plausible hypotheses. We show
    on a lung abnormalities segmentation task and on a Cityscapes segmentation
    task that our model reproduces the possible segmentation variants as well as
    the frequencies with which they occur, doing so significantly better than
    published approaches. These models could have a high impact in real-world
    applications, such as being used as clinical decision-making algorithms
    accounting for multiple plausible semantic segmentation hypotheses to
    provide possible diagnoses and recommend further actions to resolve the
    present ambiguities.
  accessed:
    - year: 2018
      month: 11
      day: 26
  author:
    - family: Kohl
      given: Simon A. A.
    - family: Romera-Paredes
      given: Bernardino
    - family: Meyer
      given: Clemens
    - family: De Fauw
      given: Jeffrey
    - family: Ledsam
      given: Joseph R.
    - family: Maier-Hein
      given: Klaus H.
    - family: Eslami
      given: S. M. Ali
    - family: Rezende
      given: Danilo Jimenez
    - family: Ronneberger
      given: Olaf
  citation-key: kohl_probabilistic_2018
  container-title: arXiv:1806.05034 [cs, stat]
  issued:
    - year: 2018
      month: 6
      day: 13
  source: arXiv.org
  title: A Probabilistic U-Net for Segmentation of Ambiguous Images
  type: article-journal
  URL: http://arxiv.org/abs/1806.05034

- id: koller_probabilistic_2009
  author:
    - family: Koller
      given: Daphne
    - family: Friedman
      given: Nir
  call-number: QA279.5 .K65 2009
  citation-key: koller_probabilistic_2009
  collection-title: Adaptive computation and machine learning
  event-place: Cambridge, MA
  ISBN: 978-0-262-01319-2
  issued:
    - year: 2009
  number-of-pages: '1231'
  publisher: MIT Press
  publisher-place: Cambridge, MA
  source: Library of Congress ISBN
  title: 'Probabilistic graphical models: principles and techniques'
  title-short: Probabilistic graphical models
  type: book

- id: kors_expert_1992
  author:
    - family: Kors
      given: Jan
  citation-key: kors_expert_1992
  event-place: S.l.
  issued:
    - year: 1992
  language: English
  note: 'OCLC: 65910805'
  publisher: s.n.]
  publisher-place: S.l.
  source: Open WorldCat
  title: Expert knowledge for computerized ECG interpretation
  type: thesis

- id: koziel_robust_2010
  author:
    - family: Koziel
      given: Slawomir
    - family: Bandler
      given: John W
    - family: Cheng
      given: Qingsha S
  citation-key: koziel_robust_2010
  container-title: IEEE Transactions on Microwave Theory and Techniques
  issue: '8'
  issued:
    - year: 2010
  page: 2166–2174
  title: >-
    Robust trust-region space-mapping algorithms for microwave design
    optimization
  type: article-journal
  volume: '58'

- id: koziel_robust_2010a
  author:
    - family: Koziel
      given: Slawomir
    - family: Bandler
      given: John W
    - family: Cheng
      given: Qingsha S
  citation-key: koziel_robust_2010a
  container-title: IEEE Transactions on Microwave Theory and Techniques
  DOI: 10/dk8kvf
  issue: '8'
  issued:
    - year: 2010
  note: '00063'
  page: 2166-2174
  title: >-
    Robust Trust-Region Space-Mapping Algorithms for Microwave Design
    Optimization
  type: article-journal
  volume: '58'

- id: kraft_software_1988
  author:
    - family: Kraft
      given: Dieter
  citation-key: kraft_software_1988
  container-title: >-
    Forschungsbericht- Deutsche Forschungs- und Versuchsanstalt fur Luft- und
    Raumfahrt
  container-title-short: >-
    Forschungsbericht- Deutsche Forschungs- und Versuchsanstalt fur Luft- und
    Raumfahrt
  ISSN: 0171-1342
  issued:
    - year: 1988
  title: A software package for sequential quadratic programming
  type: article-journal

- id: kristensen_parameter_2004
  author:
    - family: Kristensen
      given: Niels Rode
    - family: Madsen
      given: Henrik
    - family: Jørgensen
      given: Sten Bay
  citation-key: kristensen_parameter_2004
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10.1016/j.automatica.2003.10.001
  ISSN: 0005-1098
  issue: '2'
  issued:
    - year: 2004
  page: 225-237
  title: Parameter estimation in stochastic grey-box models
  type: article-journal
  volume: '40'

- id: krizhevsky_imagenet_2012
  author:
    - family: Krizhevsky
      given: Alex
    - family: Sutskever
      given: Ilya
    - family: Hinton
      given: Geoffrey E.
  citation-key: krizhevsky_imagenet_2012
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2012
  page: 1097–1105
  source: Google Scholar
  title: Imagenet classification with deep convolutional neural networks
  type: paper-conference

- id: kukreja_least_2006
  accessed:
    - year: 2017
      month: 9
      day: 13
  author:
    - family: Kukreja
      given: Sunil L.
    - family: Löfberg
      given: Johan
    - family: Brenner
      given: Martin J.
  citation-key: kukreja_least_2006
  container-title: IFAC proceedings volumes
  DOI: 10.3182/20060329-3-AU-2901.00128
  issue: '1'
  issued:
    - year: 2006
  page: 814–819
  source: Google Scholar
  title: >-
    A least absolute shrinkage and selection operator (LASSO) for nonlinear
    system identification
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S1474667015353647
  volume: '39'

- id: kumar_comparative_2019
  abstract: >-
    In this paper, a comparative study is performed to test the approximation
    ability of different neural network structures. It involves three neural
    networks multilayer feedforward neural network (MLFFNN), diagonal recurrent
    neural network (DRNN), and nonlinear autoregressive with exogenous inputs
    (NARX) neural network. Their robustness is also tested and compared when the
    system is subjected to parameter variations and disturbance signals.
    Further, dynamic back-propagation algorithm is used to update the parameters
    associated with these neural networks. Four dynamical systems of different
    complexities including motor-driven robotic link are considered on which the
    comparative study is performed. The simulation results show the superior
    performance of DRNN identification model over NARX and MLFFNN identification
    models.
  author:
    - family: Kumar
      given: Rajesh
    - family: Srivastava
      given: Smriti
    - family: Gupta
      given: J. R. P.
    - family: Mohindru
      given: Amit
  citation-key: kumar_comparative_2019
  container-title: Soft Computing
  container-title-short: Soft Computing
  DOI: 10/gfwvbb
  ISSN: 1433-7479
  issue: '1'
  issued:
    - year: 2019
      month: 1
      day: 1
  page: 101-114
  title: >-
    Comparative study of neural networks for dynamic nonlinear systems
    identification
  type: article-journal
  URL: https://doi.org/10.1007/s00500-018-3235-5
  volume: '23'

- id: kurakin_adversarial_2018
  abstract: >-
    To accelerate research on adversarial examples and robustness of machine
    learning classifiers, Google Brain organized a NIPS 2017 competition that
    encouraged researchers to develop new methods to generate adversarial
    examples as well as to develop new ways to defend against them. In this
    chapter, we describe the structure and organization of the competition and
    the solutions developed by several of the top-placing teams.
  author:
    - family: Kurakin
      given: Alexey
    - family: Goodfellow
      given: Ian
    - family: Bengio
      given: Samy
    - family: Dong
      given: Yinpeng
    - family: Liao
      given: Fangzhou
    - family: Liang
      given: Ming
    - family: Pang
      given: Tianyu
    - family: Zhu
      given: Jun
    - family: Hu
      given: Xiaolin
    - family: Xie
      given: Cihang
    - family: Wang
      given: Jianyu
    - family: Zhang
      given: Zhishuai
    - family: Ren
      given: Zhou
    - family: Yuille
      given: Alan
    - family: Huang
      given: Sangxia
    - family: Zhao
      given: Yao
    - family: Zhao
      given: Yuzhe
    - family: Han
      given: Zhonglin
    - family: Long
      given: Junjiajia
    - family: Berdibekov
      given: Yerkebulan
    - family: Akiba
      given: Takuya
    - family: Tokui
      given: Seiya
    - family: Abe
      given: Motoki
  citation-key: kurakin_adversarial_2018
  container-title: 'The NIPS ''17 competition: Building Intelligent Systems'
  editor:
    - family: Escalera
      given: Sergio
    - family: Weimer
      given: Markus
  ISBN: 978-3-319-94042-7
  issued:
    - year: 2018
  page: 195–231
  title: Adversarial attacks and defences competition
  type: paper-conference

- id: kuznetsov_elements_2004
  author:
    - family: Kuznetsov
      given: Yu A
  citation-key: kuznetsov_elements_2004
  issued:
    - year: 2004
  title: Elements of applied bifurcation theory
  type: article-journal

- id: labvolt_mobile_2015
  author:
    - literal: LabVolt
  citation-key: labvolt_mobile_2015
  issued:
    - year: 2015
  publisher: Festo
  title: Mobile Instrumentation and Process Control Training Systems
  type: report

- id: labvolt_mobile_2015a
  author:
    - literal: LabVolt
  citation-key: labvolt_mobile_2015a
  issued:
    - year: 2015
  note: '00000'
  publisher: Festo
  title: Mobile Instrumentation and Process Control Training Systems
  type: report

- id: lai_extended_1986
  abstract: >-
    Herein strong consistency of recursive extended least squares is established
    under considerably weaker assumptions than previously assumed in the
    literature. The argument used to establish consistency also leads to certain
    basic properties of adaptive predictors based on these recursive estimators.
    Making use of these properties of the adaptive predictors, simple
    modifications of the Åström-Wittenmark self-tuning regulator are proposed
    and shown to be asymptotically optimal.
  author:
    - family: Lai
      given: Tze
    - family: Wei
      given: Ching-Zong
  citation-key: lai_extended_1986
  container-title: IEEE Transactions on Automatic Control
  DOI: 10.1109/TAC.1986.1104138
  ISSN: 0018-9286
  issue: '10'
  issued:
    - year: 1986
      month: 10
  page: 898-906
  source: IEEE Xplore
  title: >-
    Extended least squares and their applications to adaptive control and
    prediction in linear systems
  type: article-journal
  volume: '31'

- id: lalee_implementation_1998
  accessed:
    - year: 2017
      month: 8
      day: 20
  author:
    - family: Lalee
      given: Marucha
    - family: Nocedal
      given: Jorge
    - family: Plantenga
      given: Todd
  citation-key: lalee_implementation_1998
  container-title: SIAM Journal on Optimization
  DOI: 10.1137/S1052623493262993
  issue: '3'
  issued:
    - year: 1998
  page: 682–706
  source: Google Scholar
  title: >-
    On the implementation of an algorithm for large-scale equality constrained
    optimization
  type: article-journal
  URL: http://epubs.siam.org/doi/abs/10.1137/S1052623493262993
  volume: '8'

- id: lalee_implementation_1998a
  accessed:
    - year: 2017
      month: 8
      day: 20
  author:
    - family: Lalee
      given: Marucha
    - family: Nocedal
      given: Jorge
    - family: Plantenga
      given: Todd
  citation-key: lalee_implementation_1998a
  container-title: SIAM Journal on Optimization
  DOI: 10/dhjrhk
  issue: '3'
  issued:
    - year: 1998
  page: 682-706
  title: >-
    On the Implementation of an Algorithm for Large-Scale Equality Constrained
    Optimization
  type: article-journal
  volume: '8'

- id: lambert_numerical_1991
  author:
    - family: Lambert
      given: John Denholm
  citation-key: lambert_numerical_1991
  ISBN: 0-471-92990-5
  issued:
    - year: 1991
  publisher: John Wiley & Sons, Inc.
  title: >-
    Numerical methods for ordinary differential systems: the initial value
    problem
  type: book

- id: lange_lottery_2020
  author:
    - family: Lange
      given: Robert Tjarko
  citation-key: lange_lottery_2020
  container-title: >-
    https://roberttlange.github.io/year-archive/posts/2020/06/lottery-ticket-hypothesis/
  issued:
    - year: 2020
  title: 'The lottery ticket hypothesis: A survey'
  type: article-journal
  URL: https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/

- id: langone_incremental_2014
  abstract: >-
    In this work a new model for online clustering named Incremental kernel
    spectral clustering (IKSC) is presented. It is based on kernel spectral
    clustering (KSC), a model designed in the Least Squares Support Vector
    Machines (LS-SVMs) framework, with primal-dual setting. The IKSC model is
    developed to quickly adapt itself to a changing environment, in order to
    learn evolving clusters with high accuracy. In contrast with other existing
    incremental spectral clustering approaches, the eigen-updating is performed
    in a model-based manner, by exploiting one of the Karush–Kuhn–Tucker (KKT)
    optimality conditions of the KSC problem. We test the capacities of IKSC
    with some experiments conducted on computer-generated data and a real-world
    data-set of PM10 concentrations registered during a pollution episode
    occurred in Northern Europe in January 2010. We observe that our model is
    able to precisely recognize the dynamics of shifting patterns in a
    non-stationary context.
  author:
    - family: Langone
      given: Rocco
    - family: Mauricio Agudelo
      given: Oscar
    - family: De Moor
      given: Bart
    - family: Suykens
      given: Johan A. K.
  citation-key: langone_incremental_2014
  container-title: Neurocomputing
  container-title-short: Neurocomputing
  DOI: 10.1016/j.neucom.2014.02.036
  ISSN: 0925-2312
  issued:
    - year: 2014
      month: 9
      day: 2
  page: 246-260
  source: ScienceDirect
  title: >-
    Incremental kernel spectral clustering for online learning of non-stationary
    data
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0925231214004433
  volume: '139'

- id: laranja_chagas_1956
  abstract: >-
    A study of the most important clinical and pathologic aspects of Chagas'
    disease has been presented, on the basis of the analysis of 180 cases of
    acute infection (11 with autopsy), 657 cases of chronic asymptomatic
    infection, and 683 cases of chronic Chagas' heart disease (21 autopsied
    cases with
                  Schizotrypanum cruzi
                  in myocardium).
  accessed:
    - year: 2021
      month: 11
      day: 25
  author:
    - family: Laranja
      given: F. S.
    - family: Dias
      given: E.
    - family: Nobrega
      given: G.
    - family: Miranda
      given: A.
  citation-key: laranja_chagas_1956
  container-title: Circulation
  container-title-short: Circulation
  DOI: 10.1161/01.CIR.14.6.1035
  ISSN: 0009-7322, 1524-4539
  issue: '6'
  issued:
    - year: 1956
      month: 12
  language: en
  page: 1035-1060
  source: DOI.org (Crossref)
  title: 'Chagas'' Disease: A Clinical, Epidemiologic, and Pathologic Study'
  title-short: Chagas' Disease
  type: article-journal
  URL: https://www.ahajournals.org/doi/10.1161/01.CIR.14.6.1035
  volume: '14'

- id: laurent_deep_
  abstract: >-
    We consider deep linear networks with arbitrary convex differentiable loss.
    We provide a short and elementary proof of the fact that all local minima
    are global minima if the hidden layers are either 1) at least as wide as the
    input layer, or 2) at least as wide as the output layer. This result is the
    strongest possible in the following sense: If the loss is convex and
    Lipschitz but not differentiable then deep linear networks can have
    sub-optimal local minima.
  author:
    - family: Laurent
      given: Thomas
  citation-key: laurent_deep_
  language: en
  page: '6'
  source: Zotero
  title: 'Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global'
  type: article-journal

- id: laurent_deep_2018
  abstract: >-
    We consider deep linear networks with arbitrary convex differentiable loss.
    We provide a short and elementary proof of the fact that all local minima
    are global minima if the hidden layers are eith...
  accessed:
    - year: 2020
      month: 9
      day: 7
  author:
    - family: Laurent
      given: Thomas
    - family: Brecht
      given: James
  citation-key: laurent_deep_2018
  container-title: Proceedings of the International Conference on Machine Learning
  event-title: International Conference on Machine Learning
  ISSN: 2640-3498
  issued:
    - year: 2018
      month: 7
      day: 3
  language: en
  page: 2902-2907
  source: proceedings.mlr.press
  title: 'Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global'
  title-short: Deep Linear Networks with Arbitrary Loss
  type: paper-conference
  URL: http://proceedings.mlr.press/v80/laurent18a.html

- id: laurent_recurrent_2016
  abstract: >-
    We introduce an exceptionally simple gated recurrent neural network (RNN)
    that achieves performance comparable to well-known gated architectures, such
    as LSTMs and GRUs, on the word-level language modeling task. We prove that
    our model has simple, predicable and non-chaotic dynamics. This stands in
    stark contrast to more standard gated architectures, whose underlying
    dynamical systems exhibit chaotic behavior.
  accessed:
    - year: 2019
      month: 3
      day: 8
  author:
    - family: Laurent
      given: Thomas
    - family: Brecht
      given: James
      non-dropping-particle: von
  citation-key: laurent_recurrent_2016
  container-title: arXiv:1612.06212 [cs]
  issued:
    - year: 2016
      month: 12
      day: 19
  source: arXiv.org
  title: A recurrent neural network without chaos
  type: article-journal
  URL: http://arxiv.org/abs/1612.06212

- id: lax_functional_2002
  author:
    - family: Lax
      given: Peter D.
  call-number: QA320 .L345 2002
  citation-key: lax_functional_2002
  event-place: New York
  ISBN: 978-0-471-55604-6
  issued:
    - year: 2002
  language: en
  number-of-pages: '580'
  publisher: Wiley
  publisher-place: New York
  source: Library of Congress ISBN
  title: Functional analysis
  type: book

- id: lecun_deep_2015
  abstract: >-
    Deep learning allows computational models that are composed of multiple
    processing layers to learn representations of data with multiple levels of
    abstraction. These methods have dramatically improved the state-of-the-art
    in speech recognition, visual object recognition, object detection and many
    other domains such as drug discovery and genomics. Deep learning discovers
    intricate structure in large data sets by using the backpropagation
    algorithm to indicate how a machine should change its internal parameters
    that are used to compute the representation in each layer from the
    representation in the previous layer. Deep convolutional nets have brought
    about breakthroughs in processing images, video, speech and audio, whereas
    recurrent nets have shone light on sequential data such as text and speech.
  author:
    - family: LeCun
      given: Yann
    - family: Bengio
      given: Yoshua
    - family: Hinton
      given: Geoffrey
  citation-key: lecun_deep_2015
  container-title: Nature
  DOI: 10/bmqp
  ISSN: 1476-4687
  issue: '7553'
  issued:
    - year: 2015
  page: 436-444
  title: Deep learning
  type: article-journal
  volume: '521'

- id: lecun_deep_2015a
  abstract: >-
    Deep learning allows computational models that are composed of multiple
    processing layers to learn representations of data with multiple levels of
    abstraction. These methods have dramatically improved the state-of-the-art
    in speech recognition, visual object recognition, object detection and many
    other domains such as drug discovery and genomics. Deep learning discovers
    intricate structure in large data sets by using the backpropagation
    algorithm to indicate how a machine should change its internal parameters
    that are used to compute the representation in each layer from the
    representation in the previous layer. Deep convolutional nets have brought
    about breakthroughs in processing images, video, speech and audio, whereas
    recurrent nets have shone light on sequential data such as text and speech.
  author:
    - family: LeCun
      given: Yann
    - family: Bengio
      given: Yoshua
    - family: Hinton
      given: Geoffrey
  citation-key: lecun_deep_2015a
  container-title: Nature
  DOI: 10.1038/nature14539
  ISBN: 1476-4687
  issue: '7553'
  issued:
    - year: 2015
  page: 436–444
  title: Deep learning
  type: article-journal
  URL: https://doi.org/10.1038/nature14539
  volume: '521'

- id: lecun_efficient_1998
  abstract: >-
    The convergence of back-propagation learning is analyzed so as to explain
    common phenomenon observedb y practitioners. Many undesirable behaviors of
    backprop can be avoided with tricks that are rarely exposedin serious
    technical publications. This paper gives some of those tricks, ando.ers
    explanations of why they work. Many authors have suggested that second-order
    optimization methods are advantageous for neural net training. It is shown
    that most “classical” second-order methods are impractical for large neural
    networks. A few methods are proposed that do not have these limitations.
  accessed:
    - year: 2017
      month: 9
      day: 11
  author:
    - family: LeCun
      given: Yann
    - family: Bottou
      given: Leon
    - family: Orr
      given: Genevieve B.
    - family: Müller
      given: Klaus-Robert
  citation-key: lecun_efficient_1998
  collection-title: Lecture Notes in Computer Science
  container-title: 'Neural Networks: Tricks of the Trade'
  DOI: 10.1007/3-540-49430-8_2
  ISBN: 978-3-540-65311-0 978-3-540-49430-0
  issued:
    - year: 1998
  language: en
  page: 9-50
  publisher: Springer, Berlin, Heidelberg
  source: link.springer.com
  title: Efficient BackProp
  type: chapter
  URL: https://link.springer.com/chapter/10.1007/3-540-49430-8_2

- id: lecun_gradientbased_1998
  author:
    - family: Lecun
      given: Y.
    - family: Bottou
      given: L.
    - family: Bengio
      given: Y.
    - family: Haffner
      given: P.
  citation-key: lecun_gradientbased_1998
  container-title: Proceedings of the IEEE
  container-title-short: Proceedings of the IEEE
  DOI: 10.1109/5.726791
  ISSN: 0018-9219
  issue: '11'
  issued:
    - year: 1998
      month: 11
  note: '14818'
  page: 2278-2324
  title: Gradient-based learning applied to document recognition
  type: article-journal
  volume: '86'

- id: lecun_handwritten_1990
  author:
    - family: Le Cun
      given: Yann
    - family: Matan
      given: Ofer
    - family: Boser
      given: Bernhard
    - family: Denker
      given: John S
    - family: Henderson
      given: Don
    - family: Howard
      given: Richard E
    - family: Hubbard
      given: Wayne
    - family: Jacket
      given: LD
    - family: Baird
      given: Henry S
  citation-key: lecun_handwritten_1990
  container-title: >-
    Proceedings of the 10th International Conference on Pattern Recognition
    (ICPR)
  issued:
    - year: 1990
  page: 35-40
  title: Handwritten zip code recognition with multilayer networks
  type: paper-conference
  volume: '2'

- id: lee_deep_2018
  abstract: >-
    It has long been known that a single-layer fully-connected neural network
    with an i.i.d. prior over its parameters is equivalent to a Gaussian process
    (GP), in the limit of inﬁnite network width. This correspondence enables
    exact Bayesian inference for inﬁnite width neural networks on regression
    tasks by means of evaluating the corresponding GP. Recently, kernel
    functions which mimic multi-layer random neural networks have been
    developed, but only outside of a Bayesian framework. As such, previous work
    has not identiﬁed that these kernels can be used as covariance functions for
    GPs and allow fully Bayesian prediction with a deep neural network.
  author:
    - family: Lee
      given: Jaehoon
    - family: Bahri
      given: Yasaman
    - family: Novak
      given: Roman
    - family: Schoenholz
      given: Samuel S.
    - family: Pennington
      given: Jeffrey
    - family: Sohl-Dickstein
      given: Jascha
  citation-key: lee_deep_2018
  container-title: ICLR
  event-title: ICLR
  issued:
    - year: 2018
  title: Deep Neural Networks as Gaussian Processes
  type: paper-conference

- id: lee_generalizing_2016
  abstract: >-
    We seek to improve deep neural networks by generalizing the pooling
    operations that play a central role in current architectures. We pursue a
    careful exploration of approaches to allow pooling to learn and to adapt to
    complex and variable patterns. The two primary directions lie in (1)
    learning a pooling function via (two strategies of) combining of max and
    average pooling, and (2) learning a pooling function in the form of a
    tree-structured fusion of pooling ﬁlters that are themselves learned. In our
    experiments every generalized pooling operation we explore improves
    performance when used in place of average or max pooling. We experimentally
    demonstrate that the proposed pooling operations provide a boost in
    invariance properties relative to conventional pooling and set the state of
    the art on several widely adopted benchmark datasets; they are also easy to
    implement, and can be applied within various deep neural network
    architectures. These beneﬁts come with only a light increase in
    computational overhead during training and a very modest increase in the
    number of model parameters.
  author:
    - family: Lee
      given: Chen-Yu
    - family: Gallagher
      given: Patrick W
    - family: Tu
      given: Zhuowen
  citation-key: lee_generalizing_2016
  container-title: >-
    Proceedings of the 19th International Conference on Artificial Intelligence
    and Statistics (AISTATS)
  issued:
    - year: 2016
  language: en
  page: '9'
  source: Zotero
  title: >-
    Generalizing Pooling Functions in Convolutional Neural Networks: Mixed,
    Gated, and Tree
  type: paper-conference

- id: lee_introduction_2001
  author:
    - family: Lee
      given: John M
  citation-key: lee_introduction_2001
  issued:
    - year: 2001
  language: en
  title: Introduction to Smooth Manifolds
  type: article-journal

- id: lee_wide_2020
  abstract: >-
    A longstanding goal in deep learning research has been to precisely
    characterize training and generalization. However, the often complex loss
    landscapes of neural networks (NNs) have made a theory of learning dynamics
    elusive. In this work, we show that for wide NNs the learning dynamics
    simplify considerably and that, in the infinite width limit, they are
    governed by a linear model obtained from the first-order Taylor expansion of
    the network around its initial parameters. Furthermore, mirroring the
    correspondence between wide Bayesian NNs and Gaussian processes (GPs),
    gradient-based training of wide NNs with a squared loss produces test set
    predictions drawn from a GP with a particular compositional kernel. While
    these theoretical results are only exact in the infinite width limit, we
    nevertheless find excellent empirical agreement between the predictions of
    the original network and those of the linearized version even for finite
    practically-sized networks. This agreement is robust across different
    architectures, optimization methods, and loss functions.
  accessed:
    - year: 2021
      month: 5
      day: 4
  author:
    - family: Lee
      given: Jaehoon
    - family: Xiao
      given: Lechao
    - family: Schoenholz
      given: Samuel S.
    - family: Bahri
      given: Yasaman
    - family: Novak
      given: Roman
    - family: Sohl-Dickstein
      given: Jascha
    - family: Pennington
      given: Jeffrey
  citation-key: lee_wide_2020
  container-title: 'Journal of Statistical Mechanics: Theory and Experiment'
  container-title-short: J. Stat. Mech.
  DOI: 10.1088/1742-5468/abc62b
  ISSN: 1742-5468
  issue: '12'
  issued:
    - year: 2020
      month: 12
  language: en
  page: '124002'
  publisher: IOP Publishing
  source: Institute of Physics
  title: >-
    Wide neural networks of any depth evolve as linear models under gradient
    descent
  type: article-journal
  URL: https://doi.org/10.1088/1742-5468/abc62b
  volume: '2020'

- id: lejeune_implicit_2020
  abstract: >-
    Ensemble methods that average over a collection of independent predictors
    that are each limited to a subsampling of both the examples and features of
    the training data command a significant presence in machine learning, such
    as the ever-popular random forest, yet the nature of the subsampling effect,
    particularly of the features, is not well understood. We study the case of
    an ensemble of linear predictors, where each individual predictor is fit
    using ordinary least squares on a random submatrix of the data matrix. We
    show that, under standard Gaussianity assumptions, when the number of
    features selected for each predictor is optimally tuned, the asymptotic risk
    of a large ensemble is equal to the asymptotic ridge regression risk, which
    is known to be optimal among linear predictors in this setting. In addition
    to eliciting this implicit regularization that results from subsampling, we
    also connect this ensemble to the dropout technique used in training deep
    (neural) networks, another strategy that has been shown to have a ridge-like
    regularizing effect.
  author:
    - family: LeJeune
      given: Daniel
    - family: Javadi
      given: Hamid
    - family: Baraniuk
      given: Richard
  citation-key: lejeune_implicit_2020
  collection-title: PMLR
  container-title: >-
    Proceedings of the 23rd International Conference on Artificial Intelligence
    and Statistics (AISTATS)
  editor:
    - family: Chiappa
      given: Silvia
    - family: Calandra
      given: Roberto
  issued:
    - year: 2020
  page: 3525–3535
  title: The implicit regularization of ordinary least squares ensembles
  type: paper-conference
  URL: http://proceedings.mlr.press/v108/lejeune20b.html
  volume: '108'

- id: lenders_trlib_2016
  author:
    - family: Lenders
      given: Felix
    - family: Kirches
      given: Christian
    - family: Potschka
      given: Andreas
  citation-key: lenders_trlib_2016
  container-title: arXiv preprint arXiv:1611.04718
  container-title-short: arXiv preprint arXiv:1611.04718
  issued:
    - year: 2016
  title: >-
    trlib: A vector-free implementation of the GLTR method for iterative
    solution of the trust region problem
  type: article-journal

- id: lensink_fully_2019
  abstract: >-
    Convolutional Neural Networks (CNN) have recently seen tremendous success in
    various computer vision tasks. However, their application to problems with
    high dimensional input and output, such as high-resolution image and video
    segmentation or 3D medical imaging, has been limited by various factors.
    Primarily, in the training stage, it is necessary to store network
    activations for back propagation. In these settings, the memory requirements
    associated with storing activations can exceed what is feasible with current
    hardware, especially for problems in 3D. Previously proposed reversible
    architectures allow one to recalculate activations in the backwards pass
    instead of storing them. For computer visions tasks, only block reversible
    networks have been possible because pooling operations are not reversible.
    Block-reversibility still requires storing a number of activations that
    grows with the number of blocks. Motivated by the propagation of signals
    over physical networks, that are governed by the hyperbolic Telegraph
    equation, in this work we introduce a fully conservative hyperbolic network
    for problems with high dimensional input and output. We introduce a
    coarsening operation that allows completely reversible CNNs by using the
    Discrete Wavelet Transform and its inverse to both coarsen and interpolate
    the network state and change the number of channels. This means that during
    training we do not need to store any of the activations from the forward
    pass, and can train arbitrarily deep networks. We show that fully reversible
    networks are able to achieve results comparable to the state of the art in
    image depth estimation and full 3D video segmentation, with a much lower
    memory footprint that is a constant independent of the network depth.
  accessed:
    - year: 2020
      month: 7
      day: 7
  author:
    - family: Lensink
      given: Keegan
    - family: Haber
      given: Eldad
    - family: Peters
      given: Bas
  citation-key: lensink_fully_2019
  container-title: arXiv:1905.10484 [cs]
  issued:
    - year: 2019
      month: 9
      day: 17
  source: arXiv.org
  title: Fully Hyperbolic Convolutional Neural Networks
  type: article-journal
  URL: http://arxiv.org/abs/1905.10484

- id: leonhard_control_2001
  author:
    - family: Leonhard
      given: Werner
  citation-key: leonhard_control_2001
  issued:
    - year: 2001
  publisher: Springer Science & Business Media
  source: Google Scholar
  title: Control of electrical drives
  type: book

- id: leonov_lyapunov_1992
  abstract: >-
    This paper surveys results of the authors and others concerningestimatesfor
    the Hausdorffdimensionof strange attractors,particularlyin the case of
    (generalized)Lorenz systems and ROsslersystems. A key idea is the
    interpretationof Hausdorffmeasure as an analogue of a Lyapunovfunction.
  accessed:
    - year: 2019
      month: 12
      day: 27
  author:
    - family: Leonov
      given: G. A.
    - family: Boichenko
      given: V. A.
  citation-key: leonov_lyapunov_1992
  container-title: Acta Applicandae Mathematicae
  container-title-short: Acta Appl Math
  DOI: 10.1007/BF00046607
  ISSN: 0167-8019, 1572-9036
  issue: '1'
  issued:
    - year: 1992
      month: 1
  language: en
  page: 1-60
  source: DOI.org (Crossref)
  title: >-
    Lyapunov's direct method in the estimation of the Hausdorff dimension of
    attractors
  type: article-journal
  URL: http://link.springer.com/10.1007/BF00046607
  volume: '26'

- id: leontaritis_experimental_1987
  author:
    - family: Leontaritis
      given: I. J.
    - family: Billings
      given: S. A.
  citation-key: leontaritis_experimental_1987
  container-title: International Journal of Systems Science
  DOI: 10.1080/00207728708963958
  issue: '1'
  issued:
    - year: 1987
  page: 189–202
  title: Experimental design and identifiability for non-linear systems
  type: article-journal
  volume: '18'

- id: levenberg_method_1944
  author:
    - family: Levenberg
      given: Kenneth
  citation-key: levenberg_method_1944
  container-title: Journal of the Society for Industrial and Applied Mathematics
  issued:
    - year: 1944
  title: A method for the solution of certain non–linear problems in least squares
  type: article-journal

- id: levy_online_2018
  accessed:
    - year: 2018
      month: 12
      day: 13
  author:
    - family: Levy
      given: Yehuda Kfir
    - family: Yurtsever
      given: Alp
    - family: Cevher
      given: Volkan
  citation-key: levy_online_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 6501–6510
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Online Adaptive Methods, Universality and Acceleration
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/7885-online-adaptive-methods-universality-and-acceleration.pdf

- id: lewis_polygenic_2020
  author:
    - family: Lewis
      given: Cathryn M
    - family: Vassos
      given: Evangelos
  citation-key: lewis_polygenic_2020
  container-title: Genome medicine
  container-title-short: Genome medicine
  ISSN: 1756-994X
  issue: '1'
  issued:
    - year: 2020
  page: 1-11
  publisher: BioMed Central
  title: 'Polygenic risk scores: from research tools to clinical instruments'
  type: article-journal
  volume: '12'

- id: lewis_robot_2004
  author:
    - family: Lewis
      given: Frank L.
    - family: Abdallah
      given: C. T.
    - family: Dawson
      given: D. M.
    - family: Lewis
      given: Frank L.
  call-number: TJ211.35 .L48 2004
  citation-key: lewis_robot_2004
  collection-title: Control engineering series
  edition: 2nd ed., rev. and expanded
  event-place: New York
  ISBN: 978-0-8247-4072-6
  issued:
    - year: 2004
  number-of-pages: '614'
  publisher: Marcel Dekker
  publisher-place: New York
  source: Library of Congress ISBN
  title: 'Robot manipulator control: theory and practice'
  title-short: Robot manipulator control
  type: book

- id: lezcano-casado_cheap_2019
  abstract: >-
    We introduce a novel approach to perform first-order optimization with
    orthogonal and unitary constraints. This approach is based on a
    parametrization stemming from Lie group theory through the exponential map.
    The parametrization transforms the constrained optimization problem into an
    unconstrained one over a Euclidean space, for which common first-order
    optimization methods can be used. The theoretical results presented are
    general enough to cover the special orthogonal group, the unitary group and,
    in general, any connected compact Lie group. We discuss how this and other
    parametrizations can be computed efficiently through an implementation
    trick, making numerically complex parametrizations usable at a negligible
    runtime cost in neural networks. In particular, we apply our results to RNNs
    with orthogonal recurrent weights, yielding a new architecture called
    expRNN. We demonstrate how our method constitutes a more robust approach to
    optimization with orthogonal constraints, showing faster, accurate, and more
    stable convergence in several tasks designed to test RNNs.
  accessed:
    - year: 2019
      month: 9
      day: 19
  author:
    - family: Lezcano-Casado
      given: Mario
    - family: Martínez-Rubio
      given: David
  citation-key: lezcano-casado_cheap_2019
  container-title: International Conference on Machine Learning
  issued:
    - year: 2019
  language: en
  page: 3794-3803
  source: arXiv.org
  title: >-
    Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of
    the Orthogonal and Unitary Group
  title-short: Cheap Orthogonal Constraints in Neural Networks
  type: paper-conference
  URL: http://arxiv.org/abs/1901.08428

- id: lezcano-casado_trivializations_2019
  author:
    - family: Lezcano-Casado
      given: Mario
  citation-key: lezcano-casado_trivializations_2019
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2019
  title: Trivializations for Gradient-Based Optimization on Manifolds
  type: article-journal

- id: li_learning_2018
  accessed:
    - year: 2018
      month: 12
      day: 4
  author:
    - family: Li
      given: Yuanzhi
    - family: Liang
      given: Yingyu
  citation-key: li_learning_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 8167–8176
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: >-
    Learning Overparameterized Neural Networks via Stochastic Gradient Descent
    on Structured Data
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/8038-learning-overparameterized-neural-networks-via-stochastic-gradient-descent-on-structured-data.pdf

- id: li_learning_2019
  abstract: >-
    Neural networks have many successful applications, while much less
    theoretical understanding has been gained. Towards bridging this gap, we
    study the problem of learning a two-layer overparameterized ReLU neural
    network for multi-class classification via stochastic gradient descent (SGD)
    from random initialization. In the overparameterized setting, when the data
    comes from mixtures of well-separated distributions, we prove that SGD
    learns a network with a small generalization error, albeit the network has
    enough capacity to fit arbitrary labels. Furthermore, the analysis provides
    interesting insights into several aspects of learning neural networks and
    can be verified based on empirical studies on synthetic data and on the
    MNIST dataset.
  accessed:
    - year: 2020
      month: 8
      day: 10
  author:
    - family: Li
      given: Yuanzhi
    - family: Liang
      given: Yingyu
  citation-key: li_learning_2019
  container-title: arXiv:1808.01204 [cs, stat]
  issued:
    - year: 2019
      month: 8
      day: 1
  source: arXiv.org
  title: >-
    Learning Overparameterized Neural Networks via Stochastic Gradient Descent
    on Structured Data
  type: article-journal
  URL: http://arxiv.org/abs/1808.01204

- id: li_limit_2011
  abstract: >-
    Block Toeplitz and Hankel matrices arise in many aspects of applications. In
    this paper, we will research the distributions of eigenvalues for some
    models and get the semicircle law. Firstly we will give trace formulas of
    block Toeplitz and Hankel matrix. Then we will prove that the almost sure
    limit γT(m) (γH(m)) of eigenvalue distributions of random block Toeplitz
    (Hankel) matrices exist and give the moments of the limit distributions
    where m is the order of the blocks. Then we will prove the existence of
    almost sure limit of eigenvalue distributions of random block Toeplitz and
    Hankel band matrices and give the moments of the limit distributions.
    Finally we will prove that γT(m) (γH(m)) converges weakly to the semicircle
    law as m → ∞.
  accessed:
    - year: 2020
      month: 11
      day: 14
  author:
    - family: Li
      given: Yi-Ting
    - family: Liu
      given: Dang-Zheng
    - family: Wang
      given: Zheng-Dong
  citation-key: li_limit_2011
  container-title: Journal of Theoretical Probability
  container-title-short: J Theor Probab
  DOI: 10.1007/s10959-010-0326-3
  ISSN: 0894-9840, 1572-9230
  issue: '4'
  issued:
    - year: 2011
      month: 12
  language: en
  page: 1063-1086
  source: DOI.org (Crossref)
  title: >-
    Limit Distributions of Eigenvalues for Random Block Toeplitz and Hankel
    Matrices
  type: article-journal
  URL: http://link.springer.com/10.1007/s10959-010-0326-3
  volume: '24'

- id: li_neurofuzzy_2017
  accessed:
    - year: 2017
      month: 8
      day: 23
  author:
    - family: Li
      given: Feng
    - family: Jia
      given: Li
    - family: Peng
      given: Daogang
    - family: Han
      given: Chao
  citation-key: li_neurofuzzy_2017
  container-title: Neurocomputing
  DOI: 10.1016/j.neucom.2017.03.026
  ISSN: '09252312'
  issued:
    - year: 2017
      month: 6
  language: en
  page: 90-101
  source: CrossRef
  title: >-
    Neuro-fuzzy based identification method for Hammerstein output error model
    with colored noise
  type: article-journal
  URL: http://linkinghub.elsevier.com/retrieve/pii/S0925231217305210
  volume: '244'

- id: li_semisupervised_2018
  abstract: >-
    Rare diseases affect a relatively small number of people, which limits
    investment in research for treatments and cures. Developing an efficient
    method for rare disease detection is a crucial first step towards subsequent
    clinical research. In this paper, we present a semi-supervised learning
    framework for rare disease detection using generative adversarial networks.
    Our method takes advantage of the large amount of unlabeled data for disease
    detection and achieves the best results in terms of precision-recall score
    compared to baseline techniques.
  accessed:
    - year: 2018
      month: 12
      day: 13
  author:
    - family: Li
      given: Wenyuan
    - family: Wang
      given: Yunlong
    - family: Cai
      given: Yong
    - family: Arnold
      given: Corey
    - family: Zhao
      given: Emily
    - family: Yuan
      given: Yilian
  citation-key: li_semisupervised_2018
  container-title: arXiv:1812.00547 [cs, stat]
  issued:
    - year: 2018
      month: 12
      day: 2
  source: arXiv.org
  title: Semi-supervised Rare Disease Detection Using Generative Adversarial Network
  type: article-journal
  URL: http://arxiv.org/abs/1812.00547

- id: li_visualizing_2017
  abstract: >-
    Neural network training relies on our ability to find "good" minimizers of
    highly non-convex loss functions. It is well-known that certain network
    architecture designs (e.g., skip connections) produce loss functions that
    train easier, and well-chosen training parameters (batch size, learning
    rate, optimizer) produce minimizers that generalize better. However, the
    reasons for these differences, and their effects on the underlying loss
    landscape, are not well understood. In this paper, we explore the structure
    of neural loss functions, and the effect of loss landscapes on
    generalization, using a range of visualization methods. First, we introduce
    a simple "filter normalization" method that helps us visualize loss function
    curvature and make meaningful side-by-side comparisons between loss
    functions. Then, using a variety of visualizations, we explore how network
    architecture affects the loss landscape, and how training parameters affect
    the shape of minimizers.
  accessed:
    - year: 2019
      month: 4
      day: 12
  author:
    - family: Li
      given: Hao
    - family: Xu
      given: Zheng
    - family: Taylor
      given: Gavin
    - family: Studer
      given: Christoph
    - family: Goldstein
      given: Tom
  citation-key: li_visualizing_2017
  container-title: arXiv:1712.09913 [cs, stat]
  issued:
    - year: 2017
      month: 12
      day: 28
  source: arXiv.org
  title: Visualizing the Loss Landscape of Neural Nets
  type: article-journal
  URL: http://arxiv.org/abs/1712.09913

- id: li_visualizing_2018
  accessed:
    - year: 2019
      month: 4
      day: 9
  author:
    - family: Li
      given: Hao
    - family: Xu
      given: Zheng
    - family: Taylor
      given: Gavin
    - family: Studer
      given: Christoph
    - family: Goldstein
      given: Tom
  citation-key: li_visualizing_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 6389–6399
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Visualizing the Loss Landscape of Neural Nets
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf

- id: liang_multiple_2020
  abstract: >-
    We study the risk of minimum-norm interpolants of data in Reproducing Kernel
    Hilbert Spaces. Our upper bounds on the risk are of a multiple-descent shape
    for the various scalings of d = n^α^, α∈(0,1), for the input dimension d and
    sample size n. Empirical evidence supports our finding that minimum-norm
    interpolants in RKHS can exhibit this unusual non-monotonicity in sample
    size; furthermore, locations of the peaks in our experiments match our
    theoretical predictions. Since gradient flow on appropriately initialized
    wide neural networks converges to a minimum-norm interpolant with respect to
    a certain kernel, our analysis also yields novel estimation and
    generalization guarantees for these over-parametrized models. At the heart
    of our analysis is a study of spectral properties of the random kernel
    matrix restricted to a filtration of eigen-spaces of the population
    covariance operator, and may be of independent interest.
  author:
    - family: Liang
      given: Tengyuan
    - family: Rakhlin
      given: Alexander
    - family: Zhai
      given: Xiyu
  citation-key: liang_multiple_2020
  collection-title: Proceedings of machine learning research
  container-title: Proceedings of thirty third conference on learning theory
  editor:
    - family: Abernethy
      given: Jacob
    - family: Agarwal
      given: Shivani
  issued:
    - year: 2020
      month: 7
      day: 9
    - year: 2020
      month: 7
      day: 12
  page: 2683–2711
  publisher: PMLR
  title: >-
    On the multiple descent of minimum-norm interpolants and restricted lower
    isometry of kernels
  type: paper-conference
  URL: http://proceedings.mlr.press/v125/liang20a.html
  volume: '125'

- id: lima_deep_2021
  abstract: >-
    The electrocardiogram (ECG) is the most commonly used exam for the screening
    and evaluation of cardiovascular diseases. Here we propose that the age
    predicted by artificial intelligence (AI) from the raw ECG tracing (ECG-age)
    can be a measure of cardiovascular health and provide prognostic
    information. A deep convolutional neural network was trained to predict a
    patient9s age from the 12-lead ECG using data from patients that underwent
    an ECG from 2010 to 2017 - the CODE study cohort (n=1,558,415 patients). On
    the 15% hold-out CODE test split, patients with ECG-age more than 8 years
    greater than chronological age had a higher mortality rate (hazard ratio
    (HR) 1.79, p&lt;0.001) in a mean follow-up of 3.67 years, whereas those with
    ECG-age more than 8 years less than chronological age had a lower mortality
    rate (HR 0.78, p&lt;0.001). Similar results were obtained in the external
    cohorts ELSA-Brasil (n=14,236) and SaMi-Trop (n=1,631). The ability to
    predict mortality from the ECG predicted age remains even when we adjust the
    model for cardiovascular risk factors. Moreover, even for apparent normal
    ECGs, having a predicted ECG-age 8 or more years greater than chronological
    age remained a statistically significant predictor of risk (HR 1.53,
    p&lt;0.001 in CODE 15% test split). These results show that AI-enabled
    analysis of the ECG can add prognostic information to the interpretation of
    the 12-lead ECGs.
  author:
    - family: Lima
      given: Emilly M.
    - family: Ribeiro
      given: Antônio H.
    - family: Paixão
      given: Gabriela M M
    - family: Ribeiro
      given: Manoel Horta
    - family: Filho
      given: Marcelo M. Pinto
    - family: Gomes
      given: Paulo R.
    - family: Oliveira
      given: Derick M.
    - family: Sabino
      given: Ester C.
    - family: Duncan
      given: Bruce B.
    - family: Giatti
      given: Luana
    - family: Barreto
      given: Sandhi M.
    - family: Meira
      given: Wagner
    - family: Schön
      given: Thomas B.
    - family: Ribeiro
      given: Antonio Luiz P.
  citation-key: lima_deep_2021
  container-title: Nature Communications
  DOI: 10.1038/s41467-021-25351-7
  issued:
    - year: 2021
  note: 'medRxiv doi: 10.1101/2021.02.19.21251232'
  source: medRxiv.org
  title: >-
    Deep neural network estimated electrocardiographic-age as a mortality
    predictor
  type: article-journal
  volume: '12'

- id: lin_aienabled_2024
  abstract: >-
    The early identification of vulnerable patients has the potential to improve
    outcomes but poses a substantial challenge in clinical practice. This study
    evaluated the ability of an artificial intelligence (AI)-enabled
    electrocardiogram (ECG) to identify hospitalized patients with a high risk
    of mortality in a multisite randomized controlled trial involving 39
    physicians and 15,965 patients. The AI-ECG alert intervention included an AI
    report and warning messages delivered to the physicians, flagging patients
    predicted to be at high risk of mortality. The trial met its primary
    outcome, finding that implementation of the AI-ECG alert was associated with
    a significant reduction in all-cause mortality within 90 days: 3.6% patients
    in the intervention group died within 90 days, compared to 4.3% in the
    control group (4.3%) (hazard ratio (HR) = 0.83, 95% confidence interval
    (CI) = 0.70–0.99). A prespecified analysis showed that reduction in
    all-cause mortality associated with the AI-ECG alert was observed primarily
    in patients with high-risk ECGs (HR = 0.69, 95% CI = 0.53–0.90). In analyses
    of secondary outcomes, patients in the intervention group with high-risk
    ECGs received increased levels of intensive care compared to the control
    group; for the high-risk ECG group of patients, implementation of the AI-ECG
    alert was associated with a significant reduction in the risk of cardiac
    death (0.2% in the intervention arm versus 2.4% in the control arm,
    HR = 0.07, 95% CI = 0.01–0.56). While the precise means by which
    implementation of the AI-ECG alert led to decreased mortality are to be
    fully elucidated, these results indicate that such implementation assists in
    the detection of high-risk patients, prompting timely clinical care and
    reducing mortality. ClinicalTrials.gov registration: NCT05118035.
  accessed:
    - year: 2024
      month: 6
      day: 8
  author:
    - family: Lin
      given: Chin-Sheng
    - family: Liu
      given: Wei-Ting
    - family: Tsai
      given: Dung-Jang
    - family: Lou
      given: Yu-Sheng
    - family: Chang
      given: Chiao-Hsiang
    - family: Lee
      given: Chiao-Chin
    - family: Fang
      given: Wen-Hui
    - family: Wang
      given: Chih-Chia
    - family: Chen
      given: Yen-Yuan
    - family: Lin
      given: Wei-Shiang
    - family: Cheng
      given: Cheng-Chung
    - family: Lee
      given: Chia-Cheng
    - family: Wang
      given: Chih-Hung
    - family: Tsai
      given: Chien-Sung
    - family: Lin
      given: Shih-Hua
    - family: Lin
      given: Chin
  citation-key: lin_aienabled_2024
  container-title: Nature Medicine
  container-title-short: Nat Med
  DOI: 10.1038/s41591-024-02961-4
  ISSN: 1546-170X
  issue: '5'
  issued:
    - year: 2024
      month: 5
  language: en
  license: 2024 The Author(s), under exclusive licence to Springer Nature America, Inc.
  page: 1461-1470
  publisher: Nature Publishing Group
  source: www.nature.com
  title: >-
    AI-enabled electrocardiography alert intervention and all-cause mortality: a
    pragmatic randomized clinical trial
  title-short: AI-enabled electrocardiography alert intervention and all-cause mortality
  type: article-journal
  URL: https://www.nature.com/articles/s41591-024-02961-4
  volume: '30'

- id: lin_how_1998
  abstract: >-
    Learning long-term temporal dependencies with recurrent neural networks can
    be a difficult problem. It has recently been shown that a class of recurrent
    neural networks called NARX networks perform much better than conventional
    recurrent neural networks for learning certain simple long-term dependency
    problems. The intuitive explanation for this behavior is that the output
    memories of a NARX network can be manifested as jump-ahead connections in
    the time-unfolded network. These jump-ahead connections can propagate
    gradient information more efficiently, thus reducing the sensitivity of the
    network to long-term dependencies. This work gives empirical justification
    to our hypothesis that similar improvements in learning long-term
    dependencies can be achieved with other classes of recurrent neural network
    axchitectures simply by increasing the order of the embedded memory. In
    particular we explore the impact of learning simple long-term dependency
    problems on three classes of recurrent neural network architectures:
    globally recurrent networks, locally recurrent networks, and NARX (output
    feedback) networks. Comparing the performance of these architectures with
    different orders of embedded memory on two simple long-term dependencies
    problems shows that all of these classes of network architectures
    demonstrate significant improvement on learning long-term dependencies when
    the orders of embedded memory are increased. These results can be important
    to a user comfortable with a specific recurrent neural network architecture
    because simply increasing the embedding memory order of that architecture
    will make it more robust to the problem of long-term dependency learning.
  author:
    - family: Lin
      given: Tsungnan
    - family: Horne
      given: Bill G.
    - family: Giles
      given: C. Lee
  citation-key: lin_how_1998
  container-title: Neural Networks
  container-title-short: Neural Networks
  DOI: 10.1016/S0893-6080(98)00018-5
  ISSN: 0893-6080
  issue: '5'
  issued:
    - year: 1998
      month: 7
      day: 1
  page: 861-868
  source: ScienceDirect
  title: >-
    How embedded memory in recurrent neural network architectures helps learning
    long-term temporal dependencies
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0893608098000185
  volume: '11'

- id: lin_learning_1996
  abstract: >-
    It has previously been shown that gradient-descent learning algorithms for
    recurrent neural networks can perform poorly on tasks that involve long-term
    dependencies, i.e. those problems for which the desired output depends on
    inputs presented at times far in the past. We show that the long-term
    dependencies problem is lessened for a class of architectures called
    nonlinear autoregressive models with exogenous (NARX) recurrent neural
    networks, which have powerful representational capabilities. We have
    previously reported that gradient descent learning can be more effective in
    NARX networks than in recurrent neural network architectures that have
    “hidden states” on problems including grammatical inference and nonlinear
    system identification. Typically, the network converges much faster and
    generalizes better than other networks. The results in this paper are
    consistent with this phenomenon. We present some experimental results which
    show that NARX networks can often retain information for two to three times
    as long as conventional recurrent neural networks. We show that although
    NARX networks do not circumvent the problem of long-term dependencies, they
    can greatly improve performance on long-term dependency problems. We also
    describe in detail some of the assumptions regarding what it means to latch
    information robustly and suggest possible ways to loosen these assumptions
  author:
    - family: Lin
      given: Tsungnan
    - family: Horne
      given: B. G.
    - family: Tino
      given: P.
    - family: Giles
      given: C. L.
  citation-key: lin_learning_1996
  container-title: IEEE Transactions on Neural Networks
  DOI: 10.1109/72.548162
  ISSN: 1045-9227
  issue: '6'
  issued:
    - year: 1996
      month: 11
  page: 1329-1338
  source: IEEE Xplore
  title: Learning long-term dependencies in NARX recurrent neural networks
  type: article-journal
  volume: '7'

- id: lin_resnet_2018
  accessed:
    - year: 2018
      month: 12
      day: 6
  author:
    - family: Lin
      given: Hongzhou
    - family: Jegelka
      given: Stefanie
  citation-key: lin_resnet_2018
  issued:
    - year: 2018
      month: 6
      day: 28
  language: en
  source: arxiv.org
  title: ResNet with one-neuron hidden layers is a Universal Approximator
  type: article-journal
  URL: https://arxiv.org/abs/1806.10909

- id: lin_structured_2017
  abstract: >-
    This paper proposes a new model for extracting an interpretable sentence
    embedding by introducing self-attention. Instead of using a vector, we use a
    2-D matrix to represent the embedding, with each row of the matrix attending
    on a different part of the sentence. We also propose a self-attention
    mechanism and a special regularization term for the model. As a side effect,
    the embedding comes with an easy way of visualizing what specific parts of
    the sentence are encoded into the embedding. We evaluate our model on 3
    different tasks: author profiling, sentiment classification, and textual
    entailment. Results show that our model yields a significant performance
    gain compared to other sentence embedding methods in all of the 3 tasks.
  accessed:
    - year: 2019
      month: 5
      day: 29
  author:
    - family: Lin
      given: Zhouhan
    - family: Feng
      given: Minwei
    - family: Santos
      given: Cicero Nogueira
      dropping-particle: dos
    - family: Yu
      given: Mo
    - family: Xiang
      given: Bing
    - family: Zhou
      given: Bowen
    - family: Bengio
      given: Yoshua
  citation-key: lin_structured_2017
  container-title: arXiv:1703.03130 [cs]
  issued:
    - year: 2017
      month: 3
      day: 8
  source: arXiv.org
  title: A Structured Self-attentive Sentence Embedding
  type: article-journal
  URL: http://arxiv.org/abs/1703.03130

- id: lindholm_machine_2022
  abstract: >-
    This book introduces machine learning for readers with some background in
    basic linear algebra, statistics, probability, and programming. In a
    coherent statistical framework it covers a selection of supervised machine
    learning methods, from the most fundamental (k-NN, decision trees, linear
    and logistic regression) to more advanced methods (deep neural networks,
    support vector machines, Gaussian processes, random forests and boosting),
    plus commonly-used unsupervised methods (generative modeling, k-means, PCA,
    autoencoders and generative adversarial networks). Careful explanations and
    pseudo-code are presented for all methods. The authors maintain a focus on
    the fundamentals by drawing connections between methods and discussing
    general concepts such as loss functions, maximum likelihood, the
    bias-variance decomposition, ensemble averaging, kernels and the Bayesian
    approach along with generally useful tools such as regularization, cross
    validation, evaluation metrics and optimization methods. The final chapters
    offer practical advice for solving real-world supervised machine learning
    problems and on ethical aspects of modern machine learning.
  archive: Cambridge Core
  author:
    - family: Lindholm
      given: Andreas
    - family: Wahlström
      given: Niklas
    - family: Lindsten
      given: Fredrik
    - family: Schön
      given: Thomas B.
  citation-key: lindholm_machine_2022
  event-place: Cambridge
  ISBN: 978-1-108-84360-7
  issued:
    - year: 2022
  publisher: Cambridge University Press
  publisher-place: Cambridge
  source: Cambridge University Press
  title: 'Machine Learning: A First Course for Engineers and Scientists'
  type: book
  URL: https://smlbook.org

- id: lindow_heart_2023
  author:
    - family: Lindow
      given: Thomas
    - family: Maanja
      given: Maren
    - family: Schelbert
      given: Erik B
    - family: Ribeiro
      given: Antonio H
    - family: Ribeiro
      given: Antonio Luiz P
    - family: Schlegel
      given: Todd T
    - family: Ugander
      given: Martin
  citation-key: lindow_heart_2023
  container-title: European Heart Journal - Digital Health
  DOI: 10.1093/ehjdh/ztad045
  issued:
    - year: 2023
  license: All rights reserved
  title: >-
    Heart age gap by explainable advanced electrocardiography is associated with
    cardiovascular risk factors and survival
  type: article-journal

- id: lindsten_particle_2014
  abstract: >-
    Particle Markov chain Monte Carlo (PMCMC) is a systematic way of combining
    the two main tools used for Monte Carlo statistical inference: sequential
    Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). We present a new
    PMCMC algorithm that we refer to as particle Gibbs with ancestor sampling
    (PGAS). PGAS provides the data analyst with an oﬀ-the-shelf class of Markov
    kernels that can be used to simulate, for instance, the typically
    high-dimensional and highly autocorrelated state trajectory in a state-space
    model. The ancestor sampling procedure enables fast mixing of the PGAS
    kernel even when using seemingly few particles in the underlying SMC
    sampler. This is important as it can signiﬁcantly reduce the computational
    burden that is typically associated with using SMC. PGAS is conceptually
    similar to the existing PG with backward simulation (PGBS) procedure.
    Instead of using separate forward and backward sweeps as in PGBS, however,
    we achieve the same eﬀect in a single forward sweep. This makes PGAS well
    suited for addressing inference problems not only in state-space models, but
    also in models with more complex dependencies, such as non-Markovian,
    Bayesian nonparametric, and general probabilistic graphical models.
  author:
    - family: Lindsten
      given: Fredrik
    - family: Jordan
      given: Michael I
    - family: Schon
      given: Thomas B
  citation-key: lindsten_particle_2014
  issued:
    - year: 2014
  language: en
  page: '40'
  source: Zotero
  title: Particle Gibbs with Ancestor Sampling
  type: article-journal

- id: ling_spectrum_2019
  abstract: >-
    We revisit the weight initialization of deep residual networks (ResNets) by
    introducing a novel analytical tool in free probability to the community of
    deep learning. This tool deals with the limiting spectral distribution of
    non-Hermitian random matrices, rather than their conventional Hermitian
    counterparts in the literature. This new tool enables us to evaluate the
    singular value spectrum of the input-output Jacobian of a fully connected
    deep ResNet in both linear and nonlinear cases. With the powerful tool of
    free probability, we conduct an asymptotic analysis of the (limiting)
    spectrum on the single-layer case, and then extend this analysis to the
    multi-layer case of an arbitrary number of layers. The asymptotic analysis
    illustrates the necessity and university of rescaling the classical random
    initialization by the number of residual units L, so that the squared
    singular value of the associated Jacobian remains of order O(1), when
    compared with the large width and depth of the network. We empirically
    demonstrate that the proposed initialization scheme learns at a speed of
    orders of magnitudes faster than the classical ones, and thus attests a
    strong practical relevance of this investigation.
  author:
    - family: Ling
      given: Zenan
    - family: Qiu
      given: Robert C.
  citation-key: ling_spectrum_2019
  container-title: IEEE Access
  DOI: 10.1109/ACCESS.2019.2931991
  ISSN: 2169-3536
  issued:
    - year: 2019
  page: 105212-105223
  source: IEEE Xplore
  title: >-
    Spectrum Concentration in Deep Residual Learning: A Free Probability
    Approach
  title-short: Spectrum Concentration in Deep Residual Learning
  type: article-journal
  volume: '7'

- id: lipton_troubling_2018
  author:
    - family: Lipton
      given: Zachary C.
    - family: Steinhardt
      given: Jacob
  citation-key: lipton_troubling_2018
  container-title: arXiv preprint arXiv:1807.03341
  issued:
    - year: 2018
  source: Google Scholar
  title: Troubling Trends in Machine Learning Scholarship
  type: article-journal

- id: liu_adaptive_2018
  accessed:
    - year: 2018
      month: 12
      day: 13
  author:
    - family: Liu
      given: Mingrui
    - family: Li
      given: Zhe
    - family: Wang
      given: Xiaoyu
    - family: Yi
      given: Jinfeng
    - family: Yang
      given: Tianbao
  citation-key: liu_adaptive_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 4858–4867
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: >-
    Adaptive Negative Curvature Descent with Applications in Non-convex
    Optimization
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/7734-adaptive-negative-curvature-descent-with-applications-in-non-convex-optimization.pdf

- id: liu_blockwise_2009
  accessed:
    - year: 2017
      month: 9
      day: 18
  author:
    - family: Liu
      given: Han
    - family: Palatucci
      given: Mark
    - family: Zhang
      given: Jian
  citation-key: liu_blockwise_2009
  container-title: Proceedings of the 26th Annual International Conference on Machine Learning
  issued:
    - year: 2009
  page: 649–656
  publisher: ACM
  source: Google Scholar
  title: >-
    Blockwise coordinate descent procedures for the multi-task lasso, with
    applications to neural semantic basis discovery
  type: paper-conference
  URL: http://dl.acm.org/citation.cfm?id=1553458

- id: liu_double_2022
  abstract: >-
    We study generalization properties of random features (RF) regression in
    high dimensions optimized by stochastic gradient descent (SGD) in
    under-/over-parameterized regime. In this work, we derive precise
    non-asymptotic error bounds of RF regression under both constant and
    polynomial-decay step-size SGD setting, and observe the double descent
    phenomenon both theoretically and empirically. Our analysis shows how to
    cope with multiple randomness sources of initialization, label noise, and
    data sampling (as well as stochastic gradients) with no closed-form
    solution, and also goes beyond the commonly-used Gaussian/spherical data
    assumption. Our theoretical results demonstrate that, with SGD training, RF
    regression still generalizes well for interpolation learning, and is able to
    characterize the double descent behavior by the unimodality of variance and
    monotonic decrease of bias. Besides, we also prove that the constant
    step-size SGD setting incurs no loss in convergence rate when compared to
    the exact minimum-norm interpolator, as a theoretical justification of using
    SGD in practice.
  accessed:
    - year: 2022
      month: 11
      day: 25
  author:
    - family: Liu
      given: Fanghui
    - family: Suykens
      given: Johan A. K.
    - family: Cevher
      given: Volkan
  citation-key: liu_double_2022
  DOI: 10.48550/arXiv.2110.06910
  issued:
    - year: 2022
      month: 10
      day: 16
  number: arXiv:2110.06910
  publisher: arXiv
  source: arXiv.org
  title: On the Double Descent of Random Features Models Trained with SGD
  type: article
  URL: http://arxiv.org/abs/2110.06910

- id: liu_frequencydomain_2018
  accessed:
    - year: 2018
      month: 12
      day: 10
  author:
    - family: Liu
      given: Zhenhua
    - family: Xu
      given: Jizheng
    - family: Peng
      given: Xiulian
    - family: Xiong
      given: Ruiqin
  citation-key: liu_frequencydomain_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 1051–1061
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Frequency-Domain Dynamic Pruning for Convolutional Neural Networks
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/7382-frequency-domain-dynamic-pruning-for-convolutional-neural-networks.pdf

- id: liu_fuzzy_2016
  author:
    - family: Liu
      given: Yan-Jun
    - family: Gao
      given: Ying
    - family: Tong
      given: Shaocheng
    - family: Li
      given: Yongming
  citation-key: liu_fuzzy_2016
  container-title: IEEE Transactions on Fuzzy Systems
  container-title-short: IEEE Transactions on Fuzzy Systems
  DOI: 10.1109/TFUZZ.2015.2418000
  ISSN: 1063-6706
  issue: '1'
  issued:
    - year: 2016
  page: 16-28
  title: >-
    Fuzzy approximation-based adaptive backstepping optimal control for a class
    of nonlinear discrete-time systems with dead-zone
  type: article-journal
  volume: '24'

- id: liu_kernel_2021
  abstract: >-
    In this paper, we provide a precise characterization of generalization
    properties of high dimensional kernel ridge regression across the under- and
    over-parameterized regimes, depending on whether...
  accessed:
    - year: 2021
      month: 6
      day: 29
  author:
    - family: Liu
      given: Fanghui
    - family: Liao
      given: Zhenyu
    - family: Suykens
      given: Johan
  citation-key: liu_kernel_2021
  container-title: International Conference on Artificial Intelligence and Statistics
  event-title: International Conference on Artificial Intelligence and Statistics
  ISSN: 2640-3498
  issued:
    - year: 2021
      month: 3
      day: 18
  language: en
  page: 649-657
  publisher: PMLR
  source: proceedings.mlr.press
  title: 'Kernel regression in high dimensions: Refined analysis beyond double descent'
  title-short: Kernel regression in high dimensions
  type: paper-conference
  URL: http://proceedings.mlr.press/v130/liu21b.html

- id: liu_limited_1989
  accessed:
    - year: 2020
      month: 2
      day: 7
  author:
    - family: Liu
      given: Dong C
    - family: Nocedal
      given: Jorge
  citation-key: liu_limited_1989
  container-title: Mathematical programming
  issue: 1-3
  issued:
    - year: 1989
  page: 503-528
  title: On the limited memory BFGS method for large scale optimization
  type: article-journal
  URL: https://people.sc.fsu.edu/~inavon/5420a/liu89limited.pdf
  volume: '45'

- id: liu_multilevel_2019
  abstract: >-
    In computer vision, convolutional networks (CNNs) often adopt pooling to
    enlarge receptive field which has the advantage of low computational
    complexity. However, pooling can cause information loss and thus is
    detrimental to further operations such as features extraction and analysis.
    Recently, dilated filter has been proposed to tradeoff between receptive
    field size and efficiency. But the accompanying gridding effect can cause a
    sparse sampling of input images with checkerboard patterns. To address this
    problem, in this paper, we propose a novel multi-level wavelet CNN (MWCNN)
    model to achieve a better tradeoff between receptive field size and
    computational efficiency. The core idea is to embed wavelet transform into
    CNN architecture to reduce the resolution of feature maps while at the same
    time, increasing receptive field. Specifically, MWCNN for image restoration
    is based on U-Net architecture, and inverse wavelet transform (IWT) is
    deployed to reconstruct the high resolution (HR) feature maps. The proposed
    MWCNN can also be viewed as an improvement of dilated filter and a
    generalization of average pooling and can be applied to not only image
    restoration tasks, but also any CNNs requiring a pooling operation. The
    experimental results demonstrate the effectiveness of the proposed MWCNN for
    tasks, such as image denoising, single image super-resolution, JPEG image
    artifacts removal and object classification.
  author:
    - family: Liu
      given: Pengju
    - family: Zhang
      given: Hongzhi
    - family: Lian
      given: Wei
    - family: Zuo
      given: Wangmeng
  citation-key: liu_multilevel_2019
  container-title: IEEE Access
  DOI: 10.1109/ACCESS.2019.2921451
  ISSN: 2169-3536
  issued:
    - year: 2019
  page: 74973-74985
  source: IEEE Xplore
  title: Multi-Level Wavelet Convolutional Neural Networks
  type: article-journal
  volume: '7'

- id: liu_multiview_2013
  abstract: >-
    Clustering by integrating multiview representations has become a crucial
    issue for knowledge discovery in heterogeneous environments. However, most
    prior approaches assume that the multiple representations share the same
    dimension, limiting their applicability to homogeneous environments. In this
    paper, we present a novel tensor-based framework for integrating
    heterogeneous multiview data in the context of spectral clustering. Our
    framework includes two novel formulations; that is multiview clustering
    based on the integration of the Frobenius-norm objective function (MC-FR-OI)
    and that based on matrix integration in the Frobenius-norm objective
    function (MC-FR-MI). We show that the solutions for both formulations can be
    computed by tensor decompositions. We evaluated our methods on synthetic
    data and two real-world data sets in comparison with baseline methods.
    Experimental results demonstrate that the proposed formulations are
    effective in integrating multiview data in heterogeneous environments.
  author:
    - family: Liu
      given: X.
    - family: Ji
      given: S.
    - family: Glänzel
      given: W.
    - family: Moor
      given: B. De
  citation-key: liu_multiview_2013
  container-title: IEEE Transactions on Knowledge and Data Engineering
  DOI: 10.1109/TKDE.2012.95
  ISSN: 1041-4347
  issue: '5'
  issued:
    - year: 2013
      month: 5
  page: 1056-1069
  source: IEEE Xplore
  title: Multiview Partitioning via Tensor Methods
  type: article-journal
  volume: '25'

- id: liu_open_2018
  abstract: >-
    Over the past few decades, methods for classification and detection of
    rhythm or morphology abnormalities in ECG signals have been widely studied.
    However, it lacks the comprehensive performance evaluation on an open
    database. This paper presents a detailed introduction for the database

    used for the 1st China Physiological Signal Challenge 2018 (CPSC 2018),
    which will be run as a special section during the ICBEB 2018. CPSC 2018 aims
    to encourage the development of algorithms to identify the rhythm/morphology
    abnormalities from 12-lead ECGs. The data used in CPSC 2018 include

    one normal ECG type and eight abnormal types. This paper details the data
    source, recording information, patients' clinical baseline parameters as
    age, gender and so on. Meanwhile, it also presents the commonly used
    detection/classification methods for the abovementioned abnormal ECG types.

    We hope this paper could be a guide reference for the CPSC 2018, to
    facilitate the researchers familiar with the data and the related research
    advances.
  author:
    - family: Liu
      given: Feifei
    - family: Liu
      given: Chengyu
    - family: Zhao
      given: Lina
    - family: Zhang
      given: Xiangyu
    - family: Wu
      given: Xiaoling
    - family: Xu
      given: Xiaoyan
    - family: Liu
      given: Yulin
    - family: Ma
      given: Caiyun
    - family: Wei
      given: Shoushui
    - family: He
      given: Zhiqiang
    - family: Li
      given: Jianqing
    - family: Yin Kwee
      given: Eddie Ng
  citation-key: liu_open_2018
  container-title: Journal of Medical Imaging and Health Informatics
  container-title-short: Journal of Medical Imaging and Health Informatics
  DOI: 10.1166/jmihi.2018.2442
  issue: '7'
  issued:
    - year: 2018
      month: 9
      day: 1
  page: 1368-1373
  source: IngentaConnect
  title: >-
    An Open Access Database for Evaluating the Algorithms of Electrocardiogram
    Rhythm and Morphology Abnormality Detection
  type: article-journal
  volume: '8'

- id: liu_reliable_2018
  abstract: >-
    Semi-supervised learning methods are motivated by the availability of large
    datasets with unlabeled features in addition to labeled data. Unlabeled data
    is, however, not guaranteed to improve classification performance and has in
    fact been reported to impair the performance in certain cases. A fundamental
    source of error arises from restrictive assumptions about the unlabeled
    features, which result in unreliable classifiers. In this paper, we develop
    a semi-supervised learning approach that relaxes such assumptions and is
    capable of providing classifiers that reliably measure the label
    uncertainty. The approach is applicable using any generative model with a
    supervised learning algorithm. We illustrate the approach using both
    handwritten digit and cloth classification data where the labels are missing
    at random.
  accessed:
    - year: 2019
      month: 3
      day: 21
  author:
    - family: Liu
      given: Xiuming
    - family: Zachariah
      given: Dave
    - family: Wågberg
      given: Johan
    - family: Schön
      given: Thomas
  citation-key: liu_reliable_2018
  container-title: arXiv:1811.10947 [cs, stat]
  issued:
    - year: 2018
      month: 11
      day: 27
  source: arXiv.org
  title: Reliable Semi-Supervised Learning when Labels are Missing at Random
  type: article-journal
  URL: http://arxiv.org/abs/1811.10947

- id: livan_introduction_2018
  abstract: >-
    This is a book for absolute beginners. If you have heard about random matrix
    theory, commonly denoted RMT, but you do not know what that is, then
    welcome!, this is the place for you. Our aim is to provide a truly
    accessible introductory account of RMT for physicists and mathematicians at
    the beginning of their research career. We tried to write the sort of text
    we would have loved to read when we were beginning Ph.D. students ourselves.
    Our book is structured with light and short chapters, and the style is
    informal. The calculations we found most instructive are spelt out in full.
    Particular attention is paid to the numerical verification of most
    analytical results. Our book covers standard material - classical ensembles,
    orthogonal polynomial techniques, spectral densities and spacings - but also
    more advanced and modern topics - replica approach and free probability -
    that are not normally included in elementary accounts on RMT. This book is
    dedicated to the fond memory of Oriol Bohigas.
  accessed:
    - year: 2020
      month: 11
      day: 12
  author:
    - family: Livan
      given: Giacomo
    - family: Novaes
      given: Marcel
    - family: Vivo
      given: Pierpaolo
  citation-key: livan_introduction_2018
  container-title: arXiv:1712.07903 [cond-mat, physics:math-ph]
  DOI: 10.1007/978-3-319-70885-0
  issued:
    - year: 2018
  source: arXiv.org
  title: Introduction to Random Matrices - Theory and Practice
  type: article-journal
  URL: http://arxiv.org/abs/1712.07903
  volume: '26'

- id: livne_aeroelasticity_2003
  accessed:
    - year: 2020
      month: 1
      day: 28
  author:
    - family: Livne
      given: E.
    - family: Weisshaar
      given: Terrence A.
  citation-key: livne_aeroelasticity_2003
  container-title: Journal of Aircraft
  DOI: 10.2514/2.7217
  issue: '6'
  issued:
    - year: 2003
  page: 1047-1065
  source: American Institute of Aeronautics and Astronautics
  title: Aeroelasticity of Nonconventional Airplane Configurations-Past and Future
  type: article-journal
  URL: https://doi.org/10.2514/2.7217
  volume: '40'

- id: liwan_regularization_2013
  abstract: >-
    We introduce DropConnect, a generalization of DropOut, for regularizing
    large fully-connected layers within neural networks. When training with
    Dropout, a randomly selected subset of activations are set to zero within
    each layer. DropConnect instead sets a randomly selected subset of weights
    within the network to zero. Each unit thus receives input from a random
    subset of units in the previous layer. We derive a bound on the
    generalization performance of both Dropout and DropConnect. We then evaluate
    DropConnect on a range of datasets, comparing to Dropout, and show
    state-of-the-art results on several image recoginition benchmarks can be
    obtained by aggregating multiple DropConnect-trained models.
  author:
    - literal: Li Wan
    - literal: Matthew Zeiler
    - literal: Sixin Zhang
    - literal: Yann Le Cun
    - literal: Rob Fergus
  citation-key: liwan_regularization_2013
  container-title: Proceedings of the 30th International Conference on Machine Learning
  editor:
    - literal: Sanjoy Dasgupta
    - literal: David McAllester
  issued:
    - year: 2013
      month: 2
      day: 13
  page: 1058-1066
  publisher: PMLR
  source: PMLR
  title: Regularization of Neural Networks using DropConnect
  type: paper-conference
  URL: http://proceedings.mlr.press/v28/wan13.html

- id: ljung_asymptotic_1980
  author:
    - family: Ljung
      given: Lennart
    - family: Caines
      given: Peter E.
  citation-key: ljung_asymptotic_1980
  container-title: Stochastics
  container-title-short: Stochastics
  DOI: 10.1080/17442507908833135
  ISSN: 0090-9491
  issue: 1-4
  issued:
    - year: 1980
      month: 1
      day: 1
  page: 29-46
  title: >-
    Asymptotic normality of prediction error estimators for approximate system
    models
  type: article-journal
  URL: https://doi.org/10.1080/17442507908833135
  volume: '3'

- id: ljung_consistency_1976
  abstract: >-
    The problem of identification is to determine a model that describes
    input–output data obtained from a certain system. In this chapter, strong
    consistency for general prediction error methods, including the
    maximum-likelihood (ML) method is considered. The results are valid for
    general process models: linear and nonlinear. An error identification method
    is discussed in the chapter along with a general model for stochastic
    dynamic systems. Different identifiability concepts are also introduced,
    where a procedure to prove consistency is outlined. Consistency is shown for
    a general system structure, as well as for linear systems. The application
    of the results to linear time-invariant systems is also discussed in the
    chapter.
  author:
    - family: Ljung
      given: Lennart
  citation-key: ljung_consistency_1976
  collection-title: System Identification Advances and Case Studies
  container-title: Mathematics in Science and Engineering
  DOI: 10.1016/S0076-5392(08)60871-1
  editor:
    - family: Mehra
      given: Raman K.
    - family: Lainiotis
      given: Dimitri G.
  issued:
    - year: 1976
      month: 1
      day: 1
  page: 121-164
  publisher: Elsevier
  source: ScienceDirect
  title: On The Consistency of Prediction Error Identification Methods
  type: chapter
  URL: http://www.sciencedirect.com/science/article/pii/S0076539208608711
  volume: '126'

- id: ljung_convergence_1978
  abstract: >-
    A certain class of methods to select suitable models of dynamical stochastic
    systems from measured input-output data is considered. The methods are based
    on a comparison between the measured outputs and the outputs of a candidate
    model. Depending on the set of models that is used, such methods are known
    under a variety of names, like output-error methods, equation-error methods,
    maximum-likelihood methods, etc. General results are proved concerning the
    models that are selected asymptotically as the number of observed data tends
    to infinity. For these results it is not assumed that the true system
    necessarily can be exactly represented within the chosen set of models. In
    the particular case when the model set contains the system, general
    consistency results are obtained and commented upon. Rather than to seek an
    exact description of the system, it is usually more realistic to be content
    with a suitable approximation of the true system with reasonable complexity
    properties. Here, the consequences of such a viewpoint are discussed.
  author:
    - family: Ljung
      given: L.
  citation-key: ljung_convergence_1978
  container-title: IEEE Transactions on Automatic Control
  DOI: 10.1109/TAC.1978.1101840
  ISSN: 0018-9286
  issue: '5'
  issued:
    - year: 1978
      month: 10
  page: 770-783
  source: IEEE Xplore
  title: Convergence analysis of parametric identification methods
  type: article-journal
  volume: '23'

- id: ljung_deep_2020
  abstract: >-
    Deep learning is a topic of considerable interest today. Since it deals with
    estimating – or learning – models, there are connections to the area of
    System Identiﬁcation developed in the Automatic Control community. Such
    connections are explored and exploited in this contribution. It is stressed
    that common deep nets such as feedforward and cascadeforward nets are
    nonlinear ARX (NARX) models, and can thus be easily incorporated in System
    Identiﬁcation code and practice. The case of LSTM nets is an example of
    NonLinear State-Space (NLSS) models. It performs worse than the
    cascadeforwardnet for a standard benchmark example.
  author:
    - family: Ljung
      given: Lennart
    - family: Andersson
      given: Carl
    - family: Tiels
      given: Koen
    - family: Schon
      given: Thomas B
  citation-key: ljung_deep_2020
  container-title: Proceedings of the IFAC Congress, Berlin
  issued:
    - year: 2020
  language: en
  page: '8'
  source: Zotero
  title: Deep Learning and System Identiﬁcation
  type: paper-conference

- id: ljung_perspectives_2010
  author:
    - family: Ljung
      given: Lennart
  citation-key: ljung_perspectives_2010
  container-title: Annual Reviews in Control
  DOI: 10.1016/j.arcontrol.2009.12.001
  issue: '1'
  issued:
    - year: 2010
  page: 1–12
  title: Perspectives on system identification
  type: article-journal
  volume: '34'

- id: ljung_perspectives_2010a
  abstract: >-
    System identiﬁcation is the art and science of building mathematical models
    of dynamic systems from observed input–output data. It can be seen as the
    interface between the real world of applications and the mathematical world
    of control theory and model abstractions. As such, it is an ubiquitous
    necessity for successful applications. System identiﬁcation is a very large
    topic, with different techniques that depend on the character of the models
    to be estimated: linear, nonlinear, hybrid, nonparametric, etc. At the same
    time, the area can be characterized by a small number of leading principles,
    e.g. to look for sustainable descriptions by proper decisions in the
    triangle of model complexity, information contents in the data, and
    effective validation. The area has many facets and there are many approaches
    and methods. A tutorial or a survey in a few pages is not quite possible.
    Instead, this presentation aims at giving an overview of the ‘‘science’’
    side, i.e. basic principles and results and at pointing to open problem
    areas in the practical, ‘‘art’’, side of how to approach and solve a real
    problem.
  accessed:
    - year: 2020
      month: 7
      day: 7
  author:
    - family: Ljung
      given: Lennart
  citation-key: ljung_perspectives_2010a
  container-title: Annual Reviews in Control
  container-title-short: Annual Reviews in Control
  DOI: 10.1016/j.arcontrol.2009.12.001
  ISSN: '13675788'
  issue: '1'
  issued:
    - year: 2010
      month: 4
  language: en
  page: 1-12
  source: DOI.org (Crossref)
  title: Perspectives on system identification
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S1367578810000027
  volume: '34'

- id: ljung_prediction_2002
  author:
    - family: Ljung
      given: Lennart
  citation-key: ljung_prediction_2002
  container-title: Circuits, Systems and Signal Processing
  DOI: 10.1007/BF01211648
  issue: '1'
  issued:
    - year: 2002
  page: 11–21
  title: Prediction error estimation methods
  type: article-journal
  volume: '21'

- id: ljung_system_1998
  author:
    - family: Ljung
      given: Lennart
  citation-key: ljung_system_1998
  issued:
    - year: 1998
  publisher: Springer
  title: System identification
  type: book

- id: long_fully_2015
  author:
    - family: Long
      given: Jonathan
    - family: Shelhamer
      given: Evan
    - family: Darrell
      given: Trevor
  citation-key: long_fully_2015
  event-title: >-
    Proceedings of the IEEE conference on computer vision and pattern
    recognition
  issued:
    - year: 2015
  page: 3431-3440
  title: Fully convolutional networks for semantic segmentation
  type: paper-conference

- id: loop_computing_1999
  author:
    - family: Loop
      given: Charles
    - family: Zhang
      given: Zhengyou
  citation-key: loop_computing_1999
  container-title: >-
    Computer Vision and Pattern Recognition, 1999. IEEE Computer Society
    Conference on.
  issued:
    - year: 1999
  publisher: IEEE
  title: Computing rectifying homographies for stereo vision
  type: paper-conference
  volume: '1'

- id: lorenz_deterministic_1963
  author:
    - family: Lorenz
      given: Edward N
  citation-key: lorenz_deterministic_1963
  container-title: Journal of atmospheric sciences
  container-title-short: Journal of atmospheric sciences
  ISSN: 1520-0469
  issue: '2'
  issued:
    - year: 1963
  page: 130-141
  title: Deterministic nonperiodic flow
  type: article-journal
  volume: '20'

- id: loubaton_almost_2016
  abstract: >-
    This paper studies the almost sure location of the eigenvalues of matrices
    ${\bf W}_N {\bf W}_N\^{\*}$ where ${\bf W}_N = ({\bf W}_N\^{(1)T}, ..., {\bf
    W}_N\^{(M)T})\^{T}$ is a $ML \times N$ block-line matrix whose block-lines
    $({\bf W}_N\^{(m)})_{m=1, ..., M}$ are independent identically distributed
    $L \times N$ Hankel matrices built from i.i.d. standard complex Gaussian
    sequences. It is shown that if $M \rightarrow +\infty$ and $\frac{ML}{N}
    \rightarrow c_\*$ ($c_\* \in (0, \infty)$), then the empirical eigenvalue
    distribution of ${\bf W}_N {\bf W}_N\^{\*}$ converges almost surely towards
    the Marcenko-Pastur distribution. More importantly, it is established that
    if $L = \mathcal{O}(N\^{\alpha})$ with $\alpha < 2/3$, then, almost surely,
    for $N$ large enough, the eigenvalues of ${\bf W}_N {\bf W}_N\^{\*}$ are
    located in the neighbourhood of the Marcenko-Pastur distribution.
  accessed:
    - year: 2020
      month: 11
      day: 23
  author:
    - family: Loubaton
      given: Philippe
  citation-key: loubaton_almost_2016
  container-title: Journal of Theoretical Probability
  container-title-short: J Theor Probab
  DOI: 10.1007/s10959-015-0614-z
  ISSN: 0894-9840, 1572-9230
  issue: '4'
  issued:
    - year: 2016
      month: 12
  page: 1339-1443
  source: arXiv.org
  title: >-
    On the almost sure location of the singular values of certain Gaussian
    block-Hankel large random matrices
  type: article-journal
  URL: http://arxiv.org/abs/1405.2006
  volume: '29'

- id: loukas_how_2017
  abstract: >-
    How many samples are sufficient to guarantee that the eigenvectors of the
    sample covariance matrix are close to those of the actual covariance matrix?
    For a wide family of distributions, including distributions with finite
    second moment and sub-gaussian distributions supported in a centered
    Euclidean ball, we prove that the inner product between eigenvectors of the
    sample and actual covariance matrices decreases proportionally to the
    respective eigenvalue distance and the number of samples. Our findings imply
    ¡em¿non-asymptotic¡/em¿ concentration bounds for eigenvectors and
    eigenvalues and carry strong consequences for the non-asymptotic analysis of
    PCA and its applications. For instance, they provide conditions for
    separating components estimated from O(1) samples and show that even few
    samples can be sufficient to perform dimensionality reduction, especially
    for low-rank covariances.
  author:
    - family: Loukas
      given: Andreas
  citation-key: loukas_how_2017
  collection-title: Proceedings of machine learning research
  container-title: Proceedings of the 34th international conference on machine learning
  editor:
    - family: Precup
      given: Doina
    - family: Teh
      given: Yee Whye
  issued:
    - year: 2017
      month: 8
      day: 6
    - year: 2017
      month: 8
      day: 11
  page: 2228–2237
  publisher: PMLR
  title: How close are the eigenvectors of the sample and actual covariance matrices?
  type: paper-conference
  URL: https://proceedings.mlr.press/v70/loukas17a.html
  volume: '70'

- id: love_linux_2010
  author:
    - family: Love
      given: R.
  citation-key: love_linux_2010
  collection-title: Developer's Library
  ISBN: 978-0-7686-9679-0
  issued:
    - year: 2010
  publisher: Pearson Education
  title: Linux Kernel Development
  type: book
  URL: https://books.google.com.br/books?id=3MWRMYRwulIC

- id: lu_attractor_2018
  accessed:
    - year: 2021
      month: 2
      day: 19
  author:
    - family: Lu
      given: Zhixin
    - family: Hunt
      given: Brian R.
    - family: Ott
      given: Edward
  citation-key: lu_attractor_2018
  container-title: 'Chaos: An Interdisciplinary Journal of Nonlinear Science'
  container-title-short: Chaos
  DOI: 10.1063/1.5039508
  ISSN: 1054-1500, 1089-7682
  issue: '6'
  issued:
    - year: 2018
      month: 6
  language: en
  page: '061104'
  source: DOI.org (Crossref)
  title: Attractor reconstruction by machine learning
  type: article-journal
  URL: http://aip.scitation.org/doi/10.1063/1.5039508
  volume: '28'

- id: lu_decoding_2024
  abstract: >-
    Electrocardiogram (ECG) is widely considered the primary test for evaluating
    cardiovascular diseases. However, the use of artificial intelligence to
    advance these medical practices and learn new clinical insights from ECGs
    remains largely unexplored. Utilising a dataset of 2,322,513 ECGs collected
    from 1,558,772 patients with 7 years of follow-up, we developed a deep
    learning model with state-of-the-art granularity for the interpretable
    diagnosis of cardiac abnormalities, gender identification, and hyper-
    tension screening solely from ECGs, which are then used to stratify the risk
    of mortality. The model achieved the area under the receiver operating
    characteristic curve (AUC) scores of 0.998 (95% confidence interval (CI),
    0.995-0.999), 0.964 (0.963-0.965), and 0.839 (0.837-0.841) for the three
    diagnostic tasks separately. Using ECG-predicted results, we find high risks
    of mortality for subjects with sinus tachycardia (adjusted hazard ratio (HR)
    of 2.24, 1.96-2.57), and atrial fibrillation (adjusted HR of 2.22,
    1.99-2.48). We further use salient morphologies produced by the deep
    learning model to identify key ECG leads that achieved similar performance
    for the three diagnoses, and we find that the V1 ECG lead is important for
    hypertension screening and mortality risk stratification of hypertensive
    cohorts, with an AUC of 0.816 (0.814-0.818) and a univariate HR of 1.70
    (1.61-1.79) for the two tasks separately. Using ECGs alone, our developed
    model showed cardiologist-level accuracy in interpretable cardiac diagnosis,
    and the advancement in mortality risk stratification; In addition, the
    potential to facilitate clinical knowledge discovery for gender and
    hypertension detection which are not readily available.
  accessed:
    - year: 2024
      month: 2
      day: 26
  author:
    - family: Lu
      given: Lei
    - family: Zhu
      given: Tingting
    - family: Ribeiro
      given: Antonio H
    - family: Clifton
      given: Lei
    - family: Zhao
      given: Erying
    - family: Zhou
      given: Jiandong
    - family: Ribeiro
      given: Antonio Luiz P
    - family: Zhang
      given: Yuan-Ting
    - family: Clifton
      given: David A
  citation-key: lu_decoding_2024
  container-title: European Heart Journal - Digital Health
  container-title-short: European Heart Journal - Digital Health
  DOI: 10.1093/ehjdh/ztae014
  ISSN: 2634-3916
  issued:
    - year: 2024
      month: 2
      day: 19
  license: All rights reserved
  page: ztae014
  source: Silverchair
  title: >-
    Decoding 2.3 Million ECGs: Interpretable Deep Learning for Advancing
    Cardiovascular Diagnosis and Mortality Risk Stratification
  title-short: Decoding 2.3 Million ECGs
  type: article-journal
  URL: https://doi.org/10.1093/ehjdh/ztae014

- id: lu_knowledge_2022
  abstract: >-
    Despite the potentials of artificial intelligence (AI) in healthcare, very
    little work focuses on the extraction of clinical information or knowledge
    discovery from clinical measurements. Here we propose a novel deep learning
    model to extract characteristics in electrocardiogram (ECG) and explore its
    usage in knowledge discovery. Utilising a 12-lead ECG dataset (nECGs =
    2,322,513) collected from unique subjects (nSubjects = 1,558,772) in primary
    care, we performed three independent medical tasks with the proposed model:
    (i) cardiac abnormality diagnosis, (ii) gender identification, and (iii)
    hypertension screening. We achieved an area under the curve (AUC) score of
    0.998 (95% confidence interval (CI), 0.995-0.999), 0.964 (95% CI,
    0.963-0.965), and 0.839 (95% CI, 0.837-0.841) for each task, respectively;
    We provide interpretation of salient morphologies and further identified key
    ECG leads that achieve similar performance for the three tasks: (i) AVR and
    V1 leads (AUC=0.990 (95% CI, 0.982-0.995); (ii) V5 lead (AUC=0.900 (95% CI,
    0.899-0.902)); and (iii) V1 lead (AUC=0.816 (95% CI, 0.814-0.818)). Using
    ECGs, our model not only has demonstrated cardiologist-level accuracy in
    heart diagnosis with interpretability, but also shows its potentials in
    facilitating clinical knowledge discovery for gender and hypertension
    detection which are not readily available.
  accessed:
    - year: 2022
      month: 11
      day: 17
  author:
    - family: Lu
      given: Lei
    - family: Zhu
      given: Tingting
    - family: Ribeiro
      given: Antônio H.
    - family: Clifton
      given: Lei
    - family: Zhao
      given: Erying
    - family: Ribeiro
      given: Antonio Luiz P.
    - family: Zhang
      given: Yuan-Ting
    - family: Clifton
      given: David A.
  citation-key: lu_knowledge_2022
  container-title: 'Under review at Nature Comunications (preprint: medRxiv)'
  DOI: 10.1101/2022.11.01.22281722
  issued:
    - year: 2022
      month: 11
      day: 1
  language: en
  license: >-
    © 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available
    under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0
    International), CC BY-NC-ND 4.0, as described at
    http://creativecommons.org/licenses/by-nc-nd/4.0/
  source: medRxiv
  title: >-
    Knowledge Discovery with Electrocardiography Using Interpretable Deep Neural
    Networks
  type: article-journal
  URL: https://www.medrxiv.org/content/10.1101/2022.11.01.22281722v1

- id: lu_multiscale_2017
  abstract: >-
    A giant leap has been made in the past couple of decades with the
    introduction of kernel-based learning as a mainstay for designing effective
    nonlinear computational learning algorithms. In view of the geometric
    interpretation of conditional expectation and the ubiquity of multiscale
    characteristics in highly complex nonlinear dynamic systems [1]-[3], this
    paper presents a new orthogonal projection operator wavelet kernel, aiming
    at developing an efficient computational learning approach for nonlinear
    dynamical system identification. In the framework of multiresolution
    analysis, the proposed projection operator wavelet kernel can fulfill the
    multiscale, multidimensional learning to estimate complex dependencies. The
    special advantage of the projection operator wavelet kernel developed in
    this paper lies in the fact that it has a closed-form expression, which
    greatly facilitates its application in kernel learning. To the best of our
    knowledge, it is the first closed-form orthogonal projection wavelet kernel
    reported in the literature. It provides a link between grid-based wavelets
    and mesh-free kernel-based methods. Simulation studies for identifying the
    parallel models of two benchmark nonlinear dynamical systems confirm its
    superiority in model accuracy and sparsity.
  author:
    - family: Lu
      given: Z.
    - family: Sun
      given: J.
    - family: Butts
      given: K.
  citation-key: lu_multiscale_2017
  container-title: IEEE Transactions on Neural Networks and Learning Systems
  DOI: 10.1109/TNNLS.2015.2513902
  ISSN: 2162-237X
  issue: '1'
  issued:
    - year: 2017
      month: 1
  page: 231-243
  source: IEEE Xplore
  title: >-
    Multiscale Support Vector Learning With Projection Operator Wavelet Kernel
    for Nonlinear Dynamical System Identification
  type: article-journal
  volume: '28'

- id: lu_reservoir_2017
  accessed:
    - year: 2021
      month: 4
      day: 1
  author:
    - family: Lu
      given: Zhixin
    - family: Pathak
      given: Jaideep
    - family: Hunt
      given: Brian
    - family: Girvan
      given: Michelle
    - family: Brockett
      given: Roger
    - family: Ott
      given: Edward
  citation-key: lu_reservoir_2017
  container-title: 'Chaos: An Interdisciplinary Journal of Nonlinear Science'
  container-title-short: Chaos
  DOI: 10.1063/1.4979665
  ISSN: 1054-1500, 1089-7682
  issue: '4'
  issued:
    - year: 2017
      month: 4
  language: en
  page: '041102'
  source: DOI.org (Crossref)
  title: >-
    Reservoir observers: Model-free inference of unmeasured variables in chaotic
    systems
  title-short: Reservoir observers
  type: article-journal
  URL: http://aip.scitation.org/doi/10.1063/1.4979665
  volume: '27'

- id: luenberger_linear_2008
  author:
    - family: Luenberger
      given: David G.
    - family: Ye
      given: Yinyu
  call-number: T57.7 .L8 2008
  citation-key: luenberger_linear_2008
  collection-title: International series in operations research and management science
  edition: 3rd ed
  event-place: New York, NY
  ISBN: 978-0-387-74502-2
  issued:
    - year: 2008
  number-of-pages: '546'
  publisher: Springer
  publisher-place: New York, NY
  source: Library of Congress ISBN
  title: Linear and nonlinear programming
  type: book

- id: lukosevicius_practical_2012
  abstract: >-
    Reservoir computing has emerged in the last decade as an alternative to
    gradient descent methods for training recurrent neural networks. Echo State
    Network (ESN) is one of the key reservoir computing “ﬂavors”. While being
    practical, conceptually simple, and easy to implement, ESNs require some
    experience and insight to achieve the hailed good performance in many tasks.
    Here we present practical techniques and recommendations for successfully
    applying ESNs, as well as some more advanced application-speciﬁc
    modiﬁcations.
  accessed:
    - year: 2021
      month: 3
      day: 30
  author:
    - family: Lukoševičius
      given: Mantas
  citation-key: lukosevicius_practical_2012
  container-title: 'Neural Networks: Tricks of the Trade'
  DOI: 10.1007/978-3-642-35289-8_36
  editor:
    - family: Montavon
      given: Grégoire
    - family: Orr
      given: Geneviève B.
    - family: Müller
      given: Klaus-Robert
  event-place: Berlin, Heidelberg
  ISBN: 978-3-642-35288-1 978-3-642-35289-8
  issued:
    - year: 2012
  language: en
  page: 659-686
  publisher: Springer Berlin Heidelberg
  publisher-place: Berlin, Heidelberg
  source: DOI.org (Crossref)
  title: A Practical Guide to Applying Echo State Networks
  type: chapter
  URL: http://link.springer.com/10.1007/978-3-642-35289-8_36
  volume: '7700'

- id: luo_companion_2014
  abstract: >-
    In this paper we describe some properties of companion matrices and
    demonstrate some special patterns that arise when a Toeplitz or a Hankel
    matrix is multiplied by a related companion matrix. We present a new
    condition, generalizing known results, for a Toeplitz or a Hankel matrix to
    be the transforming matrix for a similarity between a pair of companion
    matrices. A special case of our main result shows that a Toeplitz or a
    Hankel matrix can be extended using associated companion matrices,
    preserving the Toeplitz or Hankel structure respectively.
  accessed:
    - year: 2020
      month: 12
      day: 29
  author:
    - family: Luo
      given: Yousong
    - family: Hill
      given: Robin
  citation-key: luo_companion_2014
  container-title: arXiv:1411.4592 [math]
  issued:
    - year: 2014
      month: 11
      day: 12
  source: arXiv.org
  title: Companion Matrices and Their Relations to Toeplitz and Hankel Matrices
  type: article-journal
  URL: http://arxiv.org/abs/1411.4592

- id: luo_review_2010
  abstract: >-
    Analog filtering and digital signal processing algorithms in the
    preprocessing modules of an electrocardiographic device play a pivotal role
    in providing high-quality electrocardiogram (ECG) signals for analysis,
    interpretation, and presentation (display, printout, and storage). In this
    article, issues relating to inaccuracy of ECG preprocessing filters are
    investigated in the context of facilitating efficient ECG interpretation and
    diagnosis. The discussion covers 4 specific ECG preprocessing applications:
    anti-aliasing and upper-frequency cutoff, baseline wander suppression and
    lower-frequency cutoff, line frequency rejection, and muscle artifact
    reduction. Issues discussed include linear phase, aliasing, distortion,
    ringing, and attenuation of desired ECG signals. Due to the overlapping
    power spectrum of signal and noise in acquired ECG data, frequency selective
    filters must seek a delicate balance between noise removal and deformation
    of the desired signal. Most importantly, the filtering output should not
    adversely impact subsequent diagnosis and interpretation. Based on these
    discussions, several suggestions are made to improve and update existing ECG
    data preprocessing standards and guidelines.
  author:
    - family: Luo
      given: Shen
    - family: Johnston
      given: Paul
  citation-key: luo_review_2010
  container-title: Journal of Electrocardiology
  container-title-short: Journal of Electrocardiology
  DOI: 10/fp6hfc
  ISSN: 0022-0736
  issue: '6'
  issued:
    - year: 2010
      month: 11
      day: 1
  page: 486-496
  title: A review of electrocardiogram filtering
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0022073610002852
  volume: '43'

- id: luo_understanding_2022
  abstract: >-
    Diffusion models have shown incredible capabilities as generative models;
    indeed, they power the current state-of-the-art models on text-conditioned
    image generation such as Imagen and DALL-E 2. In this work we review,
    demystify, and unify the understanding of diffusion models across both
    variational and score-based perspectives. We first derive Variational
    Diffusion Models (VDM) as a special case of a Markovian Hierarchical
    Variational Autoencoder, where three key assumptions enable tractable
    computation and scalable optimization of the ELBO. We then prove that
    optimizing a VDM boils down to learning a neural network to predict one of
    three potential objectives: the original source input from any arbitrary
    noisification of it, the original source noise from any arbitrarily
    noisified input, or the score function of a noisified input at any arbitrary
    noise level. We then dive deeper into what it means to learn the score
    function, and connect the variational perspective of a diffusion model
    explicitly with the Score-based Generative Modeling perspective through
    Tweedie's Formula. Lastly, we cover how to learn a conditional distribution
    using diffusion models via guidance.
  accessed:
    - year: 2023
      month: 7
      day: 27
  author:
    - family: Luo
      given: Calvin
  citation-key: luo_understanding_2022
  DOI: 10.48550/arXiv.2208.11970
  issued:
    - year: 2022
      month: 8
      day: 25
  number: arXiv:2208.11970
  publisher: arXiv
  source: arXiv.org
  title: 'Understanding Diffusion Models: A Unified Perspective'
  title-short: Understanding Diffusion Models
  type: article
  URL: http://arxiv.org/abs/2208.11970

- id: luong_effective_2015
  accessed:
    - year: 2019
      month: 5
      day: 29
  author:
    - family: Luong
      given: Thang
    - family: Pham
      given: Hieu
    - family: Manning
      given: Christopher D.
  citation-key: luong_effective_2015
  container-title: >-
    Proceedings of the 2015 Conference on Empirical Methods in Natural Language
    Processing
  DOI: 10/gdpd6w
  event-place: Lisbon, Portugal
  issued:
    - year: 2015
      month: 9
  page: 1412–1421
  publisher: Association for Computational Linguistics
  publisher-place: Lisbon, Portugal
  source: ACLWeb
  title: Effective Approaches to Attention-based Neural Machine Translation
  type: paper-conference
  URL: https://www.aclweb.org/anthology/D15-1166

- id: lyon_computational_2018
  abstract: >-
    Widely developed for clinical screening, electrocardiogram (ECG) recordings
    capture the cardiac electrical activity from the body surface. ECG analysis
    can therefore be a crucial first step to help diagnose, understand and
    predict cardiovascular disorders responsible for 30% of deaths worldwide.
    Computational techniques, and more specifically machine learning techniques
    and computational modelling are powerful tools for classification,
    clustering and simulation, and they have recently been applied to address
    the analysis of medical data, especially ECG data. This review describes the
    computational methods in use for ECG analysis, with a focus on machine
    learning and 3D computer simulations, as well as their accuracy, clinical
    implications and contributions to medical advances. The first section
    focuses on heartbeat classification and the techniques developed to extract
    and classify abnormal from regular beats. The second section focuses on
    patient diagnosis from whole recordings, applied to different diseases. The
    third section presents real-time diagnosis and applications to wearable
    devices. The fourth section highlights the recent field of personalized ECG
    computer simulations and their interpretation. Finally, the discussion
    section outlines the challenges of ECG analysis and provides a critical
    assessment of the methods presented. The computational methods reported in
    this review are a strong asset for medical discoveries and their translation
    to the clinical world may lead to promising advances.
  accessed:
    - year: 2018
      month: 10
      day: 22
  author:
    - family: Lyon
      given: Aurore
    - family: Mincholé
      given: Ana
    - family: Martínez
      given: Juan Pablo
    - family: Laguna
      given: Pablo
    - family: Rodriguez
      given: Blanca
  citation-key: lyon_computational_2018
  container-title: Journal of the Royal Society Interface
  container-title-short: J R Soc Interface
  DOI: 10.1098/rsif.2017.0821
  ISSN: 1742-5689
  issue: '138'
  issued:
    - year: 2018
      month: 1
  PMCID: PMC5805987
  PMID: '29321268'
  source: PubMed Central
  title: >-
    Computational techniques for ECG analysis and interpretation in light of
    their contribution to medical advances
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5805987/
  volume: '15'

- id: ma_complete_2015
  abstract: >-
    Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous
    dynamics to define a transition kernel that efficiently explores a target
    distribution. In tandem, a focus has been on devising scalable variants that
    subsample the data and use stochastic gradients in place of full-data
    gradients in the dynamic simulations. However, such stochastic gradient MCMC
    samplers have lagged behind their full-data counterparts in terms of the
    complexity of dynamics considered since proving convergence in the presence
    of the stochastic gradient noise is non-trivial. Even with simple dynamics,
    significant physical intuition is often required to modify the dynamical
    system to account for the stochastic gradient noise. In this paper, we
    provide a general recipe for constructing MCMC samplers--including
    stochastic gradient versions--based on continuous Markov processes specified
    via two matrices. We constructively prove that the framework is complete.
    That is, any continuous Markov process that provides samples from the target
    distribution can be written in our framework. We show how previous
    continuous-dynamic samplers can be trivially "reinvented" in our framework,
    avoiding the complicated sampler-specific proofs. We likewise use our recipe
    to straightforwardly propose a new state-adaptive sampler: stochastic
    gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments on
    simulated data and a streaming Wikipedia analysis demonstrate that the
    proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the
    scalability of stochastic gradient methods.
  accessed:
    - year: 2019
      month: 1
      day: 30
  author:
    - family: Ma
      given: Yi-An
    - family: Chen
      given: Tianqi
    - family: Fox
      given: Emily B.
  citation-key: ma_complete_2015
  container-title: arXiv:1506.04696 [math, stat]
  issued:
    - year: 2015
      month: 6
      day: 15
  source: arXiv.org
  title: A Complete Recipe for Stochastic Gradient MCMC
  type: article-journal
  URL: http://arxiv.org/abs/1506.04696

- id: maas_rectifier_2013
  author:
    - family: Maas
      given: Andrew L.
    - family: Hannun
      given: Awni Y.
    - family: Ng
      given: Andrew Y.
  citation-key: maas_rectifier_2013
  container-title: in ICML Workshop on Deep Learning for Audio, Speech and Language Processing
  issued:
    - year: 2013
  title: Rectifier nonlinearities improve neural network acoustic models
  type: paper-conference

- id: maass_realtime_2002
  accessed:
    - year: 2019
      month: 10
      day: 28
  author:
    - family: Maass
      given: Wolfgang
    - family: Natschläger
      given: Thomas
    - family: Markram
      given: Henry
  citation-key: maass_realtime_2002
  container-title: Neural Computation
  container-title-short: Neural Computation
  DOI: 10.1162/089976602760407955
  ISSN: 0899-7667, 1530-888X
  issue: '11'
  issued:
    - year: 2002
      month: 11
  language: en
  page: 2531-2560
  source: DOI.org (Crossref)
  title: >-
    Real-Time Computing Without Stable States: A New Framework for Neural
    Computation Based on Perturbations
  title-short: Real-Time Computing Without Stable States
  type: article-journal
  URL: http://www.mitpressjournals.org/doi/10.1162/089976602760407955
  volume: '14'

- id: macedo_eletromagnetismo_1988
  author:
    - family: Macedo
      given: Annita
  citation-key: macedo_eletromagnetismo_1988
  ISBN: 978-85-277-0100-6
  issued:
    - year: 1988
  publisher: GUANABARA
  title: Eletromagnetismo
  type: book
  URL: https://books.google.com.br/books?id=XpvCPgAACAAJ

- id: macfarlane_automated_1996
  author:
    - family: Macfarlane
      given: Peter W
    - family: Latif
      given: Shahid
  citation-key: macfarlane_automated_1996
  container-title: Journal of Electrocardiology
  container-title-short: Journal of Electrocardiology
  DOI: 10/cdh7qw
  ISSN: 0022-0736
  issued:
    - year: 1996
  page: 29-34
  title: Automated serial ECG comparison based on the Minnesota code
  type: article-journal
  volume: '29'

- id: macfarlane_methodology_1990
  author:
    - family: Macfarlane
      given: PW
    - family: Devine
      given: B
    - family: Latif
      given: S
    - family: McLaughlin
      given: S
    - family: Shoat
      given: DB
    - family: Watts
      given: MP
  citation-key: macfarlane_methodology_1990
  container-title: Methods of information in medicine
  container-title-short: Methods of information in medicine
  DOI: 10/gftz83
  ISSN: 0026-1270
  issue: '04'
  issued:
    - year: 1990
  page: 354-361
  title: Methodology of ECG interpretation in the Glasgow program
  type: article-journal
  volume: '29'

- id: macfarlane_university_2005
  author:
    - family: Macfarlane
      given: P. W.
    - family: Devine
      given: B.
    - family: Clark
      given: E.
  citation-key: macfarlane_university_2005
  container-title: Computers in Cardiology
  DOI: 10.1109/CIC.2005.1588134
  event-title: Computers in Cardiology, 2005
  ISBN: 0276-6574
  issued:
    - year: 2005
  page: 451-454
  title: The university of glasgow (Uni-G) ECG analysis program
  type: paper-conference

- id: mackay_information_2003
  author:
    - family: MacKay
      given: David JC
    - family: Mac Kay
      given: David JC
  citation-key: mackay_information_2003
  issued:
    - year: 2003
  publisher: Cambridge university press
  source: Google Scholar
  title: Information theory, inference and learning algorithms
  type: book

- id: maddox_simple_2019
  abstract: >-
    We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose

    approach for uncertainty representation and calibration in deep learning.

    Stochastic Weight Averaging (SWA), which computes the first moment of

    stochastic gradient descent (SGD) iterates with a modified learning rate

    schedule, has recently been shown to improve generalization in deep
    learning.

    With SWAG, we fit a Gaussian using the SWA solution as the first moment and
    a

    low rank plus diagonal covariance also derived from the SGD iterates,
    forming

    an approximate posterior distribution over neural network weights; we then

    sample from this Gaussian distribution to perform Bayesian model averaging.
    We

    empirically find that SWAG approximates the shape of the true posterior, in

    accordance with results describing the stationary distribution of SGD
    iterates.

    Moreover, we demonstrate that SWAG performs well on a wide variety of
    computer

    vision tasks, including out of sample detection, calibration, and transfer

    learning, in comparison to many popular alternatives including MC dropout,
    KFAC

    Laplace, and temperature scaling.
  accessed:
    - year: 2019
      month: 2
      day: 14
  author:
    - family: Maddox
      given: Wesley
    - family: Garipov
      given: Timur
    - family: Izmailov
      given: Pavel
    - family: Vetrov
      given: Dmitry
    - family: Wilson
      given: Andrew Gordon
  citation-key: maddox_simple_2019
  issued:
    - year: 2019
      month: 2
      day: 7
  language: en
  source: arxiv.org
  title: A Simple Baseline for Bayesian Uncertainty in Deep Learning
  type: article-journal
  URL: https://arxiv.org/abs/1902.02476v1

- id: madry_deep_2018
  abstract: >-
    Recent work has demonstrated that deep neural networks are vulnerable to
    adversarial examples---inputs that are almost indistinguishable from natural
    data and yet classified incorrectly by the network. In fact, some of the
    latest findings suggest that the existence of adversarial attacks may be an
    inherent weakness of deep learning models. To address this problem, we study
    the adversarial robustness of neural networks through the lens of robust
    optimization. This approach provides us with a broad and unifying view on
    much of the prior work on this topic. Its principled nature also enables us
    to identify methods for both training and attacking neural networks that are
    reliable and, in a certain sense, universal. In particular, they specify a
    concrete security guarantee that would protect against any adversary. These
    methods let us train networks with significantly improved resistance to a
    wide range of adversarial attacks. They also suggest the notion of security
    against a first-order adversary as a natural and broad security guarantee.
    We believe that robustness against such well-defined classes of adversaries
    is an important stepping stone towards fully resistant deep learning models.
    Code and pre-trained models are available at
    https://github.com/MadryLab/mnist_challenge and
    https://github.com/MadryLab/cifar10_challenge.
  author:
    - family: Madry
      given: Aleksander
    - family: Makelov
      given: Aleksandar
    - family: Schmidt
      given: Ludwig
    - family: Tsipras
      given: Dimitris
    - family: Vladu
      given: Adrian
  citation-key: madry_deep_2018
  container-title: International Conference for Learning Representations (ICLR)
  issued:
    - year: 2018
  title: Towards Deep Learning Models Resistant to Adversarial Attacks
  type: article-journal

- id: maduranga_complex_2019
  author:
    - family: Maduranga
      given: Kehelwala DG
    - family: Helfrich
      given: Kyle E
    - family: Ye
      given: Qiang
  citation-key: maduranga_complex_2019
  event-title: Proceedings of the AAAI Conference on Artificial Intelligence
  ISBN: 2374-3468
  issued:
    - year: 2019
  page: 4528-4535
  title: Complex unitary recurrent neural networks using scaled cayley transform
  type: paper-conference
  volume: '33'

- id: maheswaranathan_reverse_2019
  author:
    - family: Maheswaranathan
      given: Niru
    - family: Williams
      given: Alex
    - family: Golub
      given: Matthew
    - family: Ganguli
      given: Surya
    - family: Sussillo
      given: David
  citation-key: maheswaranathan_reverse_2019
  container-title: Advances in neural information processing systems 32
  editor:
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Beygelzimer
      given: A.
    - family: Buc
      given: F.
      non-dropping-particle: dAlché-
    - family: Fox
      given: E.
    - family: Garnett
      given: R.
  issued:
    - year: 2019
  page: 15696-15705
  publisher: Curran Associates, Inc.
  title: >-
    Reverse engineering recurrent networks for sentiment classification reveals
    line attractor dynamics
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/9700-reverse-engineering-recurrent-networks-for-sentiment-classification-reveals-line-attractor-dynamics.pdf

- id: maheswaranathan_reverse_2019a
  author:
    - family: Maheswaranathan
      given: Niru
    - family: Williams
      given: Alex
    - family: Golub
      given: Matthew
    - family: Ganguli
      given: Surya
    - family: Sussillo
      given: David
  citation-key: maheswaranathan_reverse_2019a
  container-title: Advances in neural information processing systems 32
  editor:
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Beygelzimer
      given: A.
    - family: Buc
      given: F.
      non-dropping-particle: dAlché-
    - family: Fox
      given: E.
    - family: Garnett
      given: R.
  issued:
    - year: 2019
  page: 15696-15705
  publisher: Curran Associates, Inc.
  title: >-
    Reverse engineering recurrent networks for sentiment classification reveals
    line attractor dynamics
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/9700-reverse-engineering-recurrent-networks-for-sentiment-classification-reveals-line-attractor-dynamics.pdf

- id: maheswaranathan_reverse_2019b
  abstract: >-
    Recurrent neural networks (RNNs) are a widely used tool for modeling
    sequential data, yet they are often treated as inscrutable black boxes.
    Given a trained recurrent network, we would like to reverse engineer it--to
    obtain a quantitative, interpretable description of how it solves a
    particular task. Even for simple tasks, a detailed understanding of how
    recurrent networks work, or a prescription for how to develop such an
    understanding, remains elusive. In this work, we use tools from dynamical
    systems analysis to reverse engineer recurrent networks trained to perform
    sentiment classification, a foundational natural language processing task.
    Given a trained network, we find fixed points of the recurrent dynamics and
    linearize the nonlinear system around these fixed points. Despite their
    theoretical capacity to implement complex, high-dimensional computations, we
    find that trained networks converge to highly interpretable, low-dimensional
    representations. In particular, the topological structure of the fixed
    points and corresponding linearized dynamics reveal an approximate line
    attractor within the RNN, which we can use to quantitatively understand how
    the RNN solves the sentiment analysis task. Finally, we find this mechanism
    present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs)
    trained on multiple datasets, suggesting that our findings are not unique to
    a particular architecture or dataset. Overall, these results demonstrate
    that surprisingly universal and human interpretable computations can arise
    across a range of recurrent networks.
  accessed:
    - year: 2019
      month: 6
      day: 28
  author:
    - family: Maheswaranathan
      given: Niru
    - family: Williams
      given: Alex
    - family: Golub
      given: Matthew D.
    - family: Ganguli
      given: Surya
    - family: Sussillo
      given: David
  citation-key: maheswaranathan_reverse_2019b
  container-title: arXiv:1906.10720 [cs, stat]
  issued:
    - year: 2019
      month: 6
      day: 25
  source: arXiv.org
  title: >-
    Reverse engineering recurrent networks for sentiment classification reveals
    line attractor dynamics
  type: article-journal
  URL: http://arxiv.org/abs/1906.10720

- id: mahloujifar_curse_2019
  abstract: >-
    Many modern machine learning classifiers are shown to be vulnerable to
    adversarial perturbations of the instances. Despite a massive amount of work
    focusing on making classifiers robust, the task seems quite challenging. In
    this work, through a theoretical study, we investigate the adversarial risk
    and robustness of classifiers and draw a connection to the well-known
    phenomenon of concentration of measure in metric measure spaces. We show
    that if the metric probability space of the test instance is concentrated,
    any classifier with some initial constant error is inherently vulnerable to
    adversarial perturbations. One class of concentrated metric probability
    spaces are the so-called Levy families that include many natural
    distributions. In this special case, our attacks only need to perturb the
    test instance by at most $O(\sqrt n)$ to make it misclassified, where $n$ is
    the data dimension. Using our general result about Levy instance spaces, we
    first recover as special case some of the previously proved results about
    the existence of adversarial examples. However, many more Levy families are
    known (e.g., product distribution under the Hamming distance) for which we
    immediately obtain new attacks that find adversarial examples of distance
    $O(\sqrt n)$. Finally, we show that concentration of measure for product
    spaces implies the existence of forms of "poisoning" attacks in which the
    adversary tampers with the training data with the goal of degrading the
    classifier. In particular, we show that for any learning algorithm that uses
    $m$ training examples, there is an adversary who can increase the
    probability of any "bad property" (e.g., failing on a particular test
    instance) that initially happens with non-negligible probability to $\approx
    1$ by substituting only $\tilde{O}(\sqrt m)$ of the examples with other
    (still correctly labeled) examples.
  accessed:
    - year: 2021
      month: 8
      day: 10
  author:
    - family: Mahloujifar
      given: Saeed
    - family: Diochnos
      given: Dimitrios I.
    - family: Mahmoody
      given: Mohammad
  citation-key: mahloujifar_curse_2019
  container-title: AAAI
  issued:
    - year: 2019
  source: arXiv.org
  title: >-
    The Curse of Concentration in Robust Learning: Evasion and Poisoning Attacks
    from Concentration of Measure
  title-short: The Curse of Concentration in Robust Learning
  type: article-journal
  URL: http://arxiv.org/abs/1809.03063

- id: mahloujifar_empirically_2019
  abstract: >-
    Many recent works have shown that adversarial examples that fool classiﬁers
    can be found by minimally perturbing a normal input. Recent theoretical
    results, starting with Gilmer et al. (2018b), show that if the inputs are
    drawn from a concentrated metric probability space, then adversarial
    examples with small perturbation are inevitable. A concentrated space has
    the property that any subset with Ω(1) (e.g., 1/100) measure, according to
    the imposed distribution, has small distance to almost all (e.g., 99/100) of
    the points in the space. It is not clear, however, whether these theoretical
    results apply to actual distributions such as images. This paper presents a
    method for empirically measuring and bounding the concentration of a
    concrete dataset which is proven to converge to the actual concentration. We
    use it to empirically estimate the intrinsic robustness to ∞ and 2
    perturbations of several image classiﬁcation benchmarks. Code for our
    experiments is available at
    https://github.com/xiaozhanguva/Measure-Concentration.
  author:
    - family: Mahloujifar
      given: Saeed
    - family: Zhang
      given: Xiao
    - family: Mahmoody
      given: Mohammad
    - family: Evans
      given: David
  citation-key: mahloujifar_empirically_2019
  container-title: Advances in Neural Information Processing Systems 32
  event-title: NeurIPS
  issued:
    - year: 2019
  title: >-
    Empirically Measuring Concentration: Fundamental Limits on Intrinsic
    Robustness
  type: paper-conference

- id: mak_polygenic_2017
  abstract: >-
    ABSTRACT Polygenic scores (PGS) summarize the genetic contribution of a
    person's genotype to a disease or phenotype. They can be used to group
    participants into different risk categories for diseases, and are also used
    as covariates in epidemiological analyses. A number of possible ways of
    calculating PGS have been proposed, and recently there is much interest in
    methods that incorporate information available in published summary
    statistics. As there is no inherent information on linkage disequilibrium
    (LD) in summary statistics, a pertinent question is how we can use LD
    information available elsewhere to supplement such analyses. To answer this
    question, we propose a method for constructing PGS using summary statistics
    and a reference panel in a penalized regression framework, which we call
    lassosum. We also propose a general method for choosing the value of the
    tuning parameter in the absence of validation data. In our simulations, we
    showed that pseudovalidation often resulted in prediction accuracy that is
    comparable to using a dataset with validation phenotype and was clearly
    superior to the conservative option of setting the tuning parameter of
    lassosum to its lowest value. We also showed that lassosum achieved better
    prediction accuracy than simple clumping and P-value thresholding in almost
    all scenarios. It was also substantially faster and more accurate than the
    recently proposed LDpred.
  accessed:
    - year: 2024
      month: 1
      day: 24
  author:
    - family: Mak
      given: Timothy Shin Heng
    - family: Porsch
      given: Robert Milan
    - family: Choi
      given: Shing Wan
    - family: Zhou
      given: Xueya
    - family: Sham
      given: Pak Chung
  citation-key: mak_polygenic_2017
  container-title: Genetic Epidemiology
  container-title-short: Genetic Epidemiology
  DOI: 10.1002/gepi.22050
  ISSN: 0741-0395
  issue: '6'
  issued:
    - year: 2017
      month: 9
      day: 1
  page: 469-480
  publisher: John Wiley & Sons, Ltd
  title: Polygenic scores via penalized regression on summary statistics
  type: article-journal
  URL: https://doi.org/10.1002/gepi.22050
  volume: '41'

- id: mallon_projective_2005
  author:
    - family: Mallon
      given: John
    - family: Whelan
      given: Paul F
  citation-key: mallon_projective_2005
  container-title: Image and Vision Computing
  DOI: 10.1016/j.imavis.2005.03.002
  issue: '7'
  issued:
    - year: 2005
  page: 643–650
  title: Projective rectification from the fundamental matrix
  type: article-journal
  volume: '23'

- id: man_vectorcardiographic_2015
  accessed:
    - year: 2018
      month: 7
      day: 17
  author:
    - family: Man
      given: Sumche
    - family: Maan
      given: Arie C.
    - family: Schalij
      given: Martin J.
    - family: Swenne
      given: Cees A.
  citation-key: man_vectorcardiographic_2015
  container-title: Journal of Electrocardiology
  DOI: 10.1016/j.jelectrocard.2015.05.002
  ISSN: '00220736'
  issue: '4'
  issued:
    - year: 2015
      month: 7
  language: en
  page: 463-475
  source: Crossref
  title: >-
    Vectorcardiographic diagnostic & prognostic information derived from the
    12‐lead electrocardiogram: Historical review and clinical perspective
  title-short: >-
    Vectorcardiographic diagnostic & prognostic information derived from the
    12‐lead electrocardiogram
  type: article-journal
  URL: http://linkinghub.elsevier.com/retrieve/pii/S0022073615001284
  volume: '48'

- id: manchester_input_2010
  abstract: >-
    We consider the problem of designing an excitation input for a system
    idenfication experiment. The optimization problem considered is to maximize
    a reduced Fisher information matrix in any of the classical D-, E-, or
    A-optimal senses. In contrast to the majority of published work on this
    topic, we consider the problem in the time domain and subject to constraints
    on the amplitude of the input signal. This optimization problem is
    nonconvex. The main result of the paper is a convex relaxation that gives an
    upper bound accurate to within 2/π of the true maximum. A randomized
    algorithm is presented for finding a feasible solution which, in a certain
    sense is expected to be at least 2/π as informative as the globally optimal
    input signal. In the case of a single constraint on input power, the
    proposed approach recovers the true global optimum exactly. Extensions to
    situations with both power and amplitude constraints on both inputs and
    outputs are given. A simple simulation example illustrates the technique.
  author:
    - family: Manchester
      given: I. R.
  citation-key: manchester_input_2010
  container-title: 49th IEEE Conference on Decision and Control (CDC)
  DOI: 10.1109/CDC.2010.5717097
  event-title: 49th IEEE Conference on Decision and Control (CDC)
  issued:
    - year: 2010
      month: 12
  page: 2041-2046
  source: IEEE Xplore
  title: Input design for system identification via convex relaxation
  type: paper-conference

- id: mancini_op_2003
  author:
    - family: Mancini
      given: Ron
  citation-key: mancini_op_2003
  issued:
    - year: 2003
  publisher: Newnes
  source: Google Scholar
  title: 'Op amps for everyone: design reference'
  title-short: Op amps for everyone
  type: book

- id: mandic_recurrent_2001
  author:
    - family: Mandic
      given: Danilo P.
    - family: Chambers
      given: Jonathon A.
  call-number: Q325.5 .M36 2001
  citation-key: mandic_recurrent_2001
  collection-title: >-
    Wiley series in adaptive and learning systems for signal processing,
    communications, and control
  event-place: Chichester ; New York
  ISBN: 978-0-471-49517-8
  issued:
    - year: 2001
  number-of-pages: '285'
  publisher: John Wiley
  publisher-place: Chichester ; New York
  source: Library of Congress ISBN
  title: >-
    Recurrent neural networks for prediction: learning algorithms,
    architectures, and stability
  title-short: Recurrent neural networks for prediction
  type: book

- id: manski_patientcentered_2022
  abstract: >-
    Until recently, there has been a consensus that clinicians should condition
    patient risk assessments on all observed patient covariates with predictive
    power. The broad idea is that knowing more about patients enables more
    accurate predictions of their health risks and, hence, better clinical
    decisions. This consensus has recently unraveled with respect to a specific
    covariate, namely race. There have been increasing calls for race-free risk
    assessment, arguing that using race to predict patient outcomes contributes
    to racial disparities and inequities in health care. Writers calling for
    race-free risk assessment have not studied how it would affect the quality
    of clinical decisions. Considering the matter from the patient-centered
    perspective of medical economics yields a disturbing conclusion: Race-free
    risk assessment would harm patients of all races.
  accessed:
    - year: 2023
      month: 9
      day: 4
  author:
    - family: Manski
      given: Charles F.
  citation-key: manski_patientcentered_2022
  DOI: 10.48550/arXiv.2112.01639
  issued:
    - year: 2022
      month: 2
      day: 26
  number: arXiv:2112.01639
  publisher: arXiv
  source: arXiv.org
  title: Patient-Centered Appraisal of Race-Free Clinical Risk Assessment
  type: article
  URL: http://arxiv.org/abs/2112.01639

- id: mant_accuracy_2007
  abstract: >-
    OBJECTIVE: To assess the accuracy of general practitioners, practice nurses,
    and interpretative software in the use of different types of
    electrocardiogram to diagnose atrial fibrillation.

    DESIGN: Prospective comparison with reference standard of assessment of
    electrocardiograms by two independent specialists.

    SETTING: 49 general practices in central England.

    PARTICIPANTS: 2595 patients aged 65 or over screened for atrial fibrillation
    as part of the screening for atrial fibrillation in the elderly (SAFE)
    study; 49 general practitioners and 49 practice nurses.

    INTERVENTIONS: All electrocardiograms were read with the Biolog
    interpretative software, and a random sample of 12 lead, limb lead, and
    single lead thoracic placement electrocardiograms were assessed by general
    practitioners and practice nurses independently of each other and of the
    Biolog assessment.

    MAIN OUTCOME MEASURES: Sensitivity, specificity, and positive and negative
    predictive values.

    RESULTS: General practitioners detected 79 out of 99 cases of atrial
    fibrillation on a 12 lead electrocardiogram (sensitivity 80%, 95% confidence
    interval 71% to 87%) and misinterpreted 114 out of 1355 cases of sinus
    rhythm as atrial fibrillation (specificity 92%, 90% to 93%). Practice nurses
    detected a similar proportion of cases of atrial fibrillation (sensitivity
    77%, 67% to 85%), but had a lower specificity (85%, 83% to 87%). The
    interpretative software was significantly more accurate, with a specificity
    of 99%, but missed 36 of 215 cases of atrial fibrillation (sensitivity 83%).
    Combining general practitioners' interpretation with the interpretative
    software led to a sensitivity of 92% and a specificity of 91%. Use of limb
    lead or single lead thoracic placement electrocardiograms resulted in some
    loss of specificity.

    CONCLUSIONS: Many primary care professionals cannot accurately detect atrial
    fibrillation on an electrocardiogram, and interpretative software is not
    sufficiently accurate to circumvent this problem, even when combined with
    interpretation by a general practitioner. Diagnosis of atrial fibrillation
    in the community needs to factor in the reading of electrocardiograms by
    appropriately trained people.
  author:
    - family: Mant
      given: Jonathan
    - family: Fitzmaurice
      given: David A.
    - family: Hobbs
      given: F. D. Richard
    - family: Jowett
      given: Sue
    - family: Murray
      given: Ellen T.
    - family: Holder
      given: Roger
    - family: Davies
      given: Michael
    - family: Lip
      given: Gregory Y. H.
  citation-key: mant_accuracy_2007
  container-title: BMJ (Clinical research ed.)
  container-title-short: BMJ
  DOI: 10.1136/bmj.39227.551713.AE
  ISSN: 1756-1833
  issue: '7616'
  issued:
    - year: 2007
      month: 8
      day: 25
  language: eng
  page: '380'
  PMCID: PMC1952490
  PMID: '17604299'
  source: PubMed
  title: >-
    Accuracy of diagnosing atrial fibrillation on electrocardiogram by primary
    care practitioners and interpretative diagnostic software: analysis of data
    from screening for atrial fibrillation in the elderly (SAFE) trial
  title-short: >-
    Accuracy of diagnosing atrial fibrillation on electrocardiogram by primary
    care practitioners and interpretative diagnostic software
  type: article-journal
  volume: '335'

- id: marcotte_abide_2023
  abstract: >-
    Understanding the geometric properties of gradient descent dynamics is a key
    ingredient in deciphering the recent success of very large machine learning
    models. A striking observation is that trained over-parameterized models
    retain some properties of the optimization initialization. This "implicit
    bias" is believed to be responsible for some favorable properties of the
    trained models and could explain their good generalization properties. The
    purpose of this article is threefold. First, we rigorously expose the
    definition and basic properties of "conservation laws", that define
    quantities conserved during gradient flows of a given model (e.g. of a ReLU
    network with a given architecture) with any training data and any loss. Then
    we explain how to find the maximal number of independent conservation laws
    by performing finite-dimensional algebraic manipulations on the Lie algebra
    generated by the Jacobian of the model. Finally, we provide algorithms to:
    a) compute a family of polynomial laws; b) compute the maximal number of
    (not necessarily polynomial) independent conservation laws. We provide
    showcase examples that we fully work out theoretically. Besides, applying
    the two algorithms confirms for a number of ReLU network architectures that
    all known laws are recovered by the algorithm, and that there are no other
    independent laws. Such computational tools pave the way to understanding
    desirable properties of optimization initialization in large machine
    learning models.
  accessed:
    - year: 2023
      month: 12
      day: 12
  author:
    - family: Marcotte
      given: Sibylle
    - family: Gribonval
      given: Rémi
    - family: Peyré
      given: Gabriel
  citation-key: marcotte_abide_2023
  event-title: Thirty-seventh Conference on Neural Information Processing Systems
  issued:
    - year: 2023
      month: 11
      day: 2
  language: en
  source: openreview.net
  title: 'Abide by the law and follow the flow: conservation laws for gradient flows'
  title-short: Abide by the law and follow the flow
  type: paper-conference
  URL: https://openreview.net/forum?id=kMueEV8Eyy

- id: marcus_building_1993
  author:
    - family: Marcus
      given: Mitchell P.
    - family: Santorini
      given: Beatrice
    - family: Marcinkiewicz
      given: Mary Ann
  citation-key: marcus_building_1993
  container-title: Computational Linguistics
  issue: '2'
  issued:
    - year: 1993
  page: 313–330
  title: 'Building a large annotated corpus of English: The Penn Treebank'
  type: article-journal
  URL: https://www.aclweb.org/anthology/J93-2004
  volume: '19'

- id: marin-neto_pathogenesis_2007
  abstract: >-
    Background— Chagas disease remains a significant public health issue and a
    major cause of morbidity and mortality in Latin America. Despite nearly 1
    century of research, the pathogenesis of chronic Chagas cardiomyopathy is
    incompletely understood, the most intriguing challenge of which is the
    complex host-parasite interaction.


    Methods and Results— A systematic review of the literature found in MEDLINE,
    EMBASE, BIREME, LILACS, and SCIELO was performed to search for relevant
    references on pathogenesis and pathophysiology of Chagas disease. Evidence
    from studies in animal models and in anima nobile points to 4 main
    pathogenetic mechanisms to explain the development of chronic Chagas heart
    disease: autonomic nervous system derangements, microvascular disturbances,
    parasite-dependent myocardial aggression, and immune-mediated myocardial
    injury. Despite its prominent peculiarities, the role of autonomic
    derangements and microcirculatory disturbances is probably ancillary among
    causes of chronic myocardial damage. The pathogenesis of chronic Chagas
    heart disease is dependent on a low-grade but incessant systemic infection
    with documented immune-adverse reaction. Parasite persistence and
    immunological mechanisms are inextricably related in the myocardial
    aggression in the chronic phase of Chagas heart disease.


    Conclusions— Most clinical studies have been performed in very small number
    of patients. Future research should explore the clinical potential
    implications and therapeutic opportunities of these 2 fundamental underlying
    pathogenetic mechanisms.
  accessed:
    - year: 2021
      month: 11
      day: 25
  author:
    - family: Marin-Neto
      given: Jose Antonio
    - family: Cunha-Neto
      given: Edécio
    - family: Maciel
      given: Benedito C.
    - family: Simões
      given: Marcus V.
  citation-key: marin-neto_pathogenesis_2007
  container-title: Circulation
  DOI: 10.1161/CIRCULATIONAHA.106.624296
  issue: '9'
  issued:
    - year: 2007
      month: 3
      day: 6
  page: 1109-1123
  publisher: American Heart Association
  source: ahajournals.org (Atypon)
  title: Pathogenesis of Chronic Chagas Heart Disease
  type: article-journal
  URL: https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.106.624296
  volume: '115'

- id: markovsky_exact_2006
  author:
    - family: Markovsky
      given: Ivan
    - family: Willems
      given: Jan C
    - family: Van Huffel
      given: Sabine
    - family: De Moor
      given: Bart
  citation-key: markovsky_exact_2006
  issued:
    - year: 2006
  publisher: SIAM
  title: 'Exact and approximate modeling of linear systems: A behavioral approach'
  type: book

- id: maros_repository_1999
  accessed:
    - year: 2017
      month: 8
      day: 20
  author:
    - family: Maros
      given: Istvan
    - family: Mészáros
      given: Csaba
  citation-key: maros_repository_1999
  container-title: Optimization Methods and Software
  DOI: 10.1080/10556789908805768
  issue: 1-4
  issued:
    - year: 1999
  page: 671–681
  source: Google Scholar
  title: A repository of convex quadratic programming problems
  type: article-journal
  URL: http://www.tandfonline.com/doi/abs/10.1080/10556789908805768
  volume: '11'

- id: maros_repository_1999a
  accessed:
    - year: 2017
      month: 8
      day: 20
  author:
    - family: Maros
      given: Istvan
    - family: Mészáros
      given: Csaba
  citation-key: maros_repository_1999a
  container-title: Optimization Methods and Software
  DOI: 10/cvxwrj
  issue: 1-4
  issued:
    - year: 1999
  page: 671-681
  title: A Repository of Convex Quadratic Programming Problems
  type: article-journal
  volume: '11'

- id: marquardt_algorithm_1963
  author:
    - family: Marquardt
      given: Donald W
  citation-key: marquardt_algorithm_1963
  container-title: Journal of the Society for Industrial and Applied Mathematics
  DOI: 10.1137/0111030
  issue: '2'
  issued:
    - year: 1963
  page: 431–441
  title: An algorithm for least-squares estimation of nonlinear parameters
  type: article-journal
  volume: '11'

- id: martens_learning_2011
  abstract: >-
    In this work we resolve the long-outstanding problem of how to effectively
    train recurrent neural networks (RNNs) on complex and difficult sequence
    modeling problems which may contain long-term data dependencies. Utilizing
    recent advances in the Hessian-free optimization approach (Martens, 2010),
    together with a novel damping scheme, we successfully train RNNs on two sets
    of challenging problems. First, a collection of pathological synthetic
    datasets which are known to be impossible for standard optimization
    approaches (due to their extremely long-term dependencies), and second, on
    three natural and highly complex real-world sequence datasets where we find
    that our method significantly outperforms the previous state-of-the-art
    method for training neural sequence models: the Long Short-term Memory
    approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new
    interpretation of the generalized Gauss-Newton matrix of Schraudolph (2002)
    which is used within the HF approach of Martens.
  accessed:
    - year: 2019
      month: 4
      day: 23
  author:
    - family: Martens
      given: James
    - family: Sutskever
      given: Ilya
  citation-key: martens_learning_2011
  collection-title: ICML'11
  container-title: >-
    Proceedings of the 28th International Conference on International Conference
    on Machine Learning
  event-place: Bellevue, Washington, USA
  ISBN: 978-1-4503-0619-5
  issued:
    - year: 2011
  page: 1033–1040
  publisher: Omnipress
  publisher-place: USA
  source: ACM Digital Library
  title: Learning Recurrent Neural Networks with Hessian-free Optimization
  type: paper-conference
  URL: http://dl.acm.org/citation.cfm?id=3104482.3104612

- id: masti_learning_2018
  author:
    - family: Masti
      given: D.
    - family: Bemporad
      given: A.
  citation-key: masti_learning_2018
  container-title: 2018 IEEE Conference on Decision and Control (CDC)
  DOI: 10/gfwtwq
  event-title: 2018 IEEE Conference on Decision and Control (CDC)
  ISBN: 2576-2370
  issued:
    - year: 2018
  page: 3862-3867
  title: Learning Nonlinear State-Space Models Using Deep Autoencoders
  type: paper-conference

- id: matthews_gaussian_2018
  abstract: >-
    Whilst deep neural networks have shown great empirical success, there is
    still much work to be done to understand their theoretical properties. In
    this paper, we study the relationship between random, wide, fully connected,
    feedforward networks with more than one hidden layer and Gaussian processes
    with a recursive kernel definition. We show that, under broad conditions, as
    we make the architecture increasingly wide, the implied random function
    converges in distribution to a Gaussian process, formalising and extending
    existing results by Neal (1996) to deep networks. To evaluate convergence
    rates empirically, we use maximum mean discrepancy. We then compare finite
    Bayesian deep networks from the literature to Gaussian processes in terms of
    the key predictive quantities of interest, finding that in some cases the
    agreement can be very close. We discuss the desirability of Gaussian process
    behaviour and review non-Gaussian alternative models from the literature.
  accessed:
    - year: 2018
      month: 10
      day: 24
  author:
    - family: Matthews
      given: Alexander G. de G.
    - family: Rowland
      given: Mark
    - family: Hron
      given: Jiri
    - family: Turner
      given: Richard E.
    - family: Ghahramani
      given: Zoubin
  citation-key: matthews_gaussian_2018
  container-title: arXiv:1804.11271 [cs, stat]
  issued:
    - year: 2018
      month: 4
      day: 30
  source: arXiv.org
  title: Gaussian Process Behaviour in Wide Deep Neural Networks
  type: article-journal
  URL: http://arxiv.org/abs/1804.11271

- id: mattuck_introduction_1999
  author:
    - family: Mattuck
      given: A.
  citation-key: mattuck_introduction_1999
  ISBN: 978-0-13-081132-5
  issued:
    - year: 1999
  publisher: Prentice Hall
  title: Introduction to Analysis
  type: book
  URL: https://books.google.com.br/books?id=N0FkQgAACAAJ

- id: may_simple_1976
  author:
    - family: May
      given: Robert M
  citation-key: may_simple_1976
  container-title: Nature
  DOI: 10.1038/261459a0
  issue: '5560'
  issued:
    - year: 1976
  page: 459–467
  title: Simple mathematical models with very complicated dynamics
  type: article-journal
  volume: '261'

- id: mccool_structured_2012
  author:
    - family: McCool
      given: Michael D.
    - family: Robison
      given: Arch D.
    - family: Reinders
      given: James
  call-number: QA76.76.P37 M34 2012
  citation-key: mccool_structured_2012
  event-place: Amsterdam
  ISBN: 978-0-12-415993-8
  issued:
    - year: 2012
  number-of-pages: '406'
  publisher: Elsevier, Morgan Kaufmann
  publisher-place: Amsterdam
  source: Library of Congress ISBN
  title: 'Structured parallel programing: patterns for efficient computation'
  title-short: Structured parallel programing
  type: book

- id: mccullagh_generalized_1989
  author:
    - family: McCullagh
      given: P.
    - family: Nelder
      given: J.A.
  citation-key: mccullagh_generalized_1989
  collection-title: Chapman & Hall/CRC Monographs on Statistics & Applied Probability
  ISBN: 978-0-412-31760-6
  issued:
    - year: 1989
  publisher: Taylor & Francis
  title: Generalized Linear Models, Second Edition
  type: book
  URL: https://books.google.com.br/books?id=h9kFH2_FfBkC

- id: mcculloch_logical_1943
  author:
    - family: McCulloch
      given: Warren S
    - family: Pitts
      given: Walter
  citation-key: mcculloch_logical_1943
  container-title: The bulletin of mathematical biophysics
  container-title-short: The bulletin of mathematical biophysics
  ISSN: 0007-4985
  issue: '4'
  issued:
    - year: 1943
  page: 115-133
  title: A logical calculus of the ideas immanent in nervous activity
  type: article-journal
  volume: '5'

- id: mcdonald_instabilities_1973
  abstract: >-
    The instability of ordinary least squares estimates of linear regression
    coefficients is demonstrated for mortality rates regressed around various
    socioeconomic, weather and pollution variables. A ridge regression technique
    presented by Hoerl and Kennard (Technometrics 12 (1970) 69-82) is employed
    to arrive at "stable" regression coefficients which, in some instances,
    differ considerably from the ordinary least squares estimates. In addition,
    two methods of variable elimination are compared-one based on total squared
    error and the other on a ridge trace analysis.
  accessed:
    - year: 2024
      month: 5
      day: 16
  author:
    - family: McDonald
      given: Gary C.
    - family: Schwing
      given: Richard C.
  citation-key: mcdonald_instabilities_1973
  container-title: Technometrics
  DOI: 10.2307/1266852
  ISSN: 0040-1706
  issue: '3'
  issued:
    - year: 1973
  page: 463-481
  publisher: >-
    [Taylor & Francis, Ltd., American Statistical Association, American Society
    for Quality]
  source: JSTOR
  title: Instabilities of Regression Estimates Relating Air Pollution to Mortality
  type: article-journal
  URL: https://www.jstor.org/stable/1266852
  volume: '15'

- id: mckinney_international_2020
  abstract: "Screening mammography aims to identify breast cancer at earlier stages of the disease, when treatment can be more successful1. Despite the existence of screening programmes worldwide, the interpretation of mammograms is affected by high rates of false positives and false negatives2. Here we present an artificial intelligence (AI) system that is capable of surpassing human experts in breast cancer prediction. To assess its performance in the clinical setting, we curated a large representative dataset from the UK and a large enriched dataset from the USA. We show an absolute reduction of 5.7%\_and 1.2% (USA and UK) in false positives and 9.4%\_and 2.7% in false negatives. We provide evidence of the ability of the system to generalize from the UK to the USA. In an independent study of six radiologists, the AI system outperformed all of the human readers: the area under the receiver operating characteristic curve (AUC-ROC) for the AI system was greater than the AUC-ROC for the average radiologist by an absolute margin of 11.5%. We ran a simulation in which the AI system participated in the double-reading process that is used in the UK, and found that the AI system maintained non-inferior performance and reduced the workload of the second reader by 88%. This robust assessment of the AI system paves the way for clinical trials to improve the accuracy and efficiency of breast cancer screening."
  author:
    - family: McKinney
      given: Scott Mayer
    - family: Sieniek
      given: Marcin
    - family: Godbole
      given: Varun
    - family: Godwin
      given: Jonathan
    - family: Antropova
      given: Natasha
    - family: Ashrafian
      given: Hutan
    - family: Back
      given: Trevor
    - family: Chesus
      given: Mary
    - family: Corrado
      given: Greg C.
    - family: Darzi
      given: Ara
    - family: Etemadi
      given: Mozziyar
    - family: Garcia-Vicente
      given: Florencia
    - family: Gilbert
      given: Fiona J.
    - family: Halling-Brown
      given: Mark
    - family: Hassabis
      given: Demis
    - family: Jansen
      given: Sunny
    - family: Karthikesalingam
      given: Alan
    - family: Kelly
      given: Christopher J.
    - family: King
      given: Dominic
    - family: Ledsam
      given: Joseph R.
    - family: Melnick
      given: David
    - family: Mostofi
      given: Hormuz
    - family: Peng
      given: Lily
    - family: Reicher
      given: Joshua Jay
    - family: Romera-Paredes
      given: Bernardino
    - family: Sidebottom
      given: Richard
    - family: Suleyman
      given: Mustafa
    - family: Tse
      given: Daniel
    - family: Young
      given: Kenneth C.
    - family: De Fauw
      given: Jeffrey
    - family: Shetty
      given: Shravya
  citation-key: mckinney_international_2020
  container-title: Nature
  container-title-short: Nature
  DOI: 10.1038/s41586-019-1799-6
  ISSN: 1476-4687
  issue: '7788'
  issued:
    - year: 2020
      month: 1
      day: 1
  page: 89-94
  title: International evaluation of an AI system for breast cancer screening
  type: article-journal
  URL: https://doi.org/10.1038/s41586-019-1799-6
  volume: '577'

- id: mcnemar_note_1947
  abstract: >-
    Two formulas are presented for judging the significance of the difference
    between correlated proportions. The chi square equivalent of one of the
    developed formulas is pointed out.
  accessed:
    - year: 2019
      month: 1
      day: 28
  author:
    - family: McNemar
      given: Quinn
  citation-key: mcnemar_note_1947
  container-title: Psychometrika
  container-title-short: Psychometrika
  DOI: 10/d9pvhs
  ISSN: 1860-0980
  issue: '2'
  issued:
    - year: 1947
      month: 6
      day: 1
  language: en
  page: 153-157
  source: Springer Link
  title: >-
    Note on the sampling error of the difference between correlated proportions
    or percentages
  type: article-journal
  URL: https://doi.org/10.1007/BF02295996
  volume: '12'

- id: mcsharry_dynamical_2003
  abstract: >-
    A dynamical model based on three coupled ordinary differential equations is
    introduced which is capable of generating realistic synthetic
    electrocardiogram (ECG) signals. The operator can specify the mean and
    standard deviation of the heart rate, the morphology of the PQRST cycle, and
    the power spectrum of the RR tachogram. In particular, both respiratory
    sinus arrhythmia at the high frequencies (HFs) and Mayer waves at the low
    frequencies (LFs) together with the LF/HF ratio are incorporated in the
    model. Much of the beat-to-beat variation in morphology and timing of the
    human ECG, including QT dispersion and R-peak amplitude modulation are shown
    to result. This model may be employed to assess biomedical signal processing
    techniques which are used to compute clinical statistics from the ECG.
  author:
    - family: McSharry
      given: Patrick E
    - family: Clifford
      given: Gari D
    - family: Tarassenko
      given: Lionel
    - family: Smith
      given: Leonard A
  citation-key: mcsharry_dynamical_2003
  container-title: IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING
  issue: '3'
  issued:
    - year: 2003
  language: en
  page: '6'
  source: Zotero
  title: A Dynamical Model for Generating Synthetic Electrocardiogram Signals
  type: article-journal
  volume: '50'

- id: medioni_segmentbased_1985
  author:
    - family: Medioni
      given: Gerard
    - family: Nevatia
      given: Ramakant
  citation-key: medioni_segmentbased_1985
  container-title: Computer Vision, Graphics, and Image Processing
  DOI: 10.1016/S0734-189X(85)80073-6
  issue: '1'
  issued:
    - year: 1985
  page: 2–18
  title: Segment-based stereo matching
  type: article-journal
  volume: '31'

- id: medioni_segmentbased_1985a
  author:
    - family: Medioni
      given: Gerard
    - family: Nevatia
      given: Ramakant
  citation-key: medioni_segmentbased_1985a
  container-title: Computer Vision, Graphics, and Image Processing
  DOI: 10/dc7k3t
  issue: '1'
  issued:
    - year: 1985
  note: '00598'
  page: 2-18
  title: Segment-Based Stereo Matching
  type: article-journal
  volume: '31'

- id: mehrkanoon_regularized_2017
  abstract: >-
    Domain adaptation learning is one of the fundamental research topics in
    pattern recognition and machine learning. This paper introduces a
    regularized semipaired kernel canonical correlation analysis formulation for
    learning a latent space for the domain adaptation problem. The optimization
    problem is formulated in the primal-dual least squares support vector
    machine setting where side information can be readily incorporated through
    regularization terms. The proposed model learns a joint representation of
    the data set across different domains by solving a generalized eigenvalue
    problem or linear system of equations in the dual. The approach is naturally
    equipped with out-of-sample extension property, which plays an important
    role for model selection. Furthermore, the Nyström approximation technique
    is used to make the computational issues due to the large size of the
    matrices involved in the eigendecomposition feasible. The learned latent
    space of the source domain is fed to a multiclass semisupervised kernel
    spectral clustering model that can learn from both labeled and unlabeled
    data points of the source domain in order to classify the data instances of
    the target domain. Experimental results are given to illustrate the
    effectiveness of the proposed approaches on synthetic and real-life data
    sets.
  author:
    - family: Mehrkanoon
      given: S.
    - family: Suykens
      given: J. A. K.
  citation-key: mehrkanoon_regularized_2017
  container-title: IEEE Transactions on Neural Networks and Learning Systems
  DOI: 10.1109/TNNLS.2017.2728719
  ISSN: 2162-237X
  issue: '99'
  issued:
    - year: 2017
  page: 1-15
  source: IEEE Xplore
  title: Regularized Semipaired Kernel CCA for Domain Adaptation
  type: article-journal
  volume: PP

- id: mei_building_2011
  author:
    - family: Mei
      given: Xing
    - family: Sun
      given: Xun
    - family: Zhou
      given: Mingcai
    - family: Jiao
      given: Shaohui
    - family: Wang
      given: Haitao
    - family: Zhang
      given: Xiaopeng
  citation-key: mei_building_2011
  container-title: >-
    Computer Vision Workshops (ICCV Workshops), 2011 IEEE International
    Conference on
  issued:
    - year: 2011
  page: 467–474
  publisher: IEEE
  title: On building an accurate stereo matching system on graphics hardware
  type: paper-conference

- id: mei_generalization_2022
  abstract: >-
    Deep learning methods operate in regimes that defy the traditional
    statistical mindset. The neural network architectures often contain more
    parameters than training samples, and are so rich that they can interpolate
    the observed labels, even if the latter are replaced by pure noise. Despite
    their huge complexity, the same architectures achieve small generalization
    error on real data. This phenomenon has been rationalized in terms of a
    so-called `double descent' curve. As the model complexity increases, the
    generalization error follows the usual U-shaped curve at the beginning,
    first decreasing and then peaking around the interpolation threshold (when
    the model achieves vanishing training error). However, it descends again as
    model complexity exceeds this threshold. The global minimum of the
    generalization error is found in this overparametrized regime, often when
    the number of parameters is much larger than the number of samples. Far from
    being a peculiar property of deep neural networks, elements of this behavior
    have been demonstrated in much simpler settings, including linear regression
    with random covariates. In this paper we consider the problem of learning an
    unknown function over the $d$-dimensional sphere $\mathbb S^{d-1}$, from $n$
    i.i.d. samples $(\boldsymbol x_i, y_i) \in \mathbb S^{d-1} \times \mathbb
    R$, $i \le n$. We perform ridge regression on $N$ random features of the
    form $\sigma(\boldsymbol w_a^{\mathsf T}\boldsymbol x)$, $a \le N$. This can
    be equivalently described as a two-layers neural network with random
    first-layer weights. We compute the precise asymptotics of the
    generalization error, in the limit $N, n, d \to \infty$ with $N/d$ and $n/d$
    fixed. This provides the first analytically tractable model that captures
    all the features of the double descent phenomenon without assuming ad hoc
    misspecification structures.
  accessed:
    - year: 2020
      month: 7
      day: 22
  author:
    - family: Mei
      given: Song
    - family: Montanari
      given: Andrea
  citation-key: mei_generalization_2022
  container-title: Communications on Pure and Applied Mathematics
  DOI: https://doi.org/10.1002/cpa.22008
  issue: '4'
  issued:
    - year: 2022
  page: 667-766
  source: arXiv.org
  title: >-
    The Generalization Error of Random Features Regression: Precise Asymptotics
    and the Double Descent Curve
  title-short: The generalization error of random features regression
  type: article-journal
  URL: http://arxiv.org/abs/1908.05355
  volume: '75'

- id: meier_group_2008
  abstract: >-
    The group lasso is an extension of the lasso to do variable selection on
    (predefined) groups of variables in linear regression models. The estimates
    have the attractive property of being invariant under groupwise orthogonal
    reparameterizations. We extend the group lasso to logistic regression models
    and present an efficient algorithm, that is especially suitable for high
    dimensional problems, which can also be applied to generalized linear models
    to solve the corresponding convex optimization problem. The group lasso
    estimator for logistic regression is shown to be statistically consistent
    even if the number of predictors is much larger than sample size but with
    sparse true underlying structure. We further use a two-stage procedure which
    aims for sparser models than the group lasso, leading to improved prediction
    performance for some cases. Moreover, owing to the two-stage nature, the
    estimates can be constructed to be hierarchical. The methods are used on
    simulated and real data sets about splice site detection in DNA sequences.
  author:
    - family: Meier
      given: Lukas
    - family: Geer
      given: Sara
      non-dropping-particle: van de
    - family: Bühlmann
      given: Peter
  citation-key: meier_group_2008
  container-title: Journal of the Royal Statistical Society. Series B (Statistical Methodology)
  DOI: 10.1111/j.1467-9868.2007.00627.x
  ISSN: 1369-7412
  issue: '1'
  issued:
    - year: 2008
  page: 53-71
  source: JSTOR
  title: The Group Lasso for Logistic Regression
  type: article-journal
  URL: http://www.jstor.org/stable/20203811
  volume: '70'

- id: meirajr_contextualized_2020
  author:
    - family: Meira Jr
      given: Wagner
    - family: Ribeiro
      given: Antonio L. P.
    - family: Oliveira
      given: Derick M.
    - family: Ribeiro
      given: Antonio H.
  citation-key: meirajr_contextualized_2020
  container-title: Communications of the ACM
  DOI: 10.1145/3416965
  issued:
    - year: 2020
  license: All rights reserved
  title: Contextualized Interpretable Machine Learning for Medical Diagnosis
  type: article-journal

- id: menezes_longterm_2008
  abstract: >-
    The NARX network is a dynamical neural architecture commonly used for
    input–output modeling of nonlinear dynamical systems. When applied to time
    series prediction, the NARX network is designed as a feedforward time delay
    neural network (TDNN), i.e., without the feedback loop of delayed outputs,
    reducing substantially its predictive performance. In this paper, we show
    that the original architecture of the NARX network can be easily and
    efficiently applied to long-term (multi-step-ahead) prediction of univariate
    time series. We evaluate the proposed approach using two real-world data
    sets, namely the well-known chaotic laser time series and a variable bit
    rate (VBR) video traffic time series. All the results show that the proposed
    approach consistently outperforms standard neural network based predictors,
    such as the TDNN and Elman architectures.
  author:
    - family: Menezes
      given: José Maria P.
    - family: Barreto
      given: Guilherme A.
  citation-key: menezes_longterm_2008
  collection-title: >-
    Advances in Neural Information Processing (ICONIP 2006) / Brazilian
    Symposium on Neural Networks (SBRN 2006)
  container-title: Neurocomputing
  container-title-short: Neurocomputing
  DOI: 10.1016/j.neucom.2008.01.030
  ISSN: 0925-2312
  issue: '16'
  issued:
    - year: 2008
      month: 10
      day: 1
  page: 3335-3343
  source: ScienceDirect
  title: >-
    Long-term time series prediction with the NARX network: An empirical
    evaluation
  title-short: Long-term time series prediction with the NARX network
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0925231208003081
  volume: '71'

- id: menick_generating_2019
  abstract: >-
    The unconditional generation of high ﬁdelity images is a longstanding
    benchmark for testing the performance of image decoders. Autoregressive
    image models have been able to generate small images unconditionally, but
    the extension of these methods to large images where ﬁdelity can be more
    readily assessed has remained an open problem. Among the major challenges
    are the capacity to encode the vast previous context and the sheer difﬁculty
    of learning a distribution that preserves both global semantic coherence and
    exactness of detail. To address the former challenge, we propose the
    Subscale Pixel Network (SPN), a conditional decoder architecture that
    generates an image as a sequence of sub-images of equal size. The SPN
    compactly captures image-wide spatial dependencies and requires a fraction
    of the memory and the computation required by other fully autoregressive
    models. To address the latter challenge, we propose to use Multidimensional
    Upscaling to grow an image in both size and depth via intermediate stages
    utilising distinct SPNs. We evaluate SPNs on the unconditional generation of
    CelebAHQ of size 256 and of ImageNet from size 32 to 256. We achieve
    state-of-the-art likelihood results in multiple settings, set up new
    benchmark results in previously unexplored settings and are able to generate
    very high ﬁdelity large scale samples on the basis of both datasets.
  author:
    - family: Menick
      given: Jacob
    - family: Kalchbrenner
      given: Nal
  citation-key: menick_generating_2019
  issued:
    - year: 2019
  language: en
  page: '15'
  source: Zotero
  title: >-
    GENERATING HIGH FIDELITY IMAGES WITH SUBSCALE PIXEL NETWORKS AND
    MULTIDIMENSIONAL UPSCALING
  type: article-journal

- id: merity_pointer_2016
  abstract: >-
    Recent neural network sequence models with softmax classifiers have achieved
    their best language modeling performance only with very large hidden states
    and large vocabularies. Even then they struggle to predict rare or unseen
    words even if the context makes the prediction unambiguous. We introduce the
    pointer sentinel mixture architecture for neural sequence models which has
    the ability to either reproduce a word from the recent context or produce a
    word from a standard softmax classifier. Our pointer sentinel-LSTM model
    achieves state of the art language modeling performance on the Penn Treebank
    (70.9 perplexity) while using far fewer parameters than a standard softmax
    LSTM. In order to evaluate how well language models can exploit longer
    contexts and deal with more realistic vocabularies and larger corpora we
    also introduce the freely available WikiText corpus.
  accessed:
    - year: 2019
      month: 5
      day: 31
  author:
    - family: Merity
      given: Stephen
    - family: Xiong
      given: Caiming
    - family: Bradbury
      given: James
    - family: Socher
      given: Richard
  citation-key: merity_pointer_2016
  container-title: arXiv:1609.07843
  issued:
    - year: 2016
      month: 9
      day: 26
  source: arXiv.org
  title: Pointer Sentinel Mixture Models
  type: article-journal
  URL: http://arxiv.org/abs/1609.07843

- id: meronen_periodic_2021
  abstract: >-
    Neural network models are known to reinforce hidden data biases, making them
    unreliable and difficult to interpret. We seek to build models that `know
    what they do not know' by introducing inductive biases in the function
    space. We show that periodic activation functions in Bayesian neural
    networks establish a connection between the prior on the network weights and
    translation-invariant, stationary Gaussian process priors. Furthermore, we
    show that this link goes beyond sinusoidal (Fourier) activations by also
    covering triangular wave and periodic ReLU activation functions. In a series
    of experiments, we show that periodic activation functions obtain comparable
    performance for in-domain data and capture sensitivity to perturbed inputs
    in deep neural networks for out-of-domain detection.
  accessed:
    - year: 2021
      month: 11
      day: 22
  author:
    - family: Meronen
      given: Lassi
    - family: Trapp
      given: Martin
    - family: Solin
      given: Arno
  citation-key: meronen_periodic_2021
  container-title: arXiv:2110.13572 [cs, stat]
  issued:
    - year: 2021
      month: 10
      day: 26
  source: arXiv.org
  title: Periodic Activation Functions Induce Stationarity
  type: article-journal
  URL: http://arxiv.org/abs/2110.13572

- id: meronen_stationary_2020
  abstract: >-
    We introduce a new family of non-linear neural network activation functions
    that mimic the properties induced by the widely-used Mat\'ern family of
    kernels in Gaussian process (GP) models. This class spans a range of locally
    stationary models of various degrees of mean-square differentiability. We
    show an explicit link to the corresponding GP models in the case that the
    network consists of one infinitely wide hidden layer. In the limit of
    infinite smoothness the Mat\'ern family results in the RBF kernel, and in
    this case we recover RBF activations. Mat\'ern activation functions result
    in similar appealing properties to their counterparts in GP models, and we
    demonstrate that the local stationarity property together with limited
    mean-square differentiability shows both good performance and uncertainty
    calibration in Bayesian deep learning tasks. In particular, local
    stationarity helps calibrate out-of-distribution (OOD) uncertainty. We
    demonstrate these properties on classification and regression benchmarks and
    a radar emitter classification task.
  accessed:
    - year: 2021
      month: 11
      day: 24
  author:
    - family: Meronen
      given: Lassi
    - family: Irwanto
      given: Christabella
    - family: Solin
      given: Arno
  citation-key: meronen_stationary_2020
  container-title: arXiv:2010.09494 [cs]
  issued:
    - year: 2020
      month: 10
      day: 19
  source: arXiv.org
  title: Stationary Activations for Uncertainty Calibration in Deep Learning
  type: article-journal
  URL: http://arxiv.org/abs/2010.09494

- id: meyers_comparison_2021
  abstract: "BACKGROUND: The current ST-elevation myocardial infarction (STEMI) vs. non-STEMI (NSTEMI) paradigm prevents some NSTEMI patients with acute coronary occlusion from receiving emergent reperfusion, in spite of their known increased mortality compared with NSTEMI without occlusion. We have proposed a new paradigm known as occlusion MI vs. nonocclusion MI (OMI vs. NOMI).\nOBJECTIVE: We aimed to compare the two paradigms within a single population. We hypothesized that STEMI(-) OMI would have characteristics similar to STEMI(+) OMI but longer time to catheterization.\nMETHODS: We performed a retrospective review of a prospectively collected acute coronary syndrome population. OMI was defined as an acute culprit and either TIMI 0-2 flow or TIMI 3 flow plus peak troponin T\_>\_1.0\_ng/mL. We collected electrocardiograms, demographic characteristics, laboratory results, angiographic data, and outcomes.\nRESULTS: Among 467 patients, there were 108 OMIs, with only 60% (67 of 108) meeting STEMI criteria. Median peak troponin T for the STEMI(+) OMI, STEMI(-) OMI, and no occlusion groups were 3.78 (interquartile range \\[IQR] 2.18-7.63), 1.87 (IQR 1.12-5.48), and 0.00 (IQR 0.00-0.08). Median time from arrival to catheterization was 41\_min (IQR 23-86\_min) for STEMI(+) OMI compared with 437\_min (IQR 85-1590\_min) for STEMI(-) OMI (p\_<\_0.001). STEMI(+) OMI was more likely than STEMI(-) OMI to undergo catheterization within 90\_min (76% vs. 28%; p\_<\_0.001).\nCONCLUSIONS: STEMI(-) OMI patients had significant delays to catheterization but adverse outcomes more similar to STEMI(+) OMI than those with no occlusion. These data support the OMI/NOMI paradigm and the importance of further research into emergent reperfusion for STEMI(-) OMI."
  author:
    - family: Meyers
      given: H. Pendell
    - family: Bracey
      given: Alexander
    - family: Lee
      given: Daniel
    - family: Lichtenheld
      given: Andrew
    - family: Li
      given: Wei J.
    - family: Singer
      given: Daniel D.
    - family: Kane
      given: Jesse A.
    - family: Dodd
      given: Kenneth W.
    - family: Meyers
      given: Kristen E.
    - family: Thode
      given: Henry C.
    - family: Shroff
      given: Gautam R.
    - family: Singer
      given: Adam J.
    - family: Smith
      given: Stephen W.
  citation-key: meyers_comparison_2021
  container-title: The Journal of Emergency Medicine
  container-title-short: J Emerg Med
  DOI: 10.1016/j.jemermed.2020.10.026
  ISSN: 0736-4679
  issue: '3'
  issued:
    - year: 2021
      month: 3
  language: eng
  page: 273-284
  PMID: '33308915'
  source: PubMed
  title: >-
    Comparison of the ST-Elevation Myocardial Infarction (STEMI) vs. NSTEMI and
    Occlusion MI (OMI) vs. NOMI Paradigms of Acute MI
  type: article-journal
  volume: '60'

- id: mezic_spectral_2005
  abstract: >-
    In this paper we discuss two issues related to model reduction of
    deterministic or stochastic processes. The first is the relationship of the
    spectral properties of the dynamics on the attractor of the original,
    high-dimensional dynamical system with the properties and possibilities for
    model reduction. We review some elements of the spectral theory of dynamical
    systems. We apply this theory to obtain a decomposition of the process that
    utilizes spectral properties of the linear Koopman operator associated with
    the asymptotic dynamics on the attractor. This allows us to extract the
    almost periodic part of the evolving process. The remainder of the process
    has continuous spectrum. The second topic we discuss is that of model
    validation, where the original, possibly high-dimensional dynamics and the
    dynamics of the reduced model – that can be deterministic or stochastic –
    are compared in some norm. Using the “statistical Takens theorem” proven in
    (Mezić, I. and Banaszuk, A. Physica D, 2004) we argue that comparison of
    average energy contained in the finite-dimensional projection is one in the
    hierarchy of functionals of the field that need to be checked in order to
    assess the accuracy of the projection.
  accessed:
    - year: 2020
      month: 12
      day: 15
  author:
    - family: Mezić
      given: Igor
  citation-key: mezic_spectral_2005
  container-title: Nonlinear Dynamics
  container-title-short: Nonlinear Dyn
  DOI: 10.1007/s11071-005-2824-x
  ISSN: 1573-269X
  issue: '1'
  issued:
    - year: 2005
      month: 8
      day: 1
  language: en
  page: 309-325
  source: Springer Link
  title: Spectral Properties of Dynamical Systems, Model Reduction and Decompositions
  type: article-journal
  URL: https://doi.org/10.1007/s11071-005-2824-x
  volume: '41'

- id: mhammedi_efficient_2016
  abstract: >-
    The problem of learning long-term dependencies in sequences using Recurrent
    Neural Networks (RNNs) is still a major challenge. Recent methods have been
    suggested to solve this problem by constraining the transition matrix to be
    unitary during training which ensures that its norm is equal to one and
    prevents exploding gradients. These methods either have limited
    expressiveness or scale poorly with the size of the network when compared
    with the simple RNN case, especially when using stochastic gradient descent
    with a small mini-batch size. Our contributions are as follows; we ﬁrst show
    that constraining the transition matrix to be unitary is a special case of
    an orthogonal constraint. Then we present a new parametrisation of the
    transition matrix which allows efﬁcient training of an RNN while ensuring
    that the matrix is always orthogonal. Our results show that the orthogonal
    constraint on the transition matrix applied through our parametrisation
    gives similar beneﬁts to the unitary constraint, without the time complexity
    limitations.
  accessed:
    - year: 2019
      month: 9
      day: 19
  author:
    - family: Mhammedi
      given: Zakaria
    - family: Hellicar
      given: Andrew
    - family: Rahman
      given: Ashfaqur
    - family: Bailey
      given: James
  citation-key: mhammedi_efficient_2016
  container-title: arXiv:1612.00188 [cs]
  issued:
    - year: 2016
      month: 12
      day: 1
  language: en
  source: arXiv.org
  title: >-
    Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using
    Householder Reflections
  type: article-journal
  URL: http://arxiv.org/abs/1612.00188

- id: mhammedi_efficient_2017
  author:
    - family: Mhammedi
      given: Zakaria
    - family: Hellicar
      given: Andrew
    - family: Rahman
      given: Ashfaqur
    - family: Bailey
      given: James
  citation-key: mhammedi_efficient_2017
  event-title: >-
    Proceedings of the 34th International Conference on Machine Learning-Volume
    70
  issued:
    - year: 2017
  page: 2401-2409
  publisher: JMLR. org
  title: >-
    Efficient orthogonal parametrisation of recurrent neural networks using
    householder reflections
  type: paper-conference

- id: mikolov_distributed_2013
  author:
    - family: Mikolov
      given: Tomas
    - family: Sutskever
      given: Ilya
    - family: Chen
      given: Kai
    - family: Corrado
      given: Greg S.
    - family: Dean
      given: Jeff
  citation-key: mikolov_distributed_2013
  container-title: Advances in neural information processing systems
  issued:
    - year: 2013
  page: 3111–3119
  source: Google Scholar
  title: Distributed representations of words and phrases and their compositionality
  type: paper-conference

- id: mikolov_efficient_2013
  abstract: >-
    We propose two novel model architectures for computing continuous vector
    representations of words from very large data sets. The quality of these
    representations is measured in a word similarity task, and the results are
    compared to the previously best performing techniques based on different
    types of neural networks. We observe large improvements in accuracy at much
    lower computational cost, i.e. it takes less than a day to learn high
    quality word vectors from a 1.6 billion words data set. Furthermore, we show
    that these vectors provide state-of-the-art performance on our test set for
    measuring syntactic and semantic word similarities.
  author:
    - family: Mikolov
      given: Tomas
    - family: Chen
      given: Kai
    - family: Corrado
      given: Greg
    - family: Dean
      given: Jeffrey
  citation-key: mikolov_efficient_2013
  container-title: arXiv:1301.3781 [cs]
  issued:
    - year: 2013
      month: 1
      day: 16
  source: arXiv.org
  title: Efficient Estimation of Word Representations in Vector Space
  type: article-journal
  URL: http://arxiv.org/abs/1301.3781

- id: mikolov_linguistic_2013
  author:
    - family: Mikolov
      given: Tomas
    - family: Yih
      given: Wen-tau
    - family: Zweig
      given: Geoffrey
  citation-key: mikolov_linguistic_2013
  container-title: >-
    Proceedings of the 2013 Conference of the North American Chapter of the
    Association for Computational Linguistics: Human Language Technologies
  event-place: Atlanta, Georgia
  issued:
    - year: 2013
      month: 6
  page: 746–751
  publisher: Association for Computational Linguistics
  publisher-place: Atlanta, Georgia
  source: ACLWeb
  title: Linguistic Regularities in Continuous Space Word Representations
  type: paper-conference
  URL: http://www.aclweb.org/anthology/N13-1090

- id: milanese_model_2005
  author:
    - family: Milanese
      given: Mario
    - family: Novara
      given: Carlo
  citation-key: milanese_model_2005
  container-title: IEEE Transactions on Automatic Control
  DOI: 10.1109/TAC.2005.856657
  issue: '10'
  issued:
    - year: 2005
  page: 1606–1611
  title: Model quality in identification of nonlinear systems
  type: article-journal
  volume: '50'

- id: miller_stable_2018
  abstract: >-
    Stability is a fundamental property of dynamical systems, yet to this date
    it has had little bearing on the practice of recurrent neural networks. In
    this work, we conduct a thorough investigation of stable recurrent models.
    Theoretically, we prove stable recurrent neural networks are well
    approximated by feed-forward networks for the purpose of both inference and
    training by gradient descent. Empirically, we demonstrate stable recurrent
    models often perform as well as their unstable counterparts on benchmark
    sequence tasks. Taken together, these findings shed light on the effective
    power of recurrent networks and suggest much of sequence learning happens,
    or can be made to happen, in the stable regime. Moreover, our results help
    to explain why in many cases practitioners succeed in replacing recurrent
    models by feed-forward models.
  accessed:
    - year: 2019
      month: 7
      day: 27
  author:
    - family: Miller
      given: John
    - family: Hardt
      given: Moritz
  citation-key: miller_stable_2018
  container-title: arXiv:1805.10369 [cs, stat]
  issued:
    - year: 2018
      month: 5
      day: 25
  source: arXiv.org
  title: Stable Recurrent Models
  type: article-journal
  URL: http://arxiv.org/abs/1805.10369

- id: miller_subset_2002
  author:
    - family: Miller
      given: Alan
  citation-key: miller_subset_2002
  ISBN: 1-4200-3593-2
  issued:
    - year: 2002
  publisher: CRC Press
  title: Subset selection in regression
  type: book

- id: miller_when_2018
  abstract: >-
    We prove stable recurrent neural networks are well approximated by
    feed-forward networks for the purpose of both inference and training by
    gradient descent. Our result applies to a broad range of non-linear
    recurrent neural networks under a natural stability condition, which we
    observe is also necessary. Complementing our theoretical findings, we verify
    the conclusions of our theory on both real and synthetic tasks. Furthermore,
    we demonstrate recurrent models satisfying the stability assumption of our
    theory can have excellent performance on real sequence learning tasks.
  accessed:
    - year: 2018
      month: 11
      day: 2
  author:
    - family: Miller
      given: John
    - family: Hardt
      given: Moritz
  citation-key: miller_when_2018
  container-title: arXiv:1805.10369 [cs, stat]
  issued:
    - year: 2018
      month: 5
      day: 25
  source: arXiv.org
  title: When Recurrent Models Don't Need To Be Recurrent
  type: article-journal
  URL: http://arxiv.org/abs/1805.10369

- id: min_curious_2021
  abstract: >-
    Adversarial training has shown its ability in producing models that are
    robust to perturbations on the input data, but usually at the expense of
    decrease in the standard accuracy. To mitigate this issue, it is commonly
    believed that more training data will eventually help such adversarially
    robust models generalize better on the benign/unperturbed test data. In this
    paper, however, we challenge this conventional belief and show that more
    training data can hurt the generalization of adversarially robust models in
    the classification problems. We first investigate the Gaussian mixture
    classification with a linear loss and identify three regimes based on the
    strength of the adversary. In the weak adversary regime, more data improves
    the generalization of adversarially robust models. In the medium adversary
    regime, with more training data, the generalization loss exhibits a double
    descent curve, which implies the existence of an intermediate stage where
    more training data hurts the generalization. In the strong adversary regime,
    more data almost immediately causes the generalization error to increase.
    Then we move to the analysis of a two-dimensional classification problem
    with a 0-1 loss. We prove that more data always hurts the generalization
    performance of adversarially trained models with large perturbations. To
    complement our theoretical results, we conduct empirical studies on Gaussian
    mixture classification, support vector machines (SVMs), and linear
    regression.
  author:
    - family: Min
      given: Yifei
    - family: Chen
      given: Lin
    - family: Karbasi
      given: Amin
  citation-key: min_curious_2021
  container-title: Proceedings of the Conference on Uncertainty in Artificial Intelligence
  issued:
    - year: 2021
  page: 129-139
  source: arXiv.org
  title: >-
    The Curious Case of Adversarially Robust Models: More Data Can Help, Double
    Descend, or Hurt Generalization
  title-short: The Curious Case of Adversarially Robust Models
  type: article-journal
  URL: http://arxiv.org/abs/2002.11080
  volume: '161'

- id: minar_recent_2018
  abstract: >-
    Deep Learning is one of the newest trends in Machine Learning and Artificial

    Intelligence research. It is also one of the most popular scientific
    research

    trends now-a-days. Deep learning methods have brought revolutionary advances
    in

    computer vision and machine learning. Every now and then, new and new deep

    learning techniques are being born, outperforming state-of-the-art machine

    learning and even existing deep learning techniques. In recent years, the
    world

    has seen many major breakthroughs in this field. Since deep learning is

    evolving at a huge speed, its kind of hard to keep track of the regular

    advances especially for new researchers. In this paper, we are going to
    briefly

    discuss about recent advances in Deep Learning for past few years.
  accessed:
    - year: 2019
      month: 1
      day: 7
  author:
    - family: Minar
      given: Matiur Rahman
    - family: Naher
      given: Jibon
  citation-key: minar_recent_2018
  DOI: 10/gdvmmz
  issued:
    - year: 2018
      month: 7
      day: 21
  language: en
  source: arxiv.org
  title: 'Recent Advances in Deep Learning: An Overview'
  title-short: Recent Advances in Deep Learning
  type: article-journal
  URL: https://arxiv.org/abs/1807.08169v1

- id: minchole_artificial_
  author:
    - family: Mincholé
      given: Ana
    - family: Rodriguez
      given: Blanca
  citation-key: minchole_artificial_
  container-title: Nature Medicine
  DOI: 10/gfsvzh
  language: en
  page: '2'
  source: Zotero
  title: Artificial intelligence for the electrocardiogram
  type: article-journal

- id: mirza_conditional_2014
  abstract: >-
    Generative Adversarial Nets [8] were recently introduced as a novel way to
    train generative models. In this work we introduce the conditional version
    of generative adversarial nets, which can be constructed by simply feeding
    the data, y, we wish to condition on to both the generator and
    discriminator. We show that this model can generate MNIST digits conditioned
    on class labels. We also illustrate how this model could be used to learn a
    multi-modal model, and provide preliminary examples of an application to
    image tagging in which we demonstrate how this approach can generate
    descriptive tags which are not part of training labels.
  accessed:
    - year: 2018
      month: 12
      day: 4
  author:
    - family: Mirza
      given: Mehdi
    - family: Osindero
      given: Simon
  citation-key: mirza_conditional_2014
  container-title: arXiv:1411.1784 [cs, stat]
  issued:
    - year: 2014
      month: 11
      day: 6
  source: arXiv.org
  title: Conditional Generative Adversarial Nets
  type: article-journal
  URL: http://arxiv.org/abs/1411.1784

- id: mitra_understanding_2019
  abstract: >-
    Traditionally in regression one minimizes the number of fitting parameters
    or uses smoothing/regularization to trade training (TE) and generalization
    error (GE). Driving TE to zero by increasing fitting degrees of freedom
    (dof) is expected to increase GE. However modern big-data approaches,
    including deep nets, seem to over-parametrize and send TE to zero (data
    interpolation) without impacting GE. Overparametrization has the benefit
    that global minima of the empirical loss function proliferate and become
    easier to find. These phenomena have drawn theoretical attention. Regression
    and classification algorithms have been shown that interpolate data but also
    generalize optimally. An interesting related phenomenon has been noted: the
    existence of non-monotonic risk curves, with a peak in GE with increasing
    dof. It was suggested that this peak separates a classical regime from a
    modern regime where over-parametrization improves performance. Similar
    over-fitting peaks were reported previously (statistical physics approach to
    learning) and attributed to increased fitting model flexibility. We
    introduce a generative and fitting model pair ("Misparametrized Sparse
    Regression" or MiSpaR) and show that the overfitting peak can be dissociated
    from the point at which the fitting function gains enough dof's to match the
    data generative model and thus provides good generalization. This
    complicates the interpretation of overfitting peaks as separating a
    "classical" from a "modern" regime. Data interpolation itself cannot
    guarantee good generalization: we need to study the interpolation with
    different penalty terms. We present analytical formulae for GE curves for
    MiSpaR with $l_2$ and $l_1$ penalties, in the interpolating limit
    $\lambda\rightarrow 0$.These risk curves exhibit important differences and
    help elucidate the underlying phenomena.
  accessed:
    - year: 2021
      month: 6
      day: 24
  author:
    - family: Mitra
      given: Partha P.
  citation-key: mitra_understanding_2019
  container-title: arXiv:1906.03667 [physics, stat]
  issued:
    - year: 2019
      month: 6
      day: 9
  source: arXiv.org
  title: >-
    Understanding overfitting peaks in generalization error: Analytical risk
    curves for $l_2$ and $l_1$ penalized interpolation
  title-short: Understanding overfitting peaks in generalization error
  type: article-journal
  URL: http://arxiv.org/abs/1906.03667

- id: mnih_recurrent_2014
  author:
    - family: Mnih
      given: Volodymyr
    - family: Heess
      given: Nicolas
    - family: Graves
      given: Alex
  citation-key: mnih_recurrent_2014
  container-title: Advances in neural information processing systems
  issued:
    - year: 2014
  page: 2204–2212
  source: Google Scholar
  title: Recurrent models of visual attention
  type: paper-conference

- id: mohamed_acoustic_2012
  author:
    - family: Mohamed
      given: Abdel-rahman
    - family: Dahl
      given: George E.
    - family: Hinton
      given: Geoffrey
  citation-key: mohamed_acoustic_2012
  container-title: IEEE Transactions on Audio, Speech, and Language Processing
  DOI: 10.1109/TASL.2011.2109382
  issue: '1'
  issued:
    - year: 2012
  page: 14–22
  source: Google Scholar
  title: Acoustic modeling using deep belief networks
  type: article-journal
  volume: '20'

- id: mohan_power_1995
  author:
    - family: Mohan
      given: Ned
    - family: Mohan
      given: Tore M
  citation-key: mohan_power_1995
  issued:
    - year: 1995
  publisher: John wiley & sons New York
  title: Power electronics
  type: book
  volume: '3'

- id: mokus_bayesian_1974
  author:
    - family: Mokus
      given: J
  citation-key: mokus_bayesian_1974
  container-title: Optimization Techniques
  issued:
    - year: 1974
  language: en
  page: 400-404
  source: Zotero
  title: On Bayesian Methods for Seeking the Extremum
  type: article-journal

- id: morari_model_2002
  author:
    - family: Morari
      given: Manfred
    - family: Lee
      given: Jay H.
    - family: Garcia
      given: C.
    - family: Prett
      given: D. M.
  citation-key: morari_model_2002
  container-title: Preprint
  issued:
    - year: 2002
  source: Google Scholar
  title: Model predictive control
  type: article-journal

- id: morcos_one_2019
  abstract: >-
    The success of lottery ticket initializations (Frankle and Carbin, 2019)
    suggests that small, sparsified networks can be trained so long as the
    network is initialized appropriately. Unfortunately, finding these "winning
    ticket" initializations is computationally expensive. One potential solution
    is to reuse the same winning tickets across a variety of datasets and
    optimizers. However, the generality of winning ticket initializations
    remains unclear. Here, we attempt to answer this question by generating
    winning tickets for one training configuration (optimizer and dataset) and
    evaluating their performance on another configuration. Perhaps surprisingly,
    we found that, within the natural images domain, winning ticket
    initializations generalized across a variety of datasets, including Fashion
    MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365, often achieving
    performance close to that of winning tickets generated on the same dataset.
    Moreover, winning tickets generated using larger datasets consistently
    transferred better than those generated using smaller datasets. We also
    found that winning ticket initializations generalize across optimizers with
    high performance. These results suggest that winning ticket initializations
    generated by sufficiently large datasets contain inductive biases generic to
    neural networks more broadly which improve training across many settings and
    provide hope for the development of better initialization methods.
  accessed:
    - year: 2020
      month: 7
      day: 5
  author:
    - family: Morcos
      given: Ari S.
    - family: Yu
      given: Haonan
    - family: Paganini
      given: Michela
    - family: Tian
      given: Yuandong
  citation-key: morcos_one_2019
  container-title: arXiv:1906.02773 [cs, stat]
  issued:
    - year: 2019
      month: 10
      day: 27
  source: arXiv.org
  title: >-
    One ticket to win them all: generalizing lottery ticket initializations
    across datasets and optimizers
  title-short: One ticket to win them all
  type: article-journal
  URL: http://arxiv.org/abs/1906.02773

- id: more_computing_1983
  author:
    - family: Moré
      given: Jorge J
    - family: Sorensen
      given: Danny C
  citation-key: more_computing_1983
  container-title: SIAM Journal on Scientific and Statistical Computing
  DOI: 10.1137/0904038
  issue: '3'
  issued:
    - year: 1983
  page: 553–572
  title: Computing a trust region step
  type: article-journal
  volume: '4'

- id: more_levenbergmarquardt_1978
  author:
    - family: Moré
      given: Jorge J
  citation-key: more_levenbergmarquardt_1978
  container-title: Numerical Analysis
  issued:
    - year: 1978
  page: 105–116
  publisher: Springer
  title: 'The Levenberg-Marquardt algorithm: implementation and theory'
  type: chapter

- id: morris_what_2011
  accessed:
    - year: 2019
      month: 11
      day: 13
  author:
    - family: Morris
      given: K. A.
  citation-key: morris_what_2011
  container-title: Applied Mechanics Reviews
  container-title-short: Appl. Mech. Rev
  DOI: 10.1115/1.4007112
  ISSN: 0003-6900
  issue: '5'
  issued:
    - year: 2011
      month: 9
      day: 1
  language: en
  source: asmedigitalcollection.asme.org
  title: What is Hysteresis?
  type: article-journal
  URL: >-
    https://asmedigitalcollection.asme.org/appliedmechanicsreviews/article/64/5/050801/369998/What-is-Hysteresis
  volume: '64'

- id: munkres_analysis_1997
  author:
    - family: Munkres
      given: J.R.
  citation-key: munkres_analysis_1997
  collection-title: Advanced Books Classics
  ISBN: 978-0-8133-4548-2
  issued:
    - year: 1997
  publisher: Avalon Publishing
  title: Analysis On Manifolds
  type: book
  URL: https://books.google.com.br/books?id=tGT6K6HdFfwC

- id: munkres_topology_2000
  author:
    - family: Munkres
      given: J.R.
  citation-key: munkres_topology_2000
  collection-title: Featured Titles for Topology Series
  ISBN: 978-0-13-181629-9
  issued:
    - year: 2000
  publisher: Prentice Hall, Incorporated
  title: Topology
  type: book
  URL: https://books.google.com.br/books?id=XjoZAQAAIAAJ

- id: murphy_lowcost_2007
  author:
    - family: Murphy
      given: Chris
    - family: Lindquist
      given: Daniel
    - family: Rynning
      given: Ann Marie
    - family: Cecil
      given: Thomas
    - family: Leavitt
      given: Sarah
    - family: Chang
      given: Mark L
  citation-key: murphy_lowcost_2007
  container-title: >-
    Field-Programmable Custom Computing Machines, 2007. FCCM 2007. 15th Annual
    IEEE Symposium on
  issued:
    - year: 2007
  page: 333–334
  publisher: IEEE
  title: Low-cost stereo vision on an FPGA
  type: paper-conference

- id: murphy_machine_2012
  author:
    - family: Murphy
      given: Kevin P.
  call-number: Q325.5 .M87 2012
  citation-key: murphy_machine_2012
  collection-title: Adaptive computation and machine learning series
  event-place: Cambridge, MA
  ISBN: 978-0-262-01802-9
  issued:
    - year: 2012
  number-of-pages: '1067'
  publisher: MIT Press
  publisher-place: Cambridge, MA
  source: Library of Congress ISBN
  title: 'Machine learning: a probabilistic perspective'
  title-short: Machine learning
  type: book

- id: muthukumar_classification_2021
  abstract: >-
    We compare classification and regression tasks in an overparameterized
    linear model with Gaussian features. On the one hand, we show that with
    sufficient overparameterization all training points are support vectors:
    solutions obtained by least-squares minimum-norm interpolation, typically
    used for regression, are identical to those produced by the hard-margin
    support vector machine (SVM) that minimizes the hinge loss, typically used
    for training classifiers. On the other hand, we show that there exist
    regimes where these interpolating solutions generalize well when evaluated
    by the 0-1 test loss function, but do not generalize if evaluated by the
    square loss function, i.e. they approach the null risk. Our results
    demonstrate the very different roles and properties of loss functions used
    at the training phase (optimization) and the testing phase (generalization).
  accessed:
    - year: 2022
      month: 11
      day: 23
  author:
    - family: Muthukumar
      given: Vidya
    - family: Narang
      given: Adhyyan
    - family: Subramanian
      given: Vignesh
    - family: Belkin
      given: Mikhail
    - family: Hsu
      given: Daniel
    - family: Sahai
      given: Anant
  citation-key: muthukumar_classification_2021
  DOI: 10.48550/arXiv.2005.08054
  issued:
    - year: 2021
      month: 10
      day: 14
  number: arXiv:2005.08054
  publisher: arXiv
  source: arXiv.org
  title: >-
    Classification vs regression in overparameterized regimes: Does the loss
    function matter?
  title-short: Classification vs regression in overparameterized regimes
  type: article
  URL: http://arxiv.org/abs/2005.08054

- id: muthukumar_harmless_2020
  abstract: >-
    A continuing mystery in understanding the empirical success of deep neural
    networks is their ability to achieve zero training error and generalize
    well, even when the training data is noisy and there are more parameters
    than data points. We investigate this overparameterized regime in linear
    regression, where all solutions that minimize training error interpolate the
    data, including noise. We lower-bound the fundamental generalization
    (mean-squared) error of any interpolating solution in the presence of noise,
    and show that this bound decays to zero with the number of features. Thus,
    overparameterization can be beneficial in ensuring harmless interpolation of
    noise. We discuss two root causes for poor generalization that are
    complementary in nature - signal “bleeding” into a large number of alias
    features, and overfitting of noise by parsimonious feature selectors. For
    the sparse linear model with noise, we provide a hybrid interpolating scheme
    that mitigates both these issues and achieves order-optimal MSE over all
    possible interpolating solutions.
  author:
    - family: Muthukumar
      given: Vidya
    - family: Vodrahalli
      given: Kailas
    - family: Subramanian
      given: Vignesh
    - family: Sahai
      given: Anant
  citation-key: muthukumar_harmless_2020
  container-title: IEEE Journal on Selected Areas in Information Theory
  DOI: 10.1109/JSAIT.2020.2984716
  ISSN: 2641-8770
  issue: '1'
  issued:
    - year: 2020
      month: 5
  page: 67-83
  source: IEEE Xplore
  title: Harmless Interpolation of Noisy Data in Regression
  type: article-journal
  volume: '1'

- id: mutto_timeofflight_2012
  author:
    - family: Mutto
      given: Carlo Dal
    - family: Zanuttigh
      given: Pietro
    - family: Cortelazzo
      given: Guido M
  citation-key: mutto_timeofflight_2012
  issued:
    - year: 2012
  publisher: Springer Publishing Company, Incorporated
  title: Time-of-flight cameras and microsoft kinect (TM)
  type: book

- id: nagarajan_uniform_2019
  accessed:
    - year: 2019
      month: 12
      day: 30
  author:
    - family: Nagarajan
      given: Vaishnavh
    - family: Kolter
      given: J. Zico
  citation-key: nagarajan_uniform_2019
  container-title: Advances in Neural Information Processing Systems 32
  editor:
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Beygelzimer
      given: A.
    - family: Alché-Buc
      given: F.
      dropping-particle: d\textquotesingle
    - family: Fox
      given: E.
    - family: Garnett
      given: R.
  issued:
    - year: 2019
  page: 11611–11622
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Uniform convergence may be unable to explain generalization in deep learning
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/9336-uniform-convergence-may-be-unable-to-explain-generalization-in-deep-learning.pdf

- id: nakkiran_deep_2020
  abstract: >-
    We show that a variety of modern deep learning tasks exhibit a
    "double-descent" phenomenon where, as we increase model size, performance
    first gets worse and then gets better. Moreover, we show that double descent
    occurs not just as a function of model size, but also as a function of the
    number of training epochs. We unify the above phenomena by defining a new
    complexity measure we call the effective model complexity and conjecture a
    generalized double descent with respect to this measure. Furthermore, our
    notion of model complexity allows us to identify certain regimes where
    increasing (even quadrupling) the number of train samples actually hurts
    test performance.
  author:
    - family: Nakkiran
      given: Preetum
    - family: Kaplun
      given: Gal
    - family: Bansal
      given: Yamini
    - family: Yang
      given: Tristan
    - family: Barak
      given: Boaz
    - family: Sutskever
      given: Ilya
  citation-key: nakkiran_deep_2020
  container-title: >-
    Proceedings of the 8th International Conference on Learning Representations
    (ICLR)
  event-title: International Conference on Learning Representations (ICLR)
  issued:
    - year: 2020
  source: arXiv.org
  title: 'Deep Double Descent: Where Bigger Models and More Data Hurt'
  type: paper-conference

- id: nar_step_2018
  abstract: >-
    Training a neural network with the gradient descent algorithm gives rise to
    a discrete-time nonlinear dynamical system. Consequently, behaviors that are
    typically observed in these systems emerge during training, such as
    convergence to an orbit but not to a fixed point or dependence of
    convergence on the initialization. Step size of the algorithm plays a
    critical role in these behaviors: it determines the subset of the local
    optima that the algorithm can converge to, and it specifies the magnitude of
    the oscillations if the algorithm converges to an orbit. To elucidate the
    effects of the step size on training of neural networks, we study the
    gradient descent algorithm as a discrete-time dynamical system, and by
    analyzing the Lyapunov stability of different solutions, we show the
    relationship between the step size of the algorithm and the solutions that
    can be obtained with this algorithm. The results provide an explanation for
    several phenomena observed in practice, including the deterioration in the
    training error with increased depth, the hardness of estimating linear
    mappings with large singular values, and the distinct performance of deep
    residual networks.
  accessed:
    - year: 2018
      month: 12
      day: 10
  author:
    - family: Nar
      given: Kamil
    - family: Sastry
      given: S. Shankar
  citation-key: nar_step_2018
  container-title: arXiv:1805.08890 [cs, math, stat]
  issued:
    - year: 2018
      month: 5
      day: 22
  source: arXiv.org
  title: Step Size Matters in Deep Learning
  type: article-journal
  URL: http://arxiv.org/abs/1805.08890

- id: nar_step_2018a
  accessed:
    - year: 2018
      month: 12
      day: 4
  author:
    - family: Nar
      given: Kamil
    - family: Sastry
      given: Shankar
  citation-key: nar_step_2018a
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 3439–3447
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Step Size Matters in Deep Learning
  type: chapter
  URL: http://papers.nips.cc/paper/7603-step-size-matters-in-deep-learning.pdf

- id: nardi_autoregressive_2011
  abstract: >-
    The Lasso is a popular model selection and estimation procedure for linear
    models that enjoys nice theoretical properties. In this paper, we study the
    Lasso estimator for fitting autoregressive time series models. We adopt a
    double asymptotic framework where the maximal lag may increase with the
    sample size. We derive theoretical results establishing various types of
    consistency. In particular, we derive conditions under which the Lasso
    estimator for the autoregressive coefficients is model selection consistent,
    estimation consistent and prediction consistent. Simulation study results
    are reported.
  author:
    - family: Nardi
      given: Y.
    - family: Rinaldo
      given: A.
  citation-key: nardi_autoregressive_2011
  container-title: Journal of Multivariate Analysis
  container-title-short: Journal of Multivariate Analysis
  DOI: 10.1016/j.jmva.2010.10.012
  ISSN: 0047-259X
  issue: '3'
  issued:
    - year: 2011
      month: 3
      day: 1
  page: 528-549
  source: ScienceDirect
  title: Autoregressive process modeling via the Lasso procedure
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0047259X10002186
  volume: '102'

- id: narendra_identification_1990
  author:
    - family: Narendra
      given: Kumpati S
    - family: Parthasarathy
      given: Kannan
  citation-key: narendra_identification_1990
  container-title: IEEE Transactions on Neural Networks
  DOI: 10.1109/72.80202
  issue: '1'
  issued:
    - year: 1990
  page: 4–27
  title: Identification and control of dynamical systems using neural networks
  type: article-journal
  volume: '1'

- id: nascimento_implementing_2019
  abstract: >-
    Ischaemic heart disease is the leading cause of death worldwide, with an
    increasing trend from 6.1 million deaths in 1990 to 9.5 million in 2016,
    markedly driven by rates observed in low/middle-income countries (LMIC).
    Improvements in myocardial infarction (MI) care are crucial for reducing
    premature mortality. We aimed to evaluate the main challenges for adequate
    MI care in LMIC, and possible strategies to overcome these existing
    barriers.Reperfusion is the cornerstone of MI treatment, but worldwide
    around 30% of patients are not reperfused, with even lower rates in LMIC.
    The main challenges are related to delays associated with patient education,
    late diagnosis and inadequate referral strategies, health infrastructure and
    insufficient funding. The implementation of regional MI systems of care in
    LMIC, systematising timely reperfusion strategies, access to intensive care,
    risk stratification and use of adjunctive medications have shown some
    successful strategies. Telemedicine support for remote ECG, diagnosis and
    organisation of referrals has proven to be useful, improving access to
    reperfusion even in prehospital settings. Organisation of transport and
    referral hubs based on anticipated delays and development of MI excellence
    centres have also resulted in better equality of care. Also, education of
    healthcare staff and task shifting may potentially widen access to optimal
    therapy.In conclusion, efforts have been made for the implementation of MI
    systems of care in LMIC, aiming to address particularities of the health
    systems. However, the increasing impact of MI in these countries urges the
    development of further strategies to improve reperfusion and reduce system
    delays.
  author:
    - family: Nascimento
      given: Bruno R
    - family: Brant
      given: Luisa C Caldeira
    - family: Marino
      given: Bárbara C A
    - family: Passaglia
      given: Luiz Guilherme
    - family: Ribeiro
      given: Antonio Luiz P
  citation-key: nascimento_implementing_2019
  container-title: Heart
  container-title-short: Heart
  DOI: 10/gfchxp
  issue: '1'
  issued:
    - year: 2019
      month: 1
      day: 1
  page: '20'
  title: >-
    Implementing myocardial infarction systems of care in low/middle-income
    countries
  type: article-journal
  URL: http://heart.bmj.com/content/105/1/20.abstract
  volume: '105'

- id: nash_newtontype_1984
  author:
    - family: Nash
      given: Stephen G.
  citation-key: nash_newtontype_1984
  container-title: SIAM Journal on Numerical Analysis
  DOI: 10.1137/0721052
  issue: '4'
  issued:
    - year: 1984
  page: 770–788
  source: Google Scholar
  title: Newton-type minimization via the Lanczos method
  type: article-journal
  volume: '21'

- id: nasrabadi_hopfield_1992
  author:
    - family: Nasrabadi
      given: Nasser M
    - family: Choo
      given: Chang Y
  citation-key: nasrabadi_hopfield_1992
  container-title: Neural Networks, IEEE Transactions on
  DOI: 10.1109/72.105413
  issue: '1'
  issued:
    - year: 1992
  page: 5–13
  title: Hopfield network for stereo vision correspondence
  type: article-journal
  volume: '3'

- id: naylorc_prospects_2018
  abstract: >-
    In 1976, Maxmen1 predicted that artificial intelligence (AI) in the 21st
    century would usher in “the post-physician era,” with health care provided
    by paramedics and computers. Today, the mass extinction of physicians
    remains unlikely. However, as outlined by Hinton2 in a related Viewpoint,
    the emergence of a radically different approach to AI, called deep learning,
    has the potential to effect major changes in clinical medicine and health
    care delivery. This Viewpoint reviews some of the factors driving wide
    adoption of deep learning and other forms of machine learning in the health
    ecosystem.
  author:
    - literal: Naylor C
  citation-key: naylorc_prospects_2018
  container-title: JAMA
  container-title-short: JAMA
  DOI: 10.1001/jama.2018.11103
  ISSN: 0098-7484
  issue: '11'
  issued:
    - year: 2018
      month: 9
      day: 18
  page: 1099-1100
  title: On the prospects for a (deep) learning health care system
  type: article-journal
  URL: http://dx.doi.org/10.1001/jama.2018.11103
  volume: '320'

- id: neal_bayesian_1995
  author:
    - family: Neal
      given: Radford M
  citation-key: neal_bayesian_1995
  issued:
    - year: 1995
  title: Bayesian learning for neural networks
  type: book
  volume: '118'

- id: neal_mcmc_2011
  author:
    - family: Neal
      given: Radford M.
  citation-key: neal_mcmc_2011
  container-title: Handbook of Markov Chain Monte Carlo
  DOI: 10.1201/b10905-6
  issue: '11'
  issued:
    - year: 2011
  page: '2'
  source: Google Scholar
  title: MCMC using Hamiltonian dynamics
  type: article-journal
  volume: '2'

- id: neimark_cases_1959
  author:
    - family: Neimark
      given: Ju
  citation-key: neimark_cases_1959
  event-title: Dokl. Akad. Nauk SSSR
  issued:
    - year: 1959
  page: 736-739
  title: On some cases of periodic motions depending on parameters
  type: paper-conference
  volume: '129'

- id: nelder_simplex_1965
  author:
    - family: Nelder
      given: John A
    - family: Mead
      given: Roger
  citation-key: nelder_simplex_1965
  container-title: The computer journal
  container-title-short: The computer journal
  DOI: 10.1093/comjnl/7.4.308
  ISSN: 0010-4620
  issue: '4'
  issued:
    - year: 1965
  page: 308-313
  title: A simplex method for function minimization
  type: article-journal
  volume: '7'

- id: nelles_nonlinear_2013
  author:
    - family: Nelles
      given: Oliver
  citation-key: nelles_nonlinear_2013
  ISBN: 3-662-04323-8
  issued:
    - year: 2013
  publisher: Springer Science & Business Media
  title: >-
    Nonlinear system identification: from classical approaches to neural
    networks and fuzzy models
  type: book

- id: nelles_nonlinear_2013a
  author:
    - family: Nelles
      given: Oliver
  citation-key: nelles_nonlinear_2013a
  ISBN: 3-662-04323-8
  issued:
    - year: 2013
  publisher: Springer Science & Business Media
  title: >-
    Nonlinear System Identification: From Classical Approaches to Neural
    Networks and Fuzzy Models
  type: book

- id: nesterov_introductory_1998
  abstract: 1.1.1 General formulation of the problem................... 9
  author:
    - family: Nesterov
      given: Yu
  citation-key: nesterov_introductory_1998
  issued:
    - year: 1998
  publisher: Springer Science & Business Media
  source: CiteSeer
  title: Introductory Lectures On Convex Programming
  type: book

- id: newey_large_1994
  abstract: 'null'
  author:
    - family: Newey
      given: Whitney K.
    - family: Mcfadden
      given: Daniel
  citation-key: newey_large_1994
  container-title: of Handbook of Econometrics
  issued:
    - year: 1994
  page: '2111'
  source: CiteSeer
  title: Large sample estimation and hypothesis testing
  type: chapter

- id: newey_large_1994a
  author:
    - family: Newey
      given: Whitney K.
    - family: McFadden
      given: Daniel
  citation-key: newey_large_1994a
  container-title: Handbook of econometrics
  DOI: 10.1016/S1573-4412(05)80005-4
  issued:
    - year: 1994
  page: 2111–2245
  source: Google Scholar
  title: Large sample estimation and hypothesis testing
  type: article-journal
  volume: '4'

- id: ng_cs230_2018
  abstract: >-
    Short tutorial detailing the best practices to split your dataset into
    train, dev and test sets
  accessed:
    - year: 2019
      month: 7
      day: 18
  author:
    - family: Ng
      given: Andrew Y.
    - family: Katanforoosh
      given: Kian
  citation-key: ng_cs230_2018
  container-title: CS230 Deep Learning
  issued:
    - year: 2018
      month: 1
      day: 24
  language: en
  title: CS230 Deep Learning (Stanford)
  type: webpage
  URL: https://cs230-stanford.github.io/train-dev-test-split.html

- id: ng_discriminative_2002
  author:
    - family: Ng
      given: Andrew Y.
    - family: Jordan
      given: Michael I.
  citation-key: ng_discriminative_2002
  container-title: Advances in neural information processing systems
  issued:
    - year: 2002
  page: 841–848
  source: Google Scholar
  title: >-
    On discriminative vs. generative classifiers: A comparison of logistic
    regression and naive bayes
  title-short: On discriminative vs. generative classifiers
  type: paper-conference

- id: ng_practical_
  abstract: >-
    Learn online and earn valuable credentials from top universities like Yale,
    Michigan, Stanford, and leading companies like Google and IBM. Join Coursera
    for free and transform your career with degrees, certificates,
    Specializations, & MOOCs in data science, computer science, business, and
    dozens of other topics.
  accessed:
    - year: 2019
      month: 7
      day: 18
  author:
    - family: Ng
      given: Andrew Y.
  citation-key: ng_practical_
  container-title: Coursera
  language: pt
  title: Practical aspects of Deep Learning (Coursera)
  type: webpage
  URL: >-
    https://www.coursera.org/lecture/deep-neural-network/train-dev-test-sets-cxG1s

- id: NIPS2014_ede7e2b6
  author:
    - family: Defazio
      given: Aaron
    - family: Bach
      given: Francis
    - family: Lacoste-Julien
      given: Simon
  citation-key: NIPS2014_ede7e2b6
  container-title: Advances in neural information processing systems
  editor:
    - family: Ghahramani
      given: Z.
    - family: Welling
      given: M.
    - family: Cortes
      given: C.
    - family: Lawrence
      given: N.
    - family: Weinberger
      given: K.Q.
  issued:
    - year: 2014
  publisher: Curran Associates, Inc.
  title: >-
    SAGA: A fast incremental gradient method with support for non-strongly
    convex composite objectives
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper_files/paper/2014/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf
  volume: '27'

- id: nocedal_interior_2014
  author:
    - family: Nocedal
      given: Jorge
    - family: Öztoprak
      given: Figen
    - family: Waltz
      given: Richard A
  citation-key: nocedal_interior_2014
  container-title: Optimization Methods and Software
  DOI: 10/gfjwmn
  ISSN: 1055-6788
  issue: '4'
  issued:
    - year: 2014
  page: 837-854
  title: >-
    An Interior Point Method for Nonlinear Programming with Infeasibility
    Detection Capabilities
  type: article-journal
  volume: '29'

- id: nocedal_interior_2014a
  author:
    - family: Nocedal
      given: Jorge
    - family: Öztoprak
      given: Figen
    - family: Waltz
      given: Richard A
  citation-key: nocedal_interior_2014a
  container-title: Optimization Methods and Software
  container-title-short: Optimization Methods and Software
  DOI: 10.1080/10556788.2013.858156
  ISSN: 1055-6788
  issue: '4'
  issued:
    - year: 2014
  page: 837-854
  title: >-
    An interior point method for nonlinear programming with infeasibility
    detection capabilities
  type: article-journal
  volume: '29'

- id: nocedal_numerical_2006
  author:
    - family: Nocedal
      given: Jorge
    - family: Wright
      given: Stephen J.
  call-number: QA402.5 .N62 2006
  citation-key: nocedal_numerical_2006
  collection-title: Springer series in operations research
  edition: 2nd ed
  event-place: New York
  ISBN: 978-0-387-30303-1
  issued:
    - year: 2006
  note: 'OCLC: ocm68629100'
  number-of-pages: '664'
  publisher: Springer
  publisher-place: New York
  source: Library of Congress ISBN
  title: Numerical Optimization
  type: book

- id: noel_f16_
  author:
    - family: Noel
      given: J P
    - family: Schoukens
      given: M
  citation-key: noel_f16_
  language: en
  page: '5'
  source: Zotero
  title: F-16 aircraft benchmark based on ground vibration test data
  type: article-journal

- id: noel_greybox_2017
  author:
    - family: Noël
      given: Jean-Philippe
    - family: Schoukens
      given: Johan
  citation-key: noel_greybox_2017
  container-title: International Journal of Control
  container-title-short: International Journal of Control
  ISSN: 0020-7179
  issued:
    - year: 2017
  page: 1-22
  title: Grey-box state-space identification of nonlinear mechanical vibrations
  type: article-journal

- id: noel_greybox_2018
  abstract: >-
    The present paper deals with the identification of nonlinear mechanical
    vibrations. A grey-box, or semi-physical, nonlinear state-space
    representation is introduced, expressing the nonlinear basis functions using
    a limited number of measured output variables. This representation assumes
    that the observed nonlinearities are localised in physical space, which is a
    generic case in mechanics. A two-step identification procedure is derived
    for the grey-box model parameters, integrating nonlinear subspace
    initialisation and weighted least-squares optimisation. The complete
    procedure is applied to an electrical circuit mimicking the behaviour of a
    single–input, single–output (SISO) nonlinear mechanical system and to a
    single–input, multiple–output (SIMO) geometrically nonlinear beam structure.
  accessed:
    - year: 2019
      month: 4
      day: 15
  author:
    - family: Noël
      given: J. P.
    - family: Schoukens
      given: J.
  citation-key: noel_greybox_2018
  container-title: International Journal of Control
  DOI: 10/gfzgtg
  ISSN: 0020-7179
  issue: '5'
  issued:
    - year: 2018
      month: 5
      day: 4
  page: 1118-1139
  source: Taylor and Francis+NEJM
  title: Grey-box state-space identification of nonlinear mechanical vibrations
  type: article-journal
  URL: https://doi.org/10.1080/00207179.2017.1308557
  volume: '91'

- id: noel_hysteretic_2016
  accessed:
    - year: 2017
      month: 8
      day: 28
  author:
    - family: Noël
      given: J. P.
    - family: Schoukens
      given: M.
  citation-key: noel_hysteretic_2016
  container-title: Brussels, Belgium
  issued:
    - year: 2016
  source: Google Scholar
  title: Hysteretic benchmark with a dynamic nonlinearity
  type: article-journal
  URL: >-
    https://www.researchgate.net/profile/Maarten_Schoukens/publication/307569412_Hysteretic_benchmark_with_a_dynamic_nonlinearity/links/57c92fbe08ae9d640483eda0.pdf

- id: noel_nonlinear_2017
  author:
    - family: Noël
      given: Jean-Philippe
    - family: Esfahani
      given: A Fakhrizadeh
    - family: Kerschen
      given: Gaetan
    - family: Schoukens
      given: Johan
  citation-key: noel_nonlinear_2017
  container-title: Mechanical Systems and Signal Processing
  container-title-short: Mechanical Systems and Signal Processing
  DOI: 10.1016/j.ymssp.2016.08.025
  ISSN: 0888-3270
  issued:
    - year: 2017
  page: 171-184
  title: A nonlinear state-space approach to hysteresis identification
  type: article-journal
  volume: '84'

- id: noel_nonlinear_2017a
  abstract: >-
    Nonlinear system identification is a vast research field, today attracting a
    great deal of attention in the structural dynamics community. Ten years ago,
    an MSSP paper reviewing the progress achieved until then [1] concluded that
    the identification of simple continuous structures with localised
    nonlinearities was within reach. The past decade witnessed a shift in
    emphasis, accommodating the growing industrial need for a first generation
    of tools capable of addressing complex nonlinearities in larger-scale
    structures. The objective of the present paper is to survey the key
    developments which arose in the field since 2006, and to illustrate
    state-of-the-art techniques using a real-world satellite structure. Finally,
    a broader perspective to nonlinear system identification is provided by
    discussing the central role played by experimental models in the design
    cycle of engineering structures.
  author:
    - family: Noël
      given: J. P.
    - family: Kerschen
      given: G.
  citation-key: noel_nonlinear_2017a
  container-title: Mechanical Systems and Signal Processing
  container-title-short: Mechanical Systems and Signal Processing
  DOI: 10.1016/j.ymssp.2016.07.020
  ISSN: 0888-3270
  issued:
    - year: 2017
      month: 1
      day: 15
  page: 2-35
  source: ScienceDirect
  title: >-
    Nonlinear system identification in structural dynamics: 10 more years of
    progress
  title-short: Nonlinear system identification in structural dynamics
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S088832701630245X
  volume: '83'

- id: norgaard_neural_2000
  author:
    - family: Nørgaard
      given: Peter Magnus
    - family: Ravn
      given: Ole
    - family: Poulsen
      given: Niels Kjølstad
    - family: Hansen
      given: Lars Kai
  citation-key: norgaard_neural_2000
  issued:
    - year: 2000
  publisher: Springer-London
  title: >-
    Neural Networks for Modelling and Control of Dynamic Systems-A
    Practitioner's Handbook
  type: book

- id: normey-rico_control_2007
  author:
    - family: Normey-Rico
      given: J. E.
    - family: Camacho
      given: E. F.
  call-number: TJ213 .N58 2007
  citation-key: normey-rico_control_2007
  collection-title: Advanced textbooks in control and signal processing
  event-place: London
  ISBN: 978-1-84628-828-9 978-1-84628-829-6
  issued:
    - year: 2007
  note: 'OCLC: ocm82672359'
  number-of-pages: '462'
  publisher: Springer
  publisher-place: London
  source: Library of Congress ISBN
  title: Control of dead-time processes
  type: book

- id: norton_introduction_2009
  author:
    - family: Norton
      given: John P
  citation-key: norton_introduction_2009
  issued:
    - year: 2009
  publisher: Courier Corporation
  title: An Introduction to Identification
  type: book

- id: noseworthy_artificial_2022
  abstract: >-
    Summary Background Previous atrial fibrillation screening trials have
    highlighted the need for more targeted approaches. We did a pragmatic study
    to evaluate the effectiveness of an artificial intelligence (AI)
    algorithm-guided targeted screening approach for identifying previously
    unrecognised atrial fibrillation. Methods For this non-randomised
    interventional trial, we prospectively recruited patients with stroke risk
    factors but with no known atrial fibrillation who had an electrocardiogram
    (ECG) done in routine practice. Participants wore a continuous ambulatory
    heart rhythm monitor for up to 30 days, with the data transmitted in near
    real time through a cellular connection. The AI algorithm was applied to the
    ECGs to divide patients into high-risk or low-risk groups. The primary
    outcome was newly diagnosed atrial fibrillation. In a secondary analysis,
    trial participants were propensity-score matched (1:1) to individuals from
    the eligible but unenrolled population who served as real-world controls.
    This study is registered with ClinicalTrials.gov, NCT04208971. Findings 1003
    patients with a mean age of 74 years (SD 8·8) from 40 US states completed
    the study. Over a mean 22·3 days of continuous monitoring, atrial
    fibrillation was detected in six (1·6%) of 370 patients with low risk and 48
    (7·6%) of 633 with high risk (odds ratio 4·98, 95% CI 2·11–11·75, p=0·0002).
    Compared with usual care, AI-guided screening was associated with increased
    detection of atrial fibrillation (high-risk group: 3·6% [95% CI 2·3–5·4]
    with usual care vs 10·6% [8·3–13·2] with AI-guided screening, p¡0·0001;
    low-risk group: 0·9% vs 2·4%, p=0·12) over a median follow-up of 9·9 months
    (IQR 7·1–11·0). Interpretation An AI-guided targeted screening approach that
    leverages existing clinical data increased the yield for atrial fibrillation
    detection and could improve the effectiveness of atrial fibrillation
    screening. Funding Mayo Clinic Robert D and Patricia E Kern Center for the
    Science of Health Care Delivery.
  author:
    - family: Noseworthy
      given: Peter A
    - family: Attia
      given: Zachi I
    - family: Behnken
      given: Emma M
    - family: Giblon
      given: Rachel E
    - family: Bews
      given: Katherine A
    - family: Liu
      given: Sijia
    - family: Gosse
      given: Tara A
    - family: Linn
      given: Zachery D
    - family: Deng
      given: Yihong
    - family: Yin
      given: Jun
    - family: Gersh
      given: Bernard J
    - family: Graff-Radford
      given: Jonathan
    - family: Rabinstein
      given: Alejandro A
    - family: Siontis
      given: Konstantinos C
    - family: Friedman
      given: Paul A
    - family: Yao
      given: Xiaoxi
  citation-key: noseworthy_artificial_2022
  container-title: The Lancet
  DOI: https://doi.org/10.1016/S0140-6736(22)01637-3
  ISSN: 0140-6736
  issue: '10359'
  issued:
    - year: 2022
  page: 1206-1212
  title: >-
    Artificial intelligence-guided screening for atrial fibrillation using
    electrocardiogram during sinus rhythm: a prospective non-randomised
    interventional trial
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0140673622016373
  volume: '400'

- id: novak_bayesian_2018
  abstract: >-
    There is a previously identified equivalence between wide fully connected
    neural networks (FCNs) and Gaussian processes (GPs). This equivalence
    enables, for instance, test set predictions that would have resulted from a
    fully Bayesian, infinitely wide trained FCN to be computed without ever
    instantiating the FCN, but by instead evaluating the corresponding GP. In
    this work, we derive an analogous equivalence for multi-layer convolutional
    neural networks (CNNs) both with and without pooling layers, and achieve
    state of the art results on CIFAR10 for GPs without trainable kernels. We
    also introduce a Monte Carlo method to estimate the GP corresponding to a
    given neural network architecture, even in cases where the analytic form has
    too many terms to be computationally feasible. Surprisingly, in the absence
    of pooling layers, the GPs corresponding to CNNs with and without weight
    sharing are identical. As a consequence, translation equivariance in
    finite-channel CNNs trained with stochastic gradient descent (SGD) has no
    corresponding property in the Bayesian treatment of the infinite channel
    limit - a qualitative difference between the two regimes that is not present
    in the FCN case. We confirm experimentally, that while in some scenarios the
    performance of SGD-trained finite CNNs approaches that of the corresponding
    GPs as the channel count increases, with careful tuning SGD-trained CNNs can
    significantly outperform their corresponding GPs, suggesting advantages from
    SGD training compared to fully Bayesian parameter estimation.
  accessed:
    - year: 2018
      month: 10
      day: 24
  author:
    - family: Novak
      given: Roman
    - family: Xiao
      given: Lechao
    - family: Lee
      given: Jaehoon
    - family: Bahri
      given: Yasaman
    - family: Abolafia
      given: Daniel A.
    - family: Pennington
      given: Jeffrey
    - family: Sohl-Dickstein
      given: Jascha
  citation-key: novak_bayesian_2018
  container-title: arXiv:1810.05148 [cs, stat]
  issued:
    - year: 2018
      month: 10
      day: 11
  source: arXiv.org
  title: >-
    Bayesian Convolutional Neural Networks with Many Channels are Gaussian
    Processes
  type: article-journal
  URL: http://arxiv.org/abs/1810.05148

- id: novak_fast_2022
  abstract: >-
    The Neural Tangent Kernel (NTK), defined as $\Theta_\theta^f(x_1, x_2) =
    \left[\partial f(\theta, x_1)\big/\partial \theta\right] \left[\partial
    f(\theta, x_2)\big/\partial \theta\right]^T$ where $\left[\partial f(\theta,
    \cdot)\big/\partial \theta\right]$ is a neural network (NN) Jacobian, has
    emerged as a central object of study in deep learning. In the infinite width
    limit, the NTK can sometimes be computed analytically and is useful for
    understanding training and generalization of NN architectures. At finite
    widths, the NTK is also used to better initialize NNs, compare the
    conditioning across models, perform architecture search, and do
    meta-learning. Unfortunately, the finite width NTK is notoriously expensive
    to compute, which severely limits its practical utility. We perform the
    first in-depth analysis of the compute and memory requirements for NTK
    computation in finite width networks. Leveraging the structure of neural
    networks, we further propose two novel algorithms that change the exponent
    of the compute and memory requirements of the finite width NTK, dramatically
    improving efficiency. Our algorithms can be applied in a black box fashion
    to any differentiable function, including those implementing neural
    networks. We open-source our implementations within the Neural Tangents
    package (arXiv:1912.02803) at https://github.com/google/neural-tangents.
  accessed:
    - year: 2022
      month: 7
      day: 26
  author:
    - family: Novak
      given: Roman
    - family: Sohl-Dickstein
      given: Jascha
    - family: Schoenholz
      given: Samuel S.
  citation-key: novak_fast_2022
  issued:
    - year: 2022
      month: 6
      day: 17
  number: arXiv:2206.08720
  publisher: arXiv
  source: arXiv.org
  title: Fast Finite Width Neural Tangent Kernel
  type: article
  URL: http://arxiv.org/abs/2206.08720

- id: novak_neural_2019
  abstract: >-
    Neural Tangents is a library designed to enable research into infinite-width
    neural networks. It provides a high-level API for specifying complex and
    hierarchical neural network architectures. These networks can then be
    trained and evaluated either at finite-width as usual or in their
    infinite-width limit. Infinite-width networks can be trained analytically
    using exact Bayesian inference or using gradient descent via the Neural
    Tangent Kernel. Additionally, Neural Tangents provides tools to study
    gradient descent training dynamics of wide but finite networks in either
    function space or weight space. The entire library runs out-of-the-box on
    CPU, GPU, or TPU. All computations can be automatically distributed over
    multiple accelerators with near-linear scaling in the number of devices.
    Neural Tangents is available at www.github.com/google/neural-tangents. We
    also provide an accompanying interactive Colab notebook.
  accessed:
    - year: 2021
      month: 11
      day: 22
  author:
    - family: Novak
      given: Roman
    - family: Xiao
      given: Lechao
    - family: Hron
      given: Jiri
    - family: Lee
      given: Jaehoon
    - family: Alemi
      given: Alexander A.
    - family: Sohl-Dickstein
      given: Jascha
    - family: Schoenholz
      given: Samuel S.
  citation-key: novak_neural_2019
  container-title: arXiv:1912.02803 [cs, stat]
  issued:
    - year: 2019
      month: 12
      day: 5
  source: arXiv.org
  title: 'Neural Tangents: Fast and Easy Infinite Neural Networks in Python'
  title-short: Neural Tangents
  type: article-journal
  URL: http://arxiv.org/abs/1912.02803

- id: novak_nonlinear_2010
  abstract: >-
    In this paper, we propose a method for nonlinear system (NLS) identification
    using a swept-sine input signal and based on nonlinear convolution. The
    method uses a nonlinear model, namely, the nonparametric generalized
    polynomial Hammerstein model made of power series associated with linear
    filters. Simulation results show that the method identifies the nonlinear
    model of the system under test and estimates the linear filters of the
    unknown NLS. The method has also been tested on a real-world system: an
    audio limiter. Once the nonlinear model of the limiter is identified, a test
    signal can be regenerated to compare the outputs of both the real-world
    system and its nonlinear model. The results show good agreement between both
    model-based and real-world system outputs.
  author:
    - family: Novak
      given: A.
    - family: Simon
      given: L.
    - family: Kadlec
      given: F.
    - family: Lotton
      given: P.
  citation-key: novak_nonlinear_2010
  container-title: IEEE Transactions on Instrumentation and Measurement
  DOI: 10.1109/TIM.2009.2031836
  ISSN: 0018-9456
  issue: '8'
  issued:
    - year: 2010
      month: 8
  page: 2220-2229
  source: IEEE Xplore
  title: Nonlinear System Identification Using Exponential Swept-Sine Signal
  type: article-journal
  volume: '59'

- id: nowak_fused_2011
  abstract: >-
    Array-based comparative genomic hybridization (aCGH) enables the measurement
    of DNA copy number across thousands of locations in a genome. The main goals
    of analyzing aCGH data are to identify the regions of copy number variation
    (CNV) and to quantify the amount of CNV. Although there are many methods for
    analyzing single-sample aCGH data, the analysis of multi-sample aCGH data is
    a relatively new area of research. Further, many of the current approaches
    for analyzing multi-sample aCGH data do not appropriately utilize the
    additional information present in the multiple samples. We propose a
    procedure called the Fused Lasso Latent Feature Model (FLLat) that provides
    a statistical framework for modeling multi-sample aCGH data and identifying
    regions of CNV. The procedure involves modeling each sample of aCGH data as
    a weighted sum of a fixed number of features. Regions of CNV are then
    identified through an application of the fused lasso penalty to each
    feature. Some simulation analyses show that FLLat outperforms single-sample
    methods when the simulated samples share common information. We also propose
    a method for estimating the false discovery rate. An analysis of an aCGH
    data set obtained from human breast tumors, focusing on chromosomes 8 and
    17, shows that FLLat and Significance Testing of Aberrant Copy number (an
    alternative, existing approach) identify similar regions of CNV that are
    consistent with previous findings. However, through the estimated features
    and their corresponding weights, FLLat is further able to discern specific
    relationships between the samples, for example, identifying 3 distinct
    groups of samples based on their patterns of CNV for chromosome 17.
  accessed:
    - year: 2017
      month: 9
      day: 18
  author:
    - family: Nowak
      given: Gen
    - family: Hastie
      given: Trevor
    - family: Pollack
      given: Jonathan R.
    - family: Tibshirani
      given: Robert
  citation-key: nowak_fused_2011
  container-title: Biostatistics
  container-title-short: Biostatistics
  DOI: 10.1093/biostatistics/kxr012
  ISSN: 1465-4644
  issue: '4'
  issued:
    - year: 2011
      month: 10
      day: 1
  page: 776-791
  source: academic.oup.com
  title: A fused lasso latent feature model for analyzing multi-sample aCGH data
  type: article-journal
  URL: >-
    https://academic.oup.com/biostatistics/article/12/4/776/249257/A-fused-lasso-latent-feature-model-for-analyzing
  volume: '12'

- id: nunes_chagas_2013
  abstract: >-
    Chagas disease, caused by the parasite Trypanosoma cruzi, is a serious
    health problem in Latin America and is an emerging disease in non-endemic
    countries. In recent decades, the epidemiological profile of the disease has
    changed due to new patterns of immigration and successful control in its
    transmission, leading to the urbanization and globalization of the disease.
    Dilated cardiomyopathy is the most important and severe manifestation of
    human chronic Chagas disease and is characterized by heart failure,
    ventricular arrhythmias, heart blocks, thromboembolic phenomena, and sudden
    death. This article will present an overview of the clinical and
    epidemiological aspects of Chagas disease. It will focus on several clinical
    aspects of the disease, such as chronic Chagas disease without detectable
    cardiac pathology, as well as dysautonomia, some specific features, and the
    principles of treatment of chronic cardiomyopathy.
  author:
    - family: Nunes
      given: Maria Carmo Pereira
    - family: Dones
      given: Wistremundo
    - family: Morillo
      given: Carlos A.
    - family: Encina
      given: Juan Justiniano
    - family: Ribeiro
      given: Antônio Luiz
    - literal: Council on Chagas Disease of the Interamerican Society of Cardiology
  citation-key: nunes_chagas_2013
  container-title: Journal of the American College of Cardiology
  container-title-short: J Am Coll Cardiol
  DOI: 10.1016/j.jacc.2013.05.046
  ISSN: 1558-3597
  issue: '9'
  issued:
    - year: 2013
      month: 8
      day: 27
  language: eng
  page: 767-776
  PMID: '23770163'
  source: PubMed
  title: 'Chagas disease: an overview of clinical and epidemiological aspects'
  title-short: Chagas disease
  type: article-journal
  volume: '62'

- id: nunes_incidence_2021
  abstract: >-
    BACKGROUND: There are few contemporary cohorts of Trypanosoma
    cruzi-seropositive individuals, and the basic clinical epidemiology of
    Chagas disease is poorly understood. Herein, we report the incidence of
    cardiomyopathy and death associated with T. cruzi seropositivity.

    METHODS: Participants were selected in blood banks at 2 Brazilian centers.
    Cases were defined as T. cruzi-seropositive blood donors. T.
    cruzi-seronegative controls were matched for age, sex, and period of
    donation. Patients with established Chagas cardiomyopathy were recruited
    from a tertiary outpatient service. Participants underwent medical
    examination, blood collection, ECG, and echocardiogram at enrollment
    (2008-2010) and at follow-up (2018-2019). The primary outcomes were
    all-cause mortality and development of cardiomyopathy, defined as the
    presence of a left ventricular ejection fraction <50% or QRS complex
    duration ≥120 ms, or both. To handle loss to follow-up, a sensitivity
    analysis was performed using inverse probability weights for selection.

    RESULTS: We enrolled 499 T. cruzi-seropositive donors (age 48±10 years, 52%
    male), 488 T. cruzi-seronegative donors (age 49±10 years, 49% male), and 101
    patients with established Chagas cardiomyopathy (age 48±8 years, 59% male).
    The mortality in patients with established cardiomyopathy was 80.9
    deaths/1000 person-years (py) (54/101, 53%) and 15.1 deaths/1000 py (17/114,
    15%) in T. cruzi-seropositive donors with cardiomyopathy at baseline. Among
    T. cruzi-seropositive donors without cardiomyopathy at baseline, mortality
    was 3.7 events/1000 py (15/385, 4%), which was no different from T.
    cruzi-seronegative donors with 3.6 deaths/1000 py (17/488, 3%). The
    incidence of cardiomyopathy in T. cruzi-seropositive donors was 13.8 (95%
    CI, 9.5-19.6) events/1000 py (32/262, 12%) compared with 4.6 (95% CI,
    2.3-8.3) events/1000 py (11/277, 4%) in seronegative controls, with an
    absolute incidence difference associated with T. cruzi seropositivity of 9.2
    (95% CI, 3.6-15.0) events/1000 py. T. cruzi antibody level at baseline was
    associated with development of cardiomyopathy (adjusted odds ratio, 1.4
    \[95% CI, 1.1-1.8]).

    CONCLUSIONS: We present a comprehensive description of the natural history
    of T. cruzi seropositivity in a contemporary patient population. The results
    highlight the central importance of anti-T. cruzi antibody titer as a marker
    of Chagas disease activity and risk of progression.
  author:
    - family: Nunes
      given: Maria Carmo P.
    - family: Buss
      given: Lewis F.
    - family: Silva
      given: Jose Luiz P.
    - family: Martins
      given: Larissa Natany A.
    - family: Oliveira
      given: Claudia Di Lorenzo
    - family: Cardoso
      given: Clareci Silva
    - family: Brito
      given: Bruno Oliveira de Figueiredo
    - family: Ferreira
      given: Ariela Mota
    - family: Oliveira
      given: Lea Campos
    - family: Bierrenbach
      given: Ana Luiza
    - family: Fernandes
      given: Fabio
    - family: Busch
      given: Michael P.
    - family: Hotta
      given: Viviane Tiemi
    - family: Martinelli
      given: Luiz Mario Baptista
    - family: Soeiro
      given: Maria Carolina F. Almeida
    - family: Brentegani
      given: Adriana
    - family: Salemi
      given: Vera M. C.
    - family: Menezes
      given: Marcia M.
    - family: Ribeiro
      given: Antonio Luiz P.
    - family: Sabino
      given: Ester Cerdeira
  citation-key: nunes_incidence_2021
  container-title: Circulation
  container-title-short: Circulation
  DOI: 10.1161/CIRCULATIONAHA.121.055112
  ISSN: 1524-4539
  issue: '19'
  issued:
    - year: 2021
      month: 11
      day: 9
  language: eng
  page: 1553-1566
  PMCID: PMC8578457
  PMID: '34565171'
  source: PubMed
  title: >-
    Incidence and Predictors of Progression to Chagas Cardiomyopathy: Long-Term
    Follow-Up of Trypanosoma cruzi-Seropositive Individuals
  title-short: Incidence and Predictors of Progression to Chagas Cardiomyopathy
  type: article-journal
  volume: '144'

- id: nyquist_certain_1928
  author:
    - family: Nyquist
      given: Harry
  citation-key: nyquist_certain_1928
  container-title: Transactions of the American Institute of Electrical Engineers
  issue: '2'
  issued:
    - year: 1928
  page: 617-644
  title: Certain topics in telegraph transmission theory
  type: article-journal
  volume: '47'

- id: ogata_modern_2010
  author:
    - family: Ogata
      given: Katsuhiko
  call-number: TJ213 .O28 2010
  citation-key: ogata_modern_2010
  collection-title: >-
    Prentice-Hall electrical engineering series. Instrumentation and controls
    series
  edition: 5th ed
  event-place: Boston
  ISBN: 978-0-13-615673-4
  issued:
    - year: 2010
  number-of-pages: '894'
  publisher: Prentice-Hall
  publisher-place: Boston
  source: Library of Congress ISBN
  title: Modern control engineering
  type: book

- id: oh_new_2004
  author:
    - family: Oh
      given: Sungho
  citation-key: oh_new_2004
  genre: PhD Thesis
  issued:
    - year: 2004
  publisher: University of Florida
  source: Google Scholar
  title: A new quality measure in electrocardiogram signal
  type: thesis

- id: okutomi_multiplebaseline_1993
  author:
    - family: Okutomi
      given: Masatoshi
    - family: Kanade
      given: Takeo
  citation-key: okutomi_multiplebaseline_1993
  container-title: Pattern Analysis and Machine Intelligence, IEEE Transactions on
  DOI: 10.1109/34.206955
  issue: '4'
  issued:
    - year: 1993
  page: 353–363
  title: A multiple-baseline stereo
  type: article-journal
  volume: '15'

- id: oliveira_explaining_2020
  abstract: >-
    In this work, we present a method to explain “end-toend” electrocardiogram
    (ECG) signal classiﬁers, where the explanations were built along with
    seniors cardiologist to provide meaningful features to the ﬁnal users. Our
    method focuses exclusively on automated ECG diagnosis and analyzes the
    explanation in terms of clinical accuracy for interpretability and
    robustness. The proposed method uses a noise-insertion strategy to quantify
    the impact of intervals and segments of the ECG signals on the automated
    classiﬁcation outcome. An ECG segmentation method was applied to ECG
    tracings, to obtain: (1) Intervals, Segments and Axis; (2) Rate, and (3)
    Rhythm. Noise was added to the signal to disturb the ECG features in a
    realistic way. The method was tested using Monte Carlo simulation and the
    feature impact is estimated by the change in the model prediction averaged
    over 499 executions and a feature is deﬁned as important if its mean value
    changes the result of the classiﬁer. We demonstrate our method by explaining
    diagnoses generated by a deep convolutional neural network. The proposed
    method is particularly effective and useful for modern deep learning models
    that take raw data as input.
  author:
    - family: Oliveira
      given: Derick M
    - family: Ribeiro
      given: Antonio H
    - family: Pedrosa
      given: Joao A O
    - family: Paixao
      given: Gabriela M M
    - family: Ribeiro
      given: Antonio L
    - family: Jr
      given: Wagner Meira
  citation-key: oliveira_explaining_2020
  container-title: 2020 Computing in Cardiology (CinC)
  DOI: 10.22489/CinC.2020.452
  issued:
    - year: 2020
  license: All rights reserved
  title: >-
    Explaining black-box automated electrocardiogram classiﬁcation to
    cardiologists
  type: paper-conference
  volume: '47'

- id: oliveira_explaining_2020a
  author:
    - family: Oliveira
      given: Derick M.
    - family: Ribeiro
      given: Antônio H.
    - family: Pedrosa
      given: João A. O.
    - family: Paixao
      given: Gabriela M.M.
    - family: Ribeiro
      given: Antonio Luiz P.
    - family: Meira Jr
      given: Wagner
  citation-key: oliveira_explaining_2020a
  container-title: >-
    European Conference on Machine Learning and Principles and Practice of
    Knowledge Discovery in Databases (ECML-PKDD)
  DOI: 10.1007/978-3-030-67670-4_13
  event-place: Ghent, Belgium
  issued:
    - year: 2020
      month: 9
  page: 204--219
  publisher: Springer
  publisher-place: Ghent, Belgium
  title: Explaining end-to-end ECG automated diagnosis using contextual features
  type: paper-conference
  volume: '12461'

- id: oliver_realistic_2018
  abstract: >-
    Semi-supervised learning (SSL) provides a powerful framework for leveraging
    unlabeled data when labels are limited or expensive to obtain. SSL
    algorithms based on deep neural networks have recently proven successful on
    standard benchmark tasks. However, we argue that these benchmarks fail to
    address many issues that these algorithms would face in real-world
    applications. After creating a unified reimplementation of various
    widely-used SSL techniques, we test them in a suite of experiments designed
    to address these issues. We find that the performance of simple baselines
    which do not use unlabeled data is often underreported, that SSL methods
    differ in sensitivity to the amount of labeled and unlabeled data, and that
    performance can degrade substantially when the unlabeled dataset contains
    out-of-class examples. To help guide SSL research towards real-world
    applicability, we make our unified reimplemention and evaluation platform
    publicly available.
  accessed:
    - year: 2018
      month: 12
      day: 6
  author:
    - family: Oliver
      given: Avital
    - family: Odena
      given: Augustus
    - family: Raffel
      given: Colin
    - family: Cubuk
      given: Ekin D.
    - family: Goodfellow
      given: Ian J.
  citation-key: oliver_realistic_2018
  container-title: arXiv:1804.09170 [cs, stat]
  issued:
    - year: 2018
      month: 4
      day: 24
  source: arXiv.org
  title: Realistic Evaluation of Deep Semi-Supervised Learning Algorithms
  type: article-journal
  URL: http://arxiv.org/abs/1804.09170

- id: omnivision_ov9121_2003
  author:
    - literal: OmniVision
  citation-key: omnivision_ov9121_2003
  issued:
    - year: 2003
  title: OV9121 datasheet
  type: book

- id: oord_wavenet_2016
  abstract: >-
    This paper introduces WaveNet, a deep neural network for generating raw
    audio waveforms. The model is fully probabilistic and autoregressive, with
    the predictive distribution for each audio sample conditioned on all
    previous ones; nonetheless we show that it can be efficiently trained on
    data with tens of thousands of samples per second of audio. When applied to
    text-to-speech, it yields state-of-the-art performance, with human listeners
    rating it as significantly more natural sounding than the best parametric
    and concatenative systems for both English and Mandarin. A single WaveNet
    can capture the characteristics of many different speakers with equal
    fidelity, and can switch between them by conditioning on the speaker
    identity. When trained to model music, we find that it generates novel and
    often highly realistic musical fragments. We also show that it can be
    employed as a discriminative model, returning promising results for phoneme
    recognition.
  accessed:
    - year: 2019
      month: 3
      day: 13
  author:
    - family: Oord
      given: Aaron
      dropping-particle: van den
    - family: Dieleman
      given: Sander
    - family: Zen
      given: Heiga
    - family: Simonyan
      given: Karen
    - family: Vinyals
      given: Oriol
    - family: Graves
      given: Alex
    - family: Kalchbrenner
      given: Nal
    - family: Senior
      given: Andrew
    - family: Kavukcuoglu
      given: Koray
  citation-key: oord_wavenet_2016
  container-title: arXiv:1609.03499 [cs]
  issued:
    - year: 2016
      month: 9
      day: 12
  source: arXiv.org
  title: 'WaveNet: A Generative Model for Raw Audio'
  title-short: WaveNet
  type: article-journal
  URL: http://arxiv.org/abs/1609.03499

- id: oppenheim_discretetime_1999
  author:
    - family: Oppenheim
      given: Alan V
  citation-key: oppenheim_discretetime_1999
  issued:
    - year: 1999
  publisher: Pearson Education India
  title: Discrete-time signal processing
  type: book

- id: orfanidis_introduction_1996
  author:
    - family: Orfanidis
      given: Sophocles J.
  call-number: TK5102.9 .O73 1996
  citation-key: orfanidis_introduction_1996
  collection-title: Prentice Hall signal processing series
  event-place: Englewood Cliffs, N.J
  ISBN: 978-0-13-209172-5
  issued:
    - year: 1996
  number-of-pages: '798'
  publisher: Prentice Hall
  publisher-place: Englewood Cliffs, N.J
  source: Library of Congress ISBN
  title: Introduction to signal processing
  type: book

- id: orovic_compressive_2016
  abstract: >-
    Compressive sensing has emerged as an area that opens new perspectives in
    signal acquisition and processing. It appears as an alternative to the
    traditional sampling theory, endeavoring to reduce the required number of
    samples for successful signal reconstruction. In practice, compressive
    sensing aims to provide saving in sensing resources, transmission, and
    storage capacities and to facilitate signal processing in the circumstances
    when certain data are unavailable. To that end, compressive sensing relies
    on the mathematical algorithms solving the problem of data reconstruction
    from a greatly reduced number of measurements by exploring the properties of
    sparsity and incoherence. Therefore, this concept includes the optimization
    procedures aiming to provide the sparsest solution in a suitable
    representation domain. This work, therefore, offers a survey of the
    compressive sensing idea and prerequisites, together with the commonly used
    reconstruction methods. Moreover, the compressive sensing problem
    formulation is considered in signal processing applications assuming some of
    the commonly used transformation domains, namely, the Fourier transform
    domain, the polynomial Fourier transform domain, Hermite transform domain,
    and combined time-frequency domain.
  accessed:
    - year: 2020
      month: 7
      day: 16
  author:
    - family: Orović
      given: Irena
    - family: Papić
      given: Vladan
    - family: Ioana
      given: Cornel
    - family: Li
      given: Xiumei
    - family: Stanković
      given: Srdjan
  citation-key: orovic_compressive_2016
  container-title: Mathematical Problems in Engineering
  container-title-short: Mathematical Problems in Engineering
  DOI: 10.1155/2016/7616393
  ISSN: 1024-123X, 1563-5147
  issued:
    - year: 2016
  language: en
  page: 1-16
  source: DOI.org (Crossref)
  title: >-
    Compressive Sensing in Signal Processing: Algorithms and Transform Domain
    Formulations
  title-short: Compressive Sensing in Signal Processing
  type: article-journal
  URL: https://www.hindawi.com/journals/mpe/2016/7616393/
  volume: '2016'

- id: osama_inferring_2019
  abstract: >-
    We address the problem of inferring the causal effect of an exposure on an
    outcome across space, using observational data. The data is possibly subject
    to unmeasured confounding variables which, in a standard approach, must be
    adjusted for by estimating a nuisance function. Here we develop a method
    that eliminates the nuisance function, while mitigating the resulting
    errors-in-variables. The result is a robust and accurate inference method
    for spatially varying heterogeneous causal effects. The properties of the
    method are demonstrated on synthetic as well as real data from Germany and
    the US.
  author:
    - family: Osama
      given: Muhammad
    - family: Zachariah
      given: Dave
    - family: Schön
      given: Thomas B
  citation-key: osama_inferring_2019
  container-title: Proceedings of the 36th International Conference on Machine Learning,
  issued:
    - year: 2019
  source: Zotero
  title: Inferring Heterogeneous Causal Effects in Presence of Spatial Confounding
  type: paper-conference

- id: osborne_new_2000
  abstract: >-
    The title Lasso has been suggested by Tibshirani (1996) as a colourful name
    for a technique of variable selection which requires the minimization of a
    sum of squares subject to an l1 bound κ on the solution. This forces zero
    components in the minimizing solution for small values of κ. Thus this bound
    can function as a selection parameter. This paper makes two contributions to
    computational problems associated with implementing the Lasso: (1) a compact
    descent method for solving the constrained problem for a particular value of
    κ is formulated, and (2) a homotopy method, in which the constraint bound κ
    becomes the homotopy parameter, is developed to completely describe the
    possible selection regimes. Both algorithms have a finite termination
    property. It is suggested that modified Gram-Schmidt orthogonalization
    applied to an augmented design matrix provides an effective basis for
    implementing the algorithms.
  accessed:
    - year: 2017
      month: 9
      day: 18
  author:
    - family: Osborne
      given: M. R.
    - family: Presnell
      given: B.
    - family: Turlach
      given: B. A.
  citation-key: osborne_new_2000
  container-title: IMA Journal of Numerical Analysis
  container-title-short: IMA J Numer Anal
  DOI: 10.1093/imanum/20.3.389
  ISSN: 0272-4979
  issue: '3'
  issued:
    - year: 2000
      month: 7
      day: 1
  page: 389-403
  source: academic.oup.com
  title: A new approach to variable selection in least squares problems
  type: article-journal
  URL: >-
    https://academic.oup.com/imajna/article/20/3/389/777743/A-new-approach-to-variable-selection-in-least
  volume: '20'

- id: osborne_shooting_1969
  author:
    - family: Osborne
      given: Mike R
  citation-key: osborne_shooting_1969
  container-title: Journal of Mathematical Analysis and Applications
  DOI: 10/cgt6pc
  issue: '2'
  issued:
    - year: 1969
  page: 417-433
  title: On Shooting Methods for Boundary Value Problems
  type: article-journal
  volume: '27'

- id: osborne_shooting_1969a
  author:
    - family: Osborne
      given: Mike R
  citation-key: osborne_shooting_1969a
  container-title: Journal of Mathematical Analysis and Applications
  DOI: 10.1016/0022-247X(69)90059-6
  issue: '2'
  issued:
    - year: 1969
  page: 417–433
  title: On shooting methods for boundary value problems
  type: article-journal
  volume: '27'

- id: osullivan_real_2010
  author:
    - family: O'Sullivan
      given: Bryan
    - family: Goerzen
      given: John
    - family: Stewart
      given: Donald Bruce
  citation-key: osullivan_real_2010
  edition: 1. ed., [Nachdr.]
  event-place: Beijing
  ISBN: 978-0-596-51498-3
  issued:
    - year: 2010
  language: eng
  note: 'OCLC: 837707964'
  number-of-pages: '670'
  publisher: O'Reilly
  publisher-place: Beijing
  source: Gemeinsamer Bibliotheksverbund ISBN
  title: 'Real world Haskell: code you can believe in'
  title-short: Real world Haskell
  type: book

- id: overschee_closed_1997
  abstract: >-
    We present a general framework for closed loop subspace system
    identification. This framework consists of two new projection theorems which
    allow the extraction of non-steady state Kalman filter states and of system
    related matrices directly from input output data. Three algorithms for the
    identification of the state space matrices can be derived from these
    theorems. The similarities between the theorems and algorithms, and the
    corresponding open loop theorems and algorithms in the literature are
    remarked on
  author:
    - family: Overschee
      given: P. Van
    - family: Moor
      given: B. De
  citation-key: overschee_closed_1997
  container-title: Proceedings of the 36th IEEE Conference on Decision and Control
  DOI: 10.1109/CDC.1997.657851
  event-title: Proceedings of the 36th IEEE Conference on Decision and Control
  issued:
    - year: 1997
      month: 12
  page: 1848-1853 vol.2
  source: IEEE Xplore
  title: Closed loop subspace system identification
  type: paper-conference
  volume: '2'

- id: overschee_subspace_1991
  abstract: >-
    The authors derive a novel algorithm to consistently identify stochastic
    state space models from given output data without forming the covariance
    matrix and using only semi-infinite block Hankel matrices. The algorithm is
    based on the concept of principle angles and directions. The authors
    describe how these can be calculated with only QR and QSVD decompositions.
    They also provide an interpretation of the principle directions as states of
    a non-steady-state Kalman filter. With a couple of examples, it is shown
    that the proposed algorithm is superior to the classical canonical
    correlation algorithms
  author:
    - family: Overschee
      given: P. Van
    - family: Moor
      given: B. De
  citation-key: overschee_subspace_1991
  container-title: '[1991] Proceedings of the 30th IEEE Conference on Decision and Control'
  DOI: 10.1109/CDC.1991.261604
  event-title: '[1991] Proceedings of the 30th IEEE Conference on Decision and Control'
  issued:
    - year: 1991
      month: 12
  page: 1321-1326 vol.2
  source: IEEE Xplore
  title: Subspace algorithms for the stochastic identification problem
  type: paper-conference

- id: paduart_identification_2010
  abstract: >-
    In this paper, we propose a method to model nonlinear systems using
    polynomial nonlinear state space equations. Obtaining good initial estimates
    is a major problem in nonlinear modelling. It is solved here by identifying
    first the best linear approximation of the system under test. The proposed
    identification procedure is successfully applied to measurements of two
    physical systems.
  author:
    - family: Paduart
      given: Johan
    - family: Lauwers
      given: Lieve
    - family: Swevers
      given: Jan
    - family: Smolders
      given: Kris
    - family: Schoukens
      given: Johan
    - family: Pintelon
      given: Rik
  citation-key: paduart_identification_2010
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10.1016/j.automatica.2010.01.001
  ISSN: 0005-1098
  issue: '4'
  issued:
    - year: 2010
      month: 4
      day: 1
  page: 647-656
  source: ScienceDirect
  title: >-
    Identification of nonlinear systems using Polynomial Nonlinear State Space
    models
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S000510981000021X
  volume: '46'

- id: paixao_clinical_2018
  abstract: >-
    Introduction: Telehealth system is an important tool to improve access and
    quality to health assistance.Large electrocardiogram (ECG) databases, linked
    to mortality or hospitalization data, can be useful in determining the
    prognostic value of ECG markers. Atrial fibrillation (AF) is a public health
    problem with increasing prevalence as the population ages, associated with
    cardiovascular mortality and morbidity.Hypothesis: Evaluate the association
    between the presence of AF with overall and cardiovascular mortality in a
    large electronic cohort of primary care patients of Minas Gerais.Methods:
    This is an observational retrospective study. Patients over 16 years old who
    performed digital electrocardiograms by Telehealth Network of Minas Gerais
    from 2013 to 2016 were assessed. A probabilistic linkage between data from
    the national mortality information system and our ECG database was made.
    Clinical data were self-reported, and ECGs were interpreted by a team of
    trained cardiologists and automatic software (Glasgow and Minnesota).The
    diagnosis of AF was considered if there was concordance between the
    cardiologist′s report and one of the automatic systems. In cases of
    disagreement, ECGs were reviewed manually.Only the first ECG made was
    analysed. To assess the relation between AF and mortality, Cox regression
    was used, adjusted by age, sex and clinical conditions.Results: From a
    dataset of 1,773,689 patients, 1,075,531 were included. The mean age was
    51.4 years, 40.5% male.The prevalence of AF was 1.15%. There were 2.9%
    deaths for all causes in 2.69 years of mean follow up. In univariate
    analysis, AF was a risk factor for death from all causes (HR 6.98, 95%CI
    6.68-7.28). After adjustment for age, sex and comorbidities, AF remained an
    independent risk factor for all-cause mortality (HR 2.49; 95% CI 2.39 -
    2.61). AF was also a predictor of risk for cardiovascular mortality after
    adjustment for age, sex and clinical conditions (HR 2.35, 95% CI 2.01-2.73).
    In multivariate analysis by sex, adjusted for age and comorbidities, AF
    women had higher risk of death for all causes (HR 3.06; 95% CI 2.86-3.26)
    than men (HR 2.18; 95% CI 2.06-2.32). There were no difference between sex
    in cardiovascular mortality (HR 2.34; 95% CI 2.01-2.73 for male sex e HR
    2.49; 95% CI 2.14-2.90 for female).Conclusions: AF was a strong predictor of
    mortality for all causes and cardiovascular mortality in primary care
    population with increased risk in women for deaths for all cause.
  accessed:
    - year: 2020
      month: 12
      day: 29
  author:
    - family: Paixao
      given: Gabriela
    - family: Silva
      given: Luis Gustavo Silva
      dropping-particle: e
    - family: Gomes
      given: Paulo R.
    - family: Ferreira
      given: Milton
    - family: Oliveira
      given: Derick
    - family: Ribeiro
      given: Manoel Horta
    - family: Ribeiro
      given: Antonio H.
    - family: Nascimento
      given: Jamil
    - family: Cardoso
      given: Gustavo
    - family: Araujo
      given: Rodrigo
    - family: Santos
      given: Bruno
    - family: Canazart
      given: Jessica
    - family: Ribeiro
      given: Leonardo
    - family: Ribeiro
      given: Antonio L.
  citation-key: paixao_clinical_2018
  container-title: Circulation. Abstracts from American Heart Association's.
  issue: Suppl_1
  issued:
    - year: 2018
      month: 11
      day: 6
  license: All rights reserved
  page: A16594-A16594
  publisher: American Heart Association
  title: >-
    Clinical Outcomes in Digital Electrocardiography: Evaluation of Mortality in
    Atrial Fibrillation (Code Study)
  title-short: Abstract 16594
  type: article-journal
  URL: https://www.ahajournals.org/doi/abs/10.1161/circ.138.suppl_1.16594
  volume: '138'

- id: paixao_ecgage_2020
  author:
    - family: Paixão
      given: Gabriela Miana
    - family: Ribeiro
      given: Antonio Horta
    - family: Lima
      given: Emilly
    - family: Seewald
      given: Bruna
    - family: Ribeiro
      given: Manoel Horta
    - family: Oliveira
      given: Derick
    - family: Gomes
      given: Paulo
    - family: Castro
      given: Nathalia
    - family: Meira
      given: Wagner
    - family: Schön
      given: Thomas
    - family: Ribeiro
      given: Antonio L.
  citation-key: paixao_ecgage_2020
  container-title: Journal of the American College of Cardiology
  DOI: 10.1016/S0735-1097(20)34299-6
  ISSN: 0735-1097
  issue: 11 Supplement 1
  issued:
    - year: 2020
  license: All rights reserved
  page: '3672'
  title: >-
    ECG-AGE FROM ARTIFICIAL INTELLIGENCE: A NEW PREDICTOR FOR MORTALITY? THE
    CODE (CLINICAL OUTCOMES IN DIGITAL ELECTROCARDIOGRAPHY) STUDY
  type: article-journal
  volume: '75'

- id: paixao_electrocardiographic_2021
  abstract: >-
    Computerized electrocardiography (ECG) has been widely used and allows
    linkage to electronic medical records. The present study describes the
    development and clinical applications of an electronic cohort derived from a
    digital ECG database obtained by the Telehealth Network of Minas Gerais,
    Brazil, for the period 2010–2017, linked to the mortality data from the
    national information system, the Clinical Outcomes in Digital
    Electrocardiography (CODE) dataset. From 2,470,424 ECGs, 1,773,689 patients
    were identified. A total of 1,666,778 (94%) underwent a valid ECG recording
    for the period 2010 to 2017, with 1,558,421 patients over 16 years old;
    40.2% were men, with a mean age of 51.7 [SD 17.6] years. During a mean
    follow-up of 3.7 years, the mortality rate was 3.3%. ECG abnormalities
    assessed were: atrial fibrillation (AF), right bundle branch block (RBBB),
    left bundle branch block (LBBB), atrioventricular block (AVB), and
    ventricular pre-excitation. Most ECG abnormalities (AF: Hazard ratio [HR]
    2.10; 95% CI 2.03–2.17; RBBB: HR 1.32; 95%CI 1.27–1.36; LBBB: HR 1.69; 95%
    CI 1.62–1.76; first degree AVB: Relative survival [RS]: 0.76; 95%
    CI0.71–0.81; 2:1 AVB: RS 0.21 95% CI0.09–0.52; and RS 0.36; third degree
    AVB: 95% CI 0.26–0.49) were predictors of overall mortality, except for
    ventricular pre-excitation (HR 1.41; 95% CI 0.56–3.57) and Mobitz I AVB (RS
    0.65; 95% CI 0.34–1.24). In conclusion, a large ECG database established by
    a telehealth network can be a useful tool for facilitating new advances in
    the fields of digital electrocardiography, clinical cardiology and
    cardiovascular epidemiology.
  accessed:
    - year: 2021
      month: 10
      day: 24
  author:
    - family: Paixão
      given: Gabriela M. M.
    - family: Lima
      given: Emilly M.
    - family: Gomes
      given: Paulo R.
    - family: Oliveira
      given: Derick M.
    - family: Ribeiro
      given: Manoel H.
    - family: Nascimento
      given: Jamil S.
    - family: Ribeiro
      given: Antonio H.
    - family: Macfarlane
      given: Peter W.
    - family: Ribeiro
      given: Antonio L. P.
  citation-key: paixao_electrocardiographic_2021
  container-title: Hearts
  DOI: 10.3390/hearts2040035
  issue: '4'
  issued:
    - year: 2021
      month: 12
  license: http://creativecommons.org/licenses/by/3.0/
  number: '4'
  page: 449-458
  publisher: Multidisciplinary Digital Publishing Institute
  source: www.mdpi.com
  title: >-
    Electrocardiographic Predictors of Mortality: Data from a Primary Care
    Tele-Electrocardiography Cohort of Brazilian Patients
  title-short: Electrocardiographic Predictors of Mortality
  type: article-journal
  URL: https://www.mdpi.com/2673-3846/2/4/35
  volume: '2'

- id: paixao_evaluation_2019
  author:
    - family: Paixão
      given: Gabriela M. M.
    - family: Lima
      given: Emilly M.
    - family: Gomes
      given: Paulo R.
    - family: Ferreira
      given: Milton P.
    - family: Oliveira
      given: Derick M.
    - family: Ribeiro
      given: Manoel Horta
    - family: Ribeiro
      given: Antônio H.
    - family: Nascimento
      given: Jamil
    - family: Canazart
      given: Jéssica A.
    - family: Cardoso
      given: Gustavo
    - family: Ribeiro
      given: Leonardo B.
    - family: Ribeiro
      given: Antonio Luiz P.
  citation-key: paixao_evaluation_2019
  container-title: Journal of Electrocardiology
  DOI: 10.1016/j.jelectrocard.2019.09.004
  ISSN: 0022-0736
  issued:
    - year: 2019
      month: 9
  license: All rights reserved
  title: >-
    Evaluation of mortality in bundle branch block patients from an electronic
    cohort: Clinical Outcomes in Digital Electrocardiography (CODE) study
  type: article-journal

- id: paixao_evaluation_2020
  abstract: >-
    Methods: This observational retrospective study of primary care patients was
    developed with the digital ECG database from the Telehealth Network of Minas
    Gerais, Brazil. ECGs performed from 2010 to 2017 were interpreted by
    cardiologists and the University of Glasgow automated analysis software. An
    electronic cohort was obtained linking data from ECG exams and those from a
    national mortality information system, using standard probabilistic linkage
    methods. We considered only the first ECG of each patient. Patients under 16
    years were excluded. Hazard ratios (HR) for mortality were adjusted for
    demographic and self-reported clinical factors and estimated with Cox
    regression.

    Results: From a dataset of 1,773,689 patients, 1,558,421 were included, mean
    age 51.6 years; 40.2% male. There were 3.34% deaths from all causes in 3.68
    years of median follow up. The prevalence of AF was 1.33%. AF was an
    independent risk factor for all-cause mortality (HR 2.10, 95%CI 2.03–2.17)
    and cardiovascular mortality (HR 2.06, 95%CI 1.86–2.29). Females with AF had
    a higher risk of overall and cardiovascular mortality compared with males (p
    < 0.001).

    Conclusions: AF was a strong predictor of cardiovascular and all-cause
    mortality in a primary care population, with increased risk in women.
  accessed:
    - year: 2020
      month: 8
      day: 7
  author:
    - family: Paixão
      given: Gabriela M. M.
    - family: Silva
      given: Luis Gustavo S.
    - family: Gomes
      given: Paulo R.
    - family: Lima
      given: Emilly M.
    - family: Ferreira
      given: Milton P. F.
    - family: Oliveira
      given: Derick M.
    - family: Ribeiro
      given: Manoel H.
    - family: Ribeiro
      given: Antonio H.
    - family: Nascimento
      given: Jamil S.
    - family: Canazart
      given: Jéssica A.
    - family: Ribeiro
      given: Leonardo B.
    - family: Benjamin
      given: Emelia J.
    - family: Macfarlane
      given: Peter W.
    - family: Marcolino
      given: Milena S.
    - family: Ribeiro
      given: Antonio L.
  citation-key: paixao_evaluation_2020
  container-title: Global Heart
  DOI: 10.5334/gh.772
  ISSN: 2211-8179
  issue: '1'
  issued:
    - year: 2020
      month: 7
      day: 28
  license: All rights reserved
  page: '48'
  source: DOI.org (Crossref)
  title: >-
    Evaluation of Mortality in Atrial Fibrillation: Clinical Outcomes in Digital
    Electrocardiography (CODE) Study
  title-short: Evaluation of Mortality in Atrial Fibrillation
  type: article-journal
  URL: https://globalheartjournal.com/articles/10.5334/gh.772/
  volume: '15'

- id: paixao_validation_2020
  abstract: >-
    Introduction: Aging affects the electrocardiogram (ECG) with a higher
    incidence of abnormalities in older patients. ECG-age can be predicted by
    artificial intelligence (AI) and can be used as a measure of cardiovascular
    health.Hypothesis: ECG-age predicted by AI is a risk factor for overall
    mortality.Methods: The Clinical Outcomes in Digital Electrocardiography
    (CODE) study is a retrospective cohort with a mean follow-up of 3.67
    years.The dataset consists of Brazilian patients, mainly from primary care
    centers. Two established cohorts, ELSA-Brasil, of Brazilian public servants,
    and SaMi-Trop, of Chagas disease patients, were used for external
    validation. 2,322,513 ECGs from 1,558,421 patients over 16 years old that
    underwent an ECG from 2010 to 2017 were included. A deep convolutional
    neural network was trained in order to predict the age of the patient based
    solely on ECG 12-lead tracings. The ECG database was split into 85-15%
    training and test datasets, respectively. Death was ascertained using
    probabilistic linkage with Brazil′s mortality information data. The Cox
    regression model, adjusted by age and sex, was used for statistical
    analysis. The model was validated in two cohorts: ELSA-Brasil (n=14,263) and
    SaMi-Trop (n=1,631).Results: he mean predicted ECG-age was 52.0 years
    (±18.7) with a mean absolute error of 8.38 (±7.0) years. Patients with
    ECG-age >8y older than chronological age had higher mortality rate (HR 1.79,
    95%CI 1.69-1.90; p<0.001), whereas those ECG-age >8y younger than
    chronological age were associated with a lower mortality rate (HR 0.78,
    95%CI 0.74-0.83; p<0.001). These results were similar in ELSA-Brasil and
    SaMi-Trop external validation cohorts (HR 1.75, 95%CI 1.35-2.27; p<0.001;HR
    2.42, 95%CI 1.53-3.83; p<0.001 for >8y difference, retrospectively; HR 0.74,
    95%CI 0.63-0.88; p<0.001;HR 0.89, 95%CI 0.52-1.54; p=0.68 for <8y
    difference, respectively).Conclusions: ECG-age, predicted by AI, can be
    useful as a tool for risk stratification of mortality.
  accessed:
    - year: 2020
      month: 12
      day: 29
  author:
    - family: Paixao
      given: Gabriela
    - family: Lima
      given: Emilly M
    - family: Ribeiro
      given: Antonio H
    - family: Gomes
      given: Paulo R
    - family: Oliveira
      given: Derick
    - family: Junior
      given: Marcelo M. Pinto
    - family: Sabino
      given: Ester
    - family: Barreto
      given: Sandhi
    - family: Giatti
      given: Luana
    - family: A
      given: Paulo Andrade Lotufo
    - family: Bruce
      given: Duncan
    - family: Wagner
      given: Meira Junior
    - family: Thomas B. Schon
      given: ''
    - family: Ribeiro
      given: Antonio L.
  citation-key: paixao_validation_2020
  container-title: Circulation
  container-title-short: Circulation
  DOI: 10.1161/circ.142.suppl_3.16883
  issue: Suppl_3
  issued:
    - year: 2020
      month: 11
      day: 17
  license: All rights reserved
  page: A16883-A16883
  publisher: American Heart Association
  source: ahajournals.org (Atypon)
  title: >-
    Validation of a Deep Neural Network Electrocardiographic-Age as a Mortality
    Predictor: The CODE Study
  title-short: Abstract 16883
  type: article-journal
  URL: https://www.ahajournals.org/doi/abs/10.1161/circ.142.suppl_3.16883
  volume: '142'

- id: paperno_lambada_2016
  accessed:
    - year: 2019
      month: 9
      day: 8
  author:
    - family: Paperno
      given: Denis
    - family: Kruszewski
      given: Germán
    - family: Lazaridou
      given: Angeliki
    - family: Pham
      given: Ngoc-Quan
    - family: Bernardi
      given: Raffaella
    - family: Pezzelle
      given: Sandro
    - family: Baroni
      given: Marco
    - family: Boleda
      given: Gemma
    - family: Fernández
      given: Raquel
  citation-key: paperno_lambada_2016
  DOI: 10/gf7pwc
  event-title: >-
    Proceedings of the 54th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)
  issued:
    - year: 2016
      month: 8
  language: en-us
  page: 1525-1534
  source: www.aclweb.org
  title: 'The LAMBADA dataset: Word prediction requiring a broad discourse context'
  title-short: The LAMBADA dataset
  type: paper-conference
  URL: https://www.aclweb.org/anthology/P16-1144/

- id: papineni_bleu_2002
  author:
    - family: Papineni
      given: Kishore
    - family: Roukos
      given: Salim
    - family: Ward
      given: Todd
    - family: Zhu
      given: Wei-Jing
  citation-key: papineni_bleu_2002
  container-title: >-
    Proceedings of the 40th annual meeting on association for computational
    linguistics
  issued:
    - year: 2002
  note: '00000'
  page: 311–318
  publisher: Association for Computational Linguistics
  source: Google Scholar
  title: 'BLEU: a method for automatic evaluation of machine translation'
  title-short: BLEU
  type: paper-conference

- id: papoulis_probability_2002
  author:
    - family: Papoulis
      given: Athanasios
    - family: Pillai
      given: S. Unnikrishna
  call-number: QA273 .P2 2002
  citation-key: papoulis_probability_2002
  edition: 4th ed
  event-place: Boston
  ISBN: 978-0-07-366011-0 978-0-07-112256-6
  issued:
    - year: 2002
  number-of-pages: '852'
  publisher: McGraw-Hill
  publisher-place: Boston
  source: Library of Congress ISBN
  title: Probability, random variables, and stochastic processes
  type: book

- id: parhi_banach_2021
  abstract: >-
    We develop a variational framework to understand the properties of the
    functions learned by neural networks fit to data. We propose and study a
    family of continuous-domain linear inverse problems with total
    variation-like regularization in the Radon domain subject to data fitting
    constraints. We derive a representer theorem showing that finite-width,
    single-hidden layer neural networks are solutions to these inverse problems.
    We draw on many techniques from variational spline theory and so we propose
    the notion of polynomial ridge splines, which correspond to single-hidden
    layer neural networks with truncated power functions as the activation
    function. The representer theorem is reminiscent of the classical
    reproducing kernel Hilbert space representer theorem, but we show that the
    neural network problem is posed over a non-Hilbertian Banach space. While
    the learning problems are posed in the continuous-domain, similar to kernel
    methods, the problems can be recast as finite-dimensional neural network
    training problems. These neural network training problems have regularizers
    which are related to the well-known weight decay and path-norm regularizers.
    Thus, our result gives insight into functional characteristics of trained
    neural networks and also into the design neural network regularizers. We
    also show that these regularizers promote neural network solutions with
    desirable generalization properties.
  accessed:
    - year: 2021
      month: 11
      day: 5
  author:
    - family: Parhi
      given: Rahul
    - family: Nowak
      given: Robert D.
  citation-key: parhi_banach_2021
  container-title: arXiv:2006.05626 [cs, stat]
  issued:
    - year: 2021
      month: 2
      day: 11
  source: arXiv.org
  title: Banach Space Representer Theorems for Neural Networks and Ridge Splines
  type: article-journal
  URL: http://arxiv.org/abs/2006.05626

- id: parhi_nearminimax_2021
  abstract: >-
    We study the problem of estimating an unknown function from noisy data using
    shallow (single-hidden layer) ReLU neural networks. The estimators we study
    minimize the sum of squared data-fitting errors plus a regularization term
    proportional to the Euclidean norm of the network weights. This minimization
    corresponds to the common approach of training a neural network with weight
    decay. We quantify the performance (mean-squared error) of these neural
    network estimators when the data-generating function belongs to the space of
    functions of second-order bounded variation in the Radon domain. This space
    of functions was recently proposed as the natural function space associated
    with shallow ReLU neural networks. We derive a minimax lower bound for the
    estimation problem for this function space and show that the neural network
    estimators are minimax optimal up to logarithmic factors. We also show that
    this is a "mixed variation" function space that contains classical
    multivariate function spaces including certain Sobolev spaces and certain
    spectral Barron spaces. Finally, we use these results to quantify a gap
    between neural networks and linear methods (which include kernel methods).
    This paper sheds light on the phenomenon that neural networks seem to break
    the curse of dimensionality.
  accessed:
    - year: 2021
      month: 11
      day: 5
  author:
    - family: Parhi
      given: Rahul
    - family: Nowak
      given: Robert D.
  citation-key: parhi_nearminimax_2021
  container-title: arXiv:2109.08844 [cs, math, stat]
  issued:
    - year: 2021
      month: 9
      day: 18
  source: arXiv.org
  title: Near-Minimax Optimal Estimation With Shallow ReLU Neural Networks
  type: article-journal
  URL: http://arxiv.org/abs/2109.08844

- id: parhi_role_2020
  abstract: >-
    A wide variety of activation functions have been proposed for neural
    networks. The Rectified Linear Unit (ReLU) is especially popular today.
    There are many practical reasons that motivate the use of the ReLU. This
    paper provides new theoretical characterizations that support the use of the
    ReLU, its variants such as the leaky ReLU, as well as other activation
    functions in the case of univariate, single-hidden layer feedforward neural
    networks. Our results also explain the importance of commonly used
    strategies in the design and training of neural networks such as "weight
    decay" and "path-norm" regularization, and provide a new justification for
    the use of "skip connections" in network architectures. These new insights
    are obtained through the lens of spline theory. In particular, we show how
    neural network training problems are related to infinite-dimensional
    optimizations posed over Banach spaces of functions whose solutions are
    well-known to be fractional and polynomial splines, where the particular
    Banach space (which controls the order of the spline) depends on the choice
    of activation function.
  accessed:
    - year: 2021
      month: 11
      day: 5
  author:
    - family: Parhi
      given: Rahul
    - family: Nowak
      given: Robert D.
  citation-key: parhi_role_2020
  container-title: IEEE Signal Processing Letters
  container-title-short: IEEE Signal Process. Lett.
  DOI: 10.1109/LSP.2020.3027517
  ISSN: 1070-9908, 1558-2361
  issued:
    - year: 2020
  page: 1779-1783
  source: arXiv.org
  title: The Role of Neural Network Activation Functions
  type: article-journal
  URL: http://arxiv.org/abs/1910.02333
  volume: '27'

- id: parhi_what_2021
  abstract: >-
    We develop a variational framework to understand the properties of functions
    learned by fitting deep neural networks with rectified linear unit
    activations to data. We propose a new function space, which is reminiscent
    of classical bounded variation-type spaces, that captures the compositional
    structure associated with deep neural networks. We derive a representer
    theorem showing that deep ReLU networks are solutions to regularized data
    fitting problems over functions from this space. The function space consists
    of compositions of functions from the Banach spaces of second-order bounded
    variation in the Radon domain. These are Banach spaces with
    sparsity-promoting norms, giving insight into the role of sparsity in deep
    neural networks. The neural network solutions have skip connections and rank
    bounded weight matrices, providing new theoretical support for these common
    architectural choices. The variational problem we study can be recast as a
    finite-dimensional neural network training problem with regularization
    schemes related to the notions of weight decay and path-norm regularization.
    Finally, our analysis builds on techniques from variational spline theory,
    providing new connections between deep neural networks and splines.
  accessed:
    - year: 2021
      month: 11
      day: 5
  author:
    - family: Parhi
      given: Rahul
    - family: Nowak
      given: Robert D.
  citation-key: parhi_what_2021
  container-title: arXiv:2105.03361 [cs, stat]
  issued:
    - year: 2021
      month: 9
      day: 26
  source: arXiv.org
  title: >-
    What Kinds of Functions do Deep Neural Networks Learn? Insights from
    Variational Spline Theory
  title-short: What Kinds of Functions do Deep Neural Networks Learn?
  type: article-journal
  URL: http://arxiv.org/abs/2105.03361

- id: park_realtime_2007
  author:
    - family: Park
      given: Sungchan
    - family: Jeong
      given: Hong
  citation-key: park_realtime_2007
  container-title: >-
    Multimedia and Ubiquitous Engineering, 2007. MUE'07. International
    Conference on
  issued:
    - year: 2007
  note: '00000'
  page: 751–756
  publisher: IEEE
  title: Real-time stereo vision FPGA chip with low error rate
  type: paper-conference

- id: park_universal_1991
  author:
    - family: Park
      given: Jooyoung
    - family: Sandberg
      given: Irwin W
  citation-key: park_universal_1991
  container-title: Neural computation
  DOI: 10/d3sv6t
  issue: '2'
  issued:
    - year: 1991
  note: '03486'
  page: 246-257
  title: Universal Approximation Using Radial-Basis-Function Networks
  type: article-journal
  volume: '3'

- id: park_universal_1991a
  abstract: >-
    There have been several recent studies concerning feedforward networks and
    the problem of approximating arbitrary functionals of a finite number of
    real variables. Some of these studies deal with cases in which the
    hidden-layer nonlinearity is not a sigmoid. This was motivated by successful
    applications of feedforward networks with nonsigmoidal hidden-layer units.
    This paper reports on a related study of radial-basis-function (RBF)
    networks, and it is proved that RBF networks having one hidden layer are
    capable of universal approximation. Here the emphasis is on the case of
    typical RBF networks, and the results show that a certain class of RBF
    networks with the same smoothing factor in each kernel node is broad enough
    for universal approximation.
  accessed:
    - year: 2020
      month: 11
      day: 24
  author:
    - family: Park
      given: J.
    - family: Sandberg
      given: I. W.
  citation-key: park_universal_1991a
  container-title: Neural Computation
  DOI: 10.1162/neco.1991.3.2.246
  ISSN: 0899-7667
  issue: '2'
  issued:
    - year: 1991
      month: 6
      day: 1
  page: 246-257
  publisher: MIT Press
  source: MIT Press Journals
  title: Universal Approximation Using Radial-Basis-Function Networks
  type: article-journal
  URL: https://doi.org/10.1162/neco.1991.3.2.246
  volume: '3'

- id: park_universal_1991b
  author:
    - family: Park
      given: Jooyoung
    - family: Sandberg
      given: Irwin W
  citation-key: park_universal_1991b
  container-title: Neural computation
  issue: '2'
  issued:
    - year: 1991
  note: '00000'
  page: 246–257
  title: Universal approximation using radial-basis-function networks
  type: article-journal
  volume: '3'

- id: parziale_tcp_2006
  author:
    - family: Parziale
      given: L.
    - family: Liu
      given: W.
    - family: Matthews
      given: C.
    - family: Rosselot
      given: N.
    - family: Davis
      given: C.
    - family: Forrester
      given: J.
    - family: Britt
      given: D.T.
    - family: Redbooks
      given: IBM
  citation-key: parziale_tcp_2006
  collection-title: IBM redbooks
  ISBN: 978-0-7384-9468-5
  issued:
    - year: 2006
  note: '00000'
  publisher: IBM Redbooks
  title: TCP/IP Tutorial and Technical Overview
  type: book
  URL: https://books.google.com.br/books?id=TgwWAgAAQBAJ

- id: pascanu_difficulty_2013
  abstract: >-
    There are two widely known issues with properly training recurrent neural
    networks, the vanishing and the exploding gradient problems detailed in
    Bengio et al. (1994). In this paper we attempt to improve the understanding
    of the underlying issues by exploring these problems from an analytical, a
    geometric and a dynamical systems perspective. Our analysis is used to
    justify a simple yet effective solution. We propose a gradient norm clipping
    strategy to deal with exploding gradients and a soft constraint for the
    vanishing gradients problem. We validate empirically our hypothesis and
    proposed solutions in the experimental section.
  author:
    - family: Pascanu
      given: Razvan
    - family: Mikolov
      given: Tomas
    - family: Bengio
      given: Yoshua
  citation-key: pascanu_difficulty_2013
  container-title: >-
    Proceedings of the 30th International Conference on International Conference
    on Machine Learning
  issued:
    - year: 2013
  page: 1310–1318
  title: On the Difficulty of Training Recurrent Neural Networks
  type: entry-dictionary
  volume: '28'

- id: pastika_artificial_2024
  author:
    - family: Pastika
      given: Libor
    - family: Sau
      given: Arunashis
    - family: Patlatzoglou
      given: Konstantinos
    - family: Sieliwonczyk
      given: Ewa
    - family: Ribeiro
      given: Antonio H.
    - family: McGurk
      given: Kathryn A.
    - family: Scott
      given: William R
    - family: Ware
      given: James S.
    - family: Ribeiro
      given: Antonio Luiz P.
    - family: Kramer
      given: Daniel B.
    - family: Waks
      given: Jonathan W.
    - family: Ng
      given: Fu Siong
  citation-key: pastika_artificial_2024
  container-title: Accepted at npj Digital Medicine
  issued:
    - year: 2024
  language: en
  page: 2024.01.13.24301267
  source: medRxiv
  title: >-
    Artificial intelligence–enabled electrocardiogram for mortality and
    cardiovascular risk estimation: An actionable, explainable and biologically
    plausible platform
  type: article-journal

- id: pastur_random_2020
  abstract: >-
    The paper deals with distribution of singular values of product of random
    matrices arising in the analysis of deep neural networks. The matrices
    resemble the product analogs of the sample covariance matrices, however, an
    important difference is that the population covariance matrices, which are
    assumed to be non-random in the standard setting of statistics and random
    matrix theory, are now random, moreover, are certain functions of random
    data matrices. The problem has been considered in recent work [21] by using
    the techniques of free probability theory. Since, however, free probability
    theory deals with population matrices which are independent of the data
    matrices, its applicability in this case requires an additional
    justification. We present this justification by using a version of the
    standard techniques of random matrix theory under the assumption that the
    entries of data matrices are independent Gaussian random variables. In the
    subsequent paper [18] we extend our results to the case where the entries of
    data matrices are just independent identically distributed random variables
    with several finite moments. This, in particular, extends the property of
    the so-called macroscopic universality on the considered random matrices.
  accessed:
    - year: 2021
      month: 3
      day: 12
  author:
    - family: Pastur
      given: Leonid
  citation-key: pastur_random_2020
  container-title: arXiv:2001.06188
  issued:
    - year: 2020
      month: 4
      day: 7
  source: arXiv.org
  title: On Random Matrices Arising in Deep Neural Networks. Gaussian Case
  type: article-journal
  URL: http://arxiv.org/abs/2001.06188

- id: paszke_automatic_2017
  author:
    - family: Paszke
      given: Adam
    - family: Gross
      given: Sam
    - family: Chintala
      given: Soumith
    - family: Chanan
      given: Gregory
    - family: Yang
      given: Edward
    - family: DeVito
      given: Zachary
    - family: Lin
      given: Zeming
    - family: Desmaison
      given: Alban
    - family: Antiga
      given: Luca
    - family: Lerer
      given: Adam
  citation-key: paszke_automatic_2017
  issued:
    - year: 2017
  title: Automatic differentiation in PyTorch
  type: report

- id: paszke_pytorch_2019
  author:
    - family: Paszke
      given: Adam
    - family: Gross
      given: Sam
    - family: Massa
      given: Francisco
    - family: Lerer
      given: Adam
    - family: Bradbury
      given: James
    - family: Chanan
      given: Gregory
    - family: Killeen
      given: Trevor
    - family: Lin
      given: Zeming
    - family: Gimelshein
      given: Natalia
    - family: Antiga
      given: Luca
    - family: Desmaison
      given: Alban
    - family: Kopf
      given: Andreas
    - family: Yang
      given: Edward
    - family: DeVito
      given: Zachary
    - family: Raison
      given: Martin
    - family: Tejani
      given: Alykhan
    - family: Chilamkurthy
      given: Sasank
    - family: Steiner
      given: Benoit
    - family: Fang
      given: Lu
    - family: Bai
      given: Junjie
    - family: Chintala
      given: Soumith
  citation-key: paszke_pytorch_2019
  container-title: Advances in neural information processing systems 32
  issued:
    - year: 2019
  page: 8024–8035
  title: 'PyTorch: An imperative style, high-performance deep learning library'
  type: chapter
  URL: >-
    http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf

- id: patan_nonlinear_2012
  author:
    - family: Patan
      given: Krzysztof
    - family: Korbicz
      given: Józef
  citation-key: patan_nonlinear_2012
  container-title: International Journal of Applied Mathematics and Computer Science
  DOI: 10.2478/v10006-012-0017-6
  issue: '1'
  issued:
    - year: 2012
  note: '00000'
  page: 225–237
  title: >-
    Nonlinear model predictive control of a boiler unit: A fault tolerant
    control study
  type: article-journal
  volume: '22'

- id: patan_nonlinear_2012a
  author:
    - family: Patan
      given: Krzysztof
    - family: Korbicz
      given: Józef
  citation-key: patan_nonlinear_2012a
  container-title: International Journal of Applied Mathematics and Computer Science
  DOI: 10/gfjwmj
  issue: '1'
  issued:
    - year: 2012
  page: 225-237
  title: >-
    Nonlinear Model Predictive Control of a Boiler Unit: A Fault Tolerant
    Control Study
  type: article-journal
  volume: '22'

- id: pathak_modelfree_2018
  abstract: >-
    We demonstrate the effectiveness of using machine learning for model-free
    prediction of spatiotemporally chaotic systems of arbitrarily large spatial
    extent and attractor dimension purely from observations of the system’s past
    evolution. We present a parallel scheme with an example implementation based
    on the reservoir computing paradigm and demonstrate the scalability of our
    scheme using the Kuramoto-Sivashinsky equation as an example of a
    spatiotemporally chaotic system.
  accessed:
    - year: 2021
      month: 4
      day: 2
  author:
    - family: Pathak
      given: Jaideep
    - family: Hunt
      given: Brian
    - family: Girvan
      given: Michelle
    - family: Lu
      given: Zhixin
    - family: Ott
      given: Edward
  citation-key: pathak_modelfree_2018
  container-title: Physical Review Letters
  container-title-short: Phys. Rev. Lett.
  DOI: 10.1103/PhysRevLett.120.024102
  issue: '2'
  issued:
    - year: 2018
      month: 1
      day: 12
  page: '024102'
  publisher: American Physical Society
  source: APS
  title: >-
    Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A
    Reservoir Computing Approach
  title-short: Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data
  type: article-journal
  URL: https://link.aps.org/doi/10.1103/PhysRevLett.120.024102
  volume: '120'

- id: pathak_using_2017
  accessed:
    - year: 2021
      month: 2
      day: 19
  author:
    - family: Pathak
      given: Jaideep
    - family: Lu
      given: Zhixin
    - family: Hunt
      given: Brian R.
    - family: Girvan
      given: Michelle
    - family: Ott
      given: Edward
  citation-key: pathak_using_2017
  container-title: 'Chaos: An Interdisciplinary Journal of Nonlinear Science'
  container-title-short: Chaos
  DOI: 10.1063/1.5010300
  ISSN: 1054-1500, 1089-7682
  issue: '12'
  issued:
    - year: 2017
      month: 12
  language: en
  page: '121102'
  source: DOI.org (Crossref)
  title: >-
    Using machine learning to replicate chaotic attractors and calculate
    Lyapunov exponents from data
  type: article-journal
  URL: http://aip.scitation.org/doi/10.1063/1.5010300
  volume: '27'

- id: patterson_computer_2005
  author:
    - family: Patterson
      given: David A.
    - family: Hennessy
      given: John L.
  citation-key: patterson_computer_2005
  edition: 3. ed
  event-place: Amsterdam
  ISBN: 978-1-55860-604-3 978-0-12-088433-9
  issued:
    - year: 2005
  language: eng
  note: |-
    00000 
    OCLC: 249857976
  number-of-pages: '621'
  publisher: Kaufmann
  publisher-place: Amsterdam
  source: Gemeinsamer Bibliotheksverbund ISBN
  title: 'Computer organization and design: the hardware/software interface'
  title-short: Computer organization and design
  type: book

- id: pauwels_comparison_2012
  author:
    - family: Pauwels
      given: Karl
    - family: Tomasi
      given: Matteo
    - family: Diaz Alonso
      given: Javier
    - family: Ros
      given: Eduardo
    - family: Van Hulle
      given: Marc M
  citation-key: pauwels_comparison_2012
  container-title: Computers, IEEE Transactions on
  DOI: 10.1109/TC.2011.120
  issue: '7'
  issued:
    - year: 2012
  note: '00000'
  page: 999–1012
  title: >-
    A comparison of FPGA and GPU for real-time phase-based optical flow, stereo,
    and local image features
  type: article-journal
  volume: '61'

- id: peche_note_2019
  abstract: >-
    This paper is concerned with a new expression of the so-called
    Pennington-Worah distribution, characterizing the asymptotic empirical
    eigenvalue distribution of some non linear random matrix ensembles.
  accessed:
    - year: 2021
      month: 6
      day: 24
  author:
    - family: Péché
      given: S.
  citation-key: peche_note_2019
  container-title: Electronic Communications in Probability
  container-title-short: Electron. Commun. Probab.
  DOI: 10.1214/19-ECP262
  ISSN: 1083-589X
  issue: none
  issued:
    - year: 2019
      month: 1
      day: 1
  language: en
  source: DOI.org (Crossref)
  title: A note on the Pennington-Worah distribution
  type: article-journal
  URL: >-
    https://projecteuclid.org/journals/electronic-communications-in-probability/volume-24/issue-none/A-note-on-the-Pennington-Worah-distribution/10.1214/19-ECP262.full
  volume: '24'

- id: peifer_parameter_2007
  author:
    - family: Peifer
      given: M
    - family: Timmer
      given: J
  citation-key: peifer_parameter_2007
  container-title: IET Systems Biology
  DOI: 10.1049/iet-syb:20060067
  issue: '2'
  issued:
    - year: 2007
  page: 78–88
  title: >-
    Parameter estimation in ordinary differential equations for biochemical
    processes using the method of multiple shooting
  type: article-journal
  volume: '1'

- id: peng_eventtriggered_2017
  author:
    - family: Peng
      given: Kaixiang
    - family: Wang
      given: Mengyuan
    - family: Dong
      given: Jie
  citation-key: peng_eventtriggered_2017
  container-title: Neurocomputing
  DOI: 10.1016/j.neucom.2017.02.027
  issued:
    - year: 2017
  note: '00000'
  page: 257–267
  title: >-
    Event-triggered fault detection framework based on subspace identification
    method for the networked control systems
  type: article-journal
  volume: '239'

- id: pennington_emergence_2018
  abstract: >-
    Recent work has shown that tight concentration of the entire spectrum of
    singular values of a deep network's input-output Jacobian around one at
    initialization can speed up learning by orders of magnitude. Therefore, to
    guide important design choices, it is important to build a full theoretical
    understanding of the spectra of Jacobians at initialization. To this end, we
    leverage powerful tools from free probability theory to provide a detailed
    analytic understanding of how a deep network's Jacobian spectrum depends on
    various hyperparameters including the nonlinearity, the weight and bias
    distributions, and the depth. For a variety of nonlinearities, our work
    reveals the emergence of new universal limiting spectral distributions that
    remain concentrated around one even as the depth goes to infinity.
  archive: '1802.09979'
  author:
    - family: Pennington
      given: Jeffrey
    - family: Schoenholz
      given: Samuel S.
    - family: Ganguli
      given: Surya
  citation-key: pennington_emergence_2018
  container-title: >-
    21st International Conference on Artificial Intelligence and Statistics
    (AISTATS)
  issued:
    - year: 2018
  source: arXiv.org
  title: The Emergence of Spectral Universality in Deep Networks
  type: article-journal

- id: pennington_geometry_
  author:
    - family: Pennington
      given: Jeffrey
    - family: Bahri
      given: Yasaman
  citation-key: pennington_geometry_
  language: en
  page: '9'
  source: Zotero
  title: Geometry of Neural Network Loss Surfaces via Random Matrix Theory
  type: article-journal

- id: pennington_glove_2014
  author:
    - family: Pennington
      given: Jeffrey
    - family: Socher
      given: Richard
    - family: Manning
      given: Christopher
  citation-key: pennington_glove_2014
  container-title: >-
    Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)
  event-place: Doha, Qatar
  issued:
    - year: 2014
      month: 10
  note: '00000'
  page: 1532–1543
  publisher: Association for Computational Linguistics
  publisher-place: Doha, Qatar
  source: ACLWeb
  title: 'Glove: Global Vectors for Word Representation'
  title-short: Glove
  type: paper-conference
  URL: http://www.aclweb.org/anthology/D14-1162

- id: pennington_nonlinear_2017
  author:
    - family: Pennington
      given: Jeffrey
    - family: Worah
      given: Pratik
  citation-key: pennington_nonlinear_2017
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2017
  page: 2637-2646
  title: Nonlinear random matrix theory for deep learning
  type: paper-conference

- id: pennington_spectrum_2018
  author:
    - family: Pennington
      given: Jeffrey
    - family: Worah
      given: Pratik
  citation-key: pennington_spectrum_2018
  container-title: Advances in neural information processing systems
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  publisher: Curran Associates, Inc.
  title: >-
    The spectrum of the fisher information matrix of a single-hidden-layer
    neural network
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2018/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Paper.pdf
  volume: '31'

- id: perdomo_performative_2020
  abstract: >-
    When predictions support decisions they may influence the outcome they aim
    to predict. We call such predictions performative; the prediction influences
    the target. Performativity is a well-studied phenomenon in policy-making
    that has so far been neglected in supervised learning. When ignored,
    performativity surfaces as undesirable distribution shift, routinely
    addressed with retraining. We develop a risk minimization framework for
    performative prediction bringing together concepts from statistics, game
    theory, and causality. A conceptual novelty is an equilibrium notion we call
    performative stability. Performative stability implies that the predictions
    are calibrated not against past outcomes, but against the future outcomes
    that manifest from acting on the prediction. Our main results are necessary
    and sufficient conditions for the convergence of retraining to a
    performatively stable point of nearly minimal loss. In full generality,
    performative prediction strictly subsumes the setting known as strategic
    classification. We thus also give the first sufficient conditions for
    retraining to overcome strategic feedback effects.
  author:
    - family: Perdomo
      given: Juan
    - family: Zrnic
      given: Tijana
    - family: Mendler-Dünner
      given: Celestine
    - family: Hardt
      given: Moritz
  citation-key: perdomo_performative_2020
  collection-title: Proceedings of machine learning research
  container-title: Proceedings of the 37th international conference on machine learning
  editor:
    - family: III
      given: Hal Daumé
    - family: Singh
      given: Aarti
  issued:
    - year: 2020
      month: 7
      day: 13
    - year: 2020
      month: 7
      day: 18
  page: 7599–7609
  publisher: PMLR
  title: Performative prediction
  type: paper-conference
  URL: https://proceedings.mlr.press/v119/perdomo20a.html
  volume: '119'

- id: perry_proteomic_2024
  abstract: >-
    Despite the wide effects of cardiorespiratory fitness (CRF) on metabolic,
    cardiovascular, pulmonary and neurological health, challenges in the
    feasibility and reproducibility of CRF measurements have impeded its use for
    clinical decision-making. Here we link proteomic profiles to CRF in 14,145
    individuals across four international cohorts with diverse CRF ascertainment
    methods to establish, validate and characterize a proteomic CRF score. In a
    cohort of around 22,000 individuals in the UK Biobank, a proteomic CRF score
    was associated with a reduced risk of all-cause mortality (unadjusted hazard
    ratio 0.50 (95% confidence interval 0.48–0.52) per 1 s.d. increase). The
    proteomic CRF score was also associated with multisystem disease risk and
    provided risk reclassification and discrimination beyond clinical risk
    factors, as well as modulating high polygenic risk of certain diseases.
    Finally, we observed dynamicity of the proteomic CRF score in individuals
    who undertook a 20-week exercise training program and an association of the
    score with the degree of the effect of training on CRF, suggesting potential
    use of the score for personalization of exercise recommendations. These
    results indicate that population-based proteomics provides biologically
    relevant molecular readouts of CRF that are additive to genetic risk,
    potentially modifiable and clinically translatable.
  accessed:
    - year: 2024
      month: 6
      day: 4
  author:
    - family: Perry
      given: Andrew S.
    - family: Farber-Eger
      given: Eric
    - family: Gonzales
      given: Tomas
    - family: Tanaka
      given: Toshiko
    - family: Robbins
      given: Jeremy M.
    - family: Murthy
      given: Venkatesh L.
    - family: Stolze
      given: Lindsey K.
    - family: Zhao
      given: Shilin
    - family: Huang
      given: Shi
    - family: Colangelo
      given: Laura A.
    - family: Deng
      given: Shuliang
    - family: Hou
      given: Lifang
    - family: Lloyd-Jones
      given: Donald M.
    - family: Walker
      given: Keenan A.
    - family: Ferrucci
      given: Luigi
    - family: Watts
      given: Eleanor L.
    - family: Barber
      given: Jacob L.
    - family: Rao
      given: Prashant
    - family: Mi
      given: Michael Y.
    - family: Gabriel
      given: Kelley Pettee
    - family: Hornikel
      given: Bjoern
    - family: Sidney
      given: Stephen
    - family: Houstis
      given: Nicholas
    - family: Lewis
      given: Gregory D.
    - family: Liu
      given: Gabrielle Y.
    - family: Thyagarajan
      given: Bharat
    - family: Khan
      given: Sadiya S.
    - family: Choi
      given: Bina
    - family: Washko
      given: George
    - family: Kalhan
      given: Ravi
    - family: Wareham
      given: Nick
    - family: Bouchard
      given: Claude
    - family: Sarzynski
      given: Mark A.
    - family: Gerszten
      given: Robert E.
    - family: Brage
      given: Soren
    - family: Wells
      given: Quinn S.
    - family: Nayor
      given: Matthew
    - family: Shah
      given: Ravi V.
  citation-key: perry_proteomic_2024
  container-title: Nature Medicine
  container-title-short: Nat Med
  DOI: 10.1038/s41591-024-03039-x
  ISSN: 1546-170X
  issued:
    - year: 2024
      month: 6
      day: 4
  language: en
  license: 2024 The Author(s)
  page: 1-11
  publisher: Nature Publishing Group
  source: www.nature.com
  title: >-
    Proteomic analysis of cardiorespiratory fitness for prediction of mortality
    and multisystem disease risks
  type: article-journal
  URL: https://www.nature.com/articles/s41591-024-03039-x

- id: peters_deep_2018
  abstract: >-
    We introduce a new type of deep contextualized word representation that
    models both (1) complex characteristics of word use (e.g., syntax and
    semantics), and (2) how these uses vary across linguistic contexts (i.e., to
    model polysemy). Our word vectors are learned functions of the internal
    states of a deep bidirectional language model (biLM), which is pre-trained
    on a large text corpus. We show that these representations can be easily
    added to existing models and significantly improve the state of the art
    across six challenging NLP problems, including question answering, textual
    entailment and sentiment analysis. We also present an analysis showing that
    exposing the deep internals of the pre-trained network is crucial, allowing
    downstream models to mix different types of semi-supervision signals.
  accessed:
    - year: 2019
      month: 6
      day: 8
  author:
    - family: Peters
      given: Matthew E.
    - family: Neumann
      given: Mark
    - family: Iyyer
      given: Mohit
    - family: Gardner
      given: Matt
    - family: Clark
      given: Christopher
    - family: Lee
      given: Kenton
    - family: Zettlemoyer
      given: Luke
  citation-key: peters_deep_2018
  container-title: arXiv:1802.05365 [cs]
  issued:
    - year: 2018
      month: 2
      day: 14
  source: arXiv.org
  title: Deep contextualized word representations
  type: article-journal
  URL: http://arxiv.org/abs/1802.05365

- id: peters_elements_2017
  accessed:
    - year: 2022
      month: 3
      day: 13
  author:
    - family: Peters
      given: Jonas
    - family: Janzing
      given: Dominik
    - family: Schölkopf
      given: Bernhard
  citation-key: peters_elements_2017
  issued:
    - year: 2017
  language: en
  source: DOI.org (Crossref)
  title: 'Elements of causal inference: foundations and learning algorithms'
  title-short: Elements of causal inference
  type: book
  URL: https://www.tandfonline.com/doi/full/10.1080/00949655.2018.1505197

- id: petrovic_kalman_2013
  author:
    - family: Petrović
      given: Emina
    - family: Ćojbašić
      given: Žarko
    - family: Ristić-Durrant
      given: Danijela
    - family: Nikolić
      given: Vlastimir
    - family: Ćirić
      given: Ivan
    - family: Matić
      given: Sr\d
      dropping-particle: jan
  citation-key: petrovic_kalman_2013
  container-title: 'Facta Universitatis, Series: Automatic Control And Robotics'
  issue: '1'
  issued:
    - year: 2013
  note: '00000'
  page: 43–51
  title: Kalman filter and NARX neural network for robot vision based human tracking
  type: article-journal
  volume: '12'

- id: petrovic_kalman_2013a
  author:
    - family: Petrović
      given: Emina
    - family: Ćojbašić
      given: Žarko
    - family: Ristić-Durrant
      given: Danijela
    - family: Nikolić
      given: Vlastimir
    - family: Ćirić
      given: Ivan
    - family: Matić
      given: Sr\d
      non-dropping-particle: jan
  citation-key: petrovic_kalman_2013a
  container-title: 'Facta Universitatis, Series: Automatic Control And Robotics'
  issue: '1'
  issued:
    - year: 2013
  note: '00009'
  page: 43-51
  title: Kalman Filter and NARX Neural Network for Robot Vision Based Human Tracking
  type: article-journal
  volume: '12'

- id: pfanzagl_parametric_1994
  author:
    - family: Pfanzagl
      given: Johann
  citation-key: pfanzagl_parametric_1994
  issued:
    - year: 1994
  note: '00000'
  publisher: Walter de Gruyter
  title: Parametric statistical theory
  type: book

- id: pillonetto_deep_2023
  author:
    - family: Pillonetto
      given: Gianluigi
    - family: Aravkin
      given: Aleksandr
    - family: Gedon
      given: Daniel
    - family: Ljung
      given: Lennart
    - family: Ribeiro
      given: Antonio H.
    - family: Schön
      given: Thomas Bo
  citation-key: pillonetto_deep_2023
  container-title: Provisionally accepted at Automatica
  DOI: 10.48550/arXiv.2301.12832
  issued:
    - year: 2023
  license: All rights reserved
  title: 'Deep networks for system identification: a Survey'
  type: article-journal

- id: pillonetto_kernel_2014
  abstract: >-
    Most of the currently used techniques for linear system identification are
    based on classical estimation paradigms coming from mathematical statistics.
    In particular, maximum likelihood and prediction error methods represent the
    mainstream approaches to identification of linear dynamic systems, with a
    long history of theoretical and algorithmic contributions. Parallel to this,
    in the machine learning community alternative techniques have been
    developed. Until recently, there has been little contact between these two
    worlds. The first aim of this survey is to make accessible to the control
    community the key mathematical tools and concepts as well as the
    computational aspects underpinning these learning techniques. In particular,
    we focus on kernel-based regularization and its connections with reproducing
    kernel Hilbert spaces and Bayesian estimation of Gaussian processes. The
    second aim is to demonstrate that learning techniques tailored to the
    specific features of dynamic systems may outperform conventional parametric
    approaches for identification of stable linear systems.
  accessed:
    - year: 2019
      month: 4
      day: 10
  author:
    - family: Pillonetto
      given: Gianluigi
    - family: Dinuzzo
      given: Francesco
    - family: Chen
      given: Tianshi
    - family: De Nicolao
      given: Giuseppe
    - family: Ljung
      given: Lennart
  citation-key: pillonetto_kernel_2014
  container-title: Automatica
  DOI: 10/f236r8
  issue: '3'
  issued:
    - year: 2014
  page: 657-682
  source: DOI.org (Crossref)
  title: >-
    Kernel methods in system identification, machine learning and function
    estimation: A survey
  title-short: >-
    Kernel methods in system identification, machine learning and function
    estimation
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S000510981400020X
  volume: '50'

- id: piroddi_identification_2003
  accessed:
    - year: 2019
      month: 4
      day: 14
  author:
    - family: Piroddi
      given: L.
    - family: Spinelli
      given: W.
  citation-key: piroddi_identification_2003
  container-title: International Journal of Control
  DOI: 10/cc8w93
  ISSN: 0020-7179, 1366-5820
  issue: '17'
  issued:
    - year: 2003
      month: 11
  language: en
  page: 1767-1781
  source: DOI.org (Crossref)
  title: >-
    An identification algorithm for polynomial NARX models based on simulation
    error minimization
  type: article-journal
  URL: http://www.tandfonline.com/doi/abs/10.1080/00207170310001635419
  volume: '76'

- id: piroddi_identification_2003a
  author:
    - family: Piroddi
      given: Luigi
    - family: Spinelli
      given: William
  citation-key: piroddi_identification_2003a
  container-title: International Journal of Control
  DOI: 10.1080/00207170310001635419
  issue: '17'
  issued:
    - year: 2003
  note: '00000'
  page: 1767–1781
  title: >-
    An identification algorithm for polynomial NARX models based on simulation
    error minimization
  type: article-journal
  volume: '76'

- id: piroddi_identification_2003b
  author:
    - family: Piroddi
      given: Luigi
    - family: Spinelli
      given: William
  citation-key: piroddi_identification_2003b
  container-title: International Journal of Control
  DOI: 10/cc8w93
  issue: '17'
  issued:
    - year: 2003
  page: 1767-1781
  title: >-
    An Identification Algorithm for Polynomial NARX Models Based on Simulation
    Error Minimization
  type: article-journal
  volume: '76'

- id: piroddi_simulation_2008
  author:
    - family: Piroddi
      given: Luigi
  citation-key: piroddi_simulation_2008
  container-title: International Journal of Modelling, Identification and Control
  DOI: 10/bmvskt
  issue: '4'
  issued:
    - year: 2008
  page: 392-403
  title: Simulation Error Minimisation Methods for NARX Model Identification
  type: article-journal
  volume: '3'

- id: plantenga_trust_1998
  accessed:
    - year: 2017
      month: 8
      day: 20
  author:
    - family: Plantenga
      given: Todd
  citation-key: plantenga_trust_1998
  container-title: SIAM journal on Scientific Computing
  DOI: 10.1137/S1064827595284403
  issue: '1'
  issued:
    - year: 1998
  note: '00000'
  page: 282–305
  source: Google Scholar
  title: >-
    A trust region method for nonlinear programming based on primal
    interior-point techniques
  type: article-journal
  URL: http://epubs.siam.org/doi/abs/10.1137/S1064827595284403
  volume: '20'

- id: plantenga_trust_1998a
  accessed:
    - year: 2017
      month: 8
      day: 20
  author:
    - family: Plantenga
      given: Todd
  citation-key: plantenga_trust_1998a
  container-title: SIAM journal on Scientific Computing
  DOI: 10/cg4zbf
  issue: '1'
  issued:
    - year: 1998
  page: 282-305
  title: >-
    A Trust Region Method for Nonlinear Programming Based on Primal
    Interior-Point Techniques
  type: article-journal
  volume: '20'

- id: polak_note_1969
  author:
    - family: Polak
      given: Elijah
    - family: Ribiere
      given: Gerard
  citation-key: polak_note_1969
  container-title: Revue française d'informatique et de recherche opérationnelle. Série rouge
  container-title-short: Revue française d'informatique et de recherche opérationnelle. Série rouge
  DOI: 10.1051/m2an/196903R100351
  ISSN: 0373-8000
  issue: '16'
  issued:
    - year: 1969
  note: '00000'
  page: 35-43
  title: Note sur la convergence de méthodes de directions conjuguées
  type: article-journal
  volume: '3'

- id: poljak_introduction_1987
  author:
    - family: Poljak
      given: Boris T.
  citation-key: poljak_introduction_1987
  issued:
    - year: 1987
  publisher: Optimization Software
  title: Introduction to optimization
  type: book

- id: poon_modelbased_2017
  author:
    - family: Poon
      given: Jason
    - family: Jain
      given: Palak
    - family: Konstantakopoulos
      given: Ioannis C
    - family: Spanos
      given: Costas
    - family: Panda
      given: Sanjib Kumar
    - family: Sanders
      given: Seth R
  citation-key: poon_modelbased_2017
  container-title: IEEE Transactions on Power Electronics
  DOI: 10.1109/TPEL.2016.2541342
  issue: '2'
  issued:
    - year: 2017
  note: '00000'
  page: 1419–1430
  title: >-
    Model-based fault detection and identification for switching power
    converters
  type: article-journal
  volume: '32'

- id: poorthuis_utility_2021
  abstract: "AIMS: Atrial fibrillation (AF) is associated with higher risk of stroke. While the prevalence of AF is low in the general population, risk prediction models might identify individuals for selective screening of AF. We aimed to systematically identify and compare the utility of established models to predict prevalent AF.\nMETHODS AND RESULTS: Systematic search of PubMed and EMBASE for risk prediction models for AF. We adapted established risk prediction models and assessed their predictive performance using data from 2.5M individuals who attended vascular screening clinics in the USA and the UK and in the subset of 1.2M individuals with CHA2DS2-VASc ≥2. We assessed discrimination using area under the receiver operating characteristic (AUROC) curves and agreement between observed and predicted cases using calibration plots. After screening 6959 studies, 14 risk prediction models were identified. In our cohort, 10\_464 (0.41%) participants had AF. For discrimination, six prediction model had AUROC curves of 0.70 or above in all individuals and those with CHA2DS2-VASc ≥2. In these models, calibration plots showed very good concordance between predicted and observed risks of AF. The two models with the highest observed prevalence in the highest decile of predicted risk, CHARGE-AF and MHS, showed an observed prevalence of AF of 1.6% with a number needed to screen of 63. Selective screening of the 10% highest risk identified 39% of cases with AF.\nCONCLUSION: Prediction models can reliably identify individuals at high risk of AF. The best performing models showed an almost fourfold higher prevalence of AF by selective screening of individuals in the highest decile of risk compared with systematic screening of all cases.\nREGISTRATION: This systematic review was registered (PROSPERO CRD42019123847)."
  author:
    - family: Poorthuis
      given: Michiel H. F.
    - family: Jones
      given: Nicholas R.
    - family: Sherliker
      given: Paul
    - family: Clack
      given: Rachel
    - family: Borst
      given: Gert J.
      non-dropping-particle: de
    - family: Clarke
      given: Robert
    - family: Lewington
      given: Sarah
    - family: Halliday
      given: Alison
    - family: Bulbulia
      given: Richard
  citation-key: poorthuis_utility_2021
  container-title: European Journal of Preventive Cardiology
  container-title-short: Eur J Prev Cardiol
  DOI: 10.1093/eurjpc/zwaa082
  ISSN: 2047-4881
  issue: '6'
  issued:
    - year: 2021
      month: 5
      day: 22
  language: eng
  page: 586-595
  PMCID: PMC8651014
  PMID: '33624100'
  source: PubMed
  title: >-
    Utility of risk prediction models to detect atrial fibrillation in screened
    participants
  type: article-journal
  volume: '28'

- id: powell_direct_1994
  author:
    - family: Powell
      given: Michael JD
  citation-key: powell_direct_1994
  container-title: Advances in optimization and numerical analysis
  issued:
    - year: 1994
  note: '00000'
  page: 51-67
  publisher: Springer
  title: >-
    A direct search optimization method that models the objective and constraint
    functions by linear interpolation
  type: chapter

- id: powell_direct_1998
  author:
    - family: Powell
      given: MJD
  citation-key: powell_direct_1998
  container-title: Acta numerica
  container-title-short: Acta numerica
  DOI: 10.1017/S0962492900002841
  ISSN: 1474-0508
  issued:
    - year: 1998
  note: '00000'
  page: 287-336
  title: Direct search algorithms for optimization calculations
  type: article-journal
  volume: '7'

- id: powell_efficient_1964
  author:
    - family: Powell
      given: Michael JD
  citation-key: powell_efficient_1964
  container-title: The computer journal
  container-title-short: The computer journal
  DOI: 10.1093/comjnl/7.2.155
  ISSN: 0010-4620
  issue: '2'
  issued:
    - year: 1964
  note: '00000'
  page: 155-162
  title: >-
    An efficient method for finding the minimum of a function of several
    variables without calculating derivatives
  type: article-journal
  volume: '7'

- id: powell_new_1970
  author:
    - family: Powell
      given: Michael JD
  citation-key: powell_new_1970
  container-title: Nonlinear programming
  DOI: 10.1016/B978-0-12-597050-1.50006-3
  issued:
    - year: 1970
  note: '00000'
  page: 31–65
  title: A new algorithm for unconstrained optimization
  type: article-journal

- id: powell_new_1970a
  author:
    - family: Powell
      given: Michael JD
  citation-key: powell_new_1970a
  container-title: Nonlinear programming
  DOI: 10/gfjwq4
  issued:
    - year: 1970
  note: '00551'
  page: 31-65
  title: A New Algorithm for Unconstrained Optimization
  type: article-journal

- id: powell_newuoa_2006
  author:
    - family: Powell
      given: Michael J. D.
  citation-key: powell_newuoa_2006
  container-title: Large-Scale Nonlinear Optimization
  issued:
    - year: 2006
  note: '00000'
  page: 255–297
  publisher: Springer
  title: The NEWUOA software for unconstrained optimization without derivatives
  type: chapter

- id: powell_search_1973
  author:
    - family: Powell
      given: Michael JD
  citation-key: powell_search_1973
  container-title: Mathematical Programming
  DOI: 10.1007/BF01584660
  issue: '1'
  issued:
    - year: 1973
  note: '00000'
  page: 193–201
  title: On search directions for minimization algorithms
  type: article-journal
  volume: '4'

- id: powell_view_2007
  author:
    - family: Powell
      given: Michael JD
  citation-key: powell_view_2007
  container-title: >-
    Mathematics Today-Bulletin of the Institute of Mathematics and its
    Applications
  container-title-short: >-
    Mathematics Today-Bulletin of the Institute of Mathematics and its
    Applications
  ISSN: 1361-2042
  issue: '5'
  issued:
    - year: 2007
  note: '00000'
  page: 170-174
  title: A view of algorithms for optimization without derivatives
  type: article-journal
  volume: '43'

- id: prescott_improved_2021
  abstract: >-
    Concentration of measure has been argued to be the fundamental cause of
    adversarial vulnerability. Mahloujifar et al. presented an empirical way to
    measure the concentration of a data distribution using samples, and employed
    it to find lower bounds on intrinsic robustness for several benchmark
    datasets. However, it remains unclear whether these lower bounds are tight
    enough to provide a useful approximation for the intrinsic robustness of a
    dataset. To gain a deeper understanding of the concentration of measure
    phenomenon, we first extend the Gaussian Isoperimetric Inequality to
    non-spherical Gaussian measures and arbitrary $\ell_p$-norms ($p \geq 2$).
    We leverage these theoretical insights to design a method that uses
    half-spaces to estimate the concentration of any empirical dataset under
    $\ell_p$-norm distance metrics. Our proposed algorithm is more efficient
    than Mahloujifar et al.'s, and our experiments on synthetic datasets and
    image benchmarks demonstrate that it is able to find much tighter intrinsic
    robustness bounds. These tighter estimates provide further evidence that
    rules out intrinsic dataset concentration as a possible explanation for the
    adversarial vulnerability of state-of-the-art classifiers.
  accessed:
    - year: 2021
      month: 8
      day: 6
  author:
    - family: Prescott
      given: Jack
    - family: Zhang
      given: Xiao
    - family: Evans
      given: David
  citation-key: prescott_improved_2021
  container-title: arXiv:2103.12913 [cs, stat]
  issued:
    - year: 2021
      month: 3
      day: 23
  source: arXiv.org
  title: >-
    Improved Estimation of Concentration Under $\ell_p$-Norm Distance Metrics
    Using Half Spaces
  type: article-journal
  URL: http://arxiv.org/abs/2103.12913

- id: press_numerical_1992
  call-number: QA76.73.C15 N865 1992
  citation-key: press_numerical_1992
  edition: 2nd ed
  editor:
    - family: Press
      given: William H.
  event-place: Cambridge ; New York
  ISBN: 978-0-521-43108-8 978-0-521-43720-2
  issued:
    - year: 1992
  number-of-pages: '994'
  publisher: Cambridge University Press
  publisher-place: Cambridge ; New York
  source: Library of Congress ISBN
  title: 'Numerical recipes in C: the art of scientific computing'
  title-short: Numerical recipes in C
  type: book

- id: prineas_minnesota_2009
  author:
    - family: Prineas
      given: Ronald J
    - family: Crow
      given: Richard S
    - family: Zhang
      given: Zhu-Ming
  citation-key: prineas_minnesota_2009
  ISBN: 1-84882-778-4
  issued:
    - year: 2009
  publisher: Springer Science & Business Media
  title: The Minnesota code manual of electrocardiographic findings
  type: book

- id: prineas_minnesota_2010
  accessed:
    - year: 2020
      month: 7
      day: 8
  author:
    - family: Prineas
      given: Ronald J.
    - family: Crow
      given: Richard S.
    - family: Zhang
      given: Zhu-Ming
  citation-key: prineas_minnesota_2010
  DOI: 10.1007/978-1-84882-778-3
  event-place: London
  ISBN: 978-1-84882-777-6 978-1-84882-778-3
  issued:
    - year: 2010
  language: en
  publisher: Springer London
  publisher-place: London
  source: DOI.org (Crossref)
  title: The Minnesota Code Manual of Electrocardiographic Findings
  type: book
  URL: http://link.springer.com/10.1007/978-1-84882-778-3

- id: qian_generalized_2017
  author:
    - family: Qian
      given: X.
    - family: Huang
      given: H.
    - family: Chen
      given: X.
    - family: Huang
      given: T.
  citation-key: qian_generalized_2017
  container-title: IEEE Transactions on Cybernetics
  container-title-short: IEEE Transactions on Cybernetics
  DOI: 10/gfwvbc
  ISSN: 2168-2267
  issue: '11'
  issued:
    - year: 2017
      month: 11
  page: 3634-3648
  title: >-
    Generalized Hybrid Constructive Learning Algorithm for Multioutput RBF
    Networks
  type: article-journal
  volume: '47'

- id: quam_hierarchical_1984
  author:
    - family: Quam
      given: Lynn H
    - family: Center
      given: Artificial Intelligence
  citation-key: quam_hierarchical_1984
  container-title: Readings in computer vision
  issued:
    - year: 1984
  note: '00000'
  page: 80–86
  title: Hierarchical warp stereo
  type: article-journal

- id: rabiner_tutorial_1989
  abstract: >-
    This tutorial provides an overview of the basic theory of hidden Markov
    models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives
    practical details on methods of implementation of the theory along with a
    description of selected applications of the theory to distinct problems in
    speech recognition. Results from a number of original sources are combined
    to provide a single source of acquiring the background required to pursue
    further this area of research. The author first reviews the theory of
    discrete Markov chains and shows how the concept of hidden states, where the
    observation is a probabilistic function of the state, can be used
    effectively. The theory is illustrated with two simple examples, namely
    coin-tossing, and the classic balls-in-urns system. Three fundamental
    problems of HMMs are noted and several practical techniques for solving
    these problems are given. The various types of HMMs that have been studied,
    including ergodic as well as left-right models, are described
  author:
    - family: Rabiner
      given: L. R.
  citation-key: rabiner_tutorial_1989
  container-title: Proceedings of the IEEE
  DOI: 10.1109/5.18626
  ISSN: 0018-9219
  issue: '2'
  issued:
    - year: 1989
      month: 2
  note: '00000'
  page: 257-286
  source: IEEE Xplore
  title: >-
    A tutorial on hidden Markov models and selected applications in speech
    recognition
  type: article-journal
  volume: '77'

- id: rackauckas_comparison_2018
  abstract: >-
    The derivatives of differential equation solutions are commonly used as
    model diagnostics and as part of parameter estimation routines. In this
    manuscript we investigate an implementation of Discrete local Sensitivity
    Analysis via Automatic Differentiation (DSAAD). A non-stiff Lotka-Volterra
    model, a discretization of the two dimensional ($N \times N$) Brusselator
    stiff reaction-diffusion PDE, a stiff non-linear air pollution and a
    non-stiff pharmacokinetic/pharmacodynamic (PK/PD) model were used as
    prototype models for this investigation. Our benchmarks show that on
    sufficiently small (<100 parameters) stiff and non-stiff systems of ODEs,
    forward-mode DSAAD is more efficient than both reverse-mode DSAAD and
    continuous forward/adjoint sensitivity analysis. The scalability of
    continuous adjoint methods is shown to result in better efficiency for
    larger ODE systems such as PDE discretizations. In addition to testing
    efficiency, results on test equations demonstrate the applicability of DSAAD
    to differential-algebraic equations, delay differential equations, and
    hybrid differential equation systems where the event timing and effects are
    dependent on model parameters. Together, these results show that
    language-level automatic differentiation is an efficient method for
    calculating local sensitivities of a wide range of differential equation
    models.
  accessed:
    - year: 2018
      month: 12
      day: 14
  author:
    - family: Rackauckas
      given: Christopher
    - family: Ma
      given: Yingbo
    - family: Dixit
      given: Vaibhav
    - family: Guo
      given: Xingjian
    - family: Innes
      given: Mike
    - family: Revels
      given: Jarrett
    - family: Nyberg
      given: Joakim
    - family: Ivaturi
      given: Vijay
  citation-key: rackauckas_comparison_2018
  container-title: arXiv:1812.01892 [cs]
  issued:
    - year: 2018
      month: 12
      day: 5
  source: arXiv.org
  title: >-
    A Comparison of Automatic Differentiation and Continuous Sensitivity
    Analysis for Derivatives of Differential Equation Solutions
  type: article-journal
  URL: http://arxiv.org/abs/1812.01892

- id: radford_improving_
  abstract: >-
    Natural language understanding comprises a wide range of diverse tasks such
    as textual entailment, question answering, semantic similarity assessment,
    and document classiﬁcation. Although large unlabeled text corpora are
    abundant, labeled data for learning these speciﬁc tasks is scarce, making it
    challenging for discriminatively trained models to perform adequately. We
    demonstrate that large gains on these tasks can be realized by generative
    pre-training of a language model on a diverse corpus of unlabeled text,
    followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to
    previous approaches, we make use of task-aware input transformations during
    ﬁne-tuning to achieve effective transfer while requiring minimal changes to
    the model architecture. We demonstrate the effectiveness of our approach on
    a wide range of benchmarks for natural language understanding. Our general
    task-agnostic model outperforms discriminatively trained models that use
    architectures speciﬁcally crafted for each task, signiﬁcantly improving upon
    the state of the art in 9 out of the 12 tasks studied. For instance, we
    achieve absolute improvements of 8.9% on commonsense reasoning (Stories
    Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual
    entailment (MultiNLI).
  author:
    - family: Radford
      given: Alec
    - family: Narasimhan
      given: Karthik
    - family: Salimans
      given: Tim
    - family: Sutskever
      given: Ilya
  citation-key: radford_improving_
  language: en
  page: '12'
  source: Zotero
  title: Improving Language Understanding by Generative Pre-Training
  type: article-journal

- id: radford_improving_2018
  abstract: >-
    Natural language understanding comprises a wide range of diverse tasks such
    as textual entailment, question answering, semantic similarity assessment,
    and document classiﬁcation. Although large unlabeled text corpora are
    abundant, labeled data for learning these speciﬁc tasks is scarce, making it
    challenging for discriminatively trained models to perform adequately. We
    demonstrate that large gains on these tasks can be realized by generative
    pre-training of a language model on a diverse corpus of unlabeled text,
    followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to
    previous approaches, we make use of task-aware input transformations during
    ﬁne-tuning to achieve effective transfer while requiring minimal changes to
    the model architecture. We demonstrate the effectiveness of our approach on
    a wide range of benchmarks for natural language understanding. Our general
    task-agnostic model outperforms discriminatively trained models that use
    architectures speciﬁcally crafted for each task, signiﬁcantly improving upon
    the state of the art in 9 out of the 12 tasks studied. For instance, we
    achieve absolute improvements of 8.9% on commonsense reasoning (Stories
    Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual
    entailment (MultiNLI).
  author:
    - family: Radford
      given: Alec
    - family: Narasimhan
      given: Karthik
    - family: Salimans
      given: Tim
    - family: Sutskever
      given: Ilya
  citation-key: radford_improving_2018
  issued:
    - year: 2018
  language: en
  page: '12'
  source: Zotero
  title: Improving Language Understanding by Generative Pre-Training
  type: article-journal

- id: radford_language_2019
  abstract: >-
    Natural language processing tasks, such as question answering, machine
    translation, reading comprehension, and summarization, are typically
    approached with supervised learning on taskspeciﬁc datasets. We demonstrate
    that language models begin to learn these tasks without any explicit
    supervision when trained on a new dataset of millions of webpages called
    WebText. When conditioned on a document plus questions, the answers
    generated by the language model reach 55 F1 on the CoQA dataset - matching
    or exceeding the performance of 3 out of 4 baseline systems without using
    the 127,000+ training examples. The capacity of the language model is
    essential to the success of zero-shot task transfer and increasing it
    improves performance in a log-linear fashion across tasks. Our largest
    model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art
    results on 7 out of 8 tested language modeling datasets in a zero-shot
    setting but still underﬁts WebText. Samples from the model reﬂect these
    improvements and contain coherent paragraphs of text. These ﬁndings suggest
    a promising path towards building language processing systems which learn to
    perform tasks from their naturally occurring demonstrations.
  author:
    - family: Radford
      given: Alec
    - family: Wu
      given: Jeffrey
    - family: Child
      given: Rewon
    - family: Luan
      given: David
    - family: Amodei
      given: Dario
    - family: Sutskever
      given: Ilya
  citation-key: radford_language_2019
  issued:
    - year: 2019
  title: Language Models are Unsupervised Multitask Learners
  type: article-journal

- id: raghunath_prediction_2020
  accessed:
    - year: 2020
      month: 6
      day: 16
  author:
    - family: Raghunath
      given: Sushravya
    - family: Ulloa Cerna
      given: Alvaro E.
    - family: Jing
      given: Linyuan
    - family: vanMaanen
      given: David P.
    - family: Stough
      given: Joshua
    - family: Hartzel
      given: Dustin N.
    - family: Leader
      given: Joseph B.
    - family: Kirchner
      given: H. Lester
    - family: Stumpe
      given: Martin C.
    - family: Hafez
      given: Ashraf
    - family: Nemani
      given: Arun
    - family: Carbonati
      given: Tanner
    - family: Johnson
      given: Kipp W.
    - family: Young
      given: Katelyn
    - family: Good
      given: Christopher W.
    - family: Pfeifer
      given: John M.
    - family: Patel
      given: Aalpen A.
    - family: Delisle
      given: Brian P.
    - family: Alsaid
      given: Amro
    - family: Beer
      given: Dominik
    - family: Haggerty
      given: Christopher M.
    - family: Fornwalt
      given: Brandon K.
  citation-key: raghunath_prediction_2020
  container-title: Nature Medicine
  container-title-short: Nat Med
  DOI: 10.1038/s41591-020-0870-z
  ISSN: 1078-8956, 1546-170X
  issued:
    - year: 2020
      month: 5
      day: 11
  language: en
  source: DOI.org (Crossref)
  title: >-
    Prediction of mortality from 12-lead electrocardiogram voltage data using a
    deep neural network
  type: article-journal
  URL: http://www.nature.com/articles/s41591-020-0870-z

- id: rahhal_deep_2016
  accessed:
    - year: 2018
      month: 10
      day: 21
  author:
    - family: Rahhal
      given: M.M. Al
    - family: Bazi
      given: Yakoub
    - family: AlHichri
      given: Haikel
    - family: Alajlan
      given: Naif
    - family: Melgani
      given: Farid
    - family: Yager
      given: R.R.
  citation-key: rahhal_deep_2016
  container-title: Information Sciences
  DOI: 10.1016/j.ins.2016.01.082
  ISSN: '00200255'
  issued:
    - year: 2016
      month: 6
  language: en
  page: 340-354
  source: Crossref
  title: >-
    Deep learning approach for active classification of electrocardiogram
    signals
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0020025516300184
  volume: '345'

- id: rahimi_random_2008
  accessed:
    - year: 2020
      month: 8
      day: 10
  author:
    - family: Rahimi
      given: Ali
    - family: Recht
      given: Benjamin
  citation-key: rahimi_random_2008
  container-title: Advances in Neural Information Processing Systems 20
  issued:
    - year: 2008
  page: 1177–1184
  source: Neural Information Processing Systems
  title: Random Features for Large-Scale Kernel Machines
  type: chapter

- id: rahimi_weighted_2009
  author:
    - family: Rahimi
      given: Ali
    - family: Recht
      given: Benjamin
  citation-key: rahimi_weighted_2009
  container-title: Advances in neural information processing systems
  editor:
    - family: Koller
      given: D.
    - family: Schuurmans
      given: D.
    - family: Bengio
      given: Y.
    - family: Bottou
      given: L.
  issued:
    - year: 2009
  publisher: Curran Associates, Inc.
  title: >-
    Weighted sums of random kitchen sinks: Replacing minimization with
    randomization in learning
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf
  volume: '21'

- id: rahman_neural_2000
  author:
    - family: Rahman
      given: MHR Fazlur
    - family: Devanathan
      given: Rajagopalan
    - family: Kuanyi
      given: Zhu
  citation-key: rahman_neural_2000
  container-title: IEEE Transactions on Industrial Electronics
  issue: '2'
  issued:
    - year: 2000
  note: '00000'
  page: 470–477
  title: Neural network approach for linearizing control of nonlinear process plants
  type: article-journal
  volume: '47'

- id: rahman_neural_2000a
  author:
    - family: Rahman
      given: MHR Fazlur
    - family: Devanathan
      given: Rajagopalan
    - family: Kuanyi
      given: Zhu
  citation-key: rahman_neural_2000a
  container-title: IEEE Transactions on Industrial Electronics
  DOI: 10/b7qwf3
  issue: '2'
  issued:
    - year: 2000
  note: '00030'
  page: 470-477
  title: Neural Network Approach for Linearizing Control of Nonlinear Process Plants
  type: article-journal
  volume: '47'

- id: raj_algorithmic_
  abstract: >-
    Recent studies have shown that heavy tails can emerge in stochastic
    optimization and that the heaviness of the tails have links to the
    generalization error. While these studies have shed light on interesting
    aspects of the generalization behavior in modern settings, they relied on
    strong topological and statistical regularity assumptions, which are hard to
    verify in practice. Furthermore, it has been empirically illustrated that
    the relation between heavy tails and generalization might not always be
    monotonic in practice, contrary to the conclusions of existing theory. In
    this study, we establish novel links between the tail behavior and
    generalization properties of stochastic gradient descent (SGD), through the
    lens of algorithmic stability. We consider a quadratic optimization problem
    and use a heavy-tailed stochastic differential equation (and its Euler
    discretization) as a proxy for modeling the heavy-tailed behavior emerging
    in SGD. We then prove uniform stability bounds, which reveal the following
    outcomes: (i) Without making any exotic assumptions, we show that SGD will
    not be stable if the stability is measured with the squared-loss x → x2,
    whereas it in turn becomes stable if the stability is instead measured with
    a surrogate loss x → |x|p with some p < 2. (ii) Depending on the variance of
    the data, there exists a ‘threshold of heavy-tailedness’ such that the
    generalization error decreases as the tails become heavier, as long as the
    tails are lighter than this threshold. This suggests that the relation
    between heavy tails and generalization is not globally monotonic. (iii) We
    prove matching lower-bounds on uniform stability, implying that our bounds
    are tight in terms of the heaviness of the tails. We support our theory with
    synthetic and real neural network experiments.
  author:
    - family: Raj
      given: Anant
  citation-key: raj_algorithmic_
  language: en
  source: Zotero
  title: >-
    Algorithmic Stability of Heavy-Tailed Stochastic Gradient Descent on Least
    Squares
  type: article-journal

- id: rajpurkar_ai_2022
  abstract: >-
    Artificial intelligence (AI) is poised to broadly reshape medicine,
    potentially improving the experiences of both clinicians and patients. We
    discuss key findings from a 2-year weekly effort to track and share key
    developments in medical AI. We cover prospective studies and advances in
    medical image analysis, which have reduced the gap between research and
    deployment. We also address several promising avenues for novel medical AI
    research, including non-image data sources, unconventional problem
    formulations and human–AI collaboration. Finally, we consider serious
    technical and ethical challenges in issues spanning from data scarcity to
    racial bias. As these challenges are addressed, AI’s potential may be
    realized, making healthcare more accurate, efficient and accessible for
    patients worldwide.
  accessed:
    - year: 2022
      month: 1
      day: 25
  author:
    - family: Rajpurkar
      given: Pranav
    - family: Chen
      given: Emma
    - family: Banerjee
      given: Oishi
    - family: Topol
      given: Eric J.
  citation-key: rajpurkar_ai_2022
  container-title: Nature Medicine
  container-title-short: Nat Med
  DOI: 10.1038/s41591-021-01614-0
  ISSN: 1546-170X
  issued:
    - year: 2022
      month: 1
      day: 20
  language: en
  license: 2022 Springer Nature America, Inc.
  note: |-
    Bandiera_abtest: a
    Cg_type: Nature Research Journals
    Primary_atype: Reviews
    Subject_term: Computational biology and bioinformatics;Medical research
    Subject_term_id: computational-biology-and-bioinformatics;medical-research
  page: 31–38
  publisher: Nature Publishing Group
  source: www.nature.com
  title: AI in health and medicine
  type: article-journal
  URL: https://www.nature.com/articles/s41591-021-01614-0
  volume: '28'

- id: rajpurkar_cardiologistlevel_2017
  abstract: >-
    We develop an algorithm which exceeds the performance of board certified
    cardiologists in detecting a wide range of heart arrhythmias from
    electrocardiograms recorded with a single-lead wearable monitor. We build a
    dataset with more than 500 times the number of unique patients than
    previously studied corpora. On this dataset, we train a 34-layer
    convolutional neural network which maps a sequence of ECG samples to a
    sequence of rhythm classes. Committees of board-certified cardiologists
    annotate a gold standard test set on which we compare the performance of our
    model to that of 6 other individual cardiologists. We exceed the average
    cardiologist performance in both recall (sensitivity) and precision
    (positive predictive value).
  author:
    - family: Rajpurkar
      given: Pranav
    - family: Hannun
      given: Awni Y.
    - family: Haghpanahi
      given: Masoumeh
    - family: Bourn
      given: Codie
    - family: Ng
      given: Andrew Y.
  citation-key: rajpurkar_cardiologistlevel_2017
  container-title: arXiv:1707.01836
  issued:
    - year: 2017
      month: 7
      day: 6
  source: arXiv.org
  title: Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks
  type: article-journal
  URL: http://arxiv.org/abs/1707.01836

- id: rakhlin_statistical_
  author:
    - family: Rakhlin
      given: Alexander
    - family: Sridharan
      given: Karthik
  citation-key: rakhlin_statistical_
  language: en
  page: '259'
  source: Zotero
  title: Statistical Learning and Sequential Prediction
  type: article-journal

- id: rangapuram_deep_2018
  accessed:
    - year: 2018
      month: 12
      day: 13
  author:
    - family: Rangapuram
      given: Syama Sundar
    - family: Seeger
      given: Matthias W
    - family: Gasthaus
      given: Jan
    - family: Stella
      given: Lorenzo
    - family: Wang
      given: Yuyang
    - family: Januschowski
      given: Tim
  citation-key: rangapuram_deep_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 7796–7805
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Deep State Space Models for Time Series Forecasting
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/8004-deep-state-space-models-for-time-series-forecasting.pdf

- id: rani_systematic_2018
  abstract: >-
    Compressive Sensing (CS) is a new sensing modality, which compresses the
    signal being acquired at the time of sensing. Signals can have sparse or
    compressible representation either in original domain or in some transform
    domain. Relying on the sparsity of the signals, CS allows us to sample the
    signal at a rate much below the Nyquist sampling rate. Also, the varied
    reconstruction algorithms of CS can faithfully reconstruct the original
    signal back from fewer compressive measurements. This fact has stimulated
    research interest toward the use of CS in several fields, such as magnetic
    resonance imaging, high-speed video acquisition, and ultrawideband
    communication. This paper reviews the basic theoretical concepts underlying
    CS. To bridge the gap between theory and practicality of CS, different CS
    acquisition strategies and reconstruction approaches are elaborated
    systematically in this paper. The major application areas where CS is
    currently being used are reviewed here. This paper also highlights some of
    the challenges and research directions in this field.
  author:
    - family: Rani
      given: Meenu
    - family: Dhok
      given: S. B.
    - family: Deshmukh
      given: R. B.
  citation-key: rani_systematic_2018
  container-title: IEEE Access
  DOI: 10.1109/ACCESS.2018.2793851
  ISSN: 2169-3536
  issued:
    - year: 2018
  page: 4875-4894
  source: IEEE Xplore
  title: >-
    A Systematic Review of Compressive Sensing: Concepts, Implementations and
    Applications
  title-short: A Systematic Review of Compressive Sensing
  type: article-journal
  volume: '6'

- id: rao_engineering_2009
  author:
    - family: Rao
      given: Singiresu S.
  call-number: TA342 .R36 2009
  citation-key: rao_engineering_2009
  edition: 4th ed
  event-place: Hoboken, N.J
  ISBN: 978-0-470-18352-6
  issued:
    - year: 2009
  note: 'OCLC: ocn320352991'
  number-of-pages: '813'
  publisher: John Wiley & Sons
  publisher-place: Hoboken, N.J
  source: Library of Congress ISBN
  title: 'Engineering optimization: theory and practice'
  title-short: Engineering optimization
  type: book

- id: rashid_power_2011
  abstract: >-
    "Designed to appeal to a new generation of engineering professionals, Power
    Electronics Handbook, 3rd Edition features four new chapters covering
    renewable energy, energy transmission, energy storage, as well as an
    introduction to Distributed and Cogeneration (DCG) technology, including gas
    turbines, gensets, microturbines, wind turbines, variable speed generators,
    photovoltaics and fuel cells, has been gaining momentum for quite some time
    now.smart grid technology. With this book readers should be able to provide
    technical design leadership on assigned power electronics design projects
    and lead the design from the concept to production involving significant
    scope and complexity"--
  call-number: TK7881.15 .P6733 2011
  citation-key: rashid_power_2011
  edition: 3rd ed
  editor:
    - family: Rashid
      given: Muhammad H.
  event-place: Burlington, MA
  ISBN: 978-0-12-382036-5
  issued:
    - year: 2011
  number-of-pages: '1389'
  publisher: Elsevier
  publisher-place: Burlington, MA
  source: Library of Congress ISBN
  title: 'Power electronics handbook: devices, circuits, and applications handbook'
  title-short: Power electronics handbook
  type: book

- id: rasmussen_gaussian_2006
  author:
    - family: Rasmussen
      given: Carl Edward
    - family: Williams
      given: Christopher K. I.
  call-number: QA274.4 .R37 2006
  citation-key: rasmussen_gaussian_2006
  collection-title: Adaptive computation and machine learning
  event-place: Cambridge, Mass
  ISBN: 978-0-262-18253-9
  issued:
    - year: 2006
  note: 'OCLC: ocm61285753'
  number-of-pages: '248'
  publisher: MIT Press
  publisher-place: Cambridge, Mass
  source: Library of Congress ISBN
  title: Gaussian processes for machine learning
  type: book

- id: rassi_american_2012
  accessed:
    - year: 2021
      month: 11
      day: 25
  author:
    - family: Rassi
      given: Anis
    - family: Rassi
      given: Anis
    - family: Marcondes de Rezende
      given: Joffre
  citation-key: rassi_american_2012
  collection-title: Tropical Diseases
  container-title: Infectious Disease Clinics of North America
  container-title-short: Infectious Disease Clinics of North America
  DOI: 10.1016/j.idc.2012.03.002
  ISSN: 0891-5520
  issue: '2'
  issued:
    - year: 2012
      month: 6
      day: 1
  language: en
  page: 275-291
  source: ScienceDirect
  title: American Trypanosomiasis (Chagas Disease)
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0891552012000116
  volume: '26'

- id: rautaharju_aha_2009
  author:
    - family: Rautaharju
      given: Pentti M.
    - family: Surawicz
      given: Borys
    - family: Gettes
      given: Leonard S.
  citation-key: rautaharju_aha_2009
  container-title: Journal of the American College of Cardiology
  container-title-short: Journal of the American College of Cardiology
  DOI: 10/c7vp73
  ISSN: 0735-1097
  issue: '11'
  issued:
    - year: 2009
      month: 3
      day: 17
  page: 982-991
  title: >-
    AHA/ACCF/HRS Recommendations for the Standardization and Interpretation of
    the Electrocardiogram: Part IV: The ST Segment, T and U Waves, and the QT
    Interval A Scientific Statement From the American Heart Association
    Electrocardiography and Arrhythmias Committee, Council on Clinical
    Cardiology; the American College of Cardiology Foundation; and the Heart
    Rhythm Society Endorsed by the International Society for Computerized
    Electrocardiology
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0735109708041363
  volume: '53'

- id: recht_tour_2018
  accessed:
    - year: 2018
      month: 9
      day: 19
  author:
    - family: Recht
      given: Benjamin
  citation-key: recht_tour_2018
  issued:
    - year: 2018
      month: 6
      day: 25
  language: en
  note: '00000'
  source: arxiv.org
  title: 'A Tour of Reinforcement Learning: The View from Continuous Control'
  title-short: A Tour of Reinforcement Learning
  type: article-journal
  URL: https://arxiv.org/abs/1806.09460

- id: reddi_convergence_2018
  author:
    - family: Reddi
      given: Sashank J.
    - family: Kale
      given: Satyen
    - family: Kumar
      given: Sanjiv
  citation-key: reddi_convergence_2018
  container-title: International Conference on Learning Representations
  issued:
    - year: 2018
  note: '00000'
  source: Google Scholar
  title: On the convergence of adam and beyond
  type: paper-conference

- id: redmon_yolo9000_2016
  abstract: >-
    We introduce YOLO9000, a state-of-the-art, real-time object detection system
    that can detect over 9000 object categories. First we propose various
    improvements to the YOLO detection method, both novel and drawn from prior
    work. The improved model, YOLOv2, is state-of-the-art on standard detection
    tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007.
    At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like
    Faster RCNN with ResNet and SSD while still running significantly faster.
    Finally we propose a method to jointly train on object detection and
    classification. Using this method we train YOLO9000 simultaneously on the
    COCO detection dataset and the ImageNet classification dataset. Our joint
    training allows YOLO9000 to predict detections for object classes that don't
    have labelled detection data. We validate our approach on the ImageNet
    detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation
    set despite only having detection data for 44 of the 200 classes. On the 156
    classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than
    just 200 classes; it predicts detections for more than 9000 different object
    categories. And it still runs in real-time.
  author:
    - family: Redmon
      given: Joseph
    - family: Farhadi
      given: Ali
  citation-key: redmon_yolo9000_2016
  container-title: arXiv:1612.08242 [cs]
  issued:
    - year: 2016
      month: 12
      day: 25
  note: '00000'
  source: arXiv.org
  title: 'YOLO9000: Better, Faster, Stronger'
  title-short: YOLO9000
  type: article-journal
  URL: http://arxiv.org/abs/1612.08242

- id: redmon_you_2016
  author:
    - family: Redmon
      given: Joseph
    - family: Divvala
      given: Santosh
    - family: Girshick
      given: Ross
    - family: Farhadi
      given: Ali
  citation-key: redmon_you_2016
  container-title: >-
    Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition
  issued:
    - year: 2016
  note: '00000'
  page: 779–788
  source: Google Scholar
  title: 'You only look once: Unified, real-time object detection'
  title-short: You only look once
  type: paper-conference

- id: redmond_datadriven_2002
  abstract: >-
    Semantic Scholar extracted view of "A data-driven software tool for enabling
    cooperative information sharing among police departments" by Michael Redmond
    et al.
  accessed:
    - year: 2024
      month: 5
      day: 21
  author:
    - family: Redmond
      given: Michael
    - family: Baveja
      given: Alok
  citation-key: redmond_datadriven_2002
  container-title: European Journal of Operational Research
  container-title-short: European Journal of Operational Research
  DOI: 10.1016/S0377-2217(01)00264-8
  ISSN: '03772217'
  issue: '3'
  issued:
    - year: 2002
      month: 9
  language: en
  license: https://www.elsevier.com/tdm/userlicense/1.0/
  page: 660-678
  source: Semantic Scholar
  title: >-
    A data-driven software tool for enabling cooperative information sharing
    among police departments
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0377221701002648
  volume: '141'

- id: rehmer_using_2019
  abstract: >-
    During recent years Deep Learning (DL) methods facilitated impressive
    progress on various fields of research: Deep Convolutional Neural Networks
    (CNN) enabled object classification with to this day unmatched precision,
    while state of the art results in speech recognition and natural language
    processing (NLP) were achieved via gated units such as the LSTM. Although
    recurrent neural network architectures are long established in the field of
    system identification as a realization of an internal dynamics approach [1]
    [2], little research has yet been dedicated towards gated units. The purpose
    of this paper is to evaluate the architectures of recurrent gated units from
    the viewpoint of system identification and test their performance on a
    nonlinear system identification task.
  author:
    - family: Rehmer
      given: Alexander
    - family: Kroll
      given: Andreas
  citation-key: rehmer_using_2019
  container-title: 2019 18th European Control Conference (ECC)
  DOI: 10.23919/ECC.2019.8795631
  event-title: 2019 18th European Control Conference (ECC)
  issued:
    - year: 2019
      month: 6
  page: 2504-2509
  source: IEEE Xplore
  title: On Using Gated Recurrent Units for Nonlinear System Identification
  type: paper-conference

- id: relan_datadriven_2017
  abstract: >-
    Lithium ion batteries are attracting significant and growing interest,
    because their high energy and high power density render them an excellent
    option for energy storage, particularly in hybrid and electric vehicles. In
    this brief, a data-driven polynomial nonlinear state-space model is proposed
    for the operating points at the cusp of linear and nonlinear regimes of the
    battery’s electrical operation, based on the thorough nonparametric
    frequency domain characterization and quantification of the battery’s
    behavior in terms of its linear and nonlinear behavior at different levels
    of the state of charge.
  author:
    - family: Relan
      given: R.
    - family: Firouz
      given: Y.
    - family: Timmermans
      given: J. M.
    - family: Schoukens
      given: J.
  citation-key: relan_datadriven_2017
  container-title: IEEE Transactions on Control Systems Technology
  DOI: 10.1109/TCST.2016.2616380
  ISSN: 1063-6536
  issue: '5'
  issued:
    - year: 2017
      month: 9
  note: '00000'
  page: 1825-1832
  source: IEEE Xplore
  title: >-
    Data-Driven Nonlinear Identification of Li-Ion Battery Based on a Frequency
    Domain Nonparametric Analysis
  type: article-journal
  volume: '25'

- id: ren_faster_2015
  author:
    - family: Ren
      given: Shaoqing
    - family: He
      given: Kaiming
    - family: Girshick
      given: Ross
    - family: Sun
      given: Jian
  citation-key: ren_faster_2015
  container-title: Advances in neural information processing systems
  issued:
    - year: 2015
  note: '00000'
  page: 91–99
  source: Google Scholar
  title: >-
    Faster R-CNN: Towards real-time object detection with region proposal
    networks
  title-short: Faster R-CNN
  type: paper-conference

- id: renda_comparing_2020
  abstract: >-
    Many neural network pruning algorithms proceed in three steps: train the
    network to completion, remove unwanted structure to compress the network,
    and retrain the remaining structure to recover lost accuracy. The standard
    retraining technique, fine-tuning, trains the unpruned weights from their
    final trained values using a small fixed learning rate. In this paper, we
    compare fine-tuning to alternative retraining techniques. Weight rewinding
    (as proposed by Frankle et al., (2019)), rewinds unpruned weights to their
    values from earlier in training and retrains them from there using the
    original training schedule. Learning rate rewinding (which we propose)
    trains the unpruned weights from their final values using the same learning
    rate schedule as weight rewinding. Both rewinding techniques outperform
    fine-tuning, forming the basis of a network-agnostic pruning algorithm that
    matches the accuracy and compression ratios of several more network-specific
    state-of-the-art techniques.
  accessed:
    - year: 2020
      month: 6
      day: 29
  author:
    - family: Renda
      given: Alex
    - family: Frankle
      given: Jonathan
    - family: Carbin
      given: Michael
  citation-key: renda_comparing_2020
  container-title: arXiv:2003.02389 [cs, stat]
  issued:
    - year: 2020
      month: 3
      day: 4
  source: arXiv.org
  title: Comparing Rewinding and Fine-tuning in Neural Network Pruning
  type: article-journal
  URL: http://arxiv.org/abs/2003.02389

- id: reyland_generalized_2014
  author:
    - family: Reyland
      given: John
    - family: Bai
      given: Er‐Wei
  citation-key: reyland_generalized_2014
  container-title: International Journal of Adaptive Control and Signal Processing
  container-title-short: International Journal of Adaptive Control and Signal Processing
  DOI: 10.1002/acs.2437
  ISSN: 1099-1115
  issue: '11'
  issued:
    - year: 2014
  note: '00000'
  page: 1174-1188
  title: >-
    Generalized Wiener system identification: general backlash nonlinearity and
    finite impulse response linear part
  type: article-journal
  volume: '28'

- id: ribeiro_automatic_2018
  abstract: >-
    We present a model for predicting electrocardiogram (ECG) abnormalities in
    short-duration 12-lead ECG signals which outperformed medical doctors on the
    4th year of their cardiology residency. Such exams can provide a full
    evaluation of heart activity and have not been studied in previous
    end-to-end machine learning papers. Using the database of a large telehealth
    network, we built a novel dataset with more than 2 million ECG tracings,
    orders of magnitude larger than those used in previous studies. Moreover,
    our dataset is more realistic, as it consist of 12-lead ECGs recorded during
    standard in-clinics exams. Using this data, we trained a residual neural
    network with 9 convolutional layers to map 7 to 10 second ECG signals to 6
    classes of ECG abnormalities. Future work should extend these results to
    cover a large range of ECG abnormalities, which could improve the
    accessibility of this diagnostic tool and avoid wrong diagnosis from medical
    doctors.
  author:
    - family: Ribeiro
      given: Antônio H.
    - family: Ribeiro
      given: Manoel Horta
    - family: Paixão
      given: Gabriela
    - family: Oliveira
      given: Derick
    - family: Gomes
      given: Paulo R.
    - family: Canazart
      given: Jéssica A.
    - family: Pifano
      given: Milton
    - family: Meira Jr.
      given: Wagner
    - family: Schön
      given: Thomas B.
    - family: Ribeiro
      given: Antonio Luiz
  citation-key: ribeiro_automatic_2018
  container-title: Machine Learning for Health (ML4H) Workshop at NeurIPS
  issued:
    - year: 2018
  license: All rights reserved
  source: arXiv.org
  title: >-
    Automatic Diagnosis of Short-Duration 12-Lead ECG using a Deep Convolutional
    Network
  type: article-journal

- id: ribeiro_automatic_2020
  abstract: >-
    The 12-lead electrocardiogram (ECG) is a major diagnostic test for
    cardiovascular diseases and enhanced automated analysis tools might lead to
    more reliable diagnosis and improved clinical practice. Deep neural networks
    are models composed of stacked transformations that learn tasks by examples.
    Inspired by the success of these models in computer vision, we propose an
    end-to-end approach for the task at hand. We trained deep convolutional
    neural network models in the heterogeneous dataset provided in the Physionet
    2020 Challenge and used an ensemble of seven of these convolutional models
    for the classiﬁcation of abnormalities present in the ECG records. Ensembles
    use the output of multiple models to generate a combined prediction and are
    known to improve performance and generalization when compared to the
    individual models. In our submission, we use an ensemble of neural networks
    with the architecture similar to the one described in Nat Commun 11, 1760
    (2020) for 12-lead ECGs classiﬁcation. On the partially hidden test dataset
    from the challenge, the best-scored entry for our team (the “Code Team”) had
    a performance of 0.657, which place us in the 7-th place team-wise in the
    challenge leaderboard.
  author:
    - family: Ribeiro
      given: Antonio H
    - family: Gedon
      given: Daniel
    - family: Teixeira
      given: Daniel Martins
    - family: Ribeiro
      given: Manoel Horta
    - family: Ribeiro
      given: Antonio L Pinho
    - family: Schon
      given: Thomas B
    - family: Jr
      given: Wagner Meira
  citation-key: ribeiro_automatic_2020
  container-title: 2020 Computing in Cardiology (CinC)
  DOI: 10.22489/CinC.2020.130
  issued:
    - year: 2020
  license: All rights reserved
  title: Automatic 12-lead ECG classiﬁcation using a convolutional network ensemble
  type: paper-conference

- id: ribeiro_automatic_2020a
  author:
    - family: Ribeiro
      given: Antônio H.
    - family: Ribeiro
      given: Manoel Horta
    - family: Paixão
      given: Gabriela M. M.
    - family: Oliveira
      given: Derick M.
    - family: Gomes
      given: Paulo R.
    - family: Canazart
      given: Jéssica A.
    - family: Ferreira
      given: Milton P. S.
    - family: Andersson
      given: Carl R.
    - family: Macfarlane
      given: Peter W.
    - family: Meira Jr.
      given: Wagner
    - family: Schön
      given: Thomas B.
    - family: Ribeiro
      given: Antonio Luiz P.
  citation-key: ribeiro_automatic_2020a
  container-title: Nature Communications
  DOI: 10.1038/s41467-020-15432-4
  issue: '1'
  issued:
    - year: 2020
  license: All rights reserved
  page: '1760'
  source: arXiv.org
  title: Automatic diagnosis of the 12-lead ECG using a deep neural network
  type: article-journal
  volume: '11'

- id: ribeiro_deep_2019
  author:
    - family: Ribeiro
      given: Antonio H
    - family: Andersson
      given: Carl
    - family: Tiels
      given: Koen
    - family: Wahlstrom
      given: Niklas
    - family: Schon
      given: Thomas B
  citation-key: ribeiro_deep_2019
  container-title: Workshop on Nonlinear System Identification
  issued:
    - year: 2019
  license: All rights reserved
  source: Zotero
  title: Deep Convolutional Networks are Useful in System Identiﬁcation
  type: article-journal

- id: ribeiro_exploding_2020
  author:
    - family: Ribeiro
      given: Antônio H.
    - family: Tiels
      given: Koen
    - family: Aguirre
      given: Luis A.
    - family: Schön
      given: Thomas B.
  citation-key: ribeiro_exploding_2020
  container-title: International Conference on Artificial Intelligence and Statistics (AISTATS)
  event-title: '108'
  issued:
    - year: 2020
  license: All rights reserved
  page: 2370-2380
  title: >-
    Beyond exploding and vanishing gradients: attractors and smoothness in the
    analysis of recurrent neural network training
  type: paper-conference
  URL: http://proceedings.mlr.press/v108/ribeiro20a.html
  volume: '108'

- id: ribeiro_how_2021
  author:
    - family: Ribeiro
      given: Antonio H.
    - family: Schon
      given: Thomas B.
  citation-key: ribeiro_how_2021
  container-title: >-
    IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)
  DOI: 10.1109/ICASSP39728.2021.9414627
  issued:
    - year: 2021
  page: 2755-2759
  publisher: IEEE
  title: How convolutional neural networks deal with aliasing
  type: paper-conference

- id: ribeiro_implementacao_2015
  abstract: >-
    A stereo camera is a camera that simultaneously captures two or more images 

    in order to estimate depth. A stereo camera prototype was implemented using
    two image sensors. Its project was based on v200, a commercial camera
    produced by Invent Vision (iVision), company for which this prototype was
    developed. The major steps needed to estimate depth using a stereo camera
    were followed. They are: calibration (estimation of camera geometry and
    parameters), retification (aligning images), correspondence (finding
    correspondent points on the two images and the distance between them)

    and reconstruction (estimation of features depth using triangulation). All
    algorithms needed were implemented using C++ and integrated to the
    prototype. The prototype is able to estimate depth in a range of 0.7 to 20
    meters, it has a resolution on the order of a few centimeters and can reach
    a frame rate of 5 frames per second. The industrial camera v200 has all its
    internal processing computed by a FPGA. It is suggested, as a future work,
    to use the FPGA to speed up the retification and correspondence algorithms.
    The aim of this work was to take a first step towards a stereo camera based
    on v200 to be a competitive commercial product, apt to be sold by

    iVision.
  author:
    - family: Ribeiro
      given: Antonio H.
  citation-key: ribeiro_implementacao_2015
  event-place: Belo Horizonte, Brazil
  genre: BSc Thesis
  issued:
    - year: 2015
      month: 12
      day: 1
  license: All rights reserved
  publisher: Universidade Federal de Minas Gerais
  publisher-place: Belo Horizonte, Brazil
  title: Implementação de uma Câmera Estéreo
  type: thesis

- id: ribeiro_lasso_2018
  author:
    - family: Ribeiro
      given: Antonio H.
    - family: Aguirre
      given: Luis A.
  citation-key: ribeiro_lasso_2018
  container-title: 2018 Annual American Control Conference (ACC)
  DOI: 10.23919/ACC.2018.8430924
  event-title: 2018 Annual American Control Conference (ACC)
  ISBN: 2378-5861
  issued:
    - year: 2018
      month: 6
      day: 27
    - year: 2018
      month: 6
      day: 29
  license: All rights reserved
  page: 5268-5273
  title: Lasso Regularization Paths for NARMAX Models via Coordinate Descent
  type: paper-conference

- id: ribeiro_learning_2020
  author:
    - family: Ribeiro
      given: Antônio H.
  citation-key: ribeiro_learning_2020
  event-place: Belo Horizonte, Brazil
  genre: PhD Thesis
  issued:
    - year: 2020
  license: All rights reserved
  publisher: Universidade Federal de Minas Gerais
  publisher-place: Belo Horizonte, Brazil
  title: >-
    Learning nonlinear differentiable models for signals and systems:  with
    applications
  type: thesis

- id: ribeiro_occam_2021a
  author:
    - family: Ribeiro
      given: Antônio H.
    - family: Hendriks
      given: Johannes N.
    - family: Wills
      given: Adrian G.
    - family: Schön
      given: Thomas B.
  citation-key: ribeiro_occam_2021a
  container-title: Workshop on Nonlinear System Identification
  issued:
    - year: 2021
  license: All rights reserved
  title: >-
    Beyond Occam's Razor in System Identification: Double-Descent when Modeling
    Dynamics
  type: paper-conference

- id: ribeiro_occam_2021b
  author:
    - family: Ribeiro
      given: Antônio H.
    - family: Hendriks
      given: Johannes N.
    - family: Wills
      given: Adrian G.
    - family: Schön
      given: Thomas B.
  citation-key: ribeiro_occam_2021b
  container-title: IFAC Symposium on System Identification (SYSID)
  DOI: 10.1016/j.ifacol.2021.08.341
  issued:
    - year: 2021
  page: 97-102
  source: arXiv.org
  title: >-
    Beyond Occam's Razor in System Identification: Double-Descent when Modeling
    Dynamics
  title-short: Beyond Occam's Razor in System Identification
  type: paper-conference
  volume: '54'

- id: ribeiro_overparameterized_2023
  abstract: >-
    As machine learning models start to be used in critical applications, their
    vulnerabilities and brittleness become a pressing concern. Adversarial
    attacks are a popular framework for studying these vulnerabilities. In this
    work, we study the error of linear regression in the face of adversarial
    attacks. We provide bounds of the error in terms of the traditional risk and
    the parameter norm and show how these bounds can be leveraged and make it
    possible to use analysis from non-adversarial setups to study the
    adversarial risk. The usefulness of these results is illustrated by shedding
    light on whether or not overparameterized linear models can be adversarially
    robust. We show that adding features to linear models might be either a
    source of additional robustness or brittleness. We show that these
    differences appear due to scaling and how the $\ell_1$ and $\ell_2$ norms of
    random projections concentrate. We also show how the reformulation we
    propose allows for solving adversarial training as a convex optimization
    problem. This is then used as a tool to study how adversarial training and
    other regularization methods might affect the robustness of the estimated
    models.
  accessed:
    - year: 2022
      month: 4
      day: 22
  author:
    - family: Ribeiro
      given: Antônio H.
    - family: Schön
      given: Thomas B.
  citation-key: ribeiro_overparameterized_2023
  container-title: IEEE Transactions on Signal Processing
  DOI: 10.1109/TSP.2023.3246228
  issued:
    - year: 2023
  license: All rights reserved
  source: arXiv.org
  title: Overparameterized Linear Regression under Adversarial Attacks
  type: article-journal
  URL: http://arxiv.org/abs/2204.06274

- id: ribeiro_overparametrized_2021
  author:
    - family: Ribeiro
      given: Antonio H
    - family: Schön
      given: Thomas B
  citation-key: ribeiro_overparametrized_2021
  container-title: Workshop on the Theory of Overparameterized Machine Learning (TOPML)
  event-title: Workshop on the Theory of Overparameterized Machine Learning (TOPML))
  issued:
    - year: 2021
      month: 4
  license: All rights reserved
  title: Overparametrized Regression Under L2 Adversarial Attacks
  type: paper-conference

- id: ribeiro_parallel_2017
  abstract: >-
    Neural network models for dynamic systems can be trained either in parallel
    or in series-parallel configurations. Influenced by early arguments, several
    papers justify the choice of series-parallel rather than parallel
    configuration claiming it has a lower computational cost, better stability
    properties during training and provides more accurate results. The purpose
    of this work is to review some of those arguments and to present both
    methods in an unifying framework, showing that parallel and series-parallel
    training actually results from optimal predictors that use different noise
    models. A numerical example illustrate that each method provides better
    results when the noise model they implicit consider are consistent with the
    error in the data. Furthermore, it is argued that for feedforward networks
    with bounded activation functions the possible lack of stability does not
    jeopardize the training; and, a novel complexity analysis indicates the
    computational cost in the two configurations is not significantly different.
    This is confirmed through numerical examples.
  author:
    - family: Ribeiro
      given: Antônio H.
    - family: Aguirre
      given: Luis A.
  citation-key: ribeiro_parallel_2017
  container-title: arXiv:1706.07119 [cs]
  issued:
    - year: 2017
      month: 6
  note: '00000'
  title: >-
    "Parallel Training Considered Harmful?": Comparing Series-Parallel and
    Parallel Feedforward Network Training
  title-short: '"Parallel Training Considered Harmful?'
  type: article-journal

- id: ribeiro_parallel_2018
  abstract: "Neural network models for dynamic systems can be trained either in parallel or in series-parallel configurations. Influenced by early arguments, several papers justify the choice of series-parallel rather than parallel configuration claiming it has a lower computational cost, better stability properties during training and provides more accurate results. Other published results, on the other hand, defend parallel training as being more robust and capable of yielding more accurate long-term predictions. The main contribution of this paper is to present a study comparing both methods under the same unified framework with special attention to three aspects: (i)\_robustness of the estimation in the presence of noise; (ii)\_computational cost; and, (iii)\_convergence. A unifying mathematical framework and simulation studies show situations where each training method provides superior validation results and suggest that parallel training is generally better in more realistic scenarios. An example using measured data seems to reinforce such a claim. Complexity analysis and numerical examples show that both methods have similar computational cost although series-parallel training is more amenable to parallelization. Some informal discussion about stability and convergence properties is presented and explored in the examples."
  author:
    - family: Ribeiro
      given: Antônio H.
    - family: Aguirre
      given: Luis A.
  citation-key: ribeiro_parallel_2018
  container-title: Neurocomputing
  container-title-short: Neurocomputing
  DOI: 10.1016/j.neucom.2018.07.071
  ISSN: 0925-2312
  issued:
    - year: 2018
      month: 11
      day: 17
  license: All rights reserved
  page: 222-231
  title: >-
    ''Parallel Training Considered Harmful?'': Comparing series-parallel and
    parallel feedforward network training
  type: article-journal
  volume: '316'

- id: ribeiro_recurrent_2017
  author:
    - family: Ribeiro
      given: Antônio H.
  citation-key: ribeiro_recurrent_2017
  event-place: Belo Horizonte, Brazil
  genre: MSc Dissertation
  issued:
    - year: 2017
  license: All rights reserved
  publisher: Universidade Federal de Minas Gerais
  publisher-place: Belo Horizonte, Brazil
  source: Zotero
  title: Recurrent Structures in System Identification
  type: thesis

- id: ribeiro_regularization_2023
  author:
    - family: Ribeiro
      given: Antônio H
    - family: Zachariah
      given: Dave
    - family: Bach
      given: Francis
    - family: Schön
      given: Thomas B.
  citation-key: ribeiro_regularization_2023
  container-title: Advances in Neural Information Processing Systems (NeurIPS)
  issued:
    - year: 2023
  title: Regularization properties of adversarially-trained linear regression
  type: article-journal

- id: ribeiro_relacoes_2014
  author:
    - family: Ribeiro
      given: Antônio H.
    - family: Aguirre
      given: Luis A.
  citation-key: ribeiro_relacoes_2014
  container-title: XX Congresso Brasileiro de Automática
  issued:
    - year: 2014
  license: All rights reserved
  title: Relaçoes Estáticas de Modelos NARX MISO e sua Representaçao de Hammerstein
  type: paper-conference
  URL: http://www.swge.inf.br/CBA2014/anais/PDF/1569890815.pdf

- id: ribeiro_selecting_2015
  author:
    - family: Ribeiro
      given: Antônio H.
    - family: Aguirre
      given: Luis A.
  citation-key: ribeiro_selecting_2015
  container-title: IFAC Workshop on Automatic Control in Offshore Oil and Gas Production
  DOI: 10.1016/j.ifacol.2015.08.024
  issue: '6'
  issued:
    - year: 2015
  license: All rights reserved
  page: 154–158
  title: >-
    Selecting transients automatically for the identification of models for an
    oil well
  type: article-journal
  volume: '48'

- id: ribeiro_shooting_2017
  abstract: >-
    This paper studies parameter estimation of output error (OE) models. The
    commonly used approach of minimizing the free-run simulation error is called
    single shooting in contrast with the new multiple shooting approach proposed
    in this paper, for which the free-run simulation error of sub-datasets is
    minimized subject to equality constraints. The names “single shooting” and
    “multiple shooting” are used due to the similarities with techniques for
    estimating ODE (ordinary differential equation) parameters. Examples with
    nonlinear polynomial models illustrate the advantages of OE models as well
    as the capability of the multiple shooting approach to avoid undesirable
    local minima.
  author:
    - family: Ribeiro
      given: Antônio H.
    - family: Aguirre
      given: Luis A.
  citation-key: ribeiro_shooting_2017
  container-title: IFAC World Congress
  container-title-short: IFAC-PapersOnLine
  DOI: 10.1016/j.ifacol.2017.08.2421
  ISSN: 2405-8963
  issue: '1'
  issued:
    - year: 2017
      month: 7
      day: 1
  license: All rights reserved
  page: 13998-14003
  title: Shooting Methods for Parameter Estimation of Output Error Models
  type: article-journal
  volume: '50'

- id: ribeiro_smoothness_2020
  abstract: >-
    We shed new light on the smoothness of optimization problems arising in
    prediction error parameter estimation of linear and nonlinear systems. We
    show that for regions of the parameter space where the model is not
    contractive, the Lipschitz constant and beta-smoothness of the objective
    function might blow up exponentially with the simulation length, making it
    hard to numerically find minima within those regions or, even, to escape
    from them. In addition to providing theoretical understanding of this
    problem, this paper also proposes the use of multiple shooting as a viable
    solution. The proposed method minimizes the error between a prediction model
    and the observed values. Rather than running the prediction model over the
    entire dataset, multiple shooting splits the data into smaller subsets and
    runs the prediction model over each subset, making the simulation length a
    design parameter and making it possible to solve problems that would be
    infeasible using a standard approach. The equivalence to the original
    problem is obtained by including constraints in the optimization. The new
    method is illustrated by estimating the parameters of nonlinear systems with
    chaotic or unstable behavior, as well as neural networks. We also present a
    comparative analysis of the proposed method with multi-step-ahead prediction
    error minimization.
  author:
    - family: Ribeiro
      given: Antônio H.
    - family: Tiels
      given: Koen
    - family: Umenberger
      given: Jack
    - family: Schön
      given: Thomas B.
    - family: Aguirre
      given: Luis A.
  citation-key: ribeiro_smoothness_2020
  container-title: Automatica
  DOI: 10.1016/j.automatica.2020.109158
  issued:
    - year: 2020
      month: 11
  license: All rights reserved
  page: '109158'
  source: arXiv.org
  title: On the smoothness of nonlinear system identification
  type: article-journal
  volume: '121'

- id: ribeiro_surprises_2022
  abstract: >-
    State-of-the-art machine learning models can be vulnerable to very small
    input perturbations that are adversarially constructed. Adversarial training
    is one of the most effective approaches to defend against such examples. We
    show that for linear regression problems, adversarial training can be
    formulated as a convex problem. This fact is then used to show that
    $\ell_\infty$-adversarial training produces sparse solutions and has many
    similarities to the lasso method. Similarly, $\ell_2$-adversarial training
    has similarities with ridge regression. We use a robust regression framework
    to analyze and understand these similarities and also point to some
    differences. Finally, we show how adversarial training behaves differently
    from other regularization methods when estimating overparameterized models
    (i.e., models with more parameters than datapoints). It minimizes a sum of
    three terms which regularizes the solution, but unlike lasso and ridge
    regression, it can sharply transition into an interpolation mode. We show
    that for sufficiently many features or sufficiently small regularization
    parameters, the learned model perfectly interpolates the training data while
    still exhibiting good out-of-sample performance.
  accessed:
    - year: 2022
      month: 5
      day: 26
  author:
    - family: Ribeiro
      given: Antônio H.
    - family: Zachariah
      given: Dave
    - family: Schön
      given: Thomas B.
  citation-key: ribeiro_surprises_2022
  container-title: arXiv:2205.12695
  DOI: 10.48550/arXiv.2205.12695
  issued:
    - year: 2022
      month: 5
      day: 25
  license: All rights reserved
  source: arXiv.org
  title: Surprises in adversarially-trained linear regression
  type: article-journal
  URL: http://arxiv.org/abs/2205.12695

- id: ribeiro_teleelectrocardiography_2019
  abstract: >-
    Digital electrocardiographs are now widely available and a large number of
    digital electrocardiograms (ECGs) have been recorded and stored. The present
    study describes the development and clinical applications of a large
    database of such digital ECGs, namely the CODE (Clinical Outcomes in Digital
    Electrocardiology) study. ECGs obtained by the Telehealth Network of Minas
    Gerais, Brazil, from 2010 to 17, were organized in a structured database. A
    hierarchical free-text machine learning algorithm recognized specific ECG
    diagnoses from cardiologist reports. The Glasgow ECG Analysis Program
    provided Minnesota Codes and automatic diagnostic statements. The presence
    of a specific ECG abnormality was considered when both automatic and medical
    diagnosis were concordant; cases of discordance were decided using
    heuristisc rules and manual review. The ECG database was linked to the
    national mortality information system using probabilistic linkage methods.
    From 2,470,424 ECGs, 1,773,689 patients were identified. After excluding the
    ECGs with technical problems and patients <16 years-old, 1,558,415 patients
    were studied. High performance measures were obtained using an end-to-end
    deep neural network trained to detect 6 types of ECG abnormalities, with F1
    scores >80% and specificity >99% in an independent test dataset. We also
    evaluated the risk of mortality associated with the presence of atrial
    fibrillation (AF), which showed that AF was a strong predictor of
    cardiovascular mortality and mortality for all causes, with increased risk
    in women. In conclusion, a large database that comprises all ECGs performed
    by a large telehealth network can be useful for further developments in the
    field of digital electrocardiography, clinical cardiology and cardiovascular
    epidemiology.
  author:
    - family: Ribeiro
      given: Antonio Luiz P.
    - family: Paixão
      given: Gabriela M. M.
    - family: Gomes
      given: Paulo R.
    - family: Ribeiro
      given: Manoel Horta
    - family: Ribeiro
      given: Antônio H.
    - family: Canazart
      given: Jéssica A.
    - family: Oliveira
      given: Derick M.
    - family: Ferreira
      given: Milton P.
    - family: Lima
      given: Emilly M.
    - family: Moraes
      given: Jermana Lopes
      non-dropping-particle: de
    - family: Castro
      given: Nathalia
    - family: Ribeiro
      given: Leonardo B.
    - family: MacFarlane
      given: Peter W.
  citation-key: ribeiro_teleelectrocardiography_2019
  container-title: Journal of Electrocardiology
  container-title-short: Journal of Electrocardiology
  DOI: 10/gf7pwg
  ISSN: 0022-0736
  issued:
    - year: 2019
      month: 9
      day: 7
  license: All rights reserved
  source: ScienceDirect
  title: >-
    Tele-electrocardiography and bigdata: The CODE (Clinical Outcomes in Digital
    Electrocardiography) study
  type: article-journal

- id: rigollet_lecturenotes_2015
  author:
    - family: Rigollet
      given: Philippe
  citation-key: rigollet_lecturenotes_2015
  issued:
    - year: 2015
  title: 'LectureNotes: 18.S997- HighDimensionalStatistics'
  type: book

- id: rockafellar_convex_1970
  author:
    - family: Rockafellar
      given: R.Tyrell
  citation-key: rockafellar_convex_1970
  issued:
    - year: 1970
  title: Convex Analysis
  type: book

- id: rojas_robust_2008
  author:
    - family: Rojas
      given: Cristian R.
  citation-key: rojas_robust_2008
  event-place: Australia
  issued:
    - year: 2008
  publisher: The University of Newcastle
  publisher-place: Australia
  title: Robust Experiment Design
  type: thesis

- id: rojas-carulla_invariant_2018
  abstract: >-
    Methods of transfer learning try to combine knowledge from several related
    tasks (or domains) to improve performance on a test task. Inspired by causal
    methodology, we relax the usual covariate shift assumption and assume that
    it holds true for a subset of predictor variables: the conditional
    distribution of the target variable given this subset of predictors is
    invariant over all tasks. We show how this assumption can be motivated from
    ideas in the ﬁeld of causality. We focus on the problem of Domain
    Generalization, in which no examples from the test task are observed. We
    prove that in an adversarial setting using this subset for prediction is
    optimal in Domain Generalization; we further provide examples, in which the
    tasks are suﬃciently diverse and the estimator therefore outperforms pooling
    the data, even on average. If examples from the test task are available, we
    also provide a method to transfer knowledge from the training tasks and
    exploit all available features for prediction. However, we provide no
    guarantees for this method. We introduce a practical method which allows for
    automatic inference of the above subset and provide corresponding code. We
    present results on synthetic data sets and a gene deletion data set.
  author:
    - family: Rojas-Carulla
      given: Mateo
    - family: Scholkopf
      given: Bernhard
    - family: Turner
      given: Richard
    - family: Peters
      given: Jonas
  citation-key: rojas-carulla_invariant_2018
  container-title: Journal of Machine Learning Research
  issue: '19'
  issued:
    - year: 2018
  language: en
  page: 1--34
  source: Zotero
  title: Invariant Models for Causal Transfer Learning
  type: article-journal

- id: roll_piecewise_2008
  abstract: >-
    Recently, pathfollowing algorithms for parametric optimization problems with
    piecewise linear solution paths have been developed within the field of
    regularized regression. This paper presents a generalization of these
    algorithms to a wider class of problems. It is shown that the approach can
    be applied to the nonparametric system identification method, Direct Weight
    Optimization (DWO), and be used to enhance the computational efficiency of
    this method. The most important design parameter in the DWO method is a
    parameter (λ) controlling the bias-variance trade-off, and the use of
    parametric optimization with piecewise linear solution paths means that the
    DWO estimates can be efficiently computed for all values of λ
    simultaneously. This allows for designing computationally attractive
    adaptive bandwidth selection algorithms. One such algorithm for DWO is
    proposed and demonstrated in two examples.
  author:
    - family: Roll
      given: Jacob
  citation-key: roll_piecewise_2008
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10.1016/j.automatica.2008.03.020
  ISSN: 0005-1098
  issue: '11'
  issued:
    - year: 2008
      month: 11
      day: 1
  note: '00000'
  page: 2745-2753
  source: ScienceDirect
  title: >-
    Piecewise linear solution paths with application to direct weight
    optimization
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S000510980800263X
  volume: '44'

- id: romerougalde_computational_2015
  abstract: >-
    Models play an important role in many engineering fields. Therefore, the
    goal in system identification is to find the good balance between the
    accuracy, complexity and computational cost of such identification models.
    In a previous work (Romero-Ugalde et al., 2013 [1]), we focused on the topic
    of providing balanced accuracy/complexity models by proposing a dedicated
    neural network design and a model complexity reduction approach. In this
    paper, we focus on the reduction of the computational cost required to
    achieve these balanced models. More precisely, the improvement of the
    preceding method presented here leads to a significantly computational cost
    reduction of the neural network training phase. Even if this reduction is
    achieved by a convenient choice of the activation functions and the initial
    conditions of the synaptic weights, the proposed architecture leads to a
    wide range of models among the most encountered in the literature assuring
    the interest of such a method. To validate the proposed approach, two
    different systems are identified. The first one corresponds to the
    unavoidable Wiener–Hammerstein system proposed in SYSID2009 as a benchmark.
    The second system is a flexible robot arm. Results show the interest of the
    proposed reduction methods.
  author:
    - family: Romero Ugalde
      given: Hector M.
    - family: Carmona
      given: Jean-Claude
    - family: Reyes-Reyes
      given: Juan
    - family: Alvarado
      given: Victor M.
    - family: Mantilla
      given: Juan
  citation-key: romerougalde_computational_2015
  container-title: Neurocomputing
  container-title-short: Neurocomputing
  DOI: 10.1016/j.neucom.2015.04.022
  ISSN: 0925-2312
  issued:
    - year: 2015
      month: 10
      day: 20
  note: '00000'
  page: 96-108
  source: ScienceDirect
  title: >-
    Computational cost improvement of neural network models in black box
    nonlinear system identification
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0925231215004695
  volume: '166'

- id: ronneberger_unet_2015
  abstract: >-
    There is large consent that successful training of deep networks requires
    many thousand annotated training samples. In this paper, we present a
    network and training strategy that relies on the strong use of data
    augmentation to use the available annotated samples more efficiently. The
    architecture consists of a contracting path to capture context and a
    symmetric expanding path that enables precise localization. We show that
    such a network can be trained end-to-end from very few images and
    outperforms the prior best method (a sliding-window convolutional network)
    on the ISBI challenge for segmentation of neuronal structures in electron
    microscopic stacks. Using the same network trained on transmitted light
    microscopy images (phase contrast and DIC) we won the ISBI cell tracking
    challenge 2015 in these categories by a large margin. Moreover, the network
    is fast. Segmentation of a 512x512 image takes less than a second on a
    recent GPU. The full implementation (based on Caffe) and the trained
    networks are available at
    http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .
  accessed:
    - year: 2019
      month: 6
      day: 10
  author:
    - family: Ronneberger
      given: Olaf
    - family: Fischer
      given: Philipp
    - family: Brox
      given: Thomas
  citation-key: ronneberger_unet_2015
  container-title: arXiv:1505.04597 [cs]
  issued:
    - year: 2015
      month: 5
      day: 18
  source: arXiv.org
  title: 'U-Net: Convolutional Networks for Biomedical Image Segmentation'
  title-short: U-Net
  type: article-journal
  URL: http://arxiv.org/abs/1505.04597

- id: rosenblatt_perceptron_1958
  author:
    - family: Rosenblatt
      given: Frank
  citation-key: rosenblatt_perceptron_1958
  container-title: Psychological review
  container-title-short: Psychological review
  ISSN: 1939-1471
  issue: '6'
  issued:
    - year: 1958
  page: '386'
  title: >-
    The perceptron: a probabilistic model for information storage and
    organization in the brain.
  type: article-journal
  volume: '65'

- id: rosipal_overview_2006
  accessed:
    - year: 2017
      month: 9
      day: 8
  author:
    - family: Rosipal
      given: Roman
    - family: Krämer
      given: Nicole
  citation-key: rosipal_overview_2006
  container-title: Subspace, Latent Structure and Feature Selection
  DOI: 10.1007/11752790_2
  editor:
    - family: Saunders
      given: Craig
    - family: Grobelnik
      given: Marko
    - family: Gunn
      given: Steve
    - family: Shawe-Taylor
      given: John
  event-place: Berlin, Heidelberg
  ISBN: 978-3-540-34137-6 978-3-540-34138-3
  issued:
    - year: 2006
  note: '00000'
  page: 34-51
  publisher: Springer Berlin Heidelberg
  publisher-place: Berlin, Heidelberg
  source: CrossRef
  title: Overview and Recent Advances in Partial Least Squares
  type: chapter
  URL: http://link.springer.com/10.1007/11752790_2
  volume: '3940'

- id: rossetto_improving_
  abstract: >-
    Wavelet pooling methods can improve the classiﬁcation accuracy of
    Convolutional Neural Networks (CNNs). Combining wavelet pooling with the
    Nesterov-accelerated Adam (NAdam) gradient calculation method can improve
    both the accuracy of the CNN. We have implemented wavelet pooling with NAdam
    in this work using both a Haar wavelet (WavPool-NH ) and a Shannon wavelet
    (WavPool-NS ). The WavPool-NH and WavPoolNS methods are most accurate of the
    methods we considered for the MNIST and LIDCIDRI lung tumor data-sets. The
    WavPool-NH and WavPool-NS implementations have an accuracy of 95.92% and
    95.52%, respectively, on the LIDC-IDRI data-set. This is an improvement from
    the 92.93% accuracy obtained on this data-set with the max pooling method.
    The WavPool methods also avoid overﬁtting which is a concern with max
    pooling. We also found WavPool performed fairly well on the CIFAR-10
    data-set, however, overﬁtting was an issue with all the methods we
    considered. Wavelet pooling, especially when combined with an adaptive
    gradient and wavelets chosen speciﬁcally for the data, has the potential to
    outperform current methods.
  accessed:
    - year: 2020
      month: 7
      day: 7
  author:
    - family: Rossetto
      given: Allison
    - family: Zhou
      given: Wenjin
  citation-key: rossetto_improving_
  DOI: 10.29007/9c5j
  event-title: >-
    Proceedings of 11th International Conference on Bioinformatics and
    Computational Biology
  language: en
  page: 84-73
  source: DOI.org (Crossref)
  title: >-
    Improving Classification with CNNs using Wavelet Pooling with
    Nesterov-Accelerated Adam
  type: paper-conference
  URL: https://easychair.org/publications/paper/6Mbb

- id: rossiter_modelling_2001
  accessed:
    - year: 2019
      month: 4
      day: 4
  author:
    - family: Rossiter
      given: J. A.
    - family: Kouvaritakis
      given: B.
  citation-key: rossiter_modelling_2001
  container-title: International Journal of Control
  DOI: 10/dv4phx
  ISSN: 0020-7179, 1366-5820
  issue: '11'
  issued:
    - year: 2001
      month: 1
  language: en
  page: 1085-1095
  source: Crossref
  title: Modelling and implicit modelling for predictive control
  type: article-journal
  URL: http://www.tandfonline.com/doi/full/10.1080/00207170110054129
  volume: '74'

- id: rossler_equation_1976
  abstract: >-
    A prototype equation to the Lorenz model of turbulence contains just one
    (second-order) nonlinearity in one variable. The flow in state space allows
    for a “folded” Poincaré map (horseshoe map). Many more natural and
    artificial systems are governed by this type of equation.
  accessed:
    - year: 2021
      month: 2
      day: 22
  author:
    - family: Rössler
      given: O. E.
  citation-key: rossler_equation_1976
  container-title: Physics Letters A
  container-title-short: Physics Letters A
  DOI: 10.1016/0375-9601(76)90101-8
  ISSN: 0375-9601
  issue: '5'
  issued:
    - year: 1976
      month: 7
      day: 12
  language: en
  page: 397-398
  source: ScienceDirect
  title: An equation for continuous chaos
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/0375960176901018
  volume: '57'

- id: roth_global_2018
  abstract: >-
    <h2>Summary</h2><h3>Background</h3><p>Global development goals increasingly
    rely on country-specific estimates for benchmarking a nation's progress. To
    meet this need, the Global Burden of Diseases, Injuries, and Risk Factors
    Study (GBD) 2016 estimated global, regional, national, and, for selected
    locations, subnational cause-specific mortality beginning in the year 1980.
    Here we report an update to that study, making use of newly available data
    and improved methods. GBD 2017 provides a comprehensive assessment of
    cause-specific mortality for 282 causes in 195 countries and territories
    from 1980 to 2017.</p><h3>Methods</h3><p>The causes of death database is
    composed of vital registration (VR), verbal autopsy (VA), registry, survey,
    police, and surveillance data. GBD 2017 added ten VA studies, 127
    country-years of VR data, 502 cancer-registry country-years, and an
    additional surveillance country-year. Expansions of the GBD cause of death
    hierarchy resulted in 18 additional causes estimated for GBD 2017. Newly
    available data led to subnational estimates for five additional
    countries—Ethiopia, Iran, New Zealand, Norway, and Russia. Deaths assigned
    International Classification of Diseases (ICD) codes for non-specific,
    implausible, or intermediate causes of death were reassigned to underlying
    causes by redistribution algorithms that were incorporated into uncertainty
    estimation. We used statistical modelling tools developed for GBD, including
    the Cause of Death Ensemble model (CODEm), to generate cause fractions and
    cause-specific death rates for each location, year, age, and sex. Instead of
    using UN estimates as in previous versions, GBD 2017 independently estimated
    population size and fertility rate for all locations. Years of life lost
    (YLLs) were then calculated as the sum of each death multiplied by the
    standard life expectancy at each age. All rates reported here are
    age-standardised.</p><h3>Findings</h3><p>At the broadest grouping of causes
    of death (Level 1), non-communicable diseases (NCDs) comprised the greatest
    fraction of deaths, contributing to 73·4% (95% uncertainty interval \[UI]
    72·5–74·1) of total deaths in 2017, while communicable, maternal, neonatal,
    and nutritional (CMNN) causes accounted for 18·6% (17·9–19·6), and injuries
    8·0% (7·7–8·2). Total numbers of deaths from NCD causes increased from 2007
    to 2017 by 22·7% (21·5–23·9), representing an additional 7·61 million
    (7·20–8·01) deaths estimated in 2017 versus 2007. The death rate from NCDs
    decreased globally by 7·9% (7·0–8·8). The number of deaths for CMNN causes
    decreased by 22·2% (20·0–24·0) and the death rate by 31·8% (30·1–33·3).
    Total deaths from injuries increased by 2·3% (0·5–4·0) between 2007 and
    2017, and the death rate from injuries decreased by 13·7% (12·2–15·1) to
    57·9 deaths (55·9–59·2) per 100 000 in 2017. Deaths from substance use
    disorders also increased, rising from 284 000 deaths (268 000–289 000)
    globally in 2007 to 352 000 (334 000–363 000) in 2017. Between 2007 and
    2017, total deaths from conflict and terrorism increased by 118·0%
    (88·8–148·6). A greater reduction in total deaths and death rates was
    observed for some CMNN causes among children younger than 5 years than for
    older adults, such as a 36·4% (32·2–40·6) reduction in deaths from lower
    respiratory infections for children younger than 5 years compared with a
    33·6% (31·2–36·1) increase in adults older than 70 years. Globally, the
    number of deaths was greater for men than for women at most ages in 2017,
    except at ages older than 85 years. Trends in global YLLs reflect an
    epidemiological transition, with decreases in total YLLs from enteric
    infections, respiratory infections and tuberculosis, and maternal and
    neonatal disorders between 1990 and 2017; these were generally greater in
    magnitude at the lowest levels of the Socio-demographic Index (SDI). At the
    same time, there were large increases in YLLs from neoplasms and
    cardiovascular diseases. YLL rates decreased across the five leading Level 2
    causes in all SDI quintiles. The leading causes of YLLs in 1990—neonatal
    disorders, lower respiratory infections, and diarrhoeal diseases—were ranked
    second, fourth, and fifth, in 2017. Meanwhile, estimated YLLs increased for
    ischaemic heart disease (ranked first in 2017) and stroke (ranked third),
    even though YLL rates decreased. Population growth contributed to increased
    total deaths across the 20 leading Level 2 causes of mortality between 2007
    and 2017. Decreases in the cause-specific mortality rate reduced the effect
    of population growth for all but three causes: substance use disorders,
    neurological disorders, and skin and subcutaneous
    diseases.</p><h3>Interpretation</h3><p>Improvements in global health have
    been unevenly distributed among populations. Deaths due to injuries,
    substance use disorders, armed conflict and terrorism, neoplasms, and
    cardiovascular disease are expanding threats to global health. For causes of
    death such as lower respiratory and enteric infections, more rapid progress
    occurred for children than for the oldest adults, and there is continuing
    disparity in mortality rates by sex across age groups. Reductions in the
    death rate of some common diseases are themselves slowing or have ceased,
    primarily for NCDs, and the death rate for selected causes has increased in
    the past decade.</p><h3>Funding</h3><p>Bill & Melinda Gates Foundation.</p>
  accessed:
    - year: 2019
      month: 1
      day: 14
  author:
    - family: Roth
      given: Gregory A.
    - family: Abate
      given: Degu
    - family: Abate
      given: Kalkidan Hassen
    - family: Abay
      given: Solomon M.
    - family: Abbafati
      given: Cristiana
    - family: Abbasi
      given: Nooshin
    - family: Abbastabar
      given: Hedayat
    - family: Abd-Allah
      given: Foad
    - family: Abdela
      given: Jemal
    - family: Abdelalim
      given: Ahmed
    - family: Abdollahpour
      given: Ibrahim
    - family: Abdulkader
      given: Rizwan Suliankatchi
    - family: Abebe
      given: Haftom Temesgen
    - family: Abebe
      given: Molla
    - family: Abebe
      given: Zegeye
    - family: Abejie
      given: Ayenew Negesse
    - family: Abera
      given: Semaw F.
    - family: Abil
      given: Olifan Zewdie
    - family: Abraha
      given: Haftom Niguse
    - family: Abrham
      given: Aklilu Roba
    - family: Abu-Raddad
      given: Laith Jamal
    - family: Accrombessi
      given: Manfred Mario Kokou
    - family: Acharya
      given: Dilaram
    - family: Adamu
      given: Abdu A.
    - family: Adebayo
      given: Oladimeji M.
    - family: Adedoyin
      given: Rufus Adesoji
    - family: Adekanmbi
      given: Victor
    - family: Adetokunboh
      given: Olatunji O.
    - family: Adhena
      given: Beyene Meressa
    - family: Adib
      given: Mina G.
    - family: Admasie
      given: Amha
    - family: Afshin
      given: Ashkan
    - family: Agarwal
      given: Gina
    - family: Agesa
      given: Kareha M.
    - family: Agrawal
      given: Anurag
    - family: Agrawal
      given: Sutapa
    - family: Ahmadi
      given: Alireza
    - family: Ahmadi
      given: Mehdi
    - family: Ahmed
      given: Muktar Beshir
    - family: Ahmed
      given: Sayem
    - family: Aichour
      given: Amani Nidhal
    - family: Aichour
      given: Ibtihel
    - family: Aichour
      given: Miloud Taki Eddine
    - family: Akbari
      given: Mohammad Esmaeil
    - family: Akinyemi
      given: Rufus Olusola
    - family: Akseer
      given: Nadia
    - family: Al-Aly
      given: Ziyad
    - family: Al-Eyadhy
      given: Ayman
    - family: Al-Raddadi
      given: Rajaa M.
    - family: Alahdab
      given: Fares
    - family: Alam
      given: Khurshid
    - family: Alam
      given: Tahiya
    - family: Alebel
      given: Animut
    - family: Alene
      given: Kefyalew Addis
    - family: Alijanzadeh
      given: Mehran
    - family: Alizadeh-Navaei
      given: Reza
    - family: Aljunid
      given: Syed Mohamed
    - family: Alkerwi
      given: Ala'a
    - family: Alla
      given: François
    - family: Allebeck
      given: Peter
    - family: Alonso
      given: Jordi
    - family: Altirkawi
      given: Khalid
    - family: Alvis-Guzman
      given: Nelson
    - family: Amare
      given: Azmeraw T.
    - family: Aminde
      given: Leopold N.
    - family: Amini
      given: Erfan
    - family: Ammar
      given: Walid
    - family: Amoako
      given: Yaw Ampem
    - family: Anber
      given: Nahla Hamed
    - family: Andrei
      given: Catalina Liliana
    - family: Androudi
      given: Sofia
    - family: Animut
      given: Megbaru Debalkie
    - family: Anjomshoa
      given: Mina
    - family: Ansari
      given: Hossein
    - family: Ansha
      given: Mustafa Geleto
    - family: Antonio
      given: Carl Abelardo T.
    - family: Anwari
      given: Palwasha
    - family: Aremu
      given: Olatunde
    - family: Ärnlöv
      given: Johan
    - family: Arora
      given: Amit
    - family: Arora
      given: Monika
    - family: Artaman
      given: Al
    - family: Aryal
      given: Krishna K.
    - family: Asayesh
      given: Hamid
    - family: Asfaw
      given: Ephrem Tsegay
    - family: Ataro
      given: Zerihun
    - family: Atique
      given: Suleman
    - family: Atre
      given: Sachin R.
    - family: Ausloos
      given: Marcel
    - family: Avokpaho
      given: Euripide F. G. A.
    - family: Awasthi
      given: Ashish
    - family: Quintanilla
      given: Beatriz Paulina Ayala
    - family: Ayele
      given: Yohanes
    - family: Ayer
      given: Rakesh
    - family: Azzopardi
      given: Peter S.
    - family: Babazadeh
      given: Arefeh
    - family: Bacha
      given: Umar
    - family: Badali
      given: Hamid
    - family: Badawi
      given: Alaa
    - family: Bali
      given: Ayele Geleto
    - family: Ballesteros
      given: Katherine E.
    - family: Banach
      given: Maciej
    - family: Banerjee
      given: Kajori
    - family: Bannick
      given: Marlena S.
    - family: Banoub
      given: Joseph Adel Mattar
    - family: Barboza
      given: Miguel A.
    - family: Barker-Collo
      given: Suzanne Lyn
    - family: Bärnighausen
      given: Till Winfried
    - family: Barquera
      given: Simon
    - family: Barrero
      given: Lope H.
    - family: Bassat
      given: Quique
    - family: Basu
      given: Sanjay
    - family: Baune
      given: Bernhard T.
    - family: Baynes
      given: Habtamu Wondifraw
    - family: Bazargan-Hejazi
      given: Shahrzad
    - family: Bedi
      given: Neeraj
    - family: Beghi
      given: Ettore
    - family: Behzadifar
      given: Masoud
    - family: Behzadifar
      given: Meysam
    - family: Béjot
      given: Yannick
    - family: Bekele
      given: Bayu Begashaw
    - family: Belachew
      given: Abate Bekele
    - family: Belay
      given: Ezra
    - family: Belay
      given: Yihalem Abebe
    - family: Bell
      given: Michelle L.
    - family: Bello
      given: Aminu K.
    - family: Bennett
      given: Derrick A.
    - family: Bensenor
      given: Isabela M.
    - family: Berman
      given: Adam E.
    - family: Bernabe
      given: Eduardo
    - family: Bernstein
      given: Robert S.
    - family: Bertolacci
      given: Gregory J.
    - family: Beuran
      given: Mircea
    - family: Beyranvand
      given: Tina
    - family: Bhalla
      given: Ashish
    - family: Bhattarai
      given: Suraj
    - family: Bhaumik
      given: Soumyadeeep
    - family: Bhutta
      given: Zulfiqar A.
    - family: Biadgo
      given: Belete
    - family: Biehl
      given: Molly H.
    - family: Bijani
      given: Ali
    - family: Bikbov
      given: Boris
    - family: Bilano
      given: Ver
    - family: Bililign
      given: Nigus
    - family: Sayeed
      given: Muhammad Shahdaat Bin
    - family: Bisanzio
      given: Donal
    - family: Biswas
      given: Tuhin
    - family: Blacker
      given: Brigette F.
    - family: Basara
      given: Berrak Bora
    - family: Borschmann
      given: Rohan
    - family: Bosetti
      given: Cristina
    - family: Bozorgmehr
      given: Kayvan
    - family: Brady
      given: Oliver J.
    - family: Brant
      given: Luisa C.
    - family: Brayne
      given: Carol
    - family: Brazinova
      given: Alexandra
    - family: Breitborde
      given: Nicholas J. K.
    - family: Brenner
      given: Hermann
    - family: Briant
      given: Paul Svitil
    - family: Britton
      given: Gabrielle
    - family: Brugha
      given: Traolach
    - family: Busse
      given: Reinhard
    - family: Butt
      given: Zahid A.
    - family: Callender
      given: Charlton S. K. H.
    - family: Campos-Nonato
      given: Ismael R.
    - family: Rincon
      given: Julio Cesar Campuzano
    - family: Cano
      given: Jorge
    - family: Car
      given: Mate
    - family: Cárdenas
      given: Rosario
    - family: Carreras
      given: Giulia
    - family: Carrero
      given: Juan J.
    - family: Carter
      given: Austin
    - family: Carvalho
      given: Félix
    - family: Castañeda-Orjuela
      given: Carlos A.
    - family: Rivas
      given: Jacqueline Castillo
    - family: Castle
      given: Chris D.
    - family: Castro
      given: Clara
    - family: Castro
      given: Franz
    - family: Catalá-López
      given: Ferrán
    - family: Cerin
      given: Ester
    - family: Chaiah
      given: Yazan
    - family: Chang
      given: Jung-Chen
    - family: Charlson
      given: Fiona J.
    - family: Chaturvedi
      given: Pankaj
    - family: Chiang
      given: Peggy Pei-Chia
    - family: Chimed-Ochir
      given: Odgerel
    - family: Chisumpa
      given: Vesper Hichilombwe
    - family: Chitheer
      given: Abdulaal
    - family: Chowdhury
      given: Rajiv
    - family: Christensen
      given: Hanne
    - family: Christopher
      given: Devasahayam J.
    - family: Chung
      given: Sheng-Chia
    - family: Cicuttini
      given: Flavia M.
    - family: Ciobanu
      given: Liliana G.
    - family: Cirillo
      given: Massimo
    - family: Cohen
      given: Aaron J.
    - family: Cooper
      given: Leslie Trumbull
    - family: Cortesi
      given: Paolo Angelo
    - family: Cortinovis
      given: Monica
    - family: Cousin
      given: Ewerton
    - family: Cowie
      given: Benjamin C.
    - family: Criqui
      given: Michael H.
    - family: Cromwell
      given: Elizabeth A.
    - family: Crowe
      given: Christopher Stephen
    - family: Crump
      given: John A.
    - family: Cunningham
      given: Matthew
    - family: Daba
      given: Alemneh Kabeta
    - family: Dadi
      given: Abel Fekadu
    - family: Dandona
      given: Lalit
    - family: Dandona
      given: Rakhi
    - family: Dang
      given: Anh Kim
    - family: Dargan
      given: Paul I.
    - family: Daryani
      given: Ahmad
    - family: Das
      given: Siddharth K.
    - family: Gupta
      given: Rajat Das
    - family: Neves
      given: José Das
    - family: Dasa
      given: Tamirat Tesfaye
    - family: Dash
      given: Aditya Prasad
    - family: Davis
      given: Adrian C.
    - family: Weaver
      given: Nicole Davis
    - family: Davitoiu
      given: Dragos Virgil
    - family: Davletov
      given: Kairat
    - family: Hoz
      given: Fernando Pio De La
    - family: Neve
      given: Jan-Walter De
    - family: Degefa
      given: Meaza Girma
    - family: Degenhardt
      given: Louisa
    - family: Degfie
      given: Tizta T.
    - family: Deiparine
      given: Selina
    - family: Demoz
      given: Gebre Teklemariam
    - family: Demtsu
      given: Balem Betsu
    - family: Denova-Gutiérrez
      given: Edgar
    - family: Deribe
      given: Kebede
    - family: Dervenis
      given: Nikolaos
    - family: Jarlais
      given: Don C. Des
    - family: Dessie
      given: Getenet Ayalew
    - family: Dey
      given: Subhojit
    - family: Dharmaratne
      given: Samath D.
    - family: Dicker
      given: Daniel
    - family: Dinberu
      given: Mesfin Tadese
    - family: Ding
      given: Eric L.
    - family: Dirac
      given: M. Ashworth
    - family: Djalalinia
      given: Shirin
    - family: Dokova
      given: Klara
    - family: Doku
      given: David Teye
    - family: Donnelly
      given: Christl A.
    - family: Dorsey
      given: E. Ray
    - family: Doshi
      given: Pratik P.
    - family: Douwes-Schultz
      given: Dirk
    - family: Doyle
      given: Kerrie E.
    - family: Driscoll
      given: Tim R.
    - family: Dubey
      given: Manisha
    - family: Dubljanin
      given: Eleonora
    - family: Duken
      given: Eyasu Ejeta
    - family: Duncan
      given: Bruce B.
    - family: Duraes
      given: Andre R.
    - family: Ebrahimi
      given: Hedyeh
    - family: Ebrahimpour
      given: Soheil
    - family: Edessa
      given: Dumessa
    - family: Edvardsson
      given: David
    - family: Eggen
      given: Anne Elise
    - family: Bcheraoui
      given: Charbel El
    - family: Zaki
      given: Maysaa El Sayed
    - family: El-Khatib
      given: Ziad
    - family: Elkout
      given: Hajer
    - family: Ellingsen
      given: Christian Lycke
    - family: Endres
      given: Matthias
    - family: Endries
      given: Aman Yesuf
    - family: Er
      given: Benjamin
    - family: Erskine
      given: Holly E.
    - family: Eshrati
      given: Babak
    - family: Eskandarieh
      given: Sharareh
    - family: Esmaeili
      given: Reza
    - family: Esteghamati
      given: Alireza
    - family: Fakhar
      given: Mahdi
    - family: Fakhim
      given: Hamed
    - family: Faramarzi
      given: Mahbobeh
    - family: Fareed
      given: Mohammad
    - family: Farhadi
      given: Farzaneh
    - family: Farinha
      given: Carla Sofia E.
      dropping-particle: sá
    - family: Faro
      given: Andre
    - family: Farvid
      given: Maryam S.
    - family: Farzadfar
      given: Farshad
    - family: Farzaei
      given: Mohammad Hosein
    - family: Feigin
      given: Valery L.
    - family: Feigl
      given: Andrea B.
    - family: Fentahun
      given: Netsanet
    - family: Fereshtehnejad
      given: Seyed-Mohammad
    - family: Fernandes
      given: Eduarda
    - family: Fernandes
      given: Joao C.
    - family: Ferrari
      given: Alize J.
    - family: Feyissa
      given: Garumma Tolu
    - family: Filip
      given: Irina
    - family: Finegold
      given: Samuel
    - family: Fischer
      given: Florian
    - family: Fitzmaurice
      given: Christina
    - family: Foigt
      given: Nataliya A.
    - family: Foreman
      given: Kyle J.
    - family: Fornari
      given: Carla
    - family: Frank
      given: Tahvi D.
    - family: Fukumoto
      given: Takeshi
    - family: Fuller
      given: John E.
    - family: Fullman
      given: Nancy
    - family: Fürst
      given: Thomas
    - family: Furtado
      given: João M.
    - family: Futran
      given: Neal D.
    - family: Gallus
      given: Silvano
    - family: Garcia-Basteiro
      given: Alberto L.
    - family: Garcia-Gordillo
      given: Miguel A.
    - family: Gardner
      given: William M.
    - family: Gebre
      given: Abadi Kahsu
    - family: Gebrehiwot
      given: Tsegaye Tewelde
    - family: Gebremedhin
      given: Amanuel Tesfay
    - family: Gebremichael
      given: Bereket
    - family: Gebremichael
      given: Teklu Gebrehiwo
    - family: Gelano
      given: Tilayie Feto
    - family: Geleijnse
      given: Johanna M.
    - family: Genova-Maleras
      given: Ricard
    - family: Geramo
      given: Yilma Chisha Dea
    - family: Gething
      given: Peter W.
    - family: Gezae
      given: Kebede Embaye
    - family: Ghadami
      given: Mohammad Rasoul
    - family: Ghadimi
      given: Reza
    - family: Falavarjani
      given: Khalil Ghasemi
    - family: Ghasemi-Kasman
      given: Maryam
    - family: Ghimire
      given: Mamata
    - family: Gibney
      given: Katherine B.
    - family: Gill
      given: Paramjit Singh
    - family: Gill
      given: Tiffany K.
    - family: Gillum
      given: Richard F.
    - family: Ginawi
      given: Ibrahim Abdelmageed
    - family: Giroud
      given: Maurice
    - family: Giussani
      given: Giorgia
    - family: Goenka
      given: Shifalika
    - family: Goldberg
      given: Ellen M.
    - family: Goli
      given: Srinivas
    - family: Gómez-Dantés
      given: Hector
    - family: Gona
      given: Philimon N.
    - family: Gopalani
      given: Sameer Vali
    - family: Gorman
      given: Taren M.
    - family: Goto
      given: Atsushi
    - family: Goulart
      given: Alessandra C.
    - family: Gnedovskaya
      given: Elena V.
    - family: Grada
      given: Ayman
    - family: Grosso
      given: Giuseppe
    - family: Gugnani
      given: Harish Chander
    - family: Guimaraes
      given: Andre Luiz Sena
    - family: Guo
      given: Yuming
    - family: Gupta
      given: Prakash C.
    - family: Gupta
      given: Rahul
    - family: Gupta
      given: Rajeev
    - family: Gupta
      given: Tanush
    - family: Gutiérrez
      given: Reyna Alma
    - family: Gyawali
      given: Bishal
    - family: Haagsma
      given: Juanita A.
    - family: Hafezi-Nejad
      given: Nima
    - family: Hagos
      given: Tekleberhan B.
    - family: Hailegiyorgis
      given: Tewodros Tesfa
    - family: Hailu
      given: Gessessew Bugssa
    - family: Haj-Mirzaian
      given: Arvin
    - family: Haj-Mirzaian
      given: Arya
    - family: Hamadeh
      given: Randah R.
    - family: Hamidi
      given: Samer
    - family: Handal
      given: Alexis J.
    - family: Hankey
      given: Graeme J.
    - family: Harb
      given: Hilda L.
    - family: Harikrishnan
      given: Sivadasanpillai
    - family: Haro
      given: Josep Maria
    - family: Hasan
      given: Mehedi
    - family: Hassankhani
      given: Hadi
    - family: Hassen
      given: Hamid Yimam
    - family: Havmoeller
      given: Rasmus
    - family: Hay
      given: Roderick J.
    - family: Hay
      given: Simon I.
    - family: He
      given: Yihua
    - family: Hedayatizadeh-Omran
      given: Akbar
    - family: Hegazy
      given: Mohamed I.
    - family: Heibati
      given: Behzad
    - family: Heidari
      given: Mohsen
    - family: Hendrie
      given: Delia
    - family: Henok
      given: Andualem
    - family: Henry
      given: Nathaniel J.
    - family: Herteliu
      given: Claudiu
    - family: Heydarpour
      given: Fatemeh
    - family: Heydarpour
      given: Pouria
    - family: Heydarpour
      given: Sousan
    - family: Hibstu
      given: Desalegn Tsegaw
    - family: Hoek
      given: Hans W.
    - family: Hole
      given: Michael K.
    - family: Rad
      given: Enayatollah Homaie
    - family: Hoogar
      given: Praveen
    - family: Hosgood
      given: H. Dean
    - family: Hosseini
      given: Seyed Mostafa
    - family: Hosseinzadeh
      given: Mehdi
    - family: Hostiuc
      given: Mihaela
    - family: Hostiuc
      given: Sorin
    - family: Hotez
      given: Peter J.
    - family: Hoy
      given: Damian G.
    - family: Hsiao
      given: Thomas
    - family: Hu
      given: Guoqing
    - family: Huang
      given: John J.
    - family: Husseini
      given: Abdullatif
    - family: Hussen
      given: Mohammedaman Mama
    - family: Hutfless
      given: Susan
    - family: Idrisov
      given: Bulat
    - family: Ilesanmi
      given: Olayinka Stephen
    - family: Iqbal
      given: Usman
    - family: Irvani
      given: Seyed Sina Naghibi
    - family: Irvine
      given: Caleb Mackay Salpeter
    - family: Islam
      given: Nazrul
    - family: Islam
      given: Sheikh Mohammed Shariful
    - family: Islami
      given: Farhad
    - family: Jacobsen
      given: Kathryn H.
    - family: Jahangiry
      given: Leila
    - family: Jahanmehr
      given: Nader
    - family: Jain
      given: Sudhir Kumar
    - family: Jakovljevic
      given: Mihajlo
    - family: Jalu
      given: Moti Tolera
    - family: James
      given: Spencer L.
    - family: Javanbakht
      given: Mehdi
    - family: Jayatilleke
      given: Achala Upendra
    - family: Jeemon
      given: Panniyammakal
    - family: Jenkins
      given: Kathy J.
    - family: Jha
      given: Ravi Prakash
    - family: Jha
      given: Vivekanand
    - family: Johnson
      given: Catherine O.
    - family: Johnson
      given: Sarah C.
    - family: Jonas
      given: Jost B.
    - family: Joshi
      given: Ankur
    - family: Jozwiak
      given: Jacek Jerzy
    - family: Jungari
      given: Suresh Banayya
    - family: Jürisson
      given: Mikk
    - family: Kabir
      given: Zubair
    - family: Kadel
      given: Rajendra
    - family: Kahsay
      given: Amaha
    - family: Kalani
      given: Rizwan
    - family: Karami
      given: Manoochehr
    - family: Matin
      given: Behzad Karami
    - family: Karch
      given: André
    - family: Karema
      given: Corine
    - family: Karimi-Sari
      given: Hamidreza
    - family: Kasaeian
      given: Amir
    - family: Kassa
      given: Dessalegn H.
    - family: Kassa
      given: Getachew Mullu
    - family: Kassa
      given: Tesfaye Dessale
    - family: Kassebaum
      given: Nicholas J.
    - family: Katikireddi
      given: Srinivasa Vittal
    - family: Kaul
      given: Anil
    - family: Kazemi
      given: Zhila
    - family: Karyani
      given: Ali Kazemi
    - family: Kazi
      given: Dhruv Satish
    - family: Kefale
      given: Adane Teshome
    - family: Keiyoro
      given: Peter Njenga
    - family: Kemp
      given: Grant Rodgers
    - family: Kengne
      given: Andre Pascal
    - family: Keren
      given: Andre
    - family: Kesavachandran
      given: Chandrasekharan Nair
    - family: Khader
      given: Yousef Saleh
    - family: Khafaei
      given: Behzad
    - family: Khafaie
      given: Morteza Abdullatif
    - family: Khajavi
      given: Alireza
    - family: Khalid
      given: Nauman
    - family: Khalil
      given: Ibrahim A.
    - family: Khan
      given: Ejaz Ahmad
    - family: Khan
      given: Muhammad Shahzeb
    - family: Khan
      given: Muhammad Ali
    - family: Khang
      given: Young-Ho
    - family: Khater
      given: Mona M.
    - family: Khoja
      given: Abdullah T.
    - family: Khosravi
      given: Ardeshir
    - family: Khosravi
      given: Mohammad Hossein
    - family: Khubchandani
      given: Jagdish
    - family: Kiadaliri
      given: Aliasghar A.
    - family: Kibret
      given: Getiye D.
    - family: Kidanemariam
      given: Zelalem Teklemariam
    - family: Kiirithio
      given: Daniel N.
    - family: Kim
      given: Daniel
    - family: Kim
      given: Young-Eun
    - family: Kim
      given: Yun Jin
    - family: Kimokoti
      given: Ruth W.
    - family: Kinfu
      given: Yohannes
    - family: Kisa
      given: Adnan
    - family: Kissimova-Skarbek
      given: Katarzyna
    - family: Kivimäki
      given: Mika
    - family: Knudsen
      given: Ann Kristin Skrindo
    - family: Kocarnik
      given: Jonathan M.
    - family: Kochhar
      given: Sonali
    - family: Kokubo
      given: Yoshihiro
    - family: Kolola
      given: Tufa
    - family: Kopec
      given: Jacek A.
    - family: Koul
      given: Parvaiz A.
    - family: Koyanagi
      given: Ai
    - family: Kravchenko
      given: Michael A.
    - family: Krishan
      given: Kewal
    - family: Defo
      given: Barthelemy Kuate
    - family: Bicer
      given: Burcu Kucuk
    - family: Kumar
      given: G. Anil
    - family: Kumar
      given: Manasi
    - family: Kumar
      given: Pushpendra
    - family: Kutz
      given: Michael J.
    - family: Kuzin
      given: Igor
    - family: Kyu
      given: Hmwe Hmwe
    - family: Lad
      given: Deepesh P.
    - family: Lad
      given: Sheetal D.
    - family: Lafranconi
      given: Alessandra
    - family: Lal
      given: Dharmesh Kumar
    - family: Lalloo
      given: Ratilal
    - family: Lallukka
      given: Tea
    - family: Lam
      given: Jennifer O.
    - family: Lami
      given: Faris Hasan
    - family: Lansingh
      given: Van C.
    - family: Lansky
      given: Sonia
    - family: Larson
      given: Heidi J.
    - family: Latifi
      given: Arman
    - family: Lau
      given: Kathryn Mei-Ming
    - family: Lazarus
      given: Jeffrey V.
    - family: Lebedev
      given: Georgy
    - family: Lee
      given: Paul H.
    - family: Leigh
      given: James
    - family: Leili
      given: Mostafa
    - family: Leshargie
      given: Cheru Tesema
    - family: Li
      given: Shanshan
    - family: Li
      given: Yichong
    - family: Liang
      given: Juan
    - family: Lim
      given: Lee-Ling
    - family: Lim
      given: Stephen S.
    - family: Limenih
      given: Miteku Andualem
    - family: Linn
      given: Shai
    - family: Liu
      given: Shiwei
    - family: Liu
      given: Yang
    - family: Lodha
      given: Rakesh
    - family: Lonsdale
      given: Chris
    - family: Lopez
      given: Alan D.
    - family: Lorkowski
      given: Stefan
    - family: Lotufo
      given: Paulo A.
    - family: Lozano
      given: Rafael
    - family: Lunevicius
      given: Raimundas
    - family: Ma
      given: Stefan
    - family: Macarayan
      given: Erlyn Rachelle King
    - family: Mackay
      given: Mark T.
    - family: MacLachlan
      given: Jennifer H.
    - family: Maddison
      given: Emilie R.
    - family: Madotto
      given: Fabiana
    - family: Razek
      given: Hassan Magdy Abd El
    - family: Razek
      given: Muhammed Magdy Abd El
    - family: Maghavani
      given: Dhaval P.
    - family: Majdan
      given: Marek
    - family: Majdzadeh
      given: Reza
    - family: Majeed
      given: Azeem
    - family: Malekzadeh
      given: Reza
    - family: Malta
      given: Deborah Carvalho
    - family: Manda
      given: Ana-Laura
    - family: Mandarano-Filho
      given: Luiz Garcia
    - family: Manguerra
      given: Helena
    - family: Mansournia
      given: Mohammad Ali
    - family: Mapoma
      given: Chabila Christopher
    - family: Marami
      given: Dadi
    - family: Maravilla
      given: Joemer C.
    - family: Marcenes
      given: Wagner
    - family: Marczak
      given: Laurie
    - family: Marks
      given: Ashley
    - family: Marks
      given: Guy B.
    - family: Martinez
      given: Gabriel
    - family: Martins-Melo
      given: Francisco Rogerlândio
    - family: Martopullo
      given: Ira
    - family: März
      given: Winfried
    - family: Marzan
      given: Melvin B.
    - family: Masci
      given: Joseph R.
    - family: Massenburg
      given: Benjamin Ballard
    - family: Mathur
      given: Manu Raj
    - family: Mathur
      given: Prashant
    - family: Matzopoulos
      given: Richard
    - family: Maulik
      given: Pallab K.
    - family: Mazidi
      given: Mohsen
    - family: McAlinden
      given: Colm
    - family: McGrath
      given: John J.
    - family: McKee
      given: Martin
    - family: McMahon
      given: Brian J.
    - family: Mehata
      given: Suresh
    - family: Mehndiratta
      given: Man Mohan
    - family: Mehrotra
      given: Ravi
    - family: Mehta
      given: Kala M.
    - family: Mehta
      given: Varshil
    - family: Mekonnen
      given: Tefera C.
    - family: Melese
      given: Addisu
    - family: Melku
      given: Mulugeta
    - family: Memiah
      given: Peter T. N.
    - family: Memish
      given: Ziad A.
    - family: Mendoza
      given: Walter
    - family: Mengistu
      given: Desalegn Tadese
    - family: Mengistu
      given: Getnet
    - family: Mensah
      given: George A.
    - family: Mereta
      given: Seid Tiku
    - family: Meretoja
      given: Atte
    - family: Meretoja
      given: Tuomo J.
    - family: Mestrovic
      given: Tomislav
    - family: Mezgebe
      given: Haftay Berhane
    - family: Miazgowski
      given: Bartosz
    - family: Miazgowski
      given: Tomasz
    - family: Millear
      given: Anoushka I.
    - family: Miller
      given: Ted R.
    - family: Miller-Petrie
      given: Molly Katherine
    - family: Mini
      given: G. K.
    - family: Mirabi
      given: Parvaneh
    - family: Mirarefin
      given: Mojde
    - family: Mirica
      given: Andreea
    - family: Mirrakhimov
      given: Erkin M.
    - family: Misganaw
      given: Awoke Temesgen
    - family: Mitiku
      given: Habtamu
    - family: Moazen
      given: Babak
    - family: Mohammad
      given: Karzan Abdulmuhsin
    - family: Mohammadi
      given: Moslem
    - family: Mohammadifard
      given: Noushin
    - family: Mohammed
      given: Mohammed A.
    - family: Mohammed
      given: Shafiu
    - family: Mohan
      given: Viswanathan
    - family: Mokdad
      given: Ali H.
    - family: Molokhia
      given: Mariam
    - family: Monasta
      given: Lorenzo
    - family: Moradi
      given: Ghobad
    - family: Moradi-Lakeh
      given: Maziar
    - family: Moradinazar
      given: Mehdi
    - family: Moraga
      given: Paula
    - family: Morawska
      given: Lidia
    - family: Velásquez
      given: Ilais Moreno
    - family: Morgado-Da-Costa
      given: Joana
    - family: Morrison
      given: Shane Douglas
    - family: Moschos
      given: Marilita M.
    - family: Mouodi
      given: Simin
    - family: Mousavi
      given: Seyyed Meysam
    - family: Muchie
      given: Kindie Fentahun
    - family: Mueller
      given: Ulrich Otto
    - family: Mukhopadhyay
      given: Satinath
    - family: Muller
      given: Kate
    - family: Mumford
      given: John Everett
    - family: Musa
      given: Jonah
    - family: Musa
      given: Kamarul Imran
    - family: Mustafa
      given: Ghulam
    - family: Muthupandian
      given: Saravanan
    - family: Nachega
      given: Jean B.
    - family: Nagel
      given: Gabriele
    - family: Naheed
      given: Aliya
    - family: Nahvijou
      given: Azin
    - family: Naik
      given: Gurudatta
    - family: Nair
      given: Sanjeev
    - family: Najafi
      given: Farid
    - family: Naldi
      given: Luigi
    - family: Nam
      given: Hae Sung
    - family: Nangia
      given: Vinay
    - family: Nansseu
      given: Jobert Richie
    - family: Nascimento
      given: Bruno Ramos
    - family: Natarajan
      given: Gopalakrishnan
    - family: Neamati
      given: Nahid
    - family: Negoi
      given: Ionut
    - family: Negoi
      given: Ruxandra Irina
    - family: Neupane
      given: Subas
    - family: Newton
      given: Charles R. J.
    - family: Ngalesoni
      given: Frida N.
    - family: Ngunjiri
      given: Josephine W.
    - family: Nguyen
      given: Anh Quynh
    - family: Nguyen
      given: Grant
    - family: Nguyen
      given: Ha Thu
    - family: Nguyen
      given: Huong Thanh
    - family: Nguyen
      given: Long Hoang
    - family: Nguyen
      given: Minh
    - family: Nguyen
      given: Trang Huyen
    - family: Nichols
      given: Emma
    - family: Ningrum
      given: Dina Nur Anggraini
    - family: Nirayo
      given: Yirga Legesse
    - family: Nixon
      given: Molly R.
    - family: Nolutshungu
      given: Nomonde
    - family: Nomura
      given: Shuhei
    - family: Norheim
      given: Ole F.
    - family: Noroozi
      given: Mehdi
    - family: Norrving
      given: Bo
    - family: Noubiap
      given: Jean Jacques
    - family: Nouri
      given: Hamid Reza
    - family: Shiadeh
      given: Malihe Nourollahpour
    - family: Nowroozi
      given: Mohammad Reza
    - family: Nyasulu
      given: Peter S.
    - family: Odell
      given: Christopher M.
    - family: Ofori-Asenso
      given: Richard
    - family: Ogbo
      given: Felix Akpojene
    - family: Oh
      given: In-Hwan
    - family: Oladimeji
      given: Olanrewaju
    - family: Olagunju
      given: Andrew T.
    - family: Olivares
      given: Pedro R.
    - family: Olsen
      given: Helen Elizabeth
    - family: Olusanya
      given: Bolajoko Olubukunola
    - family: Olusanya
      given: Jacob Olusegun
    - family: Ong
      given: Kanyin L.
    - family: Ong
      given: Sok King Sk
    - family: Oren
      given: Eyal
    - family: Orpana
      given: Heather M.
    - family: Ortiz
      given: Alberto
    - family: Ortiz
      given: Justin R.
    - family: Otstavnov
      given: Stanislav S.
    - family: Øverland
      given: Simon
    - family: Owolabi
      given: Mayowa Ojo
    - family: Özdemir
      given: Raziye
    - family: A
      given: Mahesh P.
    - family: Pacella
      given: Rosana
    - family: Pakhale
      given: Smita
    - family: Pakhare
      given: Abhijit P.
    - family: Pakpour
      given: Amir H.
    - family: Pana
      given: Adrian
    - family: Panda-Jonas
      given: Songhomitra
    - family: Pandian
      given: Jeyaraj Durai
    - family: Parisi
      given: Andrea
    - family: Park
      given: Eun-Kee
    - family: Parry
      given: Charles D. H.
    - family: Parsian
      given: Hadi
    - family: Patel
      given: Shanti
    - family: Pati
      given: Sanghamitra
    - family: Patton
      given: George C.
    - family: Paturi
      given: Vishnupriya Rao
    - family: Paulson
      given: Katherine R.
    - family: Pereira
      given: Alexandre
    - family: Pereira
      given: David M.
    - family: Perico
      given: Norberto
    - family: Pesudovs
      given: Konrad
    - family: Petzold
      given: Max
    - family: Phillips
      given: Michael R.
    - family: Piel
      given: Frédéric B.
    - family: Pigott
      given: David M.
    - family: Pillay
      given: Julian David
    - family: Pirsaheb
      given: Meghdad
    - family: Pishgar
      given: Farhad
    - family: Polinder
      given: Suzanne
    - family: Postma
      given: Maarten J.
    - family: Pourshams
      given: Akram
    - family: Poustchi
      given: Hossein
    - family: Pujar
      given: Ashwini
    - family: Prakash
      given: Swayam
    - family: Prasad
      given: Narayan
    - family: Purcell
      given: Caroline A.
    - family: Qorbani
      given: Mostafa
    - family: Quintana
      given: Hedley
    - family: Quistberg
      given: D. Alex
    - family: Rade
      given: Kirankumar Waman
    - family: Radfar
      given: Amir
    - family: Rafay
      given: Anwar
    - family: Rafiei
      given: Alireza
    - family: Rahim
      given: Fakher
    - family: Rahimi
      given: Kazem
    - family: Rahimi-Movaghar
      given: Afarin
    - family: Rahman
      given: Mahfuzar
    - family: Rahman
      given: Mohammad Hifz Ur
    - family: Rahman
      given: Muhammad Aziz
    - family: Rai
      given: Rajesh Kumar
    - family: Rajsic
      given: Sasa
    - family: Ram
      given: Usha
    - family: Ranabhat
      given: Chhabi Lal
    - family: Ranjan
      given: Prabhat
    - family: Rao
      given: Puja C.
    - family: Rawaf
      given: David Laith
    - family: Rawaf
      given: Salman
    - family: Razo-García
      given: Christian
    - family: Reddy
      given: K. Srinath
    - family: Reiner
      given: Robert C.
    - family: Reitsma
      given: Marissa B.
    - family: Remuzzi
      given: Giuseppe
    - family: Renzaho
      given: Andre M. N.
    - family: Resnikoff
      given: Serge
    - family: Rezaei
      given: Satar
    - family: Rezaeian
      given: Shahab
    - family: Rezai
      given: Mohammad Sadegh
    - family: Riahi
      given: Seyed Mohammad
    - family: Ribeiro
      given: Antonio Luiz P.
    - family: Rios-Blancas
      given: Maria Jesus
    - family: Roba
      given: Kedir Teji
    - family: Roberts
      given: Nicholas L. S.
    - family: Robinson
      given: Stephen R.
    - family: Roever
      given: Leonardo
    - family: Ronfani
      given: Luca
    - family: Roshandel
      given: Gholamreza
    - family: Rostami
      given: Ali
    - family: Rothenbacher
      given: Dietrich
    - family: Roy
      given: Ambuj
    - family: Rubagotti
      given: Enrico
    - family: Sachdev
      given: Perminder S.
    - family: Saddik
      given: Basema
    - family: Sadeghi
      given: Ehsan
    - family: Safari
      given: Hosein
    - family: Safdarian
      given: Mahdi
    - family: Safi
      given: Sare
    - family: Safiri
      given: Saeid
    - family: Sagar
      given: Rajesh
    - family: Sahebkar
      given: Amirhossein
    - family: Sahraian
      given: Mohammad Ali
    - family: Salam
      given: Nasir
    - family: Salama
      given: Joseph S.
    - family: Salamati
      given: Payman
    - family: Saldanha
      given: Raphael De Freitas
    - family: Saleem
      given: Zikria
    - family: Salimi
      given: Yahya
    - family: Salvi
      given: Sundeep Santosh
    - family: Salz
      given: Inbal
    - family: Sambala
      given: Evanson Zondani
    - family: Samy
      given: Abdallah M.
    - family: Sanabria
      given: Juan
    - family: Sanchez-Niño
      given: Maria Dolores
    - family: Santomauro
      given: Damian Francesco
    - family: Santos
      given: Itamar S.
    - family: Santos
      given: João Vasco
    - family: Milicevic
      given: Milena M. Santric
    - family: Jose
      given: Bruno Piassi Sao
    - family: Sarker
      given: Abdur Razzaque
    - family: Sarmiento-Suárez
      given: Rodrigo
    - family: Sarrafzadegan
      given: Nizal
    - family: Sartorius
      given: Benn
    - family: Sarvi
      given: Shahabeddin
    - family: Sathian
      given: Brijesh
    - family: Satpathy
      given: Maheswar
    - family: Sawant
      given: Arundhati R.
    - family: Sawhney
      given: Monika
    - family: Saxena
      given: Sonia
    - family: Sayyah
      given: Mehdi
    - family: Schaeffner
      given: Elke
    - family: Schmidt
      given: Maria Inês
    - family: Schneider
      given: Ione J. C.
    - family: Schöttker
      given: Ben
    - family: Schutte
      given: Aletta Elisabeth
    - family: Schwebel
      given: David C.
    - family: Schwendicke
      given: Falk
    - family: Scott
      given: James G.
    - family: Sekerija
      given: Mario
    - family: Sepanlou
      given: Sadaf G.
    - family: Serván-Mori
      given: Edson
    - family: Seyedmousavi
      given: Seyedmojtaba
    - family: Shabaninejad
      given: Hosein
    - family: Shackelford
      given: Katya Anne
    - family: Shafieesabet
      given: Azadeh
    - family: Shahbazi
      given: Mehdi
    - family: Shaheen
      given: Amira A.
    - family: Shaikh
      given: Masood Ali
    - family: Shams-Beyranvand
      given: Mehran
    - family: Shamsi
      given: Mohammadbagher
    - family: Shamsizadeh
      given: Morteza
    - family: Sharafi
      given: Kiomars
    - family: Sharif
      given: Mehdi
    - family: Sharif-Alhoseini
      given: Mahdi
    - family: Sharma
      given: Rajesh
    - family: She
      given: Jun
    - family: Sheikh
      given: Aziz
    - family: Shi
      given: Peilin
    - family: Shiferaw
      given: Mekonnen Sisay
    - family: Shigematsu
      given: Mika
    - family: Shiri
      given: Rahman
    - family: Shirkoohi
      given: Reza
    - family: Shiue
      given: Ivy
    - family: Shokraneh
      given: Farhad
    - family: Shrime
      given: Mark G.
    - family: Si
      given: Si
    - family: Siabani
      given: Soraya
    - family: Siddiqi
      given: Tariq J.
    - family: Sigfusdottir
      given: Inga Dora
    - family: Sigurvinsdottir
      given: Rannveig
    - family: Silberberg
      given: Donald H.
    - family: Silva
      given: Diego Augusto Santos
    - family: Silva
      given: João Pedro
    - family: Silva
      given: Natacha Torres Da
    - family: Silveira
      given: Dayane Gabriele Alves
    - family: Singh
      given: Jasvinder A.
    - family: Singh
      given: Narinder Pal
    - family: Singh
      given: Prashant Kumar
    - family: Singh
      given: Virendra
    - family: Sinha
      given: Dhirendra Narain
    - family: Sliwa
      given: Karen
    - family: Smith
      given: Mari
    - family: Sobaih
      given: Badr Hasan
    - family: Sobhani
      given: Soheila
    - family: Sobngwi
      given: Eugène
    - family: Soneji
      given: Samir S.
    - family: Soofi
      given: Moslem
    - family: Sorensen
      given: Reed J. D.
    - family: Soriano
      given: Joan B.
    - family: Soyiri
      given: Ireneous N.
    - family: Sposato
      given: Luciano A.
    - family: Sreeramareddy
      given: Chandrashekhar T.
    - family: Srinivasan
      given: Vinay
    - family: Stanaway
      given: Jeffrey D.
    - family: Starodubov
      given: Vladimir I.
    - family: Stathopoulou
      given: Vasiliki
    - family: Stein
      given: Dan J.
    - family: Steiner
      given: Caitlyn
    - family: Stewart
      given: Leo G.
    - family: Stokes
      given: Mark A.
    - family: Subart
      given: Michelle L.
    - family: Sudaryanto
      given: Agus
    - family: Sufiyan
      given: Mu'awiyyah Babale
    - family: Sur
      given: Patrick John
    - family: Sutradhar
      given: Ipsita
    - family: Sykes
      given: Bryan L.
    - family: Sylaja
      given: P. N.
    - family: Sylte
      given: Dillon O.
    - family: Szoeke
      given: Cassandra E. I.
    - family: Tabarés-Seisdedos
      given: Rafael
    - family: Tabuchi
      given: Takahiro
    - family: Tadakamadla
      given: Santosh Kumar
    - family: Takahashi
      given: Ken
    - family: Tandon
      given: Nikhil
    - family: Tassew
      given: Segen Gebremeskel
    - family: Taveira
      given: Nuno
    - family: Tehrani-Banihashemi
      given: Arash
    - family: Tekalign
      given: Tigist Gashaw
    - family: Tekle
      given: Merhawi Gebremedhin
    - family: Temsah
      given: Mohamad-Hani
    - family: Temsah
      given: Omar
    - family: Terkawi
      given: Abdullah Sulieman
    - family: Teshale
      given: Manaye Yihune
    - family: Tessema
      given: Belay
    - family: Tessema
      given: Gizachew Assefa
    - family: Thankappan
      given: Kavumpurathu Raman
    - family: Thirunavukkarasu
      given: Sathish
    - family: Thomas
      given: Nihal
    - family: Thrift
      given: Amanda G.
    - family: Thurston
      given: George D.
    - family: Tilahun
      given: Binyam
    - family: To
      given: Quyen G.
    - family: Tobe-Gai
      given: Ruoyan
    - family: Tonelli
      given: Marcello
    - family: Topor-Madry
      given: Roman
    - family: Torre
      given: Anna E.
    - family: Tortajada-Girbés
      given: Miguel
    - family: Touvier
      given: Mathilde
    - family: Tovani-Palone
      given: Marcos Roberto
    - family: Tran
      given: Bach Xuan
    - family: Tran
      given: Khanh Bao
    - family: Tripathi
      given: Suryakant
    - family: Troeger
      given: Christopher E.
    - family: Truelsen
      given: Thomas Clement
    - family: Truong
      given: Nu Thi
    - family: Tsadik
      given: Afewerki Gebremeskel
    - family: Tsoi
      given: Derrick
    - family: Car
      given: Lorainne Tudor
    - family: Tuzcu
      given: E. Murat
    - family: Tyrovolas
      given: Stefanos
    - family: Ukwaja
      given: Kingsley N.
    - family: Ullah
      given: Irfan
    - family: Undurraga
      given: Eduardo A.
    - family: Updike
      given: Rachel L.
    - family: Usman
      given: Muhammad Shariq
    - family: Uthman
      given: Olalekan A.
    - family: Uzun
      given: Selen Begüm
    - family: Vaduganathan
      given: Muthiah
    - family: Vaezi
      given: Afsane
    - family: Vaidya
      given: Gaurang
    - family: Valdez
      given: Pascual R.
    - family: Varavikova
      given: Elena
    - family: Vasankari
      given: Tommi Juhani
    - family: Venketasubramanian
      given: Narayanaswamy
    - family: Villafaina
      given: Santos
    - family: Violante
      given: Francesco S.
    - family: Vladimirov
      given: Sergey Konstantinovitch
    - family: Vlassov
      given: Vasily
    - family: Vollset
      given: Stein Emil
    - family: Vos
      given: Theo
    - family: Wagner
      given: Gregory R.
    - family: Wagnew
      given: Fasil Shiferaw
    - family: Waheed
      given: Yasir
    - family: Wallin
      given: Mitchell Taylor
    - family: Walson
      given: Judd L.
    - family: Wang
      given: Yanping
    - family: Wang
      given: Yuan-Pang
    - family: Wassie
      given: Molla Mesele
    - family: Weiderpass
      given: Elisabete
    - family: Weintraub
      given: Robert G.
    - family: Weldegebreal
      given: Fitsum
    - family: Weldegwergs
      given: Kidu Gidey
    - family: Werdecker
      given: Andrea
    - family: Werkneh
      given: Adhena Ayaliew
    - family: West
      given: T. Eoin
    - family: Westerman
      given: Ronny
    - family: Whiteford
      given: Harvey A.
    - family: Widecka
      given: Justyna
    - family: Wilner
      given: Lauren B.
    - family: Wilson
      given: Shadrach
    - family: Winkler
      given: Andrea Sylvia
    - family: Wiysonge
      given: Charles Shey
    - family: Wolfe
      given: Charles D. A.
    - family: Wu
      given: Shouling
    - family: Wu
      given: Yun-Chun
    - family: Wyper
      given: Grant M. A.
    - family: Xavier
      given: Denis
    - family: Xu
      given: Gelin
    - family: Yadgir
      given: Simon
    - family: Yadollahpour
      given: Ali
    - family: Jabbari
      given: Seyed Hossein Yahyazadeh
    - family: Yakob
      given: Bereket
    - family: Yan
      given: Lijing L.
    - family: Yano
      given: Yuichiro
    - family: Yaseri
      given: Mehdi
    - family: Yasin
      given: Yasin Jemal
    - family: Yentür
      given: Gökalp Kadri
    - family: Yeshaneh
      given: Alex
    - family: Yimer
      given: Ebrahim M.
    - family: Yip
      given: Paul
    - family: Yirsaw
      given: Biruck Desalegn
    - family: Yisma
      given: Engida
    - family: Yonemoto
      given: Naohiro
    - family: Yonga
      given: Gerald
    - family: Yoon
      given: Seok-Jun
    - family: Yotebieng
      given: Marcel
    - family: Younis
      given: Mustafa Z.
    - family: Yousefifard
      given: Mahmoud
    - family: Yu
      given: Chuanhua
    - family: Zadnik
      given: Vesna
    - family: Zaidi
      given: Zoubida
    - family: Zaman
      given: Sojib Bin
    - family: Zamani
      given: Mohammad
    - family: Zare
      given: Zohreh
    - family: Zeleke
      given: Ayalew Jejaw
    - family: Zenebe
      given: Zerihun Menlkalew
    - family: Zhang
      given: Anthony Lin
    - family: Zhang
      given: Kai
    - family: Zhou
      given: Maigeng
    - family: Zodpey
      given: Sanjay
    - family: Zuhlke
      given: Liesl Joanna
    - family: Naghavi
      given: Mohsen
    - family: Murray
      given: Christopher J. L.
  citation-key: roth_global_2018
  container-title: The Lancet
  container-title-short: The Lancet
  DOI: 10/gfhx3t
  ISSN: 0140-6736, 1474-547X
  issue: '10159'
  issued:
    - year: 2018
      month: 11
      day: 10
  language: English
  page: 1736-1788
  PMID: '30496103'
  source: www.thelancet.com
  title: >-
    Global, regional, and national age-sex-specific mortality for 282 causes of
    death in 195 countries and territories, 1980–2017: a systematic analysis for
    the Global Burden of Disease Study 2017
  title-short: >-
    Global, regional, and national age-sex-specific mortality for 282 causes of
    death in 195 countries and territories, 1980–2017
  type: article-journal
  URL: >-
    https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(18)32203-7/abstract
  volume: '392'

- id: roy_maximumflow_1998
  author:
    - family: Roy
      given: Sébastien
    - family: Cox
      given: Ingemar J
  citation-key: roy_maximumflow_1998
  container-title: Computer Vision, 1998. Sixth International Conference on
  issued:
    - year: 1998
  note: '00000'
  page: 492–499
  publisher: IEEE
  title: A maximum-flow formulation of the n-camera stereo correspondence problem
  type: paper-conference

- id: roy_maximumflow_1998a
  author:
    - family: Roy
      given: Sébastien
    - family: Cox
      given: Ingemar J
  citation-key: roy_maximumflow_1998a
  container-title: Computer Vision, 1998. Sixth International Conference On
  issued:
    - year: 1998
  note: '00731'
  page: 492-499
  publisher: IEEE
  title: A Maximum-Flow Formulation of the n-Camera Stereo Correspondence Problem
  type: paper-conference

- id: rubin_densely_2017
  abstract: >-
    The development of new technology such as wearables that record high-quality
    single channel ECG, provides an opportunity for ECG screening in a larger
    population, especially for atrial fibrillation screening. The main goal of
    this study is to develop an automatic classification algorithm for normal
    sinus rhythm (NSR), atrial fibrillation (AF), other rhythms (O), and noise
    from a single channel short ECG segment (9-60 seconds). For this purpose,
    signal quality index (SQI) along with dense convolutional neural networks
    was used. Two convolutional neural network (CNN) models (main model that
    accepts 15 seconds ECG and secondary model that processes 9 seconds shorter
    ECG) were trained using the training data set. If the recording is
    determined to be of low quality by SQI, it is immediately classified as
    noisy. Otherwise, it is transformed to a time-frequency representation and
    classified with the CNN as NSR, AF, O, or noise. At the final step, a
    feature-based post-processing algorithm classifies the rhythm as either NSR
    or O in case the CNN model's discrimination between the two is
    indeterminate. The best result achieved at the official phase of the
    PhysioNet/CinC challenge on the blind test set was 0.80 (F1 for NSR, AF, and
    O were 0.90, 0.80, and 0.70, respectively).
  accessed:
    - year: 2018
      month: 10
      day: 21
  author:
    - family: Rubin
      given: Jonathan
    - family: Parvaneh
      given: Saman
    - family: Rahman
      given: Asif
    - family: Conroy
      given: Bryan
    - family: Babaeizadeh
      given: Saeed
  citation-key: rubin_densely_2017
  container-title: arXiv:1710.05817
  issued:
    - year: 2017
      month: 10
      day: 10
  source: arXiv.org
  title: >-
    Densely Connected Convolutional Networks and Signal Quality Analysis to
    Detect Atrial Fibrillation Using Short Single-Lead ECG Recordings
  type: article-journal
  URL: http://arxiv.org/abs/1710.05817

- id: rubio_spectral_2011
  abstract: "Let be an complex random matrix with i.i.d.\_entries having mean zero and variance and consider the class of matrices of the type , where , and are Hermitian nonnegative definite matrices, such that and have bounded spectral norm with being diagonal, and is the nonnegative definite square-root of . Under some assumptions on the moments of the entries of , it is proved in this paper that, for any matrix with bounded trace norm and for each complex outside the positive real line, almost surely as at the same rate, where is deterministic and solely depends on and . The previous result can be particularized to the study of the limiting behavior of the Stieltjes transform as well as the eigenvectors of the random matrix model . The study is motivated by applications in the field of statistical signal processing."
  accessed:
    - year: 2020
      month: 12
      day: 22
  author:
    - family: Rubio
      given: Francisco
    - family: Mestre
      given: Xavier
  citation-key: rubio_spectral_2011
  container-title: Statistics and Probability Letters
  DOI: 10.1016/j.spl.2011.01.004
  issue: '5'
  issued:
    - year: 2011
      month: 2
  page: '592'
  publisher: Elsevier
  source: HAL Archives Ouvertes
  title: Spectral convergence for a general class of random matrices
  type: article-journal
  URL: https://hal.archives-ouvertes.fr/hal-00725102
  volume: '81'

- id: rudi_generalization_2017
  author:
    - family: Rudi
      given: Alessandro
    - family: Rosasco
      given: Lorenzo
  citation-key: rudi_generalization_2017
  container-title: Advances in neural information processing systems
  editor:
    - family: Guyon
      given: I.
    - family: Luxburg
      given: U. V.
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Fergus
      given: R.
    - family: Vishwanathan
      given: S.
    - family: Garnett
      given: R.
  issued:
    - year: 2017
  publisher: Curran Associates, Inc.
  title: Generalization properties of learning with random features
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf
  volume: '30'

- id: rudi_less_2015
  author:
    - family: Rudi
      given: Alessandro
    - family: Camoriano
      given: Raffaello
    - family: Rosasco
      given: Lorenzo
  citation-key: rudi_less_2015
  container-title: Advances in neural information processing systems
  editor:
    - family: Cortes
      given: C.
    - family: Lawrence
      given: N.
    - family: Lee
      given: D.
    - family: Sugiyama
      given: M.
    - family: Garnett
      given: R.
  issued:
    - year: 2015
  publisher: Curran Associates, Inc.
  title: 'Less is more: Nyström computational regularization'
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2015/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf
  volume: '28'

- id: rudin_principles_1964
  author:
    - family: Rudin
      given: W.
  citation-key: rudin_principles_1964
  collection-title: International series in pure and applied mathematics
  issued:
    - year: 1964
  publisher: McGraw-Hill
  title: Principles of mathematical analysis
  type: book
  URL: https://books.google.com.br/books?id=iifvAAAAMAAJ

- id: rudin_real_1987
  author:
    - family: Rudin
      given: Walter
  call-number: QA300 .R82 1987
  citation-key: rudin_real_1987
  edition: 3rd ed
  event-place: New York
  ISBN: 978-0-07-054234-1
  issued:
    - year: 1987
  language: en
  number-of-pages: '416'
  publisher: McGraw-Hill
  publisher-place: New York
  source: Library of Congress ISBN
  title: Real and complex analysis
  type: book

- id: rumelhart_learning_1985
  accessed:
    - year: 2017
      month: 9
      day: 10
  author:
    - family: Rumelhart
      given: David E.
    - family: Hinton
      given: Geoffrey E.
    - family: Williams
      given: Ronald J.
  citation-key: rumelhart_learning_1985
  issued:
    - year: 1985
  note: '00000'
  publisher: California Univ San Diego La Jolla Inst for Cognitive Science
  source: Google Scholar
  title: Learning internal representations by error propagation
  type: report
  URL: http://www.dtic.mil/docs/citations/ADA164453

- id: rumelhart_learning_1988
  author:
    - family: Rumelhart
      given: David E
    - family: Hinton
      given: Geoffrey E
    - family: Williams
      given: Ronald J
  citation-key: rumelhart_learning_1988
  container-title: Cognitive modeling
  container-title-short: Cognitive modeling
  issue: '3'
  issued:
    - year: 1988
  page: '1'
  title: Learning representations by back-propagating errors
  type: article-journal
  volume: '5'

- id: ruslan_flood_2014
  author:
    - family: Ruslan
      given: Fazlina Ahmat
    - family: Samad
      given: Abd Manan
    - family: Zain
      given: Zainazlan Md
    - family: Adnan
      given: Ramli
  citation-key: ruslan_flood_2014
  container-title: >-
    Signal Processing & its Applications (CSPA), 2014 IEEE 10th International
    Colloquium on
  issued:
    - year: 2014
  note: '00000'
  page: 204–207
  publisher: IEEE
  title: >-
    Flood water level modeling and prediction using NARX neural network: Case
    study at Kelang river
  type: paper-conference

- id: saad_adaptive_1994
  author:
    - family: Saad
      given: Maarouf
    - family: Bigras
      given: Pascal
    - family: Dessaint
      given: L-A
    - family: Al-Haddad
      given: Kamal
  citation-key: saad_adaptive_1994
  container-title: IEEE Transactions on Industrial Electronics
  DOI: 10.1109/41.293877
  issue: '2'
  issued:
    - year: 1994
  note: '00000'
  page: 173–181
  title: Adaptive robot control using neural networks
  type: article-journal
  volume: '41'

- id: sabino_tenyear_2013
  abstract: >-
    Background—


    Very few studies have measured disease penetrance and prognostic factors of
    Chagas cardiomyopathy among asymptomatic Trypanosoma cruzi–infected persons.


    Methods and Results—


    We performed a retrospective cohort study among initially healthy blood
    donors with an index T cruzi–seropositive donation and age-, sex-, and
    period-matched seronegatives in 1996 to 2002 in the Brazilian cities of São
    Paulo and Montes Claros. In 2008 to 2010, all subjects underwent medical
    history, physical examination, ECGs, and echocardiograms. ECG and
    echocardiogram results were classified by blinded core laboratories, and
    records with abnormal results were reviewed by a blinded panel of 3
    cardiologists who adjudicated the outcome of Chagas cardiomyopathy.
    Associations with Chagas cardiomyopathy were tested with multivariate
    logistic regression. Mean follow-up time between index donation and outcome
    assessment was 10.5 years for the seropositives and 11.1 years for the
    seronegatives. Among 499 T cruzi seropositives, 120 (24%) had definite
    Chagas cardiomyopathy, and among 488 T cruzi seronegatives, 24 (5%) had
    cardiomyopathy, for an incidence difference of 1.85 per 100 person-years
    attributable to T cruzi infection. Of the 120 seropositives classified as
    having Chagas cardiomyopathy, only 31 (26%) presented with ejection fraction
    <50%, and only 11 (9%) were classified as New York Heart Association class
    II or higher. Chagas cardiomyopathy was associated (P<0.01) with male sex, a
    history of abnormal ECG, and the presence of an S3 heart sound.


    Conclusions—


    There is a substantial annual incidence of Chagas cardiomyopathy among
    initially asymptomatic T cruzi–seropositive blood donors, although disease
    was mild at diagnosis.
  accessed:
    - year: 2021
      month: 11
      day: 25
  author:
    - family: Sabino
      given: Ester C.
    - family: Ribeiro
      given: Antonio L.
    - family: Salemi
      given: Vera M.C.
    - family: Di Lorenzo Oliveira
      given: Claudia
    - family: Antunes
      given: Andre P.
    - family: Menezes
      given: Marcia M.
    - family: Ianni
      given: Barbara M.
    - family: Nastari
      given: Luciano
    - family: Fernandes
      given: Fabio
    - family: Patavino
      given: Giuseppina M.
    - family: Sachdev
      given: Vandana
    - family: Capuani
      given: Ligia
    - family: Almeida-Neto
      given: Cesar
      non-dropping-particle: de
    - family: Carrick
      given: Danielle M.
    - family: Wright
      given: David
    - family: Kavounis
      given: Katherine
    - family: Goncalez
      given: Thelma T.
    - family: Carneiro-Proietti
      given: Anna Barbara
    - family: Custer
      given: Brian
    - family: Busch
      given: Michael P.
    - family: Murphy
      given: Edward L.
  citation-key: sabino_tenyear_2013
  container-title: Circulation
  DOI: 10.1161/CIRCULATIONAHA.112.123612
  issue: '10'
  issued:
    - year: 2013
      month: 3
      day: 12
  page: 1105-1115
  publisher: American Heart Association
  source: ahajournals.org (Atypon)
  title: >-
    Ten-Year Incidence of Chagas Cardiomyopathy Among Asymptomatic Trypanosoma
    cruzi–Seropositive Former Blood Donors
  type: article-journal
  URL: https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.112.123612
  volume: '127'

- id: sacker_invariant_1967
  author:
    - family: Sacker
      given: Robert John
  citation-key: sacker_invariant_1967
  issued:
    - year: 1967
  title: >-
    ON INVARIANT SURFACES AND BIFURCATION OF PERIODIC SOLUTIONS OF ORDINARY
    DIFFERENTIAL EQUATIONS.
  type: article-journal

- id: sacramento_dendritic_2018
  accessed:
    - year: 2018
      month: 12
      day: 4
  author:
    - family: Sacramento
      given: João
    - family: Ponte Costa
      given: Rui
    - family: Bengio
      given: Yoshua
    - family: Senn
      given: Walter
  citation-key: sacramento_dendritic_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 8734–8745
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Dendritic cortical microcircuits approximate the backpropagation algorithm
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/8089-dendritic-cortical-microcircuits-approximate-the-backpropagation-algorithm.pdf

- id: saggar_system_2007
  author:
    - family: Saggar
      given: Manish
    - family: Meriçli
      given: Tekin
    - family: Andoni
      given: Sari
    - family: Miikkulainen
      given: Risto
  citation-key: saggar_system_2007
  container-title: Neural Networks, 2007. IJCNN 2007. International Joint Conference on
  issued:
    - year: 2007
  note: '00000'
  page: 2239–2244
  publisher: IEEE
  title: >-
    System identification for the Hodgkin-Huxley model using artificial neural
    networks
  type: paper-conference

- id: saggar_system_2007a
  author:
    - family: Saggar
      given: Manish
    - family: Meriçli
      given: Tekin
    - family: Andoni
      given: Sari
    - family: Miikkulainen
      given: Risto
  citation-key: saggar_system_2007a
  container-title: Neural Networks, 2007. IJCNN 2007. International Joint Conference On
  issued:
    - year: 2007
  note: '00016'
  page: 2239-2244
  publisher: IEEE
  title: >-
    System Identification for the Hodgkin-Huxley Model Using Artificial Neural
    Networks
  type: paper-conference

- id: saito_precisionrecall_2015
  abstract: >-
    Binary classifiers are routinely evaluated with performance measures such as
    sensitivity and specificity, and performance is frequently illustrated with
    Receiver Operating Characteristics (ROC) plots. Alternative measures such as
    positive predictive value (PPV) and the associated Precision/Recall (PRC)
    plots are used less frequently. Many bioinformatics studies develop and
    evaluate classifiers that are to be applied to strongly imbalanced datasets
    in which the number of negatives outweighs the number of positives
    significantly. While ROC plots are visually appealing and provide an
    overview of a classifier's performance across a wide range of specificities,
    one can ask whether ROC plots could be misleading when applied in imbalanced
    classification scenarios. We show here that the visual interpretability of
    ROC plots in the context of imbalanced datasets can be deceptive with
    respect to conclusions about the reliability of classification performance,
    owing to an intuitive but wrong interpretation of specificity. PRC plots, on
    the other hand, can provide the viewer with an accurate prediction of future
    classification performance due to the fact that they evaluate the fraction
    of true positives among positive predictions. Our findings have potential
    implications for the interpretation of a large number of studies that use
    ROC plots on imbalanced datasets.
  accessed:
    - year: 2019
      month: 2
      day: 8
  author:
    - family: Saito
      given: Takaya
    - family: Rehmsmeier
      given: Marc
  citation-key: saito_precisionrecall_2015
  container-title: PLOS ONE
  container-title-short: PLOS ONE
  DOI: 10/f69237
  ISSN: 1932-6203
  issue: '3'
  issued:
    - year: 2015
      month: 4
      day: 3
  language: en
  page: e0118432
  source: PLoS Journals
  title: >-
    The Precision-Recall Plot Is More Informative than the ROC Plot When
    Evaluating Binary Classifiers on Imbalanced Datasets
  type: article-journal
  URL: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432
  volume: '10'

- id: salehinejad_recent_2017
  abstract: >-
    Recurrent neural networks (RNNs) are capable of learning features and long
    term dependencies from sequential and time-series data. The RNNs have a
    stack of non-linear units where at least one connection between units forms
    a directed cycle. A well-trained RNN can model any dynamical system;
    however, training RNNs is mostly plagued by issues in learning long-term
    dependencies. In this paper, we present a survey on RNNs and several new
    advances for newcomers and professionals in the field. The fundamentals and
    recent advances are explained and the research challenges are introduced.
  accessed:
    - year: 2019
      month: 5
      day: 23
  author:
    - family: Salehinejad
      given: Hojjat
    - family: Sankar
      given: Sharan
    - family: Barfett
      given: Joseph
    - family: Colak
      given: Errol
    - family: Valaee
      given: Shahrokh
  citation-key: salehinejad_recent_2017
  container-title: arXiv:1801.01078 [cs]
  issued:
    - year: 2017
      month: 12
      day: 28
  source: arXiv.org
  title: Recent Advances in Recurrent Neural Networks
  type: article-journal
  URL: http://arxiv.org/abs/1801.01078

- id: salman_adversarially_2020
  abstract: >-
    Transfer learning is a widely-used paradigm in deep learning, where models
    pre-trained on standard datasets can be efficiently adapted to downstream
    tasks. Typically, better pre-trained models yield better transfer results,
    suggesting that initial accuracy is a key aspect of transfer learning
    performance. In this work, we identify another such aspect: we find that
    adversarially robust models, while less accurate, often perform better than
    their standard-trained counterparts when used for transfer learning.
    Specifically, we focus on adversarially robust ImageNet classifiers, and
    show that they yield improved accuracy on a standard suite of downstream
    classification tasks. Further analysis uncovers more differences between
    robust and standard models in the context of transfer learning. Our results
    are consistent with (and in fact, add to) recent hypotheses stating that
    robustness leads to improved feature representations. Our code and models
    are available at https://github.com/Microsoft/robust-models-transfer .
  accessed:
    - year: 2020
      month: 7
      day: 26
  author:
    - family: Salman
      given: Hadi
    - family: Ilyas
      given: Andrew
    - family: Engstrom
      given: Logan
    - family: Kapoor
      given: Ashish
    - family: Madry
      given: Aleksander
  citation-key: salman_adversarially_2020
  container-title: arXiv:2007.08489 [cs, stat]
  issued:
    - year: 2020
      month: 7
      day: 16
  source: arXiv.org
  title: Do Adversarially Robust ImageNet Models Transfer Better?
  type: article-journal
  URL: http://arxiv.org/abs/2007.08489

- id: samek_evaluating_2015
  abstract: >-
    Deep Neural Networks (DNNs) have demonstrated impressive performance in
    complex machine learning tasks such as image classification or speech
    recognition. However, due to their multi-layer nonlinear structure, they are
    not transparent, i.e., it is hard to grasp what makes them arrive at a
    particular classification or recognition decision given a new unseen data
    sample. Recently, several approaches have been proposed enabling one to
    understand and interpret the reasoning embodied in a DNN for a single test
    image. These methods quantify the ''importance'' of individual pixels wrt
    the classification decision and allow a visualization in terms of a heatmap
    in pixel/input space. While the usefulness of heatmaps can be judged
    subjectively by a human, an objective quality measure is missing. In this
    paper we present a general methodology based on region perturbation for
    evaluating ordered collections of pixels such as heatmaps. We compare
    heatmaps computed by three different methods on the SUN397, ILSVRC2012 and
    MIT Places data sets. Our main result is that the recently proposed
    Layer-wise Relevance Propagation (LRP) algorithm qualitatively and
    quantitatively provides a better explanation of what made a DNN arrive at a
    particular classification decision than the sensitivity-based approach or
    the deconvolution method. We provide theoretical arguments to explain this
    result and discuss its practical implications. Finally, we investigate the
    use of heatmaps for unsupervised assessment of neural network performance.
  author:
    - family: Samek
      given: Wojciech
    - family: Binder
      given: Alexander
    - family: Montavon
      given: Grégoire
    - family: Bach
      given: Sebastian
    - family: Müller
      given: Klaus-Robert
  citation-key: samek_evaluating_2015
  container-title: arXiv:1509.06321 [cs]
  issued:
    - year: 2015
      month: 9
      day: 21
  note: '00000'
  source: arXiv.org
  title: Evaluating the visualization of what a Deep Neural Network has learned
  type: article-journal
  URL: http://arxiv.org/abs/1509.06321

- id: sangha_automated_2022
  author:
    - family: Sangha
      given: Veer
    - family: Mortazavi
      given: Bobak J.
    - family: Haimovich
      given: Adrian D.
    - family: Ribeiro
      given: Antônio H.
    - family: Brandt
      given: Cynthia A.
    - family: Jacoby
      given: Daniel L.
    - family: Schulz
      given: Wade L.
    - family: Krumholz
      given: Harlan M.
    - family: Ribeiro
      given: Antonio Luiz P.
    - family: Khera
      given: Rohan
  citation-key: sangha_automated_2022
  container-title: Nature Communications
  DOI: 10.1038/s41467-022-29153-3
  issued:
    - year: 2022
  note: https://www.medrxiv.org/content/10.1101/2021.09.22.21263926v1
  page: '1583'
  title: Automated multilabel diagnosis on electrocardiographic images and signals
  type: article-journal
  volume: '13'

- id: sangha_detection_2023
  abstract: >-
    BACKGROUND:

    Left ventricular (LV) systolic dysfunction is associated with a >8-fold
    increased risk of heart failure and a 2-fold risk of premature death. The
    use of ECG signals in screening for LV systolic dysfunction is limited by
    their availability to clinicians. We developed a novel deep learning–based
    approach that can use ECG images for the screening of LV systolic
    dysfunction.


    METHODS:

    Using 12-lead ECGs plotted in multiple different formats, and corresponding
    echocardiographic data recorded within 15 days from the Yale New Haven
    Hospital between 2015 and 2021, we developed a convolutional neural network
    algorithm to detect an LV ejection fraction <40%. The model was validated
    within clinical settings at Yale New Haven Hospital and externally on ECG
    images from Cedars Sinai Medical Center in Los Angeles, CA; Lake Regional
    Hospital in Osage Beach, MO; Memorial Hermann Southeast Hospital in Houston,
    TX; and Methodist Cardiology Clinic of San Antonio, TX. In addition, it was
    validated in the prospective Brazilian Longitudinal Study of Adult Health.
    Gradient-weighted class activation mapping was used to localize
    class-discriminating signals in ECG images.


    RESULTS:

    Overall, 385 601 ECGs with paired echocardiograms were used for model
    development. The model demonstrated high discrimination power across various
    ECG image formats and calibrations in internal validation (area under
    receiving operation characteristics \[AUROCs], 0.91; area under
    precision-recall curve \[AUPRC], 0.55); and external sets of ECG images from
    Cedars Sinai (AUROC, 0.90 and AUPRC, 0.53), outpatient Yale New Haven
    Hospital clinics (AUROC, 0.94 and AUPRC, 0.77), Lake Regional Hospital
    (AUROC, 0.90 and AUPRC, 0.88), Memorial Hermann Southeast Hospital (AUROC,
    0.91 and AUPRC 0.88), Methodist Cardiology Clinic (AUROC, 0.90 and AUPRC,
    0.74), and Brazilian Longitudinal Study of Adult Health cohort (AUROC, 0.95
    and AUPRC, 0.45). An ECG suggestive of LV systolic dysfunction portended a
    >27-fold higher odds of LV systolic dysfunction on transthoracic
    echocardiogram (odds ratio, 27.5 \[95% CI, 22.3–33.9] in the held-out set).
    Class-discriminative patterns localized to the anterior and anteroseptal
    leads (V2 to V3), corresponding to the left ventricle regardless of the ECG
    layout. A positive ECG screen in individuals with an LV ejection fraction
    ≥40% at the time of initial assessment was associated with a 3.9-fold
    increased risk of developing incident LV systolic dysfunction in the future
    (hazard ratio, 3.9 \[95% CI, 3.3–4.7]; median follow-up, 3.2 years).


    CONCLUSIONS:

    We developed and externally validated a deep learning model that identifies
    LV systolic dysfunction from ECG images. This approach represents an
    automated and accessible screening strategy for LV systolic dysfunction,
    particularly in low-resource settings.
  author:
    - family: Sangha
      given: Veer
    - family: Nargesi
      given: Arash A.
    - family: Dhingra
      given: Lovedeep S.
    - family: Mortazavi
      given: Bobak J.
    - family: Ribeiro
      given: Antônio H.
    - family: Brandt
      given: Cynthia A.
    - family: Miller
      given: Edward J.
    - family: Ribeiro
      given: Antonio Luiz P.
    - family: Velazquez
      given: Eric J.
    - family: Krumholz
      given: Harlan M.
    - family: Khera
      given: Rohan
  citation-key: sangha_detection_2023
  container-title: Circulation
  DOI: 10.1161/CIRCULATIONAHA.122.062646
  issued:
    - year: 2023
  source: medRxiv
  title: >-
    Detection of Left Ventricular Systolic Dysfunction from Electrocardiographic
    Images
  type: article-journal

- id: sangiorgio_robustness_2020
  abstract: >-
    Recurrent neurons (and in particular LSTM cells) demonstrated to be
    efficient when used as basic blocks to build sequence to sequence
    architectures, which represent the state-of-the-art approach in many
    sequential tasks related to natural language processing. In this work, these
    architectures are proposed as general purposes, multi-step predictors for
    nonlinear time series. We analyze artificial, noise-free data generated by
    chaotic oscillators and compare LSTM nets with the benchmarks set by
    feed-forward, one-step-recursive and multi-output predictors. We focus on
    two different training methods for LSTM nets. The traditional one makes use
    of the so-called teacher forcing, i.e. the ground truth data are used as
    input for each time step ahead, rather than the outputs predicted for the
    previous steps. Conversely, the second feeds the previous predictions back
    into the recurrent neurons, as it happens when the network is used in
    forecasting. LSTM predictors robustly show the strengths of the two
    benchmark competitors, i.e., the good short-term performance of
    one-step-recursive predictors and greatly improved mid-long-term predictions
    with respect to feed-forward, multi-output predictors. Training LSTM
    predictors without teacher forcing is recommended to improve accuracy and
    robustness, and ensures a more uniform distribution of the predictive power
    within the chaotic attractor. We also show that LSTM architectures maintain
    good performances when the number of time lags included in the input differs
    from the actual embedding dimension of the dataset, a feature that is very
    important when working on real data.
  accessed:
    - year: 2022
      month: 6
      day: 15
  author:
    - family: Sangiorgio
      given: Matteo
    - family: Dercole
      given: Fabio
  citation-key: sangiorgio_robustness_2020
  container-title: Chaos, Solitons & Fractals
  container-title-short: Chaos, Solitons & Fractals
  DOI: 10.1016/j.chaos.2020.110045
  ISSN: 0960-0779
  issued:
    - year: 2020
      month: 10
      day: 1
  language: en
  page: '110045'
  source: ScienceDirect
  title: >-
    Robustness of LSTM neural networks for multi-step forecasting of chaotic
    time series
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0960077920304422
  volume: '139'

- id: santurkar_how_2018
  abstract: >-
    Batch Normalization (BatchNorm) is a widely adopted technique that enables
    faster and more stable training of deep neural networks (DNNs). Despite its
    pervasiveness, the exact reasons for BatchNorm's effectiveness are still
    poorly understood. The popular belief is that this effectiveness stems from
    controlling the change of the layers' input distributions during training to
    reduce the so-called "internal covariate shift". In this work, we
    demonstrate that such distributional stability of layer inputs has little to
    do with the success of BatchNorm. Instead, we uncover a more fundamental
    impact of BatchNorm on the training process: it makes the optimization
    landscape significantly smoother. This smoothness induces a more predictive
    and stable behavior of the gradients, allowing for faster training.
  accessed:
    - year: 2018
      month: 12
      day: 4
  author:
    - family: Santurkar
      given: Shibani
    - family: Tsipras
      given: Dimitris
    - family: Ilyas
      given: Andrew
    - family: Madry
      given: Aleksander
  citation-key: santurkar_how_2018
  container-title: arXiv:1805.11604 [cs, stat]
  issued:
    - year: 2018
      month: 5
      day: 29
  source: arXiv.org
  title: How Does Batch Normalization Help Optimization?
  type: article-journal
  URL: http://arxiv.org/abs/1805.11604

- id: sarkka_bayesian_2013
  author:
    - family: Särkkä
      given: Simo
  call-number: QA279.5 .S27 2013
  citation-key: sarkka_bayesian_2013
  collection-number: '3'
  collection-title: Institute of Mathematical Statistics textbooks
  event-place: Cambridge, U.K. ; New York
  ISBN: 978-1-107-03065-7 978-1-107-61928-9
  issued:
    - year: 2013
  note: 'OCLC: ocn840462877'
  number-of-pages: '232'
  publisher: Cambridge University Press
  publisher-place: Cambridge, U.K. ; New York
  source: Library of Congress ISBN
  title: Bayesian filtering and smoothing
  type: book

- id: sarode_embedded_2015
  author:
    - family: Sarode
      given: Ketan Dinkar
    - family: Kumar
      given: V Ravi
    - family: Kulkarni
      given: BD
  citation-key: sarode_embedded_2015
  container-title: Chemical Engineering Science
  DOI: 10/f7mqwg
  ISSN: 0009-2509
  issued:
    - year: 2015
  page: 605-618
  title: >-
    Embedded Multiple Shooting Methodology in a Genetic Algorithm Framework for
    Parameter Estimation and State Identification of Complex Systems
  type: article-journal
  volume: '134'

- id: sassi_pdfecg_2017
  abstract: >-
    BACKGROUND: In clinical practice, data archiving of resting 12-lead
    electrocardiograms (ECGs) is mainly achieved by storing a PDF report in the
    hospital electronic health record (EHR). When available, digital ECG source
    data (raw samples) are only retained within the ECG management system.

    OBJECTIVE: The widespread availability of the ECG source data would
    undoubtedly permit successive analysis and facilitate longitudinal studies,
    with both scientific and diagnostic benefits.

    METHODS & RESULTS: PDF-ECG is a hybrid archival format which allows to store
    in the same file both the standard graphical report of an ECG together with
    its source ECG data (waveforms). Using PDF-ECG as a model to address the
    challenge of ECG data portability, long-term archiving and documentation, a
    real-world proof-of-concept test was conducted in a northern Italy hospital.
    A set of volunteers undertook a basic ECG using routine hospital equipment
    and the source data captured. Using dedicated web services, PDF-ECG
    documents were then generated and seamlessly uploaded in the hospital EHR,
    replacing the standard PDF reports automatically generated at the time of
    acquisition. Finally, the PDF-ECG files could be successfully retrieved and
    re-analyzed.

    CONCLUSION: Adding PDF-ECG to an existing EHR had a minimal impact on the
    hospital's workflow, while preserving the ECG digital data.
  author:
    - family: Sassi
      given: Roberto
    - family: Bond
      given: Raymond R.
    - family: Cairns
      given: Andrew
    - family: Finlay
      given: Dewar D.
    - family: Guldenring
      given: Daniel
    - family: Libretti
      given: Guido
    - family: Isola
      given: Lamberto
    - family: Vaglio
      given: Martino
    - family: Poeta
      given: Roberto
    - family: Campana
      given: Marco
    - family: Cuccia
      given: Claudio
    - family: Badilini
      given: Fabio
  citation-key: sassi_pdfecg_2017
  container-title: Journal of Electrocardiology
  container-title-short: J Electrocardiol
  DOI: 10.1016/j.jelectrocard.2017.08.001
  ISSN: 1532-8430
  issue: '6'
  issued:
    - literal: 2017 Nov - Dec
  language: eng
  page: 776-780
  PMID: '28843654'
  source: PubMed
  title: >-
    PDF-ECG in clinical practice: A model for long-term preservation of digital
    12-lead ECG data
  title-short: PDF-ECG in clinical practice
  type: article-journal
  volume: '50'

- id: sau_artificial_2024
  abstract: >-
    Background and Aims Artificial intelligence-enhanced electrocardiograms
    (AI-ECG) can be used to predict risk of future disease and mortality but has
    not yet been adopted into clinical practice. Existing model predictions lack
    actionability at an individual patient level, explainability and biological
    plausibility. We sought to address these limitations of previous AI-ECG
    approaches by developing the AI-ECG risk estimator (AIRE) platform.

    Methods and Results The AIRE platform was developed in a secondary care
    dataset of 1,163,401 ECGs from 189,539 patients, using deep learning with a
    discrete-time survival model to create a subject-specific survival curve
    using a single ECG. Therefore, AIRE predicts not only risk of mortality, but
    time-to-mortality. AIRE was validated in five diverse, transnational cohorts
    from the USA, Brazil and the UK, including volunteers, primary care and
    secondary care subjects. AIRE accurately predicts risk of all-cause
    mortality (C-index 0.775 (0.773-0.776)), cardiovascular (CV) death 0.832
    (0.831-0.834), non-CV death (0.749 (0.747-0.751)), future ventricular
    arrhythmia (0.760 (0.756-0.763)), future atherosclerotic cardiovascular
    disease (0.696 (0.694-0.698)) and future heart failure (0.787
    (0.785-0.889))). Through phenome- and genome-wide association studies, we
    identified candidate biological pathways for the prediction of increased
    risk, including changes in cardiac structure and function, and genes
    associated with cardiac structure, biological aging and metabolic syndrome.

    Conclusion AIRE is an actionable, explainable and biologically plausible
    AI-ECG risk estimation platform that has the potential for use worldwide
    across a wide range of clinical contexts for short- and long-term risk
    estimation.

    <img class="highwire-fragment fragment-image" alt="Figure"
    src="https://www.medrxiv.org/content/medrxiv/early/2024/01/15/2024.01.13.24301267/F1.medium.gif"
    width="440" height="373"/>Download figureOpen in new tab
  accessed:
    - year: 2024
      month: 2
      day: 26
  author:
    - family: Sau
      given: Arunashis
    - family: Pastika
      given: Libor
    - family: Sieliwonczyk
      given: Ewa
    - family: Patlatzoglou
      given: Konstantinos
    - family: Ribeiro
      given: Antonio H.
    - family: McGurk
      given: Kathryn A.
    - family: Zeidaabadi
      given: Boroumand
    - family: Zhang
      given: Henry
    - family: Macierzanka
      given: Krzysztof
    - family: Mandic
      given: Danilo
    - family: Sabino
      given: Ester
    - family: Giatti
      given: Luana
    - family: Barreto
      given: Sandhi M.
    - family: Camelo
      given: Lidyane do Valle
    - family: Tzoulaki
      given: Ioanna
    - family: O’Regan
      given: Declan P.
    - family: Peters
      given: Nicholas S.
    - family: Ware
      given: James S.
    - family: Ribeiro
      given: Antonio Luiz P.
    - family: Kramer
      given: Daniel B.
    - family: Waks
      given: Jonathan W.
    - family: Ng
      given: Fu Siong
  citation-key: sau_artificial_2024
  container-title: medXiv
  DOI: 10.1101/2024.01.13.24301267
  issued:
    - year: 2024
      month: 1
      day: 15
  language: en
  license: >-
    © 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available
    under a Creative Commons License (Attribution 4.0 International), CC BY 4.0,
    as described at http://creativecommons.org/licenses/by/4.0/
  page: 2024.01.13.24301267
  source: medRxiv
  title: >-
    Artificial intelligence–enabled electrocardiogram for mortality and
    cardiovascular risk estimation: An actionable, explainable and biologically
    plausible platform
  title-short: >-
    Artificial intelligence–enabled electrocardiogram for mortality and
    cardiovascular risk estimation
  type: article-journal
  URL: https://www.medrxiv.org/content/10.1101/2024.01.13.24301267v1

- id: sau_neural_2023
  author:
    - family: Sau
      given: Arunashis
    - family: Ribeiro
      given: Antônio H.
    - family: McGurk
      given: Kathryn
    - family: Pastika
      given: Libor
    - family: Bajaj
      given: Nikesh
    - family: Ardissino
      given: Maddalena
    - family: Chen
      given: Jun Yu
    - family: Wu
      given: Huiyi
    - family: Shi
      given: Xili
    - family: Hnatkova
      given: Katerina
    - family: Zheng
      given: Sean
    - family: Britton
      given: Annie
    - family: Shipley
      given: Martin
    - family: Andršová
      given: Irena
    - family: Novotný
      given: Tomáš
    - family: Sabino
      given: Ester
    - family: Giatti
      given: Luana
    - family: Barreto
      given: Sandhi
    - family: Waks
      given: Jonathan
    - family: Kramer
      given: Daniel
    - family: Mandic
      given: Danilo
    - family: Peters
      given: Nicholas
    - family: O'Regan
      given: Declan
    - family: Malik
      given: Marek
    - family: Ware
      given: James
    - family: Ribeiro
      given: Antonio L. P.
    - family: Ng
      given: Fu Siong
  citation-key: sau_neural_2023
  container-title: medRxiv
  issued:
    - year: 2023
  license: All rights reserved
  title: >-
    Neural network-derived electrocardiographic features have prognostic
    significance and important phenotypic and genotypic associations
  type: article-journal

- id: sau_neural_2023a
  abstract: >-
    Subtle, prognostically-meaningful ECG features may not be apparent to
    physicians. In the course of supervised machine learning (ML) training, many
    thousands of ECG features are identified. These are not limited to
    conventional ECG parameters and morphology.To investigate novel neural
    network (NN)-derived ECG features, that may have clinical, phenotypic and
    genotypic associations and prognostic significance.We extracted 5120
    NN-derived ECG features from an AI-ECG model trained for six simple
    diagnoses and applied unsupervised machine learning to identify three
    phenogroups. The derivation set, the Clinical Outcomes in Digital
    Electrocardiography (CODE) cohort (n = 1,558,421), is a database of ECGs
    recorded in primary care in Brazil. There were four external validation
    cohorts. A cohort of British civil servants (WH II, n = 5,066). A
    longitudinal study of volunteers in the UK (UK Biobank, n = 42,386). A
    longitudinal cohort of Brazilian public servants (ELSA-Brasil, n = 13,739).
    Lastly, a cohort of patients with chronic Chagas cardiomyopathy (SaMi-Trop,
    n = 1,631) .In the derivation cohort (CODE), the three phenogroups had
    significantly different mortality profiles (Figure 1). After adjusting for
    known covariates, phenogroup B had a 1.2-fold increase in long-term
    mortality compared to phenogroup A (HR 1.20, 95% CI 1.17-1.23, p &lt;
    0.0001). We externally validated our findings in four diverse cohorts.
    Phenogroup C was poorly represented in the volunteer cohorts and therefore
    was excluded from those analyses. We found phenogroup B had a significantly
    greater risk of mortality in all cohorts (Figure 1). We performed a
    phenome-wide association study (PheWAS) in the UK Biobank. We found ECG
    phenogroup significantly associated with cardiac and non-cardiac phenotypes,
    including cardiac chamber volumes and cardiac output (Figure 2A). A
    single-trait genome-wide association study (GWAS) was conducted. The GWAS
    yielded four loci (Figure 2B). SCN10A, SCN5A and CAV1 have well described
    roles in cardiac conduction and arrhythmia. ARHGAP24 has been previously
    associated with ECG parameters, however, our analysis has identified for the
    first time ARHGAP24 as a gene associated with a prognostically significant
    phenogroup. Mendelian randomisation demonstrated the higher risk ECG
    phenogroup was causally associated with higher odds of atrioventricular (AV)
    block but lower odds of atrial fibrillation and ischaemic heart
    disease.NN-derived ECG features have important applications beyond the
    original model from which they are derived and may be transferable and
    applicable for risk prediction in a wide range of settings, in addition to
    mortality prediction. We have shown the significant potential of NN-derived
    ECG features, as a highly transferable and potentially universal risk
    marker, that may be applied to a wide range of clinical contexts.
  accessed:
    - year: 2024
      month: 6
      day: 4
  author:
    - family: Sau
      given: A
    - family: Ribeiro
      given: A H
    - family: Mcgurk
      given: K A
    - family: Pastika
      given: L
    - family: Chen
      given: J Y
    - family: Ardissino
      given: M
    - family: Sabino
      given: E
    - family: Giatti
      given: L
    - family: Barreto
      given: S M
    - family: Mandic
      given: D
    - family: Peters
      given: N S
    - family: Malik
      given: M
    - family: Ware
      given: J
    - family: Ribeiro
      given: A L P
    - family: Ng
      given: F S
  citation-key: sau_neural_2023a
  container-title: European Heart Journal
  container-title-short: European Heart Journal
  DOI: 10.1093/eurheartj/ehad655.2921
  ISSN: 0195-668X
  issue: Supplement_2
  issued:
    - year: 2023
      month: 11
      day: 9
  page: ehad655.2921
  source: Silverchair
  title: >-
    Neural network-derived electrocardiographic features have prognostic
    significance and important phenotypic and genotypic associations
  type: article-journal
  URL: https://doi.org/10.1093/eurheartj/ehad655.2921
  volume: '44'

- id: savitch_absolute_2013
  author:
    - family: Savitch
      given: Walter J.
    - family: Mock
      given: Kenrick
  call-number: QA76.73.J38 S265 2013
  citation-key: savitch_absolute_2013
  edition: 5th ed
  event-place: Boston
  ISBN: 978-0-13-283031-7
  issued:
    - year: 2013
  number-of-pages: '1239'
  publisher: Addison-Wesley
  publisher-place: Boston
  source: Library of Congress ISBN
  title: 'Absolute Java: Walter Savitch ; contributor, Kenrick Mock'
  title-short: Absolute Java
  type: book

- id: saxe_exact_2014
  abstract: >-
    Despite the widespread practical success of deep learning methods, our
    theoretical understanding of the dynamics of learning in deep neural
    networks remains quite sparse. We attempt to bridge the gap between the
    theory and practice of deep learning by systematically analyzing learning
    dynamics for the restricted case of deep linear neural networks. Despite the
    linearity of their input-output map, such networks have nonlinear gradient
    descent dynamics on weights that change with the addition of each new hidden
    layer. We show that deep linear networks exhibit nonlinear learning
    phenomena similar to those seen in simulations of nonlinear networks,
    including long plateaus followed by rapid transitions to lower error
    solutions, and faster convergence from greedy unsupervised pretraining
    initial conditions than from random initial conditions. We provide an
    analytical description of these phenomena by finding new exact solutions to
    the nonlinear dynamics of deep learning. Our theoretical analysis also
    reveals the surprising finding that as the depth of a network approaches
    infinity, learning speed can nevertheless remain finite: for a special class
    of initial conditions on the weights, very deep networks incur only a
    finite, depth independent, delay in learning speed relative to shallow
    networks. We show that, under certain conditions on the training data,
    unsupervised pretraining can find this special class of initial conditions,
    while scaled random Gaussian initializations cannot. We further exhibit a
    new class of random orthogonal initial conditions on weights that, like
    unsupervised pre-training, enjoys depth independent learning times. We
    further show that these initial conditions also lead to faithful propagation
    of gradients even in deep nonlinear networks, as long as they operate in a
    special regime known as the edge of chaos.
  accessed:
    - year: 2020
      month: 8
      day: 27
  author:
    - family: Saxe
      given: Andrew M.
    - family: McClelland
      given: James L.
    - family: Ganguli
      given: Surya
  citation-key: saxe_exact_2014
  container-title: International Conference on Learning Representations (ICLR)
  issued:
    - year: 2014
      month: 2
      day: 19
  source: arXiv.org
  title: >-
    Exact solutions to the nonlinear dynamics of learning in deep linear neural
    networks
  type: article-journal
  URL: http://arxiv.org/abs/1312.6120

- id: scetbon_robust_2023
  abstract: >-
    In this work we study the robustness to adversarial attacks, of
    early-stopping strategies on gradient-descent (GD) methods for linear
    regression. More precisely, we show that early-stopped GD is optimally
    robust (up to an absolute constant) against Euclidean-norm adversarial
    attacks. However, we show that this strategy can be arbitrarily sub-optimal
    in the case of general Mahalanobis attacks. This observation is compatible
    with recent findings in the case of
    classification~\cite{Vardi2022GradientMP} that show that GD provably
    converges to non-robust models. To alleviate this issue, we propose to apply
    instead a GD scheme on a transformation of the data adapted to the attack.
    This data transformation amounts to apply feature-depending learning rates
    and we show that this modified GD is able to handle any Mahalanobis attack,
    as well as more general attacks under some conditions. Unfortunately,
    choosing such adapted transformations can be hard for general attacks. To
    the rescue, we design a simple and tractable estimator whose adversarial
    risk is optimal up to within a multiplicative constant of 1.1124 in the
    population regime, and works for any norm.
  accessed:
    - year: 2023
      month: 2
      day: 13
  author:
    - family: Scetbon
      given: Meyer
    - family: Dohmatob
      given: Elvis
  citation-key: scetbon_robust_2023
  DOI: 10.48550/arXiv.2301.13486
  issued:
    - year: 2023
      month: 1
      day: 31
  number: arXiv:2301.13486
  publisher: arXiv
  source: arXiv.org
  title: 'Robust Linear Regression: Gradient-descent, Early-stopping, and Beyond'
  title-short: Robust Linear Regression
  type: article
  URL: http://arxiv.org/abs/2301.13486

- id: scharstein_taxonomy_2002
  author:
    - family: Scharstein
      given: Daniel
    - family: Szeliski
      given: Richard
  citation-key: scharstein_taxonomy_2002
  container-title: International journal of computer vision
  issue: 1-3
  issued:
    - year: 2002
  note: '00000'
  page: 7–42
  title: >-
    A taxonomy and evaluation of dense two-frame stereo correspondence
    algorithms
  type: article-journal
  volume: '47'

- id: scherer_evaluation_2010
  abstract: >-
    A common practice to gain invariant features in object recognition models is
    to aggregate multiple low-level features over a small neighborhood. However,
    the diﬀerences between those models makes a comparison of the properties of
    diﬀerent aggregation functions hard. Our aim is to gain insight into
    diﬀerent functions by directly comparing them on a ﬁxed architecture for
    several common object recognition tasks. Empirical results show that a
    maximum pooling operation signiﬁcantly outperforms subsampling operations.
    Despite their shift-invariant properties, overlapping pooling windows are no
    signiﬁcant improvement over non-overlapping pooling windows. By applying
    this knowledge, we achieve state-of-the-art error rates of 4.57% on the NORB
    normalized-uniform dataset and 5.6% on the NORB jittered-cluttered dataset.
  accessed:
    - year: 2020
      month: 3
      day: 23
  author:
    - family: Scherer
      given: Dominik
    - family: Muller
      given: Andreas
    - family: Behnke
      given: Sven
  citation-key: scherer_evaluation_2010
  container-title: Artificial Neural Networks – ICANN
  DOI: 10.1007/978-3-642-15825-4_10
  event-place: Berlin, Heidelberg
  issued:
    - year: 2010
  page: 92-101
  publisher: Springer
  publisher-place: Berlin, Heidelberg
  title: >-
    Evaluation of Pooling Operations in Convolutional Architectures for Object
    Recognition
  type: chapter
  volume: '6354'

- id: schittkowski_convergence_1983
  author:
    - family: Schittkowski
      given: Klaus
  citation-key: schittkowski_convergence_1983
  container-title: Mathematische Operationsforschung und Statistik. Series Optimization
  container-title-short: Mathematische Operationsforschung und Statistik. Series Optimization
  DOI: 10.1080/02331938308842847
  ISSN: 0323-3898
  issue: '2'
  issued:
    - year: 1983
      month: 1
      day: 1
  note: '00000'
  page: 197-216
  title: >-
    On the convergence of a sequential quadratic programming method with an
    augmented lagrangian line search function
  type: article-journal
  URL: https://doi.org/10.1080/02331938308842847
  volume: '14'

- id: schittkowski_nonlinear_1982
  abstract: >-
    SummaryThe paper represents an outcome of an extensive comparative study of
    nonlinear optimization algorithms. This study indicates that quadratic
    approximation methods which are characterized by solving a sequence of
    quadratic subproblems recursively, belong to the most efficient and reliable
    nonlinear programming algorithms available at present. The purpose of this
    paper is to analyse the theoretical convergence properties and to
    investigate the numerical performance in more detail. In Part 1, the
    exactL1-penalty function of Han and Powell is replaced by a differentiable
    augmented Lagrange function for the line search computation to be able to
    prove the global convergence and to show that the steplength one is chosen
    in the neighbourhood of a solution. In Part 2, the quadratic subproblem is
    exchanged by a linear least squares problem to improve the efficiency, and
    to test the dependence of the performance from different solution methods
    for the quadratic or least squares subproblem.
  accessed:
    - year: 2018
      month: 3
      day: 28
  author:
    - family: Schittkowski
      given: Klaus
  citation-key: schittkowski_nonlinear_1982
  container-title: Numerische Mathematik
  container-title-short: Numer. Math.
  DOI: 10.1007/BF01395810
  ISSN: 0029-599X, 0945-3245
  issue: '1'
  issued:
    - year: 1982
      month: 2
      day: 1
  language: en
  note: '00000'
  page: 83-114
  source: link.springer.com
  title: >-
    The nonlinear programming method of Wilson, Han, and Powell with an
    augmented Lagrangian type line search function. Part 1: Convergence
    analysis.
  type: article-journal
  URL: https://link.springer.com/article/10.1007/BF01395810
  volume: '38'

- id: schittkowski_nonlinear_1982a
  abstract: >-
    SummaryThe paper represents an outcome of an extensive comparative study of
    nonlinear optimization algorithms. This study indicates that quadratic
    approximation methods which are characterized by solving a sequence of
    quadratic subproblems recursively, belong to the most efficient and reliable
    nonlinear programming algorithms available at present. The purpose of this
    paper is to analyse the theoretical convergence properties and to
    investigate the numerical performance in more detail. In Part 1, the
    exactL1-penalty function of Han and Powell is replaced by a differentiable
    augmented Lagrange function for the line search computation to the able to
    prove the global convergence and to show that the steplength one is chosen
    in the neighbourhood of a solution. In Part 2, the quadratic subproblem is
    exchanged by a linear least squares problem to improve the efficiency, and
    to test the dependence of the performance from different solution methods
    for the quadratic or least squares subproblems.
  accessed:
    - year: 2018
      month: 3
      day: 28
  author:
    - family: Schittkowski
      given: Klaus
  citation-key: schittkowski_nonlinear_1982a
  container-title: Numerische Mathematik
  container-title-short: Numer. Math.
  DOI: 10.1007/BF01395811
  ISSN: 0029-599X, 0945-3245
  issue: '1'
  issued:
    - year: 1982
      month: 2
      day: 1
  language: en
  note: '00000'
  page: 115-127
  source: link.springer.com
  title: >-
    The nonlinear programming method of Wilson, Han, and Powell with an
    augmented Lagrangian type line search function. Part 2: An efficient
    implementation with linear least squares subproblems
  type: article-journal
  URL: https://link.springer.com/article/10.1007/BF01395811
  volume: '38'

- id: schlapfer_computerinterpreted_2017
  abstract: "Computerized interpretation of the electrocardiogram (CIE) was introduced to improve the correct interpretation of the electrocardiogram (ECG), facilitating health care decision making and reducing costs. Worldwide, millions of ECGs are recorded annually, with the majority automatically analyzed, followed by an immediate interpretation. Limitations in the diagnostic accuracy of CIE were soon recognized and still persist, despite ongoing improvement in ECG algorithms. Unfortunately, inexperienced physicians ordering the ECG may fail to recognize interpretation mistakes and accept the automated diagnosis without criticism. Clinical mismanagement may result, with the risk of exposing patients to useless\_investigations or potentially dangerous treatment. Consequently, CIE over-reading and confirmation by an experienced ECG reader are essential and are repeatedly recommended in published reports. Implementation of new ECG knowledge is also important. The current status of automated ECG interpretation is reviewed, with suggestions for improvement."
  author:
    - family: Schläpfer
      given: Jürg
    - family: Wellens
      given: Hein J.
  citation-key: schlapfer_computerinterpreted_2017
  container-title: Journal of the American College of Cardiology
  DOI: 10/gbs2q5
  issue: '9'
  issued:
    - year: 2017
      month: 8
      day: 29
  page: '1183'
  title: 'Computer-Interpreted Electrocardiograms: Benefits and Limitations'
  type: article-journal
  URL: http://www.onlinejacc.org/content/70/9/1183.abstract
  volume: '70'

- id: scholkopf_kernel_1997
  abstract: >-
    A new method for performing a nonlinear form of Principal Component Analysis
    is proposed. By the use of integral operator kernel functions, one can e
    ciently compute principal components in high dimensional feature spaces,
    related to input space by some nonlinear map; for instance the space of all
    possible d pixel products in images. We give the derivation of the method
    and present experimental results on polynomial feature extraction for
    pattern recognition.
  author:
    - family: Scholkopf
      given: Bernhard
    - family: Smola
      given: Alexander
    - family: Muller
      given: Klaus Robert
  citation-key: scholkopf_kernel_1997
  container-title: Artificial Neural Networks — ICANN
  DOI: https://doi.org/10.1007/BFb0020217
  issued:
    - year: 1997
  page: 583–588
  title: Kernel Principal Component Analysis
  type: article-journal

- id: schon_estimation_2006
  author:
    - family: Schön
      given: Thomas B
  citation-key: schon_estimation_2006
  event-place: Linköping
  issued:
    - year: 2006
  language: Med sammanfattning på svenska.
  note: |-
    00000 
    OCLC: 185211029
  publisher: Univ.
  publisher-place: Linköping
  source: Open WorldCat
  title: 'Estimation of Nonlinear dynamic systems: theory and applications'
  title-short: Estimation of Nonlinear dynamic systems
  type: thesis

- id: schon_manipulating_2011
  author:
    - family: Schön
      given: Thomas B.
    - family: Lindsten
      given: Fredrik
  citation-key: schon_manipulating_2011
  issued:
    - year: 2011
  note: '00000'
  source: Google Scholar
  title: Manipulating the multivariate gaussian density
  type: document

- id: schon_system_2011
  abstract: >-
    This paper is concerned with the parameter estimation of a general class of
    nonlinear dynamic systems in state-space form. More specifically, a Maximum
    Likelihood (ML) framework is employed and an Expectation Maximisation (EM)
    algorithm is derived to compute these ML estimates. The Expectation (E) step
    involves solving a nonlinear state estimation problem, where the smoothed
    estimates of the states are required. This problem lends itself perfectly to
    the particle smoother, which provides arbitrarily good estimates. The
    maximisation (M) step is solved using standard techniques from numerical
    optimisation theory. Simulation examples demonstrate the efficacy of our
    proposed solution.
  author:
    - family: Schön
      given: Thomas B.
    - family: Wills
      given: Adrian
    - family: Ninness
      given: Brett
  citation-key: schon_system_2011
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10.1016/j.automatica.2010.10.013
  ISSN: 0005-1098
  issue: '1'
  issued:
    - year: 2011
      month: 1
      day: 1
  note: '00000'
  page: 39-49
  source: ScienceDirect
  title: System identification of nonlinear state-space models
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0005109810004279
  volume: '47'

- id: schoukens_cascaded_2016
  author:
    - family: Schoukens
      given: M
    - family: Mattsson
      given: P
    - family: Wigren
      given: T
    - family: Noel
      given: J P
  citation-key: schoukens_cascaded_2016
  issued:
    - year: 2016
  language: en
  page: '4'
  source: Zotero
  title: Cascaded tanks benchmark combining soft and hard nonlinearities
  type: article-journal

- id: schoukens_identification_2017
  abstract: >-
    Block-oriented nonlinear models are popular in nonlinear system
    identification because of their advantages of being simple to understand and
    easy to use. Many different identification approaches were developed over
    the years to estimate the parameters of a wide range of block-oriented
    nonlinear models. One class of these approaches uses linear approximations
    to initialize the identification algorithm. The best linear approximation
    framework and the $\epsilon$-approximation framework, or equivalent
    frameworks, allow the user to extract important information about the
    system, guide the user in selecting good candidate model structures and
    orders, and prove to be a good starting point for nonlinear system
    identification algorithms. This paper gives an overview of the different
    block-oriented nonlinear models that can be identified using linear
    approximations, and of the identification algorithms that have been
    developed in the past. A non-exhaustive overview of the most important other
    block-oriented nonlinear system identification approaches is also provided
    throughout this paper.
  author:
    - family: Schoukens
      given: Maarten
    - family: Tiels
      given: Koen
  citation-key: schoukens_identification_2017
  container-title: Automatica
  DOI: 10.1016/j.automatica.2017.06.044
  ISSN: '00051098'
  issued:
    - year: 2017
      month: 11
  page: 272-292
  source: arXiv.org
  title: >-
    Identification of block-oriented nonlinear systems starting from linear
    approximations: A survey
  title-short: >-
    Identification of block-oriented nonlinear systems starting from linear
    approximations
  type: article-journal
  URL: http://arxiv.org/abs/1607.01217
  volume: '85'

- id: schoukens_nonlinear_2019
  abstract: >-
    Nonlinear system identification is an extremely broad topic, since every
    system that is not linear is nonlinear. That makes it impossible to give a
    full overview of all aspects of the fi eld. For this reason, the selection
    of topics and the organization of the discussion are strongly colored by the
    personal journey of the authors in this nonlinear universe.
  author:
    - family: Schoukens
      given: Johan
    - family: Ljung
      given: Lennart
  citation-key: schoukens_nonlinear_2019
  container-title: IEEE Control Systems Magazine
  DOI: 10.1109/MCS.2019.2938121
  ISSN: 1941-000X
  issue: '6'
  issued:
    - year: 2019
      month: 12
  page: 28-99
  source: IEEE Xplore
  title: 'Nonlinear System Identification: A User-Oriented Road Map'
  title-short: Nonlinear System Identification
  type: article-journal
  volume: '39'

- id: schoukens_nonlinear_2019a
  abstract: >-
    The goal of this article is twofold. Firstly, nonlinear system
    identification is introduced to a wide audience, guiding practicing
    engineers and newcomers in the field to a sound solution of their data
    driven modeling problems for nonlinear dynamic systems. In addition, the
    article also provides a broad perspective on the topic to researchers that
    are already familiar with the linear system identification theory, showing
    the similarities and differences between the linear and nonlinear problem.
    The reader will be referred to the existing literature for detailed
    mathematical explanations and formal proofs. Here the focus is on the basic
    philosophy, giving an intuitive understanding of the problems and the
    solutions, by making a guided tour along the wide range of user choices in
    nonlinear system identification. Guidelines will be given in addition to
    many examples, to reach that goal.
  accessed:
    - year: 2019
      month: 4
      day: 8
  author:
    - family: Schoukens
      given: Johan
    - family: Ljung
      given: Lennart
  citation-key: schoukens_nonlinear_2019a
  container-title: arXiv:1902.00683 [cs]
  issued:
    - year: 2019
      month: 2
      day: 2
  source: arXiv.org
  title: 'Nonlinear System Identification: A User-Oriented Roadmap'
  title-short: Nonlinear System Identification
  type: article-journal
  URL: http://arxiv.org/abs/1902.00683

- id: schoukens_parametric_2015
  abstract: >-
    Block-oriented nonlinear models are popular in nonlinear modeling because of
    their advantages to be quite simple to understand and easy to use. To
    increase the flexibility of single branch block-oriented models, such as
    Hammerstein, Wiener, and Wiener-Hammerstein models, parallel block-oriented
    models can be considered. This paper presents a method to identify parallel
    Wiener-Hammerstein systems starting from input-output data only. In the
    first step, the best linear approximation is estimated for different input
    excitation levels. In the second step, the dynamics are decomposed over a
    number of parallel orthogonal branches. Next, the dynamics of each branch
    are partitioned into a linear time invariant subsystem at the input and a
    linear time invariant subsystem at the output. This is repeated for each
    branch of the model. The static nonlinear part of the model is also
    estimated during this step. The consistency of the proposed initialization
    procedure is proven. The method is validated on real-world measurements
    using a custom built parallel Wiener-Hammerstein test system.
  accessed:
    - year: 2018
      month: 11
      day: 20
  author:
    - family: Schoukens
      given: Maarten
    - family: Marconato
      given: Anna
    - family: Pintelon
      given: Rik
    - family: Vandersteen
      given: Gerd
    - family: Rolain
      given: Yves
  citation-key: schoukens_parametric_2015
  container-title: Automatica
  DOI: 10/f6w4rj
  ISSN: '00051098'
  issued:
    - year: 2015
      month: 1
  page: 111-122
  source: arXiv.org
  title: Parametric identification of parallel Wiener-Hammerstein systems
  type: article-journal
  URL: http://arxiv.org/abs/1708.06543
  volume: '51'

- id: schoukens_wienerhammerstein_
  abstract: >-
    This paper describes a benchmark for nonlinear system identification. A
    Wiener-Hammerstein system is selected as test object. In such a structure
    there is no direct access to the static nonlinearity starting from the
    measured input/output, because it is sandwiched between two unknown dynamic
    systems. The signal-to-noise ratio of the measurements is quite high, which
    puts the focus of the benchmark on the ability to identify the nonlinear
    behaviour, and not so much on the noise rejection properties. The benchmark
    is not intended as a competition, but as a tool to compare the possibilities
    of different methods to deal with this specific nonlinear structure.
  author:
    - family: Schoukens
      given: Johan
    - family: Suykens
      given: Johan
    - family: Ljung
      given: Lennart
  citation-key: schoukens_wienerhammerstein_
  language: en
  page: '4'
  source: Zotero
  title: Wiener-Hammerstein Benchmark
  type: article-journal

- id: schoukens_wienerhammerstein_2016
  author:
    - family: Schoukens
      given: M
    - family: Noel
      given: J P
  citation-key: schoukens_wienerhammerstein_2016
  issued:
    - year: 2016
  language: en
  page: '5'
  source: Zotero
  title: Wiener-Hammerstein benchmark with process noise
  type: article-journal

- id: schoukens_wienerhammerstein_2016a
  accessed:
    - year: 2017
      month: 8
      day: 28
  author:
    - family: Schoukens
      given: M.
    - family: Noël
      given: J. P.
  citation-key: schoukens_wienerhammerstein_2016a
  container-title: Workshop on Nonlinear System Identification Benchmarks, Brussels
  issued:
    - year: 2016
  note: '00000'
  page: 15–19
  source: Google Scholar
  title: Wiener-Hammerstein benchmark with process noise
  type: paper-conference
  URL: >-
    https://www.researchgate.net/profile/Maarten_Schoukens/publication/307569307_Wiener-Hammerstein_benchmark_with_process_noise/links/57c92eca08ae28c01d54ab0a.pdf

- id: schroff_facenet_2015
  accessed:
    - year: 2018
      month: 1
      day: 26
  author:
    - family: Schroff
      given: Florian
    - family: Kalenichenko
      given: Dmitry
    - family: Philbin
      given: James
  citation-key: schroff_facenet_2015
  event-title: >-
    Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition
  issued:
    - year: 2015
  note: '00000'
  page: 815-823
  source: www.cv-foundation.org
  title: 'FaceNet: A Unified Embedding for Face Recognition and Clustering'
  title-short: FaceNet
  type: paper-conference
  URL: >-
    https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html

- id: schwarz_estimating_1978
  author:
    - family: Schwarz
      given: Gideon
  citation-key: schwarz_estimating_1978
  container-title: The annals of statistics
  container-title-short: The annals of statistics
  ISSN: 0090-5364
  issue: '2'
  issued:
    - year: 1978
  note: '00000'
  page: 461-464
  title: Estimating the dimension of a model
  type: article-journal
  volume: '6'

- id: scott_limited_2021
  abstract: >-
    Selection has dramatically shaped genetic and phenotypic variation in bread
    wheat. We can assess the genomic basis of historical phenotypic changes, and
    the potential for future improvement, using experimental populations that
    attempt to undo selection through the randomizing effects of recombination.
  accessed:
    - year: 2022
      month: 2
      day: 28
  author:
    - family: Scott
      given: Michael F.
    - family: Fradgley
      given: Nick
    - family: Bentley
      given: Alison R.
    - family: Brabbs
      given: Thomas
    - family: Corke
      given: Fiona
    - family: Gardner
      given: Keith A.
    - family: Horsnell
      given: Richard
    - family: Howell
      given: Phil
    - family: Ladejobi
      given: Olufunmilayo
    - family: Mackay
      given: Ian J.
    - family: Mott
      given: Richard
    - family: Cockram
      given: James
  citation-key: scott_limited_2021
  container-title: Genome Biology
  container-title-short: Genome Biology
  DOI: 10.1186/s13059-021-02354-7
  ISSN: 1474-760X
  issue: '1'
  issued:
    - year: 2021
      month: 5
      day: 6
  page: '137'
  source: BioMed Central
  title: >-
    Limited haplotype diversity underlies polygenic trait architecture across
    70 years of wheat breeding
  type: article-journal
  URL: https://doi.org/10.1186/s13059-021-02354-7
  volume: '22'

- id: selvaraju_gradcam_2020
  abstract: >-
    We propose a technique for producing "visual explanations" for decisions
    from a large class of CNN-based models, making them more transparent. Our
    approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the
    gradients of any target concept, flowing into the final convolutional layer
    to produce a coarse localization map highlighting important regions in the
    image for predicting the concept. Grad-CAM is applicable to a wide variety
    of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used
    for structured outputs, (3) CNNs used in tasks with multimodal inputs or
    reinforcement learning, without any architectural changes or re-training. We
    combine Grad-CAM with fine-grained visualizations to create a
    high-resolution class-discriminative visualization and apply it to
    off-the-shelf image classification, captioning, and visual question
    answering (VQA) models, including ResNet-based architectures. In the context
    of image classification models, our visualizations (a) lend insights into
    their failure modes, (b) are robust to adversarial images, (c) outperform
    previous methods on localization, (d) are more faithful to the underlying
    model and (e) help achieve generalization by identifying dataset bias. For
    captioning and VQA, we show that even non-attention based models can
    localize inputs. We devise a way to identify important neurons through
    Grad-CAM and combine it with neuron names to provide textual explanations
    for model decisions. Finally, we design and conduct human studies to measure
    if Grad-CAM helps users establish appropriate trust in predictions from
    models and show that Grad-CAM helps untrained users successfully discern a
    'stronger' nodel from a 'weaker' one even when both make identical
    predictions. Our code is available at https://github.com/ramprs/grad-cam/,
    along with a demo at http://gradcam.cloudcv.org, and a video at
    youtu.be/COjUB9Izk6E.
  accessed:
    - year: 2021
      month: 3
      day: 24
  author:
    - family: Selvaraju
      given: Ramprasaath R.
    - family: Cogswell
      given: Michael
    - family: Das
      given: Abhishek
    - family: Vedantam
      given: Ramakrishna
    - family: Parikh
      given: Devi
    - family: Batra
      given: Dhruv
  citation-key: selvaraju_gradcam_2020
  container-title: International Journal of Computer Vision
  container-title-short: Int J Comput Vis
  DOI: 10.1007/s11263-019-01228-7
  ISSN: 0920-5691, 1573-1405
  issue: '2'
  issued:
    - year: 2020
      month: 2
  page: 336-359
  source: arXiv.org
  title: >-
    Grad-CAM: Visual Explanations from Deep Networks via Gradient-based
    Localization
  title-short: Grad-CAM
  type: article-journal
  URL: http://arxiv.org/abs/1610.02391
  volume: '128'

- id: sen_principles_2007
  author:
    - family: Sen
      given: P.C.
  citation-key: sen_principles_2007
  ISBN: 978-81-265-1101-3
  issued:
    - year: 2007
  note: '00000'
  publisher: Wiley India Pvt. Limited
  title: Principles of electric machines and power electronics
  type: book
  URL: https://books.google.com.br/books?id=KlN84tZ1p9cC

- id: sermanet_overfeat_2013
  abstract: >-
    We present an integrated framework for using Convolutional Networks for
    classification, localization and detection. We show how a multiscale and
    sliding window approach can be efficiently implemented within a ConvNet. We
    also introduce a novel deep learning approach to localization by learning to
    predict object boundaries. Bounding boxes are then accumulated rather than
    suppressed in order to increase detection confidence. We show that different
    tasks can be learned simultaneously using a single shared network. This
    integrated framework is the winner of the localization task of the ImageNet
    Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very
    competitive results for the detection and classifications tasks. In
    post-competition work, we establish a new state of the art for the detection
    task. Finally, we release a feature extractor from our best model called
    OverFeat.
  author:
    - family: Sermanet
      given: Pierre
    - family: Eigen
      given: David
    - family: Zhang
      given: Xiang
    - family: Mathieu
      given: Michael
    - family: Fergus
      given: Rob
    - family: LeCun
      given: Yann
  citation-key: sermanet_overfeat_2013
  container-title: arXiv:1312.6229 [cs]
  issued:
    - year: 2013
      month: 12
      day: 21
  note: '00000'
  source: arXiv.org
  title: >-
    OverFeat: Integrated Recognition, Localization and Detection using
    Convolutional Networks
  title-short: OverFeat
  type: article-journal
  URL: http://arxiv.org/abs/1312.6229

- id: shah_errors_2007
  abstract: >-
    BACKGROUND: More than 100 million computer-interpreted electrocardiograms
    (ECG-C) are obtained annually. However, there are few contemporary published
    data on the  accuracy of cardiac rhythm interpretation by this method.
    PURPOSE: The purpose of this study is to determine the accuracy of ECG-C
    rhythm interpretation in a typical patient population. METHODS: We compared
    the ECG-C rhythm interpretation  to that of 2 expert overreaders in 2112
    randomly selected standard 12-lead ECGs.  RESULTS: The ECG-C correctly
    interpreted the rhythm in 1858 and incorrectly identified the rhythm in 254
    (overall accuracy, 88.0%). Sinus rhythm was correctly interpreted in 95.0%
    of the ECGs (1666/1753) with this rhythm, whereas  nonsinus rhythms were
    correctly interpreted with an accuracy of only 53.5% (192/359) (P < .0001).
    The ECG-C interpreted sinus rhythm with a sensitivity of 95% (confidence
    interval, 93.8-96.7), specificity of 66.3%, and positive predictive value of
    93.2%. The ECG-C interpreted nonsinus rhythms with a sensitivity of 72%,
    (confidence interval, 68.7-73.7), a specificity of 93%, and a positive
    predictive value of 59.3%. Of the 254 ECGs that had incorrect rhythm
    interpretation, additional major errors were noted in 137 (54%).
    CONCLUSIONS: The
  author:
    - family: Shah
      given: Atman P.
    - family: Rubin
      given: Stanley A.
  citation-key: shah_errors_2007
  container-title: Journal of Electrocardiology
  container-title-short: J Electrocardiol
  DOI: 10.1016/j.jelectrocard.2007.03.008
  ISSN: 1532-8430 0022-0736
  issue: '5'
  issued:
    - literal: 2007 Sep-Oct
  language: eng
  page: 385-390
  PMID: '17531257'
  title: >-
    Errors in the computerized electrocardiogram interpretation of cardiac
    rhythm.
  type: article-journal
  volume: '40'

- id: shaham_understanding_2018
  author:
    - family: Shaham
      given: Uri
    - family: Yamada
      given: Yutaro
    - family: Negahban
      given: Sahand
  citation-key: shaham_understanding_2018
  container-title: Neurocomputing
  container-title-short: Neurocomputing
  ISSN: 0925-2312
  issued:
    - year: 2018
  page: 195-204
  publisher: Elsevier
  title: >-
    Understanding adversarial training: Increasing local stability of supervised
    models through robust optimization
  type: article-journal
  volume: '307'

- id: shalev-shwartz_understanding_2014
  accessed:
    - year: 2020
      month: 5
      day: 6
  author:
    - family: Shalev-Shwartz
      given: Shai
    - family: Ben-David
      given: Shai
  citation-key: shalev-shwartz_understanding_2014
  DOI: 10.1017/CBO9781107298019
  ISBN: 978-1-107-29801-9
  issued:
    - year: 2014
  language: en
  publisher: Cambridge University Press
  source: DOI.org (Crossref)
  title: 'Understanding Machine Learning: From Theory to Algorithms'
  title-short: Understanding Machine Learning
  type: book
  URL: http://ebooks.cambridge.org/ref/id/CBO9781107298019

- id: shanmugam_multiple_2018
  accessed:
    - year: 2018
      month: 12
      day: 10
  author:
    - family: Shanmugam
      given: Divya
    - family: Blalock
      given: Davis
    - family: Gong
      given: Jen G.
    - family: Guttag
      given: John
  citation-key: shanmugam_multiple_2018
  issued:
    - year: 2018
      month: 12
      day: 2
  language: en
  source: arxiv.org
  title: Multiple Instance Learning for ECG Risk Stratification
  type: article-journal
  URL: https://arxiv.org/abs/1812.00475

- id: shanmugam_multiple_2018a
  abstract: >-
    In this paper, we apply a multiple instance learning paradigm to
    signal-based risk stratiﬁcation for cardiovascular outcomes. In contrast to
    methods that require hand-crafted features or domain knowledge, our method
    learns a representation with state-of-the-art predictive power from the raw
    ECG signal. We accomplish this by leveraging the multiple instance learning
    framework. This framework is particularly valuable to learning from
    biometric signals, where patient-level labels are available but signal
    segments are rarely annotated. We make two contributions in this paper: 1)
    reframing risk stratiﬁcation for cardiovascular death (CVD) as a multiple
    instance learning problem, and 2) using this framework to design a new risk
    score, for which patients in the highest quartile are 15.9 times more likely
    to die of CVD within 90 days of hospital admission for an acute coronary
    syndrome.
  accessed:
    - year: 2018
      month: 12
      day: 10
  author:
    - family: Shanmugam
      given: Divya
    - family: Blalock
      given: Davis
    - family: Gong
      given: Jen G.
    - family: Guttag
      given: John
  citation-key: shanmugam_multiple_2018a
  container-title: arXiv:1812.00475 [cs, stat]
  issued:
    - year: 2018
      month: 12
      day: 2
  language: en
  source: arXiv.org
  title: Multiple Instance Learning for ECG Risk Stratification
  type: article-journal
  URL: http://arxiv.org/abs/1812.00475

- id: shanmugam_multiple_2018b
  abstract: >-
    In this paper, we apply a multiple instance learning paradigm to
    signal-based risk stratification for cardiovascular outcomes. In contrast to
    methods that require hand-crafted features or domain knowledge, our method
    learns a representation with state-of-the-art predictive power from the raw
    ECG signal. We accomplish this by leveraging the multiple instance learning
    framework. This framework is particularly valuable to learning from
    biometric signals, where patient-level labels are available but signal
    segments are rarely annotated. We make two contributions in this paper: 1)
    reframing risk stratification for cardiovascular death (CVD) as a multiple
    instance learning problem, and 2) using this framework to design a new risk
    score, for which patients in the highest quartile are 15.9 times more likely
    to die of CVD within 90 days of hospital admission for an acute coronary
    syndrome.
  accessed:
    - year: 2018
      month: 12
      day: 13
  author:
    - family: Shanmugam
      given: Divya
    - family: Blalock
      given: Davis
    - family: Gong
      given: Jen G.
    - family: Guttag
      given: John
  citation-key: shanmugam_multiple_2018b
  container-title: arXiv:1812.00475 [cs, stat]
  issued:
    - year: 2018
      month: 12
      day: 2
  source: arXiv.org
  title: Multiple Instance Learning for ECG Risk Stratification
  type: article-journal
  URL: http://arxiv.org/abs/1812.00475

- id: sharif_suitability_2018
  accessed:
    - year: 2021
      month: 5
      day: 15
  author:
    - family: Sharif
      given: Mahmood
    - family: Bauer
      given: Lujo
    - family: Reiter
      given: Michael K.
  citation-key: sharif_suitability_2018
  event-title: >-
    Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition Workshops
  issued:
    - year: 2018
  page: 1605-1613
  source: openaccess.thecvf.com
  title: >-
    On the Suitability of Lp-Norms for Creating and Preventing Adversarial
    Examples
  type: paper-conference
  URL: >-
    https://openaccess.thecvf.com/content_cvpr_2018_workshops/w32/html/Sharif_On_the_Suitability_CVPR_2018_paper.html

- id: shashikumar_deep_2017
  abstract: >-
    Atrial Fibrillation (AF) is the most common cardiac arrhythmia in clinical
    practice, with a prevalence of 2% in the community. Not only it is
    associated with reduced quality of life, but also increased risk of stroke
    and myocardial infarction. Unfortunately, many cases of AF are clinically
    silent and undiagnosed, but long-term monitoring is difﬁcult. Nonetheless,
    efforts at monitoring at-risk individuals and detecting clinically silent AF
    may yield signiﬁcant public health beneﬁt, as individuals with new-onset,
    asymptomatic AF would receive preventive therapies with anticoagulants and
    beta-blockers, for example.
  accessed:
    - year: 2018
      month: 10
      day: 21
  author:
    - family: Shashikumar
      given: Supreeth Prajwal
    - family: Shah
      given: Amit J.
    - family: Li
      given: Qiao
    - family: Clifford
      given: Gari D.
    - family: Nemati
      given: Shamim
  citation-key: shashikumar_deep_2017
  container-title: >-
    Proceedings of the IEEE EMBS International Conference on Biomedical & Health
    Informatics (BHI)
  DOI: 10.1109/BHI.2017.7897225
  event-place: Orland, FL, USA
  event-title: >-
    2017 IEEE EMBS International Conference on Biomedical & Health Informatics
    (BHI)
  ISBN: 978-1-5090-4179-4
  issued:
    - year: 2017
  language: en
  page: 141-144
  publisher: IEEE
  publisher-place: Orland, FL, USA
  source: Crossref
  title: >-
    A deep learning approach to monitoring and detecting atrial fibrillation
    using wearable technology
  type: paper-conference
  URL: http://ieeexplore.ieee.org/document/7897225/

- id: shashikumar_detection_2018
  abstract: >-
    Detection of atrial fibrillation (AF), a type of cardiac arrhythmia, is
    difficult since many cases of AF are usually clinically silent and
    undiagnosed. In particular paroxysmal AF is a form of AF that occurs
    occasionally, and has a higher probability of being undetected. In this
    work, we present an attention based deep learning framework for detection of
    paroxysmal AF episodes from a sequence of windows. Time-frequency
    representation of 30 seconds recording windows, over a 10 minute data
    segment, are fed sequentially into a deep convolutional neural network for
    image-based feature extraction, which are then presented to a bidirectional
    recurrent neural network with an attention layer for AF detection. To
    demonstrate the effectiveness of the proposed framework for transient AF
    detection, we use a database of 24 hour Holter Electrocardiogram (ECG)
    recordings acquired from 2850 patients at the University of Virginia heart
    station. The algorithm achieves an AUC of 0.94 on the testing set, which
    exceeds the performance of baseline models. We also demonstrate the
    cross-domain generalizablity of the approach by adapting the learned model
    parameters from one recording modality (ECG) to another (photoplethysmogram)
    with improved AF detection performance. The proposed high accuracy, low
    false alarm algorithm for detecting paroxysmal AF has potential applications
    in long-term monitoring using wearable sensors.
  accessed:
    - year: 2018
      month: 10
      day: 20
  author:
    - family: Shashikumar
      given: Supreeth P.
    - family: Shah
      given: Amit J.
    - family: Clifford
      given: Gari D.
    - family: Nemati
      given: Shamim
  citation-key: shashikumar_detection_2018
  collection-title: KDD '18
  container-title: >-
    Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
    Discovery & Data Mining
  DOI: 10.1145/3219819.3219912
  event-place: New York, NY, USA
  ISBN: 978-1-4503-5552-0
  issued:
    - year: 2018
  page: 715–723
  publisher: ACM
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    Detection of Paroxysmal Atrial Fibrillation Using Attention-based
    Bidirectional Recurrent Neural Networks
  type: paper-conference
  URL: http://doi.acm.org/10.1145/3219819.3219912

- id: shen_outofdistribution_2021
  abstract: >-
    Classic machine learning methods are built on the $i.i.d.$ assumption that
    training and testing data are independent and identically distributed.
    However, in real scenarios, the $i.i.d.$ assumption can hardly be satisfied,
    rendering the sharp drop of classic machine learning algorithms'
    performances under distributional shifts, which indicates the significance
    of investigating the Out-of-Distribution generalization problem.
    Out-of-Distribution (OOD) generalization problem addresses the challenging
    setting where the testing distribution is unknown and different from the
    training. This paper serves as the first effort to systematically and
    comprehensively discuss the OOD generalization problem, from the definition,
    methodology, evaluation to the implications and future directions. Firstly,
    we provide the formal definition of the OOD generalization problem.
    Secondly, existing methods are categorized into three parts based on their
    positions in the whole learning pipeline, namely unsupervised representation
    learning, supervised model learning and optimization, and typical methods
    for each category are discussed in detail. We then demonstrate the
    theoretical connections of different categories, and introduce the commonly
    used datasets and evaluation metrics. Finally, we summarize the whole
    literature and raise some future directions for OOD generalization problem.
    The summary of OOD generalization methods reviewed in this survey can be
    found at http://out-of-distribution-generalization.com.
  accessed:
    - year: 2023
      month: 4
      day: 4
  author:
    - family: Shen
      given: Zheyan
    - family: Liu
      given: Jiashuo
    - family: He
      given: Yue
    - family: Zhang
      given: Xingxuan
    - family: Xu
      given: Renzhe
    - family: Yu
      given: Han
    - family: Cui
      given: Peng
  citation-key: shen_outofdistribution_2021
  DOI: 10.48550/arXiv.2108.13624
  issued:
    - year: 2021
      month: 8
      day: 31
  number: arXiv:2108.13624
  publisher: arXiv
  source: arXiv.org
  title: 'Towards Out-Of-Distribution Generalization: A Survey'
  title-short: Towards Out-Of-Distribution Generalization
  type: article
  URL: http://arxiv.org/abs/2108.13624

- id: sheng_trust_2016
  author:
    - family: Sheng
      given: Wanxing
    - family: Liu
      given: Ke-Yan
    - family: Cheng
      given: Sheng
    - family: Meng
      given: Xiaoli
    - family: Dai
      given: Wei
  citation-key: sheng_trust_2016
  container-title: IEEE Transactions on Smart Grid
  DOI: 10.1109/TSG.2014.2376197
  issue: '1'
  issued:
    - year: 2016
  note: '00000'
  page: 381–391
  title: >-
    A trust region SQP method for coordinated voltage control in smart
    distribution grid
  type: article-journal
  volume: '7'

- id: shi_neural_2018
  abstract: >-
    Precise near-ground trajectory control is difficult for multi-rotor drones,
    due to the complex aerodynamic effects caused by interactions between
    multi-rotor airflow and the environment. Conventional control methods often
    fail to properly account for these complex effects and fall short in
    accomplishing smooth landing. In this paper, we present a novel
    deep-learning-based robust nonlinear controller (Neural Lander) that
    improves control performance of a quadrotor during landing. Our approach
    combines a nominal dynamics model with a Deep Neural Network (DNN) that
    learns high-order interactions. We apply spectral normalization (SN) to
    constrain the Lipschitz constant of the DNN. Leveraging this Lipschitz
    property, we design a nonlinear feedback linearization controller using the
    learned model and prove system stability with disturbance rejection. To the
    best of our knowledge, this is the first DNN-based nonlinear feedback
    controller with stability guarantees that can utilize arbitrarily large
    neural nets. Experimental results demonstrate that the proposed controller
    significantly outperforms a Baseline Nonlinear Tracking Controller in both
    landing and cross-table trajectory tracking cases. We also empirically show
    that the DNN generalizes well to unseen data outside the training domain.
  accessed:
    - year: 2019
      month: 6
      day: 1
  author:
    - family: Shi
      given: Guanya
    - family: Shi
      given: Xichen
    - family: O'Connell
      given: Michael
    - family: Yu
      given: Rose
    - family: Azizzadenesheli
      given: Kamyar
    - family: Anandkumar
      given: Animashree
    - family: Yue
      given: Yisong
    - family: Chung
      given: Soon-Jo
  citation-key: shi_neural_2018
  container-title: arXiv:1811.08027 [cs]
  issued:
    - year: 2018
      month: 11
      day: 19
  source: arXiv.org
  title: 'Neural Lander: Stable Drone Landing Control using Learned Dynamics'
  title-short: Neural Lander
  type: article-journal
  URL: http://arxiv.org/abs/1811.08027

- id: signoretto_tensor_2011
  abstract: >-
    Tensor completion recently emerged as a generalization of matrix completion
    for higher order arrays. This problem formulation allows one to exploit the
    structure of data that intrinsically have multiple dimensions. In this work,
    we recall a convex formulation for minimum (multilinear) ranks completion of
    arrays of arbitrary order. Successively we focus on completion of partially
    observed spectral images; the latter can be naturally represented as third
    order tensors and typically exhibit intraband correlations. We compare
    different convex formulations and assess them through case studies.
  author:
    - family: Signoretto
      given: M.
    - family: Plas
      given: R. Van
      dropping-particle: de
    - family: Moor
      given: B. De
    - family: Suykens
      given: J. A. K.
  citation-key: signoretto_tensor_2011
  container-title: IEEE Signal Processing Letters
  DOI: 10.1109/LSP.2011.2151856
  ISSN: 1070-9908
  issue: '7'
  issued:
    - year: 2011
      month: 7
  note: '00000'
  page: 403-406
  source: IEEE Xplore
  title: >-
    Tensor Versus Matrix Completion: A Comparison With Application to Spectral
    Data
  title-short: Tensor Versus Matrix Completion
  type: article-journal
  volume: '18'

- id: sigurgeirsson_transport_2003
  accessed:
    - year: 2017
      month: 9
      day: 18
  author:
    - family: Sigurgeirsson
      given: H.
    - family: Heyes
      given: D. M.
  citation-key: sigurgeirsson_transport_2003
  container-title: Molecular Physics
  DOI: 10.1080/0026897021000037717
  issue: '3'
  issued:
    - year: 2003
  note: '00000'
  page: 469–482
  source: Google Scholar
  title: Transport coefficients of hard sphere fluids
  type: article-journal
  URL: http://www.tandfonline.com/doi/abs/10.1080/0026897021000037717
  volume: '101'

- id: silberschatz_operating_2014
  author:
    - family: Silberschatz
      given: Abraham
    - family: Galvin
      given: Peter Baer
    - family: Gagne
      given: Greg
  citation-key: silberschatz_operating_2014
  issued:
    - year: 2014
  note: '00000'
  publisher: John Wiley & Sons, Inc.
  source: Google Scholar
  title: Operating system concepts essentials
  type: book

- id: silverstein_empirical_1995
  abstract: >-
    A stronger result on the limiting distribution of the eigenvalues of random
    Hermitian matrices of the form A+XT X∗, originally studied in Marˇcenko and
    Pastur [4], is presented. Here, X (N ×n), T (n×n), and A (N ×N ) are
    independent, with X containing i.i.d. entries having ﬁnite second moments, T
    is diagonal with real (diagonal) entries, A is Hermitian, and n/N → c > 0 as
    N → ∞. Under addtional assumptions on the eigenvalues of A and T , almost
    sure convergence of the empirical distribution function of the eigenvalues
    of A + XT X∗ is proven with the aid of Stieltjes transforms, taking a more
    direct approach than previous methods.
  accessed:
    - year: 2020
      month: 12
      day: 22
  author:
    - family: Silverstein
      given: J.W.
    - family: Bai
      given: Z.D.
  citation-key: silverstein_empirical_1995
  container-title: Journal of Multivariate Analysis
  container-title-short: Journal of Multivariate Analysis
  DOI: 10.1006/jmva.1995.1051
  ISSN: 0047259X
  issue: '2'
  issued:
    - year: 1995
      month: 8
  language: en
  page: 175-192
  source: DOI.org (Crossref)
  title: >-
    On the Empirical Distribution of Eigenvalues of a Class of Large Dimensional
    Random Matrices
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0047259X85710512
  volume: '54'

- id: simard_fixed_1989
  accessed:
    - year: 2019
      month: 10
      day: 14
  author:
    - family: Simard
      given: Patrice Y.
    - family: Ottaway
      given: Mary B.
    - family: Ballard
      given: Dana H.
  citation-key: simard_fixed_1989
  container-title: Advances in Neural Information Processing Systems 1
  editor:
    - family: Touretzky
      given: D. S.
  issued:
    - year: 1989
  page: 149–159
  publisher: Morgan-Kaufmann
  source: Neural Information Processing Systems
  title: Fixed Point Analysis for Recurrent Networks
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/181-fixed-point-analysis-for-recurrent-networks.pdf

- id: simonyan_deep_2013
  abstract: >-
    This paper addresses the visualisation of image classification models,
    learnt using deep Convolutional Networks (ConvNets). We consider two
    visualisation techniques, based on computing the gradient of the class score
    with respect to the input image. The first one generates an image, which
    maximises the class score [Erhan et al., 2009], thus visualising the notion
    of the class, captured by a ConvNet. The second technique computes a class
    saliency map, specific to a given image and class. We show that such maps
    can be employed for weakly supervised object segmentation using
    classification ConvNets. Finally, we establish the connection between the
    gradient-based ConvNet visualisation methods and deconvolutional networks
    [Zeiler et al., 2013].
  accessed:
    - year: 2018
      month: 11
      day: 15
  author:
    - family: Simonyan
      given: Karen
    - family: Vedaldi
      given: Andrea
    - family: Zisserman
      given: Andrew
  citation-key: simonyan_deep_2013
  container-title: arXiv:1312.6034 [cs]
  issued:
    - year: 2013
      month: 12
      day: 20
  source: arXiv.org
  title: >-
    Deep Inside Convolutional Networks: Visualising Image Classification Models
    and Saliency Maps
  title-short: Deep Inside Convolutional Networks
  type: article-journal
  URL: http://arxiv.org/abs/1312.6034

- id: simonyan_very_2014
  abstract: >-
    In this work we investigate the effect of the convolutional network depth on
    its accuracy in the large-scale image recognition setting. Our main
    contribution is a thorough evaluation of networks of increasing depth using
    an architecture with very small (3x3) convolution filters, which shows that
    a significant improvement on the prior-art configurations can be achieved by
    pushing the depth to 16-19 weight layers. These findings were the basis of
    our ImageNet Challenge 2014 submission, where our team secured the first and
    the second places in the localisation and classification tracks
    respectively. We also show that our representations generalise well to other
    datasets, where they achieve state-of-the-art results. We have made our two
    best-performing ConvNet models publicly available to facilitate further
    research on the use of deep visual representations in computer vision.
  author:
    - family: Simonyan
      given: Karen
    - family: Zisserman
      given: Andrew
  citation-key: simonyan_very_2014
  container-title: arXiv:1409.1556 [cs]
  issued:
    - year: 2014
      month: 9
      day: 4
  note: '00000'
  source: arXiv.org
  title: Very Deep Convolutional Networks for Large-Scale Image Recognition
  type: article-journal
  URL: http://arxiv.org/abs/1409.1556

- id: sinai_notion_1959
  author:
    - family: Sinai
      given: Yaha G
  citation-key: sinai_notion_1959
  event-title: Dokl. Akad. Nauk. SSSR
  issued:
    - year: 1959
  page: '768'
  title: On the notion of entropy of a dynamical system
  type: paper-conference
  volume: '124'

- id: singh_identification_2013
  author:
    - family: Singh
      given: Manu
    - family: Singh
      given: Isha
    - family: Verma
      given: A
  citation-key: singh_identification_2013
  container-title: MIT Int. J. Electr. Instrumen. Eng
  issue: '1'
  issued:
    - year: 2013
  note: '00000'
  page: 21–23
  title: Identification on non linear series-parallel model using neural network
  type: article-journal
  volume: '3'

- id: singya_mitigating_2017
  abstract: >-
    Efficient utilization of limited bandwidth with high-data-rate transmission
    while serving a large number of users is a prime requirement for present and
    future wireless communication systems. To meet this rising demand,
    orthogonal frequency-division multiplexing (OFDM) and cooperative
    communication have emerged as promising solutions due to their robustness in
    severely degraded channel conditions, link reliability, and spectral
    efficiency. Both OFDM and cooperative systems have challenged RF front-end
    specifications such as bandwidth and power-efficiency requirements for end
    users as well as for the base station due to the high peak-to-average power
    ratio (PAPR). In present and future communication systems, the power
    amplifier (PA) is a key component at the transmitter. To obtain maximum
    power efficiency, the PA is operated near its saturation point, which leads
    to nonlinear distortion (NLD), which is further exaggerated due to the high
    PAPR of the input signal. This NLD is expected to increase in future
    fifth-generation (5G) communication networks, due to the use of large
    bandwidth at millimeter-wave (mmW) frequencies.
  author:
    - family: Singya
      given: Praveen Kumar
    - family: Kumar
      given: Nagendra
    - family: Bhatia
      given: Vimal
  citation-key: singya_mitigating_2017
  container-title: IEEE Microwave Magazine
  DOI: 10.1109/MMM.2017.2691423
  ISSN: 1557-9581
  issue: '5'
  issued:
    - year: 2017
      month: 7
  page: 73-90
  source: IEEE Xplore
  title: >-
    Mitigating NLD for Wireless Networks: Effect of Nonlinear Power Amplifiers
    on Future Wireless Communication Networks
  title-short: Mitigating NLD for Wireless Networks
  type: article-journal
  volume: '18'

- id: sinha_efficient_2014
  author:
    - family: Sinha
      given: Sudipta N
    - family: Scharstein
      given: Daniel
    - family: Szeliski
      given: Richard
  citation-key: sinha_efficient_2014
  container-title: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on
  issued:
    - year: 2014
  note: '00000'
  page: 1582–1589
  publisher: IEEE
  title: Efficient high-resolution stereo matching using local plane sweeps
  type: paper-conference

- id: sipser_introduction_2006
  author:
    - family: Sipser
      given: Michael
  call-number: QA267 .S56 2006
  citation-key: sipser_introduction_2006
  edition: 2nd ed
  event-place: Boston
  ISBN: 978-0-534-95097-2
  issued:
    - year: 2006
  number-of-pages: '431'
  publisher: Thomson Course Technology
  publisher-place: Boston
  source: Library of Congress ISBN
  title: Introduction to the theory of computation
  type: book

- id: sjoberg_nonlinear_1995
  author:
    - family: Sjöberg
      given: Jonas
    - family: Zhang
      given: Qinghua
    - family: Ljung
      given: Lennart
    - family: Benveniste
      given: Albert
    - family: Delyon
      given: Bernard
    - family: Glorennec
      given: Pierre-Yves
    - family: Hjalmarsson
      given: H\a
      dropping-particle: akan
    - family: Juditsky
      given: Anatoli
  citation-key: sjoberg_nonlinear_1995
  container-title: Automatica
  DOI: 10.1016/0005-1098(95)00120-8
  issue: '12'
  issued:
    - year: 1995
  note: '00000'
  page: 1691–1724
  title: 'Nonlinear black-box modeling in system identification: a unified overview'
  type: article-journal
  volume: '31'

- id: slotine_applied_1991
  author:
    - family: Slotine
      given: J.-J. E.
    - family: Li
      given: Weiping
  call-number: QA402.35 .S56 1990
  citation-key: slotine_applied_1991
  event-place: Englewood Cliffs, N.J
  ISBN: 978-0-13-040890-7
  issued:
    - year: 1991
  language: en
  note: '19267'
  number-of-pages: '459'
  publisher: Prentice Hall
  publisher-place: Englewood Cliffs, N.J
  source: Library of Congress ISBN
  title: Applied nonlinear control
  type: book

- id: smith_deep_2019
  abstract: >-
    Background

    Cardiologs® has developed the first electrocardiogram (ECG) algorithm that
    uses a deep neural network (DNN) for full 12‑lead ECG analysis, including
    rhythm, QRS and ST-T-U waves. We compared the accuracy of the first version
    of Cardiologs® DNN algorithm to the Mortara/Veritas® conventional algorithm
    in emergency department (ED) ECGs.

    Methods

    Individual ECG diagnoses were prospectively mapped to one of 16
    pre-specified groups of ECG diagnoses, which were further classified as
    “major” ECG abnormality or not. Automated interpretations were compared to
    blinded experts'. The primary outcome was the performance of the algorithms
    in finding at least one “major” abnormality. The secondary outcome was the
    proportion of all ECGs for which all groups were identified, with no false
    negative or false positive groups (“accurate ECG interpretation”).
    Additionally, we measured sensitivity and positive predictive value (PPV)
    for any abnormal group.

    Results

    Cardiologs® vs. Veritas® accuracy for finding a major abnormality was 92.2%
    vs. 87.2% (p < 0.0001), with comparable sensitivity (88.7% vs. 92.0%,
    p = 0.086), improved specificity (94.0% vs. 84.7%, p < 0.0001) and improved
    positive predictive value (PPV 88.2% vs. 75.4%, p < 0.0001). Cardiologs® had
    accurate ECG interpretation for 72.0% (95% CI: 69.6–74.2) of ECGs vs. 59.8%
    (57.3–62.3) for Veritas® (P < 0.0001). Sensitivity for any abnormal group
    for Cardiologs® and Veritas®, respectively, was 69.6% (95CI 66.7–72.3) vs.
    68.3% (95CI 65.3–71.1) (NS). Positive Predictive Value was 74.0% (71.1–76.7)
    for Cardiologs® vs. 56.5% (53.7–59.3) for Veritas® (P < 0.0001).

    Conclusion

    Cardiologs' DNN was more accurate and specific in identifying ECGs with at
    least one major abnormal group. It had a significantly higher rate of
    accurate ECG interpretation, with similar sensitivity and higher PPV.
  accessed:
    - year: 2019
      month: 5
      day: 29
  author:
    - family: Smith
      given: Stephen W.
    - family: Walsh
      given: Brooks
    - family: Grauer
      given: Ken
    - family: Wang
      given: Kyuhyun
    - family: Rapin
      given: Jeremy
    - family: Li
      given: Jia
    - family: Fennell
      given: William
    - family: Taboulet
      given: Pierre
  citation-key: smith_deep_2019
  container-title: Journal of Electrocardiology
  container-title-short: Journal of Electrocardiology
  DOI: 10/gf286g
  ISSN: 0022-0736
  issued:
    - year: 2019
      month: 1
      day: 1
  page: 88-95
  source: ScienceDirect
  title: >-
    A deep neural network learning algorithm outperforms a conventional
    algorithm for emergency department electrocardiogram interpretation
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0022073618302292
  volume: '52'

- id: smith_disciplined_2018
  abstract: >-
    Although deep learning has produced dazzling successes for applications of
    image, speech, and video processing in the past few years, most trainings
    are with suboptimal hyper-parameters, requiring unnecessarily long training
    times. Setting the hyper-parameters remains a black art that requires years
    of experience to acquire. This report proposes several efficient ways to set
    the hyper-parameters that significantly reduce training time and improves
    performance. Specifically, this report shows how to examine the training
    validation/test loss function for subtle clues of underfitting and
    overfitting and suggests guidelines for moving toward the optimal balance
    point. Then it discusses how to increase/decrease the learning rate/momentum
    to speed up training. Our experiments show that it is crucial to balance
    every manner of regularization for each dataset and architecture. Weight
    decay is used as a sample regularizer to show how its optimal value is
    tightly coupled with the learning rates and momentums.
  author:
    - family: Smith
      given: Leslie N.
  citation-key: smith_disciplined_2018
  container-title: arXiv:1803.09820 [cs, stat]
  issued:
    - year: 2018
      month: 3
      day: 26
  note: '00000'
  source: arXiv.org
  title: >-
    A disciplined approach to neural network hyper-parameters: Part 1 --
    learning rate, batch size, momentum, and weight decay
  title-short: A disciplined approach to neural network hyper-parameters
  type: article-journal
  URL: http://arxiv.org/abs/1803.09820

- id: smola_tutorial_2004
  author:
    - family: Smola
      given: Alex J
    - family: Schölkopf
      given: Bernhard
  citation-key: smola_tutorial_2004
  container-title: Statistics and computing
  container-title-short: Statistics and computing
  DOI: 10.1023/B:STCO.0000035301.49549.88
  ISSN: 0960-3174
  issue: '3'
  issued:
    - year: 2004
  note: '00000'
  page: 199-222
  title: A tutorial on support vector regression
  type: article-journal
  volume: '14'

- id: soderstrom_comparison_1981
  author:
    - family: Söderström
      given: T
    - family: Stoica
      given: P
  citation-key: soderstrom_comparison_1981
  container-title: Automatica
  DOI: 10.1016/0005-1098(81)90087-X
  issue: '1'
  issued:
    - year: 1981
  note: '00000'
  page: 101–115
  title: >-
    Comparison of some instrumental variable methods consistency and accuracy
    aspects
  type: article-journal
  volume: '17'

- id: soderstrom_errorsinvariables_2007
  accessed:
    - year: 2017
      month: 8
      day: 23
  author:
    - family: Söderström
      given: Torsten
  citation-key: soderstrom_errorsinvariables_2007
  container-title: Automatica
  DOI: 10.1016/j.automatica.2006.11.025
  ISSN: '00051098'
  issue: '6'
  issued:
    - year: 2007
  language: en
  page: 939-958
  source: CrossRef
  title: Errors-in-variables methods in system identification
  type: article-journal
  URL: http://linkinghub.elsevier.com/retrieve/pii/S0005109807000714
  volume: '43'

- id: soderstrom_system_1988
  author:
    - family: Söderström
      given: Torsten
    - family: Stoica
      given: Petre
  citation-key: soderstrom_system_1988
  issued:
    - year: 1988
  publisher: Prentice-Hall, Inc.
  title: System identification
  type: book

- id: sontag_mathematical_2013
  author:
    - family: Sontag
      given: Eduardo D.
  citation-key: sontag_mathematical_2013
  issued:
    - year: 2013
  note: '00000'
  publisher: Springer Science & Business Media
  source: Google Scholar
  title: 'Mathematical control theory: deterministic finite dimensional systems'
  title-short: Mathematical control theory
  type: book
  volume: '6'

- id: sontag_vc_
  abstract: >-
    This paper presents a brief introduction to Vapnik-Chervonenkis (VC)
    dimension, a quantity which characterizes the diﬃculty of
    distribution-independent learning. The paper establishes various elementary
    results, and discusses how to estimate the VC dimension in several examples
    of interest in neural network theory.
  author:
    - family: Sontag
      given: Eduardo D
  citation-key: sontag_vc_
  language: en
  page: '26'
  source: Zotero
  title: VC Dimension of Neural Networks
  type: article-journal

- id: soudry_implicit_2017
  accessed:
    - year: 2018
      month: 12
      day: 13
  author:
    - family: Soudry
      given: Daniel
    - family: Hoffer
      given: Elad
    - family: Nacson
      given: Mor Shpigel
    - family: Gunasekar
      given: Suriya
    - family: Srebro
      given: Nathan
  citation-key: soudry_implicit_2017
  issued:
    - year: 2017
      month: 10
      day: 27
  language: en
  source: arxiv.org
  title: The Implicit Bias of Gradient Descent on Separable Data
  type: article-journal
  URL: https://arxiv.org/abs/1710.10345

- id: spivak_calculus_1998
  author:
    - family: Spivak
      given: Michael
  citation-key: spivak_calculus_1998
  collection-title: Mathematics monograph series
  event-place: Cambridge, Mass
  ISBN: 978-0-8053-9021-6
  issued:
    - year: 1998
  language: en
  number-of-pages: '146'
  publisher: Perseus Books
  publisher-place: Cambridge, Mass
  source: Gemeinsamer Bibliotheksverbund ISBN
  title: >-
    Calculus on manifolds: a modern approach to classical theorems of advanced
    calculus
  title-short: Calculus on manifolds
  type: book

- id: srivastava_dropout_2014
  author:
    - family: Srivastava
      given: Nitish
    - family: Hinton
      given: Geoffrey E.
    - family: Krizhevsky
      given: Alex
    - family: Sutskever
      given: Ilya
    - family: Salakhutdinov
      given: Ruslan
  citation-key: srivastava_dropout_2014
  container-title: Journal of Machine Learning Research
  issue: '1'
  issued:
    - year: 2014
  page: 1929–1958
  source: Google Scholar
  title: 'Dropout: a simple way to prevent neural networks from overfitting.'
  title-short: Dropout
  type: article-journal
  volume: '15'

- id: stead_clinical_2018
  abstract: >-
    Artificial intelligence (AI) and deep learning are entering the mainstream
    of clinical medicine. For example, in December 2016, Gulshan et al1 reported
    development and validation of a deep learning algorithm for detection of
    diabetic retinopathy in retinal fundus photographs. An accompanying
    editorial by Wong and Bressler2 pointed out limits of the study, the need
    for further validation of the algorithm in different populations, and
    unresolved challenges (eg, incorporating the algorithm into clinical work
    flows and convincing clinicians and patients to “trust a ‘black box’”).
    Sixteen months later, the Food and Drug Administration (FDA)3 permitted
    marketing of the first medical device to use AI to detect diabetic
    retinopathy. FDA reduced the risk of releasing the device by limiting the
    indication for use to screening adults who do not have visual symptoms for
    greater than mild retinopathy, to refer them to an eye care specialist.
  author:
    - family: Stead
      given: William W.
  citation-key: stead_clinical_2018
  container-title: JAMA
  container-title-short: JAMA
  DOI: 10/gfkhr8
  ISSN: 0098-7484
  issue: '11'
  issued:
    - year: 2018
      month: 9
      day: 18
  page: 1107-1108
  title: >-
    Clinical implications and challenges of artificial intelligence and deep
    learning
  type: article-journal
  URL: http://dx.doi.org/10.1001/jama.2018.11029
  volume: '320'

- id: steihaug_conjugate_1983
  author:
    - family: Steihaug
      given: Trond
  citation-key: steihaug_conjugate_1983
  container-title: SIAM Journal on Numerical Analysis
  DOI: 10.1137/0720042
  issue: '3'
  issued:
    - year: 1983
  note: '00000'
  page: 626–637
  title: The conjugate gradient method and trust regions in large scale optimization
  type: article-journal
  volume: '20'

- id: stein_real_2005
  author:
    - family: Stein
      given: Elias M.
    - family: Shakarchi
      given: Rami
  call-number: QA320 .S84 2005
  citation-key: stein_real_2005
  collection-number: v. 3
  collection-title: Princeton lectures in analysis
  event-place: Princeton, N.J
  ISBN: 978-0-691-11386-9
  issued:
    - year: 2005
  number-of-pages: '402'
  publisher: Princeton University Press
  publisher-place: Princeton, N.J
  source: Library of Congress ISBN
  title: 'Real analysis: measure theory, integration, and Hilbert spaces'
  title-short: Real analysis
  type: book

- id: stelzer_stereovisionbased_2012
  author:
    - family: Stelzer
      given: Annett
    - family: Hirschmüller
      given: Heiko
    - family: Görner
      given: Martin
  citation-key: stelzer_stereovisionbased_2012
  container-title: The International Journal of Robotics Research
  DOI: 10.1177/0278364911435161
  issue: '4'
  issued:
    - year: 2012
  note: '00000'
  page: 381–402
  title: >-
    Stereo-vision-based navigation of a six-legged walking robot in unknown
    rough terrain
  type: article-journal
  volume: '31'

- id: stensrud_why_2020
  author:
    - family: Stensrud
      given: Mats J.
    - family: Hernán
      given: Miguel A.
  citation-key: stensrud_why_2020
  container-title: JAMA
  container-title-short: JAMA
  DOI: 10.1001/jama.2020.1267
  ISSN: 1538-3598
  issue: '14'
  issued:
    - year: 2020
      month: 4
      day: 14
  language: eng
  page: 1401-1402
  PMID: '32167523'
  source: PubMed
  title: Why Test for Proportional Hazards?
  type: article-journal
  volume: '323'

- id: stentoumis_accurate_2014
  author:
    - family: Stentoumis
      given: C
    - family: Grammatikopoulos
      given: L
    - family: Kalisperakis
      given: I
    - family: Karras
      given: G
  citation-key: stentoumis_accurate_2014
  container-title: ISPRS Journal of Photogrammetry and Remote Sensing
  DOI: 10.1016/j.isprsjprs.2014.02.006
  issued:
    - year: 2014
  note: '00000'
  page: 29–49
  title: On accurate dense stereo-matching using a local adaptive multi-cost approach
  type: article-journal
  volume: '91'

- id: stevens_advanced_2013
  author:
    - family: Stevens
      given: W.R.
    - family: Rago
      given: S.A.
  citation-key: stevens_advanced_2013
  collection-title: Addison-Wesley Professional Computing Series
  ISBN: 978-0-321-63800-7
  issued:
    - year: 2013
  note: '00000'
  publisher: Pearson Education
  title: Advanced Programming in the UNIX Environment
  type: book
  URL: https://books.google.com.br/books?id=kCTMFpEcIOwC

- id: stewart_calculus_2015
  author:
    - family: Stewart
      given: James
  citation-key: stewart_calculus_2015
  issued:
    - year: 2015
  note: '00000'
  publisher: Cengage Learning
  title: 'Calculus: early transcendentals'
  type: book

- id: stoer_introduction_1980
  author:
    - family: Stoer
      given: J
    - family: Bulirsch
      given: R
  citation-key: stoer_introduction_1980
  issued:
    - year: 1980
  note: '00000'
  publisher: Springer-Verlag, New York
  title: Introduction to Numerical Analysis
  type: book

- id: stoer_introduction_1980a
  author:
    - family: Stoer
      given: J
    - family: Bulirsch
      given: R
  citation-key: stoer_introduction_1980a
  issued:
    - year: 1980
  publisher: Springer-Verlag, New York
  title: Introduction to Numerical Analysis
  type: book

- id: storn_differential_1997
  author:
    - family: Storn
      given: Rainer
    - family: Price
      given: Kenneth
  citation-key: storn_differential_1997
  container-title: Journal of global optimization
  DOI: 10.1023/A:1008202821328
  issue: '4'
  issued:
    - year: 1997
  note: '00000'
  page: 341–359
  title: >-
    Differential evolution–a simple and efficient heuristic for global
    optimization over continuous spaces
  type: article-journal
  volume: '11'

- id: strang_introduction_2009
  author:
    - family: Strang
      given: G.
  citation-key: strang_introduction_2009
  ISBN: 978-0-9802327-1-4
  issued:
    - year: 2009
  note: '00000'
  publisher: Wellesley-Cambridge Press
  title: Introduction to Linear Algebra
  type: book
  URL: https://books.google.com.br/books?id=M19gPgAACAAJ

- id: strang_linear_2006
  author:
    - family: Strang
      given: G.
  citation-key: strang_linear_2006
  ISBN: 978-0-03-010567-8
  issued:
    - year: 2006
  note: '00000'
  publisher: Thomson, Brooks/Cole
  title: Linear Algebra and Its Applications
  type: book
  URL: https://books.google.com.br/books?id=8QVdcRJyL2oC

- id: strang_wavelets_1996
  author:
    - family: Strang
      given: Gilbert
    - family: Nguyen
      given: Truong
  citation-key: strang_wavelets_1996
  issued:
    - year: 1996
  note: '00000'
  publisher: SIAM
  source: Google Scholar
  title: Wavelets and filter banks
  type: book

- id: street_nuclear_1993
  abstract: >-
    Interactive image processing techniques, along with a
    linear-programming-based inductive classifier, have been used to create a
    highly accurate system for diagnosis of breast tumors. A small fraction of a
    fine needle aspirate slide is selected and digitized. With an interactive
    interface, the user initializes active contour models, known as snakes, near
    the boundaries of a set of cell nuclei. The customized snakes are deformed
    to the exact shape of the nuclei. This allows for precise, automated
    analysis of nuclear size, shape and texture. Ten such features are computed
    for each nucleus, and the mean value, largest (or 'worst') value and
    standard error of each feature are found over the range of isolated cells.
    After 569 images were analyzed in this fashion, different combinations of
    features were tested to find those which best separate benign from malignant
    samples. Ten-fold cross-validation accuracy of 97% was achieved using a
    single separating plane on three of the thirty features: mean texture, worst
    area and worst smoothness. This represents an improvement over the best
    diagnostic results in the medical literature. The system is currently in use
    at the University of Wisconsin Hospitals. The same feature set has also been
    utilized in the much more difficult task of predicting distant recurrence of
    malignancy in patients, resulting in an accuracy of 86%.
  accessed:
    - year: 2024
      month: 5
      day: 16
  author:
    - family: Street
      given: W. Nick
    - family: Wolberg
      given: W. H.
    - family: Mangasarian
      given: O. L.
  citation-key: street_nuclear_1993
  container-title: Biomedical Image Processing and Biomedical Visualization
  DOI: 10.1117/12.148698
  event-title: Biomedical Image Processing and Biomedical Visualization
  issued:
    - year: 1993
      month: 7
      day: 29
  page: 861-870
  publisher: SPIE
  source: www.spiedigitallibrary.org
  title: Nuclear feature extraction for breast tumor diagnosis
  type: paper-conference
  URL: >-
    https://www.spiedigitallibrary.org/conference-proceedings-of-spie/1905/0000/Nuclear-feature-extraction-for-breast-tumor-diagnosis/10.1117/12.148698.full
  volume: '1905'

- id: strodthoff_deep_2020
  abstract: >-
    Electrocardiography is a very common, non-invasive diagnostic procedure and
    its interpretation is increasingly supported by automatic interpretation
    algorithms. The progress in the field of automatic ECG interpretation has up
    to now been hampered by a lack of appropriate datasets for training as well
    as a lack of well-defined evaluation procedures to ensure comparability of
    different algorithms. To alleviate these issues, we put forward first
    benchmarking results for the recently published, freely accessible PTB-XL
    dataset, covering a variety of tasks from different ECG statement prediction
    tasks over age and gender prediction to signal quality assessment. We find
    that convolutional neural networks, in particular resnet- and
    inception-based architectures, show the strongest performance across all
    tasks outperforming feature-based algorithms by a large margin. These
    results are complemented by deeper insights into the classification
    algorithm in terms of hidden stratification, model uncertainty and an
    exploratory interpretability analysis. We also put forward benchmarking
    results for the ICBEB2018 challenge ECG dataset and discuss prospects of
    transfer learning using classifiers pretrained on PTB-XL. With this
    resource, we aim to establish the PTB-XL dataset as a resource for
    structured benchmarking of ECG analysis algorithms and encourage other
    researchers in the field to join these efforts.
  accessed:
    - year: 2020
      month: 7
      day: 14
  author:
    - family: Strodthoff
      given: Nils
    - family: Wagner
      given: Patrick
    - family: Schaeffter
      given: Tobias
    - family: Samek
      given: Wojciech
  citation-key: strodthoff_deep_2020
  container-title: arXiv:2004.13701 [cs, stat]
  issued:
    - year: 2020
      month: 4
      day: 28
  source: arXiv.org
  title: 'Deep Learning for ECG Analysis: Benchmarks and Insights from PTB-XL'
  title-short: Deep Learning for ECG Analysis
  type: article-journal
  URL: http://arxiv.org/abs/2004.13701

- id: strogatz_nonlinear_2018
  author:
    - family: Strogatz
      given: Steven H
  citation-key: strogatz_nonlinear_2018
  ISBN: 0-429-96111-1
  issued:
    - year: 2018
  note: '00000'
  publisher: CRC Press
  title: >-
    Nonlinear dynamics and chaos: with applications to physics, biology,
    chemistry, and engineering
  type: book

- id: stroock_concise_1994
  author:
    - family: Stroock
      given: Daniel W.
  call-number: QA312 .S78 1994
  citation-key: stroock_concise_1994
  edition: 2nd ed
  event-place: Boston
  ISBN: 978-0-8176-3759-0 978-3-7643-3759-9
  issued:
    - year: 1994
  number-of-pages: '184'
  publisher: Birkhäuser
  publisher-place: Boston
  source: Library of Congress ISBN
  title: A concise introduction to the theory of integration
  type: book

- id: su_longterm_1992
  author:
    - family: Su
      given: Hong Te
    - family: McAvoy
      given: Thomas J
    - family: Werbos
      given: Paul
  citation-key: su_longterm_1992
  container-title: Industrial & Engineering Chemistry Research
  DOI: 10.1021/ie00005a014
  issue: '5'
  issued:
    - year: 1992
  page: 1338–1352
  title: >-
    Long-term predictions of chemical processes using recurrent neural networks:
    A parallel training approach
  type: article-journal
  volume: '31'

- id: su_longterm_1992a
  author:
    - family: Su
      given: Hong Te
    - family: McAvoy
      given: Thomas J
    - family: Werbos
      given: Paul
  citation-key: su_longterm_1992a
  container-title: Industrial & Engineering Chemistry Research
  DOI: 10/cc9djd
  issue: '5'
  issued:
    - year: 1992
  page: 1338-1352
  title: >-
    Long-Term Predictions of Chemical Processes Using Recurrent Neural Networks:
    A Parallel Training Approach
  type: article-journal
  volume: '31'

- id: su_neural_1993
  author:
    - family: Su
      given: H-T
    - family: McAvoy
      given: Thomas J
  citation-key: su_neural_1993
  container-title: >-
    Intelligent Control, 1993., Proceedings of the 1993 IEEE International
    Symposium on
  issued:
    - year: 1993
  note: '00000'
  page: 358–363
  publisher: IEEE
  title: Neural model predictive control of nonlinear chemical processes
  type: paper-conference

- id: su_neural_1993a
  author:
    - family: Su
      given: H-T
    - family: McAvoy
      given: Thomas J
  citation-key: su_neural_1993a
  container-title: >-
    Intelligent Control, 1993., Proceedings of the 1993 IEEE International
    Symposium On
  issued:
    - year: 1993
  page: 358-363
  publisher: IEEE
  title: Neural Model Predictive Control of Nonlinear Chemical Processes
  type: paper-conference

- id: surawicz_aha_2009
  author:
    - family: Surawicz
      given: Borys
    - family: Childers
      given: Rory
    - family: Deal
      given: Barbara J.
    - family: Gettes
      given: Leonard S.
  citation-key: surawicz_aha_2009
  container-title: Journal of the American College of Cardiology
  DOI: 10/bmv8kz
  issue: '11'
  issued:
    - year: 2009
      month: 3
      day: 17
  page: '976'
  title: >-
    AHA/ACCF/HRS Recommendations for the Standardization and Interpretation of
    the Electrocardiogram
  type: article-journal
  URL: http://www.onlinejacc.org/content/53/11/976.abstract
  volume: '53'

- id: sussillo_opening_2013
  accessed:
    - year: 2019
      month: 3
      day: 8
  author:
    - family: Sussillo
      given: David
    - family: Barak
      given: Omri
  citation-key: sussillo_opening_2013
  container-title: Neural Computation
  DOI: 10/f4kmg4
  ISSN: 0899-7667, 1530-888X
  issue: '3'
  issued:
    - year: 2013
      month: 3
  language: en
  page: 626-649
  source: Crossref
  title: >-
    Opening the Black Box: Low-Dimensional Dynamics in High-Dimensional
    Recurrent Neural Networks
  title-short: Opening the Black Box
  type: article-journal
  URL: http://www.mitpressjournals.org/doi/10.1162/NECO_a_00409
  volume: '25'

- id: sutarya_identification_2014
  author:
    - family: Sutarya
      given: Dede
    - family: Kusumoputro
      given: Benyamin
  citation-key: sutarya_identification_2014
  container-title: Science and Technology of Nuclear Installations
  DOI: 10.1155/2014/854569
  issued:
    - year: 2014
  note: '00000'
  title: >-
    Identification of industrial furnace temperature for sintering process in
    nuclear fuel fabrication using NARX neural networks
  type: article-journal
  volume: '2014'

- id: sutskever_sequence_2014
  author:
    - family: Sutskever
      given: Ilya
    - family: Vinyals
      given: Oriol
    - family: Le
      given: Quoc V.
  citation-key: sutskever_sequence_2014
  container-title: Advances in neural information processing systems
  issued:
    - year: 2014
  note: '00000'
  page: 3104–3112
  source: Google Scholar
  title: Sequence to sequence learning with neural networks
  type: paper-conference

- id: sutton_reinforcement_2011
  author:
    - family: Sutton
      given: Richard S
    - family: Barto
      given: Andrew G
  citation-key: sutton_reinforcement_2011
  issued:
    - year: 2011
  title: 'Reinforcement learning: An introduction'
  type: article-journal

- id: suykens_artificial_2012
  author:
    - family: Suykens
      given: Johan AK
    - family: Vandewalle
      given: Joos PL
    - family: Moor
      given: Bart L
      non-dropping-particle: de
  citation-key: suykens_artificial_2012
  issued:
    - year: 2012
  note: '00000'
  publisher: Springer Science & Business Media
  title: Artificial neural networks for modelling and control of non-linear systems
  type: book

- id: suykens_least_2002
  author:
    - family: Suykens
      given: Johan AK
    - family: Van Gestel
      given: Tony
    - family: De Brabanter
      given: Jos
  citation-key: suykens_least_2002
  issued:
    - year: 2002
  note: '00000'
  publisher: World Scientific
  title: Least squares support vector machines
  type: book

- id: svensson_flexible_2017
  accessed:
    - year: 2018
      month: 4
      day: 25
  author:
    - family: Svensson
      given: Andreas
    - family: Schön
      given: Thomas B.
  citation-key: svensson_flexible_2017
  container-title: Automatica
  DOI: 10.1016/j.automatica.2017.02.030
  ISSN: '00051098'
  issued:
    - year: 2017
      month: 6
  language: en
  note: '00000'
  page: 189-199
  source: Crossref
  title: A flexible state–space model for learning nonlinear dynamical systems
  type: article-journal
  URL: http://linkinghub.elsevier.com/retrieve/pii/S0005109817301048
  volume: '80'

- id: szegedy_going_2015
  author:
    - family: Szegedy
      given: Christian
    - family: Liu
      given: Wei
    - family: Jia
      given: Yangqing
    - family: Sermanet
      given: Pierre
    - family: Reed
      given: Scott
    - family: Anguelov
      given: Dragomir
    - family: Erhan
      given: Dumitru
    - family: Vanhoucke
      given: Vincent
    - family: Rabinovich
      given: Andrew
  citation-key: szegedy_going_2015
  container-title: >-
    Proceedings of the IEEE conference on computer vision and pattern
    recognition
  issued:
    - year: 2015
  note: '00000'
  page: 1–9
  source: Google Scholar
  title: Going deeper with convolutions
  type: paper-conference

- id: szeliski_stereo_1999
  author:
    - family: Szeliski
      given: Richard
    - family: Golland
      given: Polina
  citation-key: szeliski_stereo_1999
  container-title: International Journal of Computer Vision
  issue: '1'
  issued:
    - year: 1999
  note: '00000'
  page: 45–61
  title: Stereo matching with transparency and matting
  type: article-journal
  volume: '32'

- id: szumilas_explaining_2010
  accessed:
    - year: 2023
      month: 8
      day: 30
  author:
    - family: Szumilas
      given: Magdalena
  citation-key: szumilas_explaining_2010
  container-title: Journal of the Canadian Academy of Child and Adolescent Psychiatry
  container-title-short: J Can Acad Child Adolesc Psychiatry
  ISSN: 1719-8429
  issue: '3'
  issued:
    - year: 2010
      month: 8
  page: 227-229
  PMCID: PMC2938757
  PMID: '20842279'
  source: PubMed Central
  title: Explaining Odds Ratios
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2938757/
  volume: '19'

- id: taheri_asymptotic_2022
  abstract: >-
    It has been consistently reported that many machine learning models are
    susceptible to adversarial attacks i.e., small additive adversarial
    perturbations applied to data points can cause misclassification.
    Adversarial training using empirical risk minimization is considered to be
    the state-of-the-art method for defense against adversarial attacks. Despite
    being successful in practice, several problems in understanding
    generalization performance of adversarial training remain open. In this
    paper, we derive precise theoretical predictions for the performance of
    adversarial training in binary classification. We consider the
    high-dimensional regime where the dimension of data grows with the size of
    the training data-set at a constant ratio. Our results provide exact
    asymptotics for standard and adversarial errors of the estimators obtained
    by adversarial training with $\ell_q$-norm bounded perturbations ($q \ge 1$)
    for both discriminative binary models and generative Gaussian mixture
    models. Furthermore, we use these sharp predictions to uncover several
    intriguing observations on the role of various parameters including the
    over-parameterization ratio, the data model, and the attack budget on the
    adversarial and standard errors.
  accessed:
    - year: 2021
      month: 5
      day: 16
  author:
    - family: Taheri
      given: Hossein
    - family: Pedarsani
      given: Ramtin
    - family: Thrampoulidis
      given: Christos
  citation-key: taheri_asymptotic_2022
  container-title: IEEE International Symposium on Information Theory (ISIT)
  DOI: 10.1109/ISIT50566.2022.9834717
  issued:
    - year: 2022
  source: arXiv.org
  title: Asymptotic Behavior of Adversarial Training in Binary Classification
  type: article-journal
  URL: http://arxiv.org/abs/2010.13275
  volume: 127-132

- id: taigman_deepface_2014
  accessed:
    - year: 2018
      month: 1
      day: 26
  author:
    - family: Taigman
      given: Yaniv
    - family: Yang
      given: Ming
    - family: Ranzato
      given: Marc'Aurelio
    - family: Wolf
      given: Lior
  citation-key: taigman_deepface_2014
  event-title: >-
    Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition
  issued:
    - year: 2014
  note: '00000'
  page: 1701-1708
  source: www.cv-foundation.org
  title: 'DeepFace: Closing the Gap to Human-Level Performance in Face Verification'
  title-short: DeepFace
  type: paper-conference
  URL: >-
    https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Taigman_DeepFace_Closing_the_2014_CVPR_paper.html

- id: tan_efficientnet_2019
  abstract: >-
    Convolutional Neural Networks (ConvNets) are commonly developed at a fixed
    resource budget, and then scaled up for better accuracy if more resources
    are available. In this paper, we systematically study model scaling and
    identify that carefully balancing network depth, width, and resolution can
    lead to better performance. Based on this observation, we propose a new
    scaling method that uniformly scales all dimensions of
    depth/width/resolution using a simple yet highly effective compound
    coefficient. We demonstrate the effectiveness of this method on scaling up
    MobileNets and ResNet. To go even further, we use neural architecture search
    to design a new baseline network and scale it up to obtain a family of
    models, called EfficientNets, which achieve much better accuracy and
    efficiency than previous ConvNets. In particular, our EfficientNet-B7
    achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet,
    while being 8.4x smaller and 6.1x faster on inference than the best existing
    ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art
    accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer
    learning datasets, with an order of magnitude fewer parameters. Source code
    is at
    https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.
  author:
    - family: Tan
      given: Mingxing
    - family: Le
      given: Quoc V.
  citation-key: tan_efficientnet_2019
  collection-title: PMLR
  container-title: Proceedings of the 36th International Conference on Machine Learning,
  issued:
    - year: 2019
  source: arXiv.org
  title: 'EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks'
  type: article-journal
  URL: http://arxiv.org/abs/1905.11946
  volume: '97'

- id: tan_neuralnetworksbased_2000
  author:
    - family: Tan
      given: Yonghong
    - family: Saif
      given: Mehrdad
  citation-key: tan_neuralnetworksbased_2000
  container-title: Neurocomputing
  issue: '1'
  issued:
    - year: 2000
  note: '00000'
  page: 129–142
  title: Neural-networks-based nonlinear dynamic modeling for automotive engines
  type: article-journal
  volume: '30'

- id: tan_simulating_2019
  abstract: >-
    An artificial neural network architecture, parameterization networks, is
    proposed for simulating extrapolated dynamics beyond observed data in
    dynamical systems. Parameterization networks are used to ensure the long
    term integrity of extrapolated dynamics, while careful tuning of model
    hyperparameters against validation errors controls overfitting. A
    parameterization network is demonstrated on the logistic map, where chaos
    and other nonlinear phenomena consistent with the underlying model can be
    extrapolated from non-chaotic training time series with good fidelity. The
    stated results are a lot less fantastical than they appear to be because the
    neural network is only extrapolating between quadratic return maps.
    Nonetheless, the results do suggest that successful extrapolation of
    qualitatively different behaviors requires learning to occur on a level of
    abstraction where the corresponding behaviors are more similar in nature.
  accessed:
    - year: 2021
      month: 3
      day: 30
  author:
    - family: Tan
      given: James P. L.
  citation-key: tan_simulating_2019
  container-title: arXiv:1902.03440 [nlin]
  issued:
    - year: 2019
      month: 2
      day: 9
  source: arXiv.org
  title: Simulating extrapolated dynamics with parameterization networks
  type: article-journal
  URL: http://arxiv.org/abs/1902.03440

- id: tanaka_pruning_2020
  abstract: >-
    Pruning the parameters of deep neural networks has generated intense
    interest due to potential savings in time, memory and energy both during
    training and at test time. Recent works have identified, through an
    expensive sequence of training and pruning cycles, the existence of winning
    lottery tickets or sparse trainable subnetworks at initialization. This
    raises a foundational question: can we identify highly sparse trainable
    subnetworks at initialization, without ever training, or indeed without ever
    looking at the data? We provide an affirmative answer to this question
    through theory driven algorithm design. We first mathematically formulate
    and experimentally verify a conservation law that explains why existing
    gradient-based pruning algorithms at initialization suffer from
    layer-collapse, the premature pruning of an entire layer rendering a network
    untrainable. This theory also elucidates how layer-collapse can be entirely
    avoided, motivating a novel pruning algorithm Iterative Synaptic Flow
    Pruning (SynFlow). This algorithm can be interpreted as preserving the total
    flow of synaptic strengths through the network at initialization subject to
    a sparsity constraint. Notably, this algorithm makes no reference to the
    training data and consistently outperforms existing state-of-the-art pruning
    algorithms at initialization over a range of models (VGG and ResNet),
    datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to
    99.9 percent). Thus our data-agnostic pruning algorithm challenges the
    existing paradigm that data must be used to quantify which synapses are
    important.
  accessed:
    - year: 2020
      month: 6
      day: 15
  author:
    - family: Tanaka
      given: Hidenori
    - family: Kunin
      given: Daniel
    - family: Yamins
      given: Daniel L. K.
    - family: Ganguli
      given: Surya
  citation-key: tanaka_pruning_2020
  container-title: arXiv:2006.05467 [cond-mat, q-bio, stat]
  issued:
    - year: 2020
      month: 6
      day: 9
  source: arXiv.org
  title: >-
    Pruning neural networks without any data by iteratively conserving synaptic
    flow
  type: article-journal
  URL: http://arxiv.org/abs/2006.05467

- id: tanaka_wavecyclegan2_2019
  abstract: >-
    WaveCycleGAN has recently been proposed to bridge the gap between natural
    and synthesized speech waveforms in statistical parametric speech synthesis
    and provides fast inference with a moving average model rather than an
    autoregressive model and high-quality speech synthesis with the adversarial
    training. However, the human ear can still distinguish the processed speech
    waveforms from natural ones. One possible cause of this distinguishability
    is the aliasing observed in the processed speech waveform via
    down/up-sampling modules. To solve the aliasing and provide higher quality
    speech synthesis, we propose WaveCycleGAN2, which 1) uses generators without
    down/up-sampling modules and 2) combines discriminators of the waveform
    domain and acoustic parameter domain. The results show that the proposed
    method 1) alleviates the aliasing well, 2) is useful for both speech
    waveforms generated by analysis-and-synthesis and statistical parametric
    speech synthesis, and 3) achieves a mean opinion score comparable to those
    of natural speech and speech synthesized by WaveNet (open WaveNet) and
    WaveGlow while processing speech samples at a rate of more than 150 kHz on
    an NVIDIA Tesla P100.
  accessed:
    - year: 2020
      month: 3
      day: 23
  author:
    - family: Tanaka
      given: Kou
    - family: Kameoka
      given: Hirokazu
    - family: Kaneko
      given: Takuhiro
    - family: Hojo
      given: Nobukatsu
  citation-key: tanaka_wavecyclegan2_2019
  container-title: arXiv:1904.02892
  issued:
    - year: 2019
      month: 4
      day: 8
  source: arXiv.org
  title: 'WaveCycleGAN2: Time-domain Neural Post-filter for Speech Waveform Generation'
  title-short: WaveCycleGAN2
  type: article-journal
  URL: http://arxiv.org/abs/1904.02892

- id: tanenbaum_computer_2011
  author:
    - family: Tanenbaum
      given: Andrew S.
    - family: Wetherall
      given: D.
  call-number: TK5105.5 .T36 2011
  citation-key: tanenbaum_computer_2011
  edition: 5th ed
  event-place: Boston
  ISBN: 978-0-13-212695-3
  issued:
    - year: 2011
  note: 'OCLC: ocn660087726'
  number-of-pages: '933'
  publisher: Pearson Prentice Hall
  publisher-place: Boston
  source: Library of Congress ISBN
  title: Computer networks
  type: book

- id: tanenbaum_modern_2009
  author:
    - family: Tanenbaum
      given: Andrew S.
  citation-key: tanenbaum_modern_2009
  issued:
    - year: 2009
  note: '00000'
  publisher: Pearson Education, Inc
  source: Google Scholar
  title: Modern operating system
  type: book

- id: tanenbaum_structured_2006
  author:
    - family: Tanenbaum
      given: Andrew S.
  call-number: QA76.6 .T38 2006
  citation-key: tanenbaum_structured_2006
  edition: 5th ed
  event-place: Upper Saddle River, N.J
  ISBN: 978-0-13-148521-1
  issued:
    - year: 2006
  note: 'OCLC: ocm57506907'
  number-of-pages: '777'
  publisher: Pearson Prentice Hall
  publisher-place: Upper Saddle River, N.J
  source: Library of Congress ISBN
  title: Structured computer organization
  type: book

- id: tao_system_2017
  abstract: >-
    The steam generator is one of the most essential equipment of the nuclear
    power device which takes part in heat exchanging. The water level system of
    a steam generator has the peculiarities of apparent nonlinearity, large
    inertia and time-delaying. Furthermore, the water level of a steam generator
    has a direct impact on the quality of the outlet steam and the security of
    the system. Thus, it is significant and necessary to maintain the water
    level within a safety limit. the mathematical model of the steam generator,
    which is difficult to be described with integer order, can be established
    relatively clearly and precisely by using the fractional order calculus.
    With the progress of the modern technology, the merits of using a fractional
    order model become increasingly obvious due to its good performance in
    describing the actual plants and the dynamic process. This paper proposes a
    fractional order system identification method which can identify the model
    parameters concisely. This method is simple to be realized and also has a
    good adaptability to the initial value. The proposed fractional order
    identification scheme provides a new method of the important equipment
    modeling in nuclear power plant. The model that established by this method
    can provide research basis and technical support of the high precision and
    performance index control system.
  author:
    - family: Tao
      given: M.
    - family: Ke
      given: Z.
    - family: Yu
      given: Y.
  citation-key: tao_system_2017
  container-title: 2017 IEEE International Conference on Mechatronics and Automation (ICMA)
  DOI: 10.1109/ICMA.2017.8015911
  event-title: 2017 IEEE International Conference on Mechatronics and Automation (ICMA)
  issued:
    - year: 2017
      month: 8
  note: '00000'
  page: 758-763
  source: IEEE Xplore
  title: >-
    System identification of fractional-order for non-minimum-phase and
    non-self-balancing system
  type: paper-conference

- id: tao_topics_2012
  author:
    - family: Tao
      given: Terence
  citation-key: tao_topics_2012
  collection-title: Graduate Studies in Mathematics
  issued:
    - year: 2012
  language: en
  publisher: American Mathematical Society
  source: Zotero
  title: Topics in random matrix theory
  type: book
  volume: '132'

- id: tassa_controllimited_2014
  author:
    - family: Tassa
      given: Yuval
    - family: Mansard
      given: Nicolas
    - family: Todorov
      given: Emo
  citation-key: tassa_controllimited_2014
  container-title: Robotics and Automation (ICRA), 2014 IEEE International Conference on
  issued:
    - year: 2014
  note: '00000'
  page: 1168–1175
  publisher: IEEE
  source: Google Scholar
  title: Control-limited differential dynamic programming
  type: paper-conference

- id: tecnologia_vocabulario_
  author:
    - family: Tecnologia
      given: Inmetro
    - family: Filipe
      given: Eduarda
    - family: Pellegrino
      given: Olivier
    - family: Baratto
      given: Antonio Carlos
    - family: Oliveira
      given: Sérgio Pinheiro
      non-dropping-particle: de
    - family: Mendoza
      given: Victor Manuel Loayza
  citation-key: tecnologia_vocabulario_
  note: '00000'
  source: Google Scholar
  title: >-
    Vocabulário Internacional de Metrologia–Conceitos fundamentais e gerais e
    termos associados (VIM 2012)
  type: article-journal

- id: teijeiro_arrhythmia_2017
  abstract: >-
    In this work we propose a new method for the rhythm classiﬁcation of short
    single-lead ECG records, using a set of high-level and clinically meaningful
    features provided by the abductive interpretation of the records. These
    features include morphological and rhythm-related features that are used to
    build two classiﬁers: one that evaluates the record globally, using
    aggregated values for each feature; and another one that evaluates the
    record as a sequence, using a Recurrent Neural Network fed with the
    individual features for each detected heartbeat. The two classiﬁers are
    ﬁnally combined using the stacking technique, providing an answer by means
    of four target classes: Normal sinus rhythm (N), Atrial ﬁbrillation (A),
    Other anomaly (O) and Noisy (~). The approach has been validated against the
    2017 Physionet/CinC Challenge dataset, obtaining a ﬁnal score of 0.83 and
    ranking ﬁrst in the competition.
  accessed:
    - year: 2018
      month: 10
      day: 21
  author:
    - family: Teijeiro
      given: Tomas
    - family: Garcia
      given: Constantino A.
    - family: Castro
      given: Daniel
    - family: Félix
      given: Paulo
  citation-key: teijeiro_arrhythmia_2017
  container-title: Computing in Cardiology
  DOI: 10.22489/CinC.2017.166-054
  event-title: Computing in Cardiology, 2017
  issued:
    - year: 2017
      month: 9
      day: 14
  language: en
  source: Crossref
  title: >-
    Arrhythmia Classification from the Abductive Interpretation of Short
    Single-Lead ECG Records
  type: paper-conference
  URL: http://www.cinc.org/archives/2017/pdf/166-054.pdf

- id: teixeira_datadriven_2014
  author:
    - family: Teixeira
      given: Bruno O. S.
    - family: Castro
      given: Walace S
    - family: Teixeira
      given: Alex F
    - family: Aguirre
      given: Luis A
  citation-key: teixeira_datadriven_2014
  container-title: Control Engineering Practice
  DOI: 10/f5nhgb
  issued:
    - year: 2014
  page: 34-43
  title: Data-Driven Soft Sensor of Downhole Pressure for a Gas-Lift Oil Well
  type: article-journal
  volume: '22'

- id: teixeira_datadriven_2014a
  author:
    - family: Teixeira
      given: Bruno O. S.
    - family: Castro
      given: Walace S
    - family: Teixeira
      given: Alex F
    - family: Aguirre
      given: Luis A
  citation-key: teixeira_datadriven_2014a
  container-title: Control Engineering Practice
  DOI: 10/f5nhgb
  issued:
    - year: 2014
  page: 34-43
  title: Data-Driven Soft Sensor of Downhole Pressure for a Gas-Lift Oil Well
  type: article-journal
  volume: '22'

- id: ternes_identification_2017
  abstract: >-
    Stratified medicine seeks to identify biomarkers or parsimonious gene
    signatures distinguishing patients that will benefit most from a targeted
    treatment. We evaluated 12 approaches in high-dimensional Cox models in
    randomized clinical trials: penalization of the biomarker main effects and
    biomarker-by-treatment interactions (full-lasso, three kinds of adaptive
    lasso, ridge+lasso and group-lasso); dimensionality reduction of the main
    effect matrix via linear combinations (PCA+lasso (where PCA is principal
    components analysis) or PLS+lasso (where PLS is partial least squares));
    penalization of modified covariates or of the arm-specific biomarker effects
    (two-I model); gradient boosting; and univariate approach with control of
    multiple testing. We compared these methods via simulations, evaluating
    their selection abilities in null and alternative scenarios. We varied the
    number of biomarkers, of nonnull main effects and true
    biomarker-by-treatment interactions. We also proposed a novel measure
    evaluating the interaction strength of the developed gene signatures. In the
    null scenarios, the group-lasso, two-I model, and gradient boosting
    performed poorly in the presence of nonnull main effects, and performed well
    in alternative scenarios with also high interaction strength. The adaptive
    lasso with grouped weights was too conservative. The modified covariates,
    PCA+lasso, PLS+lasso, and ridge+lasso performed moderately. The full-lasso
    and adaptive lassos performed well, with the exception of the full-lasso in
    the presence of only nonnull main effects. The univariate approach performed
    poorly in alternative scenarios. We also illustrate the methods using gene
    expression data from 614 breast cancer patients treated with adjuvant
    chemotherapy.
  author:
    - family: Ternès
      given: Nils
    - family: Rotolo
      given: Federico
    - family: Heinze
      given: Georg
    - family: Michiels
      given: Stefan
  citation-key: ternes_identification_2017
  container-title: Biometrical Journal
  container-title-short: Biom. J.
  DOI: 10.1002/bimj.201500234
  ISSN: 1521-4036
  issue: '4'
  issued:
    - year: 2017
      month: 7
      day: 1
  language: en
  note: '00000'
  page: 685-701
  source: Wiley Online Library
  title: >-
    Identification of biomarker-by-treatment interactions in randomized clinical
    trials with survival outcomes and high-dimensional spaces
  type: article-journal
  URL: http://onlinelibrary.wiley.com/doi/10.1002/bimj.201500234/abstract
  volume: '59'

- id: terzi_learning_2018
  abstract: >-
    In this paper, the derivation of multi-step-ahead prediction models from
    sampled input-output data of a linear system is considered. Specifically, a
    dedicated prediction model is built for each future time step of interest.
    Each model is linearly parametrized in a suitable regressor vector, composed
    of past output values and past and future input values. In addition to a
    nominal model, the set of all models consistent with data and prior
    information is derived as well, making the approach suitable for robust
    control design within a Model Predictive Control framework. The resulting
    parameter identification problem is solved through a sequence of convex
    programs. Convergence of the identified error bounds to their theoretical
    minimum is demonstrated, under suitable assumptions on the measured data,
    and features like worst-case accuracy computation are illustrated in a
    numerical example.
  author:
    - family: Terzi
      given: E.
    - family: Fagiano
      given: L.
    - family: Farina
      given: M.
    - family: Scattolini
      given: R.
  citation-key: terzi_learning_2018
  container-title: 2018 European Control Conference (ECC)
  DOI: 10/gfxx69
  event-title: 2018 European Control Conference (ECC)
  issued:
    - year: 2018
      month: 6
  page: 1335-1340
  source: IEEE Xplore
  title: Learning multi-step prediction models for receding horizon control
  type: paper-conference

- id: teschl_ordinary_
  author:
    - family: Teschl
      given: Gerald
  citation-key: teschl_ordinary_
  language: en
  page: '364'
  source: Zotero
  title: Ordinary Differential Equations and Dynamical Systems
  type: article-journal

- id: thekumparampil_robustness_2018
  accessed:
    - year: 2018
      month: 12
      day: 4
  author:
    - family: Thekumparampil
      given: Kiran K
    - family: Khetan
      given: Ashish
    - family: Lin
      given: Zinan
    - family: Oh
      given: Sewoong
  citation-key: thekumparampil_robustness_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 10291–10302
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Robustness of conditional GANs to noisy labels
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/8229-robustness-of-conditional-gans-to-noisy-labels.pdf

- id: thomas_maximum_2014
  abstract: >-
    We propose a method, maximum likelihood estimation of generalized eigenvalue
    decomposition (MLGEVD) that employs a well known technique relying on the
    generalization of singular value decomposition (SVD). The main aim of the
    work is to show the tight equivalence between MLGEVD and generalized ridge
    regression. This relationship reveals an important mathematical property of
    GEVD in which the second argument act as prior information in the model.
    Thus we show that MLGEVD allows the incorporation of external knowledge
    about the quantities of interest into the estimation problem. We illustrate
    the importance of prior knowledge in clinical decision making/identifying
    differentially expressed genes with case studies for which microarray data
    sets with corresponding clinical/literature information are available. On
    all of these three case studies, MLGEVD outperformed GEVD on prediction in
    terms of test area under the ROC curve (test AUC). MLGEVD results in
    significantly improved diagnosis, prognosis and prediction of therapy
    response.
  author:
    - family: Thomas
      given: M.
    - family: Daemen
      given: A.
    - family: Moor
      given: B. De
  citation-key: thomas_maximum_2014
  container-title: IEEE/ACM Transactions on Computational Biology and Bioinformatics
  DOI: 10.1109/TCBB.2014.2304292
  ISSN: 1545-5963
  issue: '4'
  issued:
    - year: 2014
      month: 7
  note: '00000'
  page: 673-680
  source: IEEE Xplore
  title: 'Maximum Likelihood Estimation of GEVD: Applications in Bioinformatics'
  title-short: Maximum Likelihood Estimation of GEVD
  type: article-journal
  volume: '11'

- id: thrampoulidis_regularized_2015
  abstract: >-
    Non-smooth regularized convex optimization procedures have emerged as a
    powerful tool to recover structured signals (sparse, low-rank, etc.) from
    (possibly compressed) noisy linear measurements. We focus on the problem of
    linear regression and consider a general class of optimization methods that
    minimize a loss function measuring the misfit of the model to the
    observations with an added structured-inducing regularization term.
    Celebrated instances include the LASSO, Group-LASSO, Least-Absolute
    Deviations method, etc.. We develop a quite general framework for how to
    determine precise prediction performance guaranties (e.g. mean-square-error)
    of such methods for the case of Gaussian measurement ensemble. The 
    machinery builds upon  Gordon’s Gaussian min-max theorem under additional
    convexity assumptions that arise in many practical applications. This
    theorem associates with a primary optimization (PO) problem a simplified
    auxiliary optimization  (AO) problem from which we can tightly infer
    properties of the original (PO), such as the optimal cost, the norm of the
    optimal solution, etc. Our theory applies to general loss functions and
    regularization and provides guidelines on how to optimally tune the
    regularizer coefficient when certain structural properties (such as sparsity
    level, rank, etc.) are known.
  accessed:
    - year: 2022
      month: 8
      day: 4
  author:
    - family: Thrampoulidis
      given: Christos
    - family: Oymak
      given: Samet
    - family: Hassibi
      given: Babak
  citation-key: thrampoulidis_regularized_2015
  container-title: Proceedings of The 28th Conference on Learning Theory
  event-title: Conference on Learning Theory
  ISSN: 1938-7228
  issued:
    - year: 2015
      month: 6
      day: 26
  language: en
  page: 1683-1709
  publisher: PMLR
  source: proceedings.mlr.press
  title: 'Regularized Linear Regression: A Precise Analysis of the Estimation Error'
  title-short: Regularized Linear Regression
  type: paper-conference
  URL: https://proceedings.mlr.press/v40/Thrampoulidis15.html

- id: tibshirani_lasso_2013
  abstract: >-
    The lasso is a popular tool for sparse linear regression, especially for
    problems in which the number of variables p exceeds the number of
    observations n. But when p > n, the lasso criterion is not strictly convex,
    and hence it may not have a unique minimizer. An important question is: when
    is the lasso solution well-deﬁned (unique)? We review results from the
    literature, which show that if the predictor variables are drawn from a
    continuous probability distribution, then there is a unique lasso solution
    with probability one, regardless of the sizes of n and p. We also show that
    this result extends easily to 1 penalized minimization problems over a wide
    range of loss functions.
  author:
    - family: Tibshirani
      given: Ryan J
  citation-key: tibshirani_lasso_2013
  container-title: Electronic Journal of Statistics
  issued:
    - year: 2013
  language: en
  page: 1456-1490
  source: Zotero
  title: The Lasso Problem and Uniqueness
  type: article-journal
  volume: '7'

- id: tibshirani_regression_1996
  author:
    - family: Tibshirani
      given: Robert
  citation-key: tibshirani_regression_1996
  container-title: Journal of the Royal Statistical Society. Series B (Methodological)
  issued:
    - year: 1996
  note: '00000'
  page: 267–288
  title: Regression shrinkage and selection via the LASSO
  type: article-journal

- id: tibshirani_regression_2011
  accessed:
    - year: 2017
      month: 9
      day: 14
  author:
    - family: Tibshirani
      given: Robert
  citation-key: tibshirani_regression_2011
  container-title: 'Journal of the Royal Statistical Society: Series B (Statistical Methodology)'
  DOI: 10.1111/j.1467-9868.2011.00771.x
  issue: '3'
  issued:
    - year: 2011
  note: '00000'
  page: 273–282
  source: Google Scholar
  title: 'Regression shrinkage and selection via the lasso: a retrospective'
  title-short: Regression shrinkage and selection via the lasso
  type: article-journal
  URL: http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2011.00771.x/full
  volume: '73'

- id: tierney_markov_1994
  abstract: >-
    [Several Markov chain methods are available for sampling from a posterior
    distribution. Two important examples are the Gibbs sampler and the
    Metropolis algorithm. In addition, several strategies are available for
    constructing hybrid algorithms. This paper outlines some of the basic
    methods and strategies and discusses some related theoretical and practical
    issues. On the theoretical side, results from the theory of general state
    space Markov chains can be used to obtain convergence rates, laws of large
    numbers and central limit theorems for estimates obtained from Markov chain
    methods. These theoretical results can be used to guide the construction of
    more efficient algorithms. For the practical use of Markov chain methods,
    standard simulation methodology provides several variance reduction
    techniques and also give guidance on the choice of sample size and
    allocation.]
  accessed:
    - year: 2018
      month: 12
      day: 13
  archive: JSTOR
  author:
    - family: Tierney
      given: Luke
  citation-key: tierney_markov_1994
  container-title: The Annals of Statistics
  ISSN: 0090-5364
  issue: '4'
  issued:
    - year: 1994
  page: 1701-1728
  source: JSTOR
  title: Markov Chains for Exploring Posterior Distributions
  type: article-journal
  URL: https://www.jstor.org/stable/2242477
  volume: '22'

- id: tihonenko_st_2007
  abstract: >-
    This database consists of 75 annotated recordings extracted from 32 Holter
    records. Each record is 30 minutes long and contains 12 standard leads, each
    sampled at 257 Hz, with gains varying from 250 to 1100 analog-to-digital
    converter units per millivolt. Gains for each record are specified in its
    .hea file. The reference annotation files contain over 175,000 beat
    annotations in all.
  accessed:
    - year: 2020
      month: 11
      day: 3
  author:
    - family: Tihonenko
      given: Viktor
    - family: Khaustov
      given: Alexander
    - family: Ivanov
      given: Sergey
    - family: Rivin
      given: Alexei
  citation-key: tihonenko_st_2007
  DOI: 10.13026/C2V88N
  issued:
    - year: 2007
  publisher: physionet.org
  source: DOI.org (Datacite)
  title: >-
    St.-Petersburg Institute of Cardiological Technics 12-lead Arrhythmia
    Database
  type: dataset
  URL: https://physionet.org/content/incartdb/

- id: tijani_nonlinear_2014
  author:
    - family: Tijani
      given: Ismaila B
    - family: Akmeliawati
      given: Rini
    - family: Legowo
      given: Ari
    - family: Budiyono
      given: Agus
  citation-key: tijani_nonlinear_2014
  container-title: Engineering Applications of Artificial Intelligence
  DOI: 10.1016/j.engappai.2014.04.003
  issued:
    - year: 2014
  note: '00000'
  page: 99–115
  title: >-
    Nonlinear identification of a small scale unmanned helicopter using
    optimized NARX network with multiobjective differential evolution
  type: article-journal
  volume: '33'

- id: tijani_nonlinear_2014a
  author:
    - family: Tijani
      given: Ismaila B
    - family: Akmeliawati
      given: Rini
    - family: Legowo
      given: Ari
    - family: Budiyono
      given: Agus
  citation-key: tijani_nonlinear_2014a
  container-title: Engineering Applications of Artificial Intelligence
  DOI: 10/f58zk9
  issued:
    - year: 2014
  page: 99-115
  title: >-
    Nonlinear Identification of a Small Scale Unmanned Helicopter Using
    Optimized NARX Network with Multiobjective Differential Evolution
  type: article-journal
  volume: '33'

- id: tikk_survey_2003
  author:
    - family: Tikk
      given: Domonkos
    - family: Kóczy
      given: László T
    - family: Gedeon
      given: Tamás D
  citation-key: tikk_survey_2003
  container-title: International Journal of Approximate Reasoning
  DOI: 10.1016/S0888-613X(03)00021-5
  issue: '2'
  issued:
    - year: 2003
  note: '00000'
  page: 185–202
  title: >-
    A survey on universal approximation and its limits in soft computing
    techniques
  type: article-journal
  volume: '33'

- id: timmer_parametric_2000
  author:
    - family: Timmer
      given: J
    - family: Rust
      given: H
    - family: Horbelt
      given: W
    - family: Voss
      given: HU
  citation-key: timmer_parametric_2000
  container-title: Physics Letters A
  issue: '3'
  issued:
    - year: 2000
  page: 123–134
  title: >-
    Parametric, nonparametric and parametric modelling of a chaotic circuit time
    series
  type: article-journal
  volume: '274'

- id: tiso_new_2017
  abstract: >-
    The progress accomplished during the past decade in nonlinear system
    identification in structural dynamics is considerable. The objective of the
    present paper is to consolidate this progress by challenging the community
    through a new benchmark structure exhibiting complex nonlinear dynamics. The
    proposed structure consists of two offset cantilevered beams connected by a
    highly flexible element. For increasing forcing amplitudes, the system
    sequentially features linear behaviour, localised nonlinearity associated
    with the buckling of the connecting element, and distributed nonlinearity
    resulting from large elastic deformations across the structure. A finite
    element-based code with time integration capabilities is made available at
    https://sem.org/nonlinear-systems-imac-focus-group/. This code permits the
    numerical simulation of the benchmark dynamics in response to arbitrary
    excitation signals.
  author:
    - family: Tiso
      given: Paolo
    - family: Noël
      given: Jean-Philippe
  citation-key: tiso_new_2017
  collection-title: Recent advances in nonlinear system identification
  container-title: Mechanical Systems and Signal Processing
  container-title-short: Mechanical Systems and Signal Processing
  DOI: 10.1016/j.ymssp.2016.08.008
  ISSN: 0888-3270
  issued:
    - year: 2017
      month: 2
      day: 1
  note: '00000'
  page: 185-193
  source: ScienceDirect
  title: A new, challenging benchmark for nonlinear system identification
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0888327016302837
  volume: '84'

- id: topol_deep_2019
  author:
    - family: Topol
      given: Eric
  citation-key: topol_deep_2019
  ISBN: 1-5416-4464-6
  issued:
    - year: 2019
  publisher: Hachette UK
  title: 'Deep medicine: how artificial intelligence can make healthcare human again'
  type: book

- id: topol_medical_2024
  accessed:
    - year: 2024
      month: 6
      day: 8
  author:
    - family: Topol
      given: Eric J.
  citation-key: topol_medical_2024
  container-title: Science
  DOI: 10.1126/science.adp7977
  issue: '6698'
  issued:
    - year: 2024
      month: 5
      day: 23
  publisher: American Association for the Advancement of Science
  source: science.org (Atypon)
  title: Medical forecasting
  type: article-journal
  URL: https://www.science.org/doi/10.1126/science.adp7977
  volume: '384'

- id: torkamani_personal_2018
  author:
    - family: Torkamani
      given: Ali
    - family: Wineinger
      given: Nathan E
    - family: Topol
      given: Eric J
  citation-key: torkamani_personal_2018
  container-title: Nature Reviews Genetics
  container-title-short: Nature Reviews Genetics
  ISSN: 1471-0056
  issue: '9'
  issued:
    - year: 2018
  page: 581-590
  publisher: Nature Publishing Group UK London
  title: The personal and clinical utility of polygenic risk scores
  type: article-journal
  volume: '19'

- id: trefethen_numerical_1997
  author:
    - family: Trefethen
      given: Lloyd N.
    - family: Bau
      given: David
  call-number: QA184 .T74 1997
  citation-key: trefethen_numerical_1997
  event-place: Philadelphia
  ISBN: 978-0-89871-361-9
  issued:
    - year: 1997
  number-of-pages: '361'
  publisher: Society for Industrial and Applied Mathematics
  publisher-place: Philadelphia
  source: Library of Congress ISBN
  title: Numerical linear algebra
  type: book

- id: tripathy_novel_2019
  abstract: >-
    Myocardial infarction (MI) is also called the heart attack, and it results
    in the death of heart muscle cells due to the lacking in the supply of
    oxygen and other nutrients. The early and accurate detection of MI using the
    12-lead electrocardiogram (ECG) is helpful in the clinical standard for
    saving the lives of the patients suffering from this pathology. This paper
    proposes a novel approach for the detection of MI pathology using the
    multiresolution analysis of 12-lead ECG signals. The approach is based on
    the use of Fourier–Bessel series expansion-based empirical wavelet transform
    (FBSE-EWT) for the time-scale decomposition of 12-lead ECG signals. For each
    lead ECG signal, nine subband signals are evaluated using FBSE-EWT. The
    statistical features such as the kurtosis, the skewness, and the entropy are
    evaluated from the subband signals of each ECG lead. The deep neural network
    such as the deep layer least-square support-vector machine (DL-LSSVM) which
    is formulated using the hidden layers of sparse auto-encoders and the LSSVM
    is used for the detection of MI from the feature vector of 12-lead ECG. The
    experimental results demonstrate that the combination of FBSE-EWT-based
    entropy features and DL-LSSVM has the mean accuracy, the mean sensitivity,
    and the mean specificity values of 99.74%, 99.87%, and 99.60%, respectively,
    for the detection of MI. The accuracy value of the proposed method is
    improved by more than 3% as compared to the wavelet-based features for the
    detection of MI.
  author:
    - family: Tripathy
      given: R. K.
    - family: Bhattacharyya
      given: A.
    - family: Pachori
      given: R. B.
  citation-key: tripathy_novel_2019
  container-title: IEEE Sensors Journal
  DOI: 10/gf286v
  ISSN: 1530-437X
  issue: '12'
  issued:
    - year: 2019
      month: 6
  page: 4509-4517
  source: IEEE Xplore
  title: >-
    A Novel Approach for Detection of Myocardial Infarction From ECG Signals of
    Multiple Electrodes
  type: article-journal
  volume: '19'

- id: trucco_introductory_1998
  author:
    - family: Trucco
      given: Emanuele
    - family: Verri
      given: Alessandro
  citation-key: trucco_introductory_1998
  issued:
    - year: 1998
  note: '00000'
  publisher: Prentice Hall Englewood Cliffs
  title: Introductory techniques for 3-D computer vision
  type: book
  volume: '201'

- id: tsai_versatile_1987
  author:
    - family: Tsai
      given: Roger Y
  citation-key: tsai_versatile_1987
  container-title: Robotics and Automation, IEEE Journal of
  DOI: 10.1109/JRA.1987.1087109
  issue: '4'
  issued:
    - year: 1987
  note: '00000'
  page: 323–344
  title: >-
    A versatile camera calibration technique for high-accuracy 3D machine vision
    metrology using off-the-shelf TV cameras and lenses
  type: article-journal
  volume: '3'

- id: tseng_convergence_2001
  accessed:
    - year: 2017
      month: 9
      day: 13
  author:
    - family: Tseng
      given: Paul
  citation-key: tseng_convergence_2001
  container-title: Journal of Optimization Theory and Applications
  DOI: 10.1023/A:1017501703105
  issue: '3'
  issued:
    - year: 2001
  note: '00000'
  page: 475–494
  source: Google Scholar
  title: >-
    Convergence of a block coordinate descent method for nondifferentiable
    minimization
  type: article-journal
  URL: http://www.springerlink.com/index/q327675221126243.pdf
  volume: '109'

- id: tsipras_robustness_2019
  abstract: >-
    We show that there exists an inherent tension between the goal of
    adversarial robustness and that of standard generalization. Speciﬁcally,
    training robust models may not only be more resource-consuming, but also
    lead to a reduction of standard accuracy. We demonstrate that this trade-off
    between the standard accuracy of a model and its robustness to adversarial
    perturbations provably exists even in a fairly simple and natural setting.
    These ﬁndings also corroborate a similar phenomenon observed in practice.
    Further, we argue that this phenomenon is a consequence of robust classiﬁers
    learning fundamentally different feature representations than standard
    classiﬁers. These differences, in particular, seem to result in unexpected
    beneﬁts: the features learned by robust models tend to align better with
    salient data characteristics and human perception.
  author:
    - family: Tsipras
      given: Dimitris
    - family: Santurkar
      given: Shibani
    - family: Engstrom
      given: Logan
    - family: Turner
      given: Alexander
    - family: Ma
      given: Aleksander
  citation-key: tsipras_robustness_2019
  container-title: International Conference for Learning Representations
  issued:
    - year: 2019
  language: en
  source: Zotero
  title: Robustness May Be At Odds with Accuracy
  type: article-journal

- id: tulleken_greybox_1993
  author:
    - family: Tulleken
      given: Herbert JAF
  citation-key: tulleken_greybox_1993
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10.1016/0005-1098(93)90124-C
  ISSN: 0005-1098
  issue: '2'
  issued:
    - year: 1993
  note: '00000'
  page: 285-308
  title: >-
    Grey-box modelling and identification using physical knowledge and Bayesian
    techniques
  type: article-journal
  volume: '29'

- id: turner_residential_2017
  author:
    - family: Turner
      given: W. J. N.
    - family: Staino
      given: A
    - family: Basu
      given: B
  citation-key: turner_residential_2017
  container-title: Energy and Buildings
  DOI: 10/gbxqqf
  issued:
    - year: 2017
  note: '00013'
  title: Residential HVAC Fault Detection Using a System Identification Approach
  type: article-journal

- id: ugray_scatter_2007
  author:
    - family: Ugray
      given: Zsolt
    - family: Lasdon
      given: Leon
    - family: Plummer
      given: John
    - family: Glover
      given: Fred
    - family: Kelly
      given: James
    - family: Martí
      given: Rafael
  citation-key: ugray_scatter_2007
  container-title: INFORMS Journal on Computing
  DOI: 10.1287/ijoc.1060.0175
  issue: '3'
  issued:
    - year: 2007
  note: '00000'
  page: 328–340
  title: >-
    Scatter search and local NLP solvers: A multistart framework for global
    optimization
  type: article-journal
  volume: '19'

- id: ulicny_harmonic_2018
  abstract: >-
    Convolutional neural networks (CNNs) learn filters in order to capture local
    correlation patterns in feature space. In contrast, in this paper we propose
    harmonic blocks that produce features by learning optimal combinations of
    spectral filters defined by the Discrete Cosine Transform. The harmonic
    blocks are used to replace conventional convolutional layers to construct
    partial or fully harmonic CNNs. We extensively validate our approach and
    show that the introduction of harmonic blocks into state-of-the-art CNN
    baseline architectures results in comparable or better performance in
    classification tasks on small NORB, CIFAR10 and CIFAR100 datasets.
  accessed:
    - year: 2020
      month: 7
      day: 7
  author:
    - family: Ulicny
      given: Matej
    - family: Krylov
      given: Vladimir A.
    - family: Dahyot
      given: Rozenn
  citation-key: ulicny_harmonic_2018
  container-title: arXiv:1812.03205 [cs]
  issued:
    - year: 2018
      month: 12
      day: 7
  source: arXiv.org
  title: 'Harmonic Networks: Integrating Spectral Information into CNNs'
  title-short: Harmonic Networks
  type: article-journal
  URL: http://arxiv.org/abs/1812.03205

- id: umenberger_convex_2017
  abstract: >-
    This thesis concerns the scalable application of convex optimization to
    data-driven mod- eling of dynamical systems, termed system identification in
    the control community. Two problems commonly arising in system
    identification are model instability (e.g. unreliability of long-term,
    open-loop predictions), and nonconvexity of quality-of-fit criteria, such as
    sim- ulation error (a.k.a. output error). To address these problems, this
    thesis presents convex parametrizations of stable dynamical systems, convex
    quality-of-fit criteria, and e cient algorithms to optimize the latter over
    the former.

    In particular, this thesis makes extensive use of Lagrangian relaxation, a
    technique for gen- erating convex approximations to nonconvex optimization
    problems. Recently, Lagrangian relaxation has been used to approximate
    simulation error and guarantee nonlinear model stability via semidefinite
    programming (SDP), however, the resulting SDPs have large di- mension,
    limiting their practical utility. The first contribution of this thesis is a
    custom interior point algorithm that exploits structure in the problem to
    significantly reduce com- putational complexity. The new algorithm enables
    empirical comparisons to established methods including Nonlinear ARX, in
    which superior generalization to new data is demon- strated.

    Equipped with this algorithmic machinery, the second contribution of this
    thesis is the in- corporation of model stability constraints into the
    maximum likelihood framework. Specifi- cally, Lagrangian relaxation is
    combined with the expectation maximization (EM) algorithm to derive tight
    bounds on the likelihood function, that can be optimized over a convex
    parametrization of all stable linear dynamical systems. Two di↵erent
    formulations are pre- sented, one of which gives higher fidelity bounds when
    disturbances (a.k.a. process noise) dominate measurement noise, and vice
    versa.

    Finally, identification of positive systems is considered. Such systems
    enjoy substantially simpler stability and performance analysis compared to
    the general linear time-invariant(LTI) case, and appear frequently in
    applications where physical constraints imply nonneg- ativity of the
    quantities of interest. Lagrangian relaxation is used to derive new convex
    parametrizations of stable positive systems and quality-of-fit criteria, and
    substantial im- provements in accuracy of the identified models, compared to
    existing approaches based on weighted equation error, are demonstrated.
    Furthermore, the convex parametrizations of stable systems based on linear
    Lyapunov functions are shown to be amenable to distributed optimization,
    which is useful for identification of large-scale networked dynamical
    systems.
  author:
    - family: Umenberger
      given: Jack
  citation-key: umenberger_convex_2017
  event-place: Sydney
  issued:
    - year: 2017
  language: English
  note: '00000'
  number-of-pages: '173'
  publisher: The University of Sydney
  publisher-place: Sydney
  source: Google Scholar
  title: Convex Identifcation of Stable Dynamical Systems
  type: thesis

- id: umenberger_identification_2015
  abstract: >-
    In the application of the Expectation Maximization (EM) algorithm to
    identification of dynamical systems, latent variables are typically taken as
    system states, for simplicity. In this work, we propose a different choice
    of latent variables, namely, system disturbances. Such a formulation is
    shown, under certain circumstances, to improve the fidelity of bounds on the
    likelihood, and circumvent difficulties related to intractable model
    transition densities. To access these benefits, we propose a Lagrangian
    relaxation of the challenging optimization problem that arises when
    formulating over latent disturbances, and fully develop the method for
    linear models.
  author:
    - family: Umenberger
      given: Jack
    - family: Wågberg
      given: Johan
    - family: Manchester
      given: Ian R.
    - family: Schön
      given: Thomas B.
  citation-key: umenberger_identification_2015
  collection-title: 17th IFAC Symposium on System Identification SYSID 2015
  container-title: IFAC-PapersOnLine
  container-title-short: IFAC-PapersOnLine
  DOI: 10.1016/j.ifacol.2015.12.102
  ISSN: 2405-8963
  issue: '28'
  issued:
    - year: 2015
      month: 1
      day: 1
  note: '00000'
  page: 69-74
  source: ScienceDirect
  title: On Identification via EM with Latent Disturbances and Lagrangian Relaxation
  title-short: On Identification via EM with Latent Disturbances and Lagrangian Relaxation
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S2405896315027263
  volume: '48'

- id: umenberger_learning_2018
  abstract: >-
    Learning to make decisions from observed data in dynamic environments
    remains a problem of fundamental importance in a number of fields, from
    artificial intelligence and robotics, to medicine and finance. This paper
    concerns the problem of learning control policies for unknown linear
    dynamical systems so as to maximize a quadratic reward function. We present
    a method to optimize the expected value of the reward over the posterior
    distribution of the unknown system parameters, given data. The algorithm
    involves sequential convex programing, and enjoys reliable local convergence
    and robust stability guarantees. Numerical simulations and stabilization of
    a real-world inverted pendulum are used to demonstrate the approach, with
    strong performance and robustness properties observed in both.
  author:
    - family: Umenberger
      given: Jack
    - family: Schön
      given: Thomas B.
  citation-key: umenberger_learning_2018
  container-title: arXiv:1806.00319 [cs, math, stat]
  issued:
    - year: 2018
      month: 6
      day: 1
  note: '00000'
  source: arXiv.org
  title: Learning convex bounds for linear quadratic control policy synthesis
  type: article-journal
  URL: http://arxiv.org/abs/1806.00319

- id: umenberger_linear_2016
  abstract: >-
    In the application of the Expectation Maximization algorithm to
    identification of dynamical systems, internal states are typically chosen as
    latent variables, for simplicity. In this work, we propose a different
    choice of latent variables, namely, system disturbances. Such a formulation
    elegantly handles the problematic case of singular state space models, and
    is shown, under certain circumstances, to improve the fidelity of bounds on
    the likelihood, leading to convergence in fewer iterations. To access these
    benefits we develop a Lagrangian relaxation of the nonconvex optimization
    problems that arise in the latent disturbances formulation, and proceed via
    semidefinite programming.
  author:
    - family: Umenberger
      given: Jack
    - family: Wågberg
      given: Johan
    - family: Manchester
      given: Ian R.
    - family: Schön
      given: Thomas B.
  citation-key: umenberger_linear_2016
  container-title: arXiv:1603.09157 [cs, stat]
  issued:
    - year: 2016
      month: 3
      day: 30
  note: '00000'
  source: arXiv.org
  title: >-
    Linear System Identification via EM with Latent Disturbances and Lagrangian
    Relaxation
  type: article-journal
  URL: http://arxiv.org/abs/1603.09157

- id: umenberger_maximum_2017
  author:
    - family: Umenberger
      given: Jack
    - family: Wågberg
      given: Johan
    - family: Manchester
      given: Ian
    - family: Schön
      given: Thomas
  citation-key: umenberger_maximum_2017
  issued:
    - year: 2017
      month: 6
      day: 7
  note: '00000'
  title: Maximum likelihood identification of stable linear dynamical systems
  type: book

- id: umenberger_scalable_2016
  author:
    - family: Umenberger
      given: Jack
    - family: Manchester
      given: Ian R
  citation-key: umenberger_scalable_2016
  container-title: 2016 IEEE 55th Conference on Decision and Control (CDC)
  DOI: 10.1109/CDC.2016.7798974
  event-title: 2016 IEEE 55th Conference on Decision and Control (CDC)
  issued:
    - year: 2016
      month: 12
  note: '00000'
  page: 4630-4635
  title: Scalable identification of stable positive systems
  type: paper-conference

- id: umenberger_specialized_2016
  author:
    - family: Umenberger
      given: Jack
    - family: Manchester
      given: Ian R
  citation-key: umenberger_specialized_2016
  container-title: 2016 American Control Conference (ACC)
  DOI: 10.1109/ACC.2016.7525034
  event-title: 2016 American Control Conference (ACC)
  issued:
    - year: 2016
      month: 7
  note: '00000'
  page: 930-935
  title: >-
    Specialized algorithm for identification of stable linear systems using
    Lagrangian relaxation
  type: paper-conference

- id: unwin_iris_2021
  abstract: >-
    The iris data set is one of the best-known and most widely used data sets in
    statistics and data science. But the origins of at least part of the data
    have been something of a mystery for decades. Antony Unwin and Kim Kleinman
    believe they have traced the source
  accessed:
    - year: 2024
      month: 5
      day: 21
  author:
    - family: Unwin
      given: Antony
    - family: Kleinman
      given: Kim
  citation-key: unwin_iris_2021
  container-title: Significance
  container-title-short: Significance
  DOI: 10.1111/1740-9713.01589
  ISSN: 1740-9705
  issue: '6'
  issued:
    - year: 2021
      month: 12
      day: 1
  page: 26-29
  source: Silverchair
  title: 'The Iris Data Set: In Search of the Source of Virginica'
  title-short: The Iris Data Set
  type: article-journal
  URL: https://doi.org/10.1111/1740-9713.01589
  volume: '18'

- id: vaicenavicius_evaluating_2019
  abstract: >-
    Probabilistic classifiers output a probability distribution on target
    classes rather than just a class prediction. Besides providing a clear
    separation of prediction and decision making, the main advantage of
    probabilistic models is their ability to represent uncertainty about
    predictions. In safety-critical applications, it is pivotal for a model to
    possess an adequate sense of uncertainty, which for probabilistic
    classifiers translates into outputting probability distributions that are
    consistent with the empirical frequencies observed from realized outcomes. A
    classifier with such a property is called calibrated. In this work, we
    develop a general theoretical calibration evaluation framework grounded in
    probability theory, and point out subtleties present in model calibration
    evaluation that lead to refined interpretations of existing evaluation
    techniques. Lastly, we propose new ways to quantify and visualize
    miscalibration in probabilistic classification, including novel
    multidimensional reliability diagrams.
  accessed:
    - year: 2019
      month: 3
      day: 20
  author:
    - family: Vaicenavicius
      given: Juozas
    - family: Widmann
      given: David
    - family: Andersson
      given: Carl
    - family: Lindsten
      given: Fredrik
    - family: Roll
      given: Jacob
    - family: Schön
      given: Thomas B.
  citation-key: vaicenavicius_evaluating_2019
  container-title: arXiv:1902.06977 [cs, stat]
  issued:
    - year: 2019
      month: 2
      day: 19
  source: arXiv.org
  title: Evaluating model calibration in classification
  type: article-journal
  URL: http://arxiv.org/abs/1902.06977

- id: valko_lectures_
  author:
    - family: Valko
      given: B
  citation-key: valko_lectures_
  language: en
  page: '7'
  source: Zotero
  title: 'Lectures 6 – 7 : Marchenko-Pastur Law'
  type: article-journal

- id: vandeleur_automatic_2020
  abstract: >-
    BACKGROUND: The correct interpretation of the ECG is pivotal for the
    accurate diagnosis of many cardiac abnormalities, and conventional
    computerized interpretation has not been able to reach physician-­level
    accuracy in detecting (acute) cardiac abnormalities. This study aims to
    develop and validate a deep neural network for comprehensive automated ECG
    triage in daily practice.

    METHODS AND RESULTS: We developed a 37-­layer convolutional residual deep
    neural network on a data set of free-­text physician-a­nnotated 12-l­ead
    ECGs. The deep neural network was trained on a data set with 336.835
    recordings from 142.040 patients and validated on an independent validation
    data set (n=984), annotated by a panel of 5 cardiologists
    electrophysiologists. The 12-­lead ECGs were acquired in all noncardiology
    departments of the University Medical Center Utrecht. The algorithm learned
    to classify these ECGs into the following 4 triage categories: normal,
    abnormal not acute, subacute, and acute. Discriminative performance is
    presented with overall and category-s­ pecific concordance statistics,
    polytomous discrimination indexes, sensitivities, specificities, and
    positive and negative predictive values. The patients in the validation data
    set had a mean age of 60.4 years and 54.3% were men. The deep neural network
    showed excellent overall discrimination with an overall concordance
    statistic of 0.93 (95% CI, 0.92–0.95) and a polytomous discriminatory index
    of 0.83 (95% CI, 0.79–0.87).

    CONCLUSIONS: This study demonstrates that an end-­to-­end deep neural
    network can be accurately trained on unstructured free-t­ext physician
    annotations and used to consistently triage 12-l­ead ECGs. When further
    fine-­tuned with other clinical outcomes and externally validated in
    clinical practice, the demonstrated deep learning–based ECG interpretation
    can potentially improve time to treatment and decrease healthcare burden.
  accessed:
    - year: 2020
      month: 7
      day: 28
  author:
    - family: Leur
      given: Rutger R.
      non-dropping-particle: van de
    - family: Blom
      given: Lennart J.
    - family: Gavves
      given: Efstratios
    - family: Hof
      given: Irene E.
    - family: Heijden
      given: Jeroen F.
      non-dropping-particle: van der
    - family: Clappers
      given: Nick C.
    - family: Doevendans
      given: Pieter A.
    - family: Hassink
      given: Rutger J.
    - family: Es
      given: René
      non-dropping-particle: van
  citation-key: vandeleur_automatic_2020
  container-title: Journal of the American Heart Association
  container-title-short: JAHA
  DOI: 10.1161/JAHA.119.015138
  ISSN: 2047-9980
  issue: '10'
  issued:
    - year: 2020
      month: 5
      day: 18
  language: en
  source: DOI.org (Crossref)
  title: Automatic Triage of 12‐Lead ECGs Using Deep Convolutional Neural Networks
  type: article-journal
  URL: https://www.ahajournals.org/doi/10.1161/JAHA.119.015138
  volume: '9'

- id: vandervaart_asymptotic_2000
  author:
    - family: Vaart
      given: A. W.
      non-dropping-particle: van der
  citation-key: vandervaart_asymptotic_2000
  collection-title: Cambridge series in statistical and probabilistic mathematics
  ISBN: 0-521-78450-6 978-0-521-78450-4 0-521-49603-9 978-0-521-49603-2
  issued:
    - year: 2000
  publisher: Cambridge University Press
  title: Asymptotic statistics
  type: book

- id: vandomselaar_nonlinear_1975
  author:
    - family: Van Domselaar
      given: B
    - family: Hemker
      given: Piet W
  citation-key: vandomselaar_nonlinear_1975
  issued:
    - year: 1975
  note: '00002'
  publisher: SIS-76-1121
  title: Nonlinear Parameter Estimation in Initial Value Problems
  type: report

- id: vandomselaar_nonlinear_1975a
  author:
    - family: Van Domselaar
      given: B
    - family: Hemker
      given: Piet W
  citation-key: vandomselaar_nonlinear_1975a
  issued:
    - year: 1975
  publisher: SIS-76-1121
  title: Nonlinear Parameter Estimation in Initial Value Problems
  type: report

- id: vanmulders_two_2009
  abstract: >-
    In this paper, two nonlinear optimization methods for the identification of
    nonlinear systems are compared. Both methods estimate all the parameters of
    a polynomial nonlinear state-space model by means of a nonlinear
    least-squares optimization. While the first method does not estimate the
    states explicitly, the second estimates both states and parameters adding an
    extra constraint equation. Both methods are introduced and their
    similarities and differences are discussed utilizing simulation and
    experimental data. The unconstrained method appears to be faster and more
    memory efficient, while the constrained method is robust towards
    instabilities.
  author:
    - family: Van Mulders
      given: Anne
    - family: Schoukens
      given: Johan
    - family: Volckaert
      given: Marnix
    - family: Diehl
      given: Moritz
  citation-key: vanmulders_two_2009
  container-title: 15th IFAC Symposium on System Identification
  container-title-short: IFAC Proceedings Volumes
  DOI: 10/dt72p8
  ISSN: 1474-6670
  issue: '10'
  issued:
    - year: 2009
      month: 1
      day: 1
  page: 1086-1091
  title: Two Nonlinear Optimization Methods for Black Box Identification Compared
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S1474667016387948
  volume: '42'

- id: vanmulders_two_2010
  abstract: >-
    In this paper, two nonlinear optimization methods for the identification of
    nonlinear systems are compared. Both methods estimate the parameters of e.g.
    a polynomial nonlinear state-space model by means of a nonlinear
    least-squares optimization of the same cost function. While the first method
    does not estimate the states explicitly, the second method estimates both
    states and parameters adding an extra constraint equation. Both methods are
    introduced and their similarities and differences are discussed utilizing
    simulation data. The unconstrained method appears to be faster and more
    memory efficient, but the constrained method has a significant advantage as
    well: it is robust for unstable systems of which bounded input–output data
    can be measured (e.g. a system captured in a stabilizing feedback loop).
    Both methods have successfully been applied on real-life measurement data.
  accessed:
    - year: 2019
      month: 3
      day: 19
  author:
    - family: Van Mulders
      given: Anne
    - family: Schoukens
      given: Johan
    - family: Volckaert
      given: Marnix
    - family: Diehl
      given: Moritz
  citation-key: vanmulders_two_2010
  container-title: Automatica
  DOI: 10/dzv9r8
  ISSN: '00051098'
  issue: '10'
  issued:
    - year: 2010
      month: 10
  language: en
  page: 1675-1681
  source: Crossref
  title: Two nonlinear optimization methods for black box identification compared
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0005109810002712
  volume: '46'

- id: vanoverschee_n4sid_1994
  abstract: >-
    Recently a great deal of attention has been given to numerical algorithms
    for subspace state space system identification (N4SID). In this paper, we
    derive two new N4SID algorithms to identify mixed deterministic-stochastic
    systems. Both algorithms determine state sequences through the projection of
    input and output data. These state sequences are shown to be outputs of
    non-steady state Kalman filter banks. From these it is easy to determine the
    state space system matrices. The N4SID algorithms are always convergent
    (non-iterative) and numerically stable since they only make use of QR and
    Singular Value Decompositions. Both N4SID algorithms are similar, but the
    second one trades off accuracy for simplicity. These new algorithms are
    compared with existing subspace algorithms in theory and in practice.
  author:
    - family: Van Overschee
      given: Peter
    - family: De Moor
      given: Bart
  citation-key: vanoverschee_n4sid_1994
  collection-title: Special issue on statistical signal processing and control
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10.1016/0005-1098(94)90230-5
  ISSN: 0005-1098
  issue: '1'
  issued:
    - year: 1994
      month: 1
      day: 1
  note: '00000'
  page: 75-93
  source: ScienceDirect
  title: >-
    N4SID: Subspace algorithms for the identification of combined
    deterministic-stochastic systems
  title-short: N4SID
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/0005109894902305
  volume: '30'

- id: vanoverschee_subspace_2012
  author:
    - family: Van Overschee
      given: Peter
    - family: De Moor
      given: B. L.
  citation-key: vanoverschee_subspace_2012
  issued:
    - year: 2012
  note: '00000'
  publisher: Springer Science & Business Media
  source: Google Scholar
  title: >-
    Subspace identification for linear systems:
    Theory—Implementation—Applications
  title-short: Subspace identification for linear systems
  type: book

- id: vanoverschee_unifying_1995
  abstract: >-
    The aim of this paper is to indicate and explore the similarities between
    three different subspace algorithms for the identification of combined
    deterministic-stochastic systems. The similarities between these algorithms
    have been obscured, due to different notations and backgrounds. It is shown
    that all three algorithms are special cases of one unifying theorem. The
    comparison also reveals that the three algorithms use exactly the same
    subspace to determine the order and the extended observability matrix, but
    that the weighting matrix, used to calculate a basis for the column space of
    the observability matrix is different in the three cases.
  author:
    - family: Van Overschee
      given: Peter
    - family: De Moor
      given: Bart
  citation-key: vanoverschee_unifying_1995
  collection-title: Trends in System Identification
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10.1016/0005-1098(95)00072-0
  ISSN: 0005-1098
  issue: '12'
  issued:
    - year: 1995
      month: 12
      day: 1
  note: '00000'
  page: 1853-1864
  source: ScienceDirect
  title: A unifying theorem for three subspace system identification algorithms
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/0005109895000720
  volume: '31'

- id: vanschoren_openml_2014
  abstract: >-
    Many sciences have made significant breakthroughs by adopting online tools
    that help organize, structure and mine information that is too detailed to
    be printed in journals. In this paper, we introduce OpenML, a place for
    machine learning researchers to share and organize data in fine detail, so
    that they can work more effectively, be more visible, and collaborate with
    others to tackle harder problems. We discuss how OpenML relates to other
    examples of networked science and what benefits it brings for machine
    learning research, individual scientists, as well as students and
    practitioners.
  accessed:
    - year: 2024
      month: 5
      day: 16
  author:
    - family: Vanschoren
      given: Joaquin
    - family: Rijn
      given: Jan N.
      non-dropping-particle: van
    - family: Bischl
      given: Bernd
    - family: Torgo
      given: Luis
  citation-key: vanschoren_openml_2014
  container-title: ACM SIGKDD Explorations Newsletter
  container-title-short: SIGKDD Explor. Newsl.
  DOI: 10.1145/2641190.2641198
  ISSN: 1931-0145
  issue: '2'
  issued:
    - year: 2014
      month: 6
      day: 16
  page: 49–60
  source: ACM Digital Library
  title: 'OpenML: networked science in machine learning'
  title-short: OpenML
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/2641190.2641198
  volume: '15'

- id: vapnik_nature_2013
  author:
    - family: Vapnik
      given: V.
  citation-key: vapnik_nature_2013
  collection-title: Information Science and Statistics
  ISBN: 978-1-4757-3264-1
  issued:
    - year: 2013
  note: '00000'
  publisher: Springer New York
  title: The Nature of Statistical Learning Theory
  type: book
  URL: https://books.google.com.br/books?id=EqgACAAAQBAJ

- id: vargas_improved_2019
  abstract: >-
    This study is concerned with the asymptotic identification of nonlinear
    systems based on Lyapunov theory and two-layer neural networks. An improved
    identification model enhanced with a feedback term and a novel adaptation
    law for the threshold offset, associated with the output weight matrix, is
    introduced to assure the convergence of the online prediction error, even in
    the presence of approximation error and bounded disturbances and when upper
    bounds for these perturbations are not known in advance. The effectiveness
    of the proposed method and its application to the identification of a
    hyperchaotic system and control of a welding system is investigated.
  author:
    - family: Vargas
      given: José A.R.
    - family: Pedrycz
      given: Witold
    - family: Hemerly
      given: Elder M.
  citation-key: vargas_improved_2019
  container-title: Neurocomputing
  container-title-short: Neurocomputing
  DOI: 10/gfwt99
  ISSN: 0925-2312
  issued:
    - year: 2019
      month: 2
      day: 15
  page: 86-96
  title: >-
    Improved learning algorithm for two-layer neural networks for identification
    of nonlinear systems
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0925231218311846
  volume: '329'

- id: vasconcelos_effective_2020
  abstract: >-
    Image pre-processing in the frequency domain has traditionally played a
    vital role in computer vision and was even part of the standard pipeline in
    the early days of deep learning. However, with the advent of large datasets,
    many practitioners concluded that this was unnecessary due to the belief
    that these priors can be learned from the data itself. Frequency aliasing is
    a phenomenon that may occur when sub-sampling any signal, such as an image
    or feature map, causing distortion in the sub-sampled output. We show that
    we can mitigate this effect by placing non-trainable blur filters and using
    smooth activation functions at key locations, particularly where networks
    lack the capacity to learn them. These simple architectural changes lead to
    substantial improvements in out-of-distribution generalization on both image
    classification under natural corruptions on ImageNet-C [10] and few-shot
    learning on Meta-Dataset [17], without introducing additional trainable
    parameters and using the default hyper-parameters of open source codebases.
  author:
    - family: Vasconcelos
      given: Cristina
    - family: Larochelle
      given: Hugo
    - family: Dumoulin
      given: Vincent
    - family: Roux
      given: Nicolas Le
    - family: Goroshin
      given: Ross
  citation-key: vasconcelos_effective_2020
  container-title: arXiv:2011.10675
  issued:
    - year: 2020
      month: 11
      day: 20
  title: An Effective Anti-Aliasing Approach for Residual Networks
  type: article-journal

- id: vaswani_attention_2017
  accessed:
    - year: 2019
      month: 4
      day: 24
  author:
    - family: Vaswani
      given: Ashish
    - family: Shazeer
      given: Noam
    - family: Parmar
      given: Niki
    - family: Uszkoreit
      given: Jakob
    - family: Jones
      given: Llion
    - family: Gomez
      given: Aidan N
    - family: Kaiser
      given: Łukasz
    - family: Polosukhin
      given: Illia
  citation-key: vaswani_attention_2017
  container-title: Advances in Neural Information Processing Systems 30
  issued:
    - year: 2017
  page: 5998–6008
  source: Neural Information Processing Systems
  title: Attention is All you Need
  type: paper-conference
  URL: http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf

- id: veloso_lazy_2006
  abstract: >-
    Decision tree classifiers perform a greedy search for rules by heuristically
    selecting the most promising features. Such greedy (local) search may
    discard important rules. Associative classifiers, on the other hand, perform
    a global search for rules satisfying some quality constraints (i.e., minimum
    support). This global search, however, may generate a large number of rules.
    Further, many of these rules may be useless during classification, and
    worst, important rules may never be mined. Lazy (non-eager) associative
    classification overcomes this problem by focusing on the features of the
    given test instance, increasing the chance of generating more rules that are
    useful for classifying the test instance. In this paper we assess the
    performance of lazy associative classification. First we demonstrate that an
    associative classifier performs no worse than the corresponding decision
    tree classifier. Also we demonstrate that lazy classifiers outperform the
    corresponding eager ones. Our claims are empirically confirmed by an
    extensive set of experimental results. We show that our proposed lazy
    associative classifier is responsible for an error rate reduction of
    approximately 10% when compared against its eager counterpart, and for a
    reduction of 20% when compared against a decision tree classifier. A simple
    caching mechanism makes lazy associative classification fast, and thus
    improvements in the execution time are also observed.
  author:
    - family: Veloso
      given: Adriano
    - family: Meira Jr
      given: Wagner
    - family: Zaki
      given: M. J.
  citation-key: veloso_lazy_2006
  container-title: Proceedingsof the 6th International Conference on Data Mining (ICDM)
  DOI: 10/bhq7p6
  event-title: Sixth International Conference on Data Mining (ICDM'06)
  issued:
    - year: 2006
  page: 645-654
  source: IEEE Xplore
  title: Lazy Associative Classification
  type: paper-conference

- id: verdie_tilde_2015
  author:
    - family: Verdie
      given: Yannick
    - family: Yi
      given: Kwang
    - family: Fua
      given: Pascal
    - family: Lepetit
      given: Vincent
  citation-key: verdie_tilde_2015
  container-title: >-
    Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition
  issued:
    - year: 2015
  note: '00000'
  page: 5279-5288
  title: 'TILDE: A Temporally Invariant Learned DEtector'
  type: paper-conference

- id: veronese_emergency_2016
  abstract: >-
    BACKGROUND: Electrocardiogram (ECG) interpretation is widely performed by
    emergency physicians. We aimed to determine the accuracy of interpretation
    of potential ST-segment elevation myocardial infarction (STEMI) ECGs by
    emergency physicians.

    METHODS: Thirty-six ECGs resulted in putative STEMI diagnoses were selected.
    Participants were asked to focus on whether or not the ECG in question met
    the diagnostic criteria for an acutely blocked coronary artery causing a
    STEMI. Based on the coronary angiogram, a binary outcome of accurate versus
    inaccurate ECG interpretation was defined. We computed the overall
    sensitivity, specificity, accuracy and 95% confidence intervals (95%CIs) for
    ECG interpretation. Data on participant training level, working experience
    and place were collected.

    RESULTS: 135 participants interpreted 4603 ECGs. Overall sensitivity to
    identify 'true' STEMI ECGs was 64.5% (95%CI: 62.8-66.3); specificity in
    determining 'false' ECGs was 78% (95%CI: 76-80.1). Overall accuracy was
    modest (69.1, 95%CI: 67.8-70.4). Higher accuracy in ECG interpretation was
    observed for attending physicians, participants working in tertiary care
    hospitals and those more experienced.

    CONCLUSION: The accuracy of interpretation of potential STEMI ECGs was
    modest among emergency physicians. The study supports the notion that ECG
    interpretation for establishing a STEMI diagnosis lacks the necessary
    sensitivity and specificity to be considered a reliable 'stand-alone'
    diagnostic test.
  author:
    - family: Veronese
      given: Giacomo
    - family: Germini
      given: Federico
    - family: Ingrassia
      given: Stella
    - family: Cutuli
      given: Ombretta
    - family: Donati
      given: Valeria
    - family: Bonacchini
      given: Luca
    - family: Marcucci
      given: Maura
    - family: Fabbri
      given: Andrea
    - literal: Italian Society of Emergency Medicine (SIMEU)
  citation-key: veronese_emergency_2016
  container-title: Acute Cardiac Care
  container-title-short: Acute Card Care
  DOI: 10.1080/17482941.2016.1234058
  ISSN: 1748-295X
  issue: '1'
  issued:
    - year: 2016
      month: 3
  language: eng
  page: 7-10
  PMID: '27759433'
  source: PubMed
  title: >-
    Emergency physician accuracy in interpreting electrocardiograms with
    potential ST-segment elevation myocardial infarction: Is it enough?
  title-short: >-
    Emergency physician accuracy in interpreting electrocardiograms with
    potential ST-segment elevation myocardial infarction
  type: article-journal
  volume: '18'

- id: vershynin_highdimensional_2018
  author:
    - family: Vershynin
      given: Roman
  citation-key: vershynin_highdimensional_2018
  collection-title: Cambridge series in statistical and probabilistic mathematics
  ISBN: 978-1-108-41519-4 1-108-41519-9 978-1-108-23159-6
  issued:
    - year: 2018
  publisher: Cambridge University Press
  title: High-Dimensional Probability
  type: book
  URL: http://gen.lib.rus.ec/book/index.php?md5=e0ec86c023c4d37ad5c1d1428c69ca31

- id: verstraete_lorentz_2002
  abstract: >-
    All mixed states of two qubits can be brought into normal form by the action
    of local operations and classical communication operations of the kind
    ρ′=(A⊗B)ρ(A⊗B)†. These normal forms can be obtained by considering a Lorentz
    singular-value decomposition on a real parametrization of the density
    matrix. We show that the Lorentz singular values are variationally defined
    and give rise to entanglement monotones, with as a special case the
    concurrence. Next a necessary and sufficient criterion is conjectured for a
    mixed state to be convertible into another specific one with a nonzero
    probability. Finally the formalism of the Lorentz singular-value
    decomposition is applied to tripartite pure states of qubits. New proofs are
    given for the existence of the Greenberger-Horne-Zeilinger (GHZ) class and W
    class of states, and a rigorous proof for the optimal distillation of a GHZ
    state is derived.
  author:
    - family: Verstraete
      given: Frank
    - family: Dehaene
      given: Jeroen
    - family: De Moor
      given: Bart
  citation-key: verstraete_lorentz_2002
  container-title: Physical Review A
  container-title-short: Phys. Rev. A
  DOI: 10.1103/PhysRevA.65.032308
  issue: '3'
  issued:
    - year: 2002
      month: 2
      day: 14
  note: '00000'
  page: '032308'
  source: APS
  title: >-
    Lorentz singular-value decomposition and its applications to pure states of
    three qubits
  type: article-journal
  URL: https://link.aps.org/doi/10.1103/PhysRevA.65.032308
  volume: '65'

- id: vidyasagar_nonlinear_1993
  author:
    - family: Vidyasagar
      given: M.
  call-number: QA402 .V53 1993
  citation-key: vidyasagar_nonlinear_1993
  edition: 2nd ed
  event-place: Englewood Cliffs, N.J
  ISBN: 978-0-13-623463-0
  issued:
    - year: 1993
  language: en
  note: '01559'
  number-of-pages: '498'
  publisher: Prentice Hall
  publisher-place: Englewood Cliffs, N.J
  source: Library of Congress ISBN
  title: Nonlinear systems analysis
  type: book

- id: vilhjalmsson_modeling_2015
  abstract: >-
    Polygenic risk scores have shown great promise in predicting complex disease
    risk and will become more accurate as training sample sizes increase. The
    standard approach for calculating risk scores involves linkage
    disequilibrium (LD)-based marker pruning ...
  accessed:
    - year: 2024
      month: 1
      day: 24
  author:
    - family: Vilhjálmsson
      given: Bjarni J.
    - family: Yang
      given: Jian
    - family: Finucane
      given: Hilary K.
    - family: Gusev
      given: Alexander
    - family: Lindström
      given: Sara
    - family: Ripke
      given: Stephan
    - family: Genovese
      given: Giulio
    - family: Loh
      given: Po-Ru
    - family: Bhatia
      given: Gaurav
    - family: Do
      given: Ron
    - family: Hayeck
      given: Tristan
    - family: Won
      given: Hong-Hee
    - family: Consortium
      given: Schizophrenia Working Group of the Psychiatric Genomics
    - family: Discovery
      given: Biology
    - family: Kathiresan
      given: Sekar
    - family: Pato
      given: Michele
    - family: Pato
      given: Carlos
    - family: Tamimi
      given: Rulla
    - family: Stahl
      given: Eli
    - family: Zaitlen
      given: Noah
    - family: Pasaniuc
      given: Bogdan
    - family: Belbin
      given: Gillian
    - family: Kenny
      given: Eimear E.
    - family: Schierup
      given: Mikkel H.
    - family: Jager
      given: Philip De
    - family: Patsopoulos
      given: Nikolaos A.
    - family: McCarroll
      given: Steve
    - family: Daly
      given: Mark
    - family: Purcell
      given: Shaun
    - family: Chasman
      given: Daniel
    - family: Neale
      given: Benjamin
    - family: Goddard
      given: Michael
    - family: Visscher
      given: Peter M.
    - family: Kraft
      given: Peter
    - family: Patterson
      given: Nick
    - family: Price
      given: Alkes L.
  citation-key: vilhjalmsson_modeling_2015
  container-title: American Journal of Human Genetics
  DOI: 10.1016/j.ajhg.2015.09.001
  issue: '4'
  issued:
    - year: 2015
      month: 10
      day: 10
  language: en
  page: '576'
  PMID: '26430803'
  publisher: Elsevier
  source: www.ncbi.nlm.nih.gov
  title: Modeling Linkage Disequilibrium Increases Accuracy of Polygenic Risk Scores
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4596916/
  volume: '97'

- id: virmaux_lipschitz_2018
  accessed:
    - year: 2020
      month: 6
      day: 5
  author:
    - family: Virmaux
      given: Aladin
    - family: Scaman
      given: Kevin
  citation-key: virmaux_lipschitz_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 3835–3844
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: >-
    Lipschitz regularity of deep neural networks: analysis and efficient
    estimation
  title-short: Lipschitz regularity of deep neural networks
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/7640-lipschitz-regularity-of-deep-neural-networks-analysis-and-efficient-estimation.pdf

- id: virtanen_scipy_2020
  abstract: >-
    SciPy is an open source scientific computing library for the Python
    programming language. SciPy 1.0 was released in late 2017, about 16 years
    after the original version 0.1 release. SciPy has become a de facto standard
    for leveraging scientific algorithms in the Python programming language,
    with more than 600 unique code contributors, thousands of dependent
    packages, over 100,000 dependent repositories, and millions of downloads per
    year. This includes usage of SciPy in almost half of all machine learning
    projects on GitHub, and usage by high profile projects including LIGO
    gravitational wave analysis and creation of the first-ever image of a black
    hole (M87). The library includes functionality spanning clustering, Fourier
    transforms, integration, interpolation, file I/O, linear algebra, image
    processing, orthogonal distance regression, minimization algorithms, signal
    processing, sparse matrix handling, computational geometry, and statistics.
    In this work, we provide an overview of the capabilities and development
    practices of the SciPy library and highlight some recent technical
    developments.
  author:
    - family: Virtanen
      given: Pauli
    - family: Gommers
      given: Ralf
    - family: Oliphant
      given: Travis E.
    - family: Haberland
      given: Matt
    - family: Reddy
      given: Tyler
    - family: Cournapeau
      given: David
    - family: Burovski
      given: Evgeni
    - family: Peterson
      given: Pearu
    - family: Weckesser
      given: Warren
    - family: Bright
      given: Jonathan
    - family: Walt
      given: Stéfan J.
      non-dropping-particle: van der
    - family: Brett
      given: Matthew
    - family: Wilson
      given: Joshua
    - family: Millman
      given: K. Jarrod
    - family: Mayorov
      given: Nikolay
    - family: Nelson
      given: Andrew R. J.
    - family: Jones
      given: Eric
    - family: Kern
      given: Robert
    - family: Larson
      given: Eric
    - family: Carey
      given: C. J.
    - family: Polat
      given: İlhan
    - family: Feng
      given: Yu
    - family: Moore
      given: Eric W.
    - family: VanderPlas
      given: Jake
    - family: Laxalde
      given: Denis
    - family: Perktold
      given: Josef
    - family: Cimrman
      given: Robert
    - family: Henriksen
      given: Ian
    - family: Quintero
      given: E. A.
    - family: Harris
      given: Charles R.
    - family: Archibald
      given: Anne M.
    - family: Ribeiro
      given: Antônio H.
    - family: Pedregosa
      given: Fabian
    - family: Mulbregt
      given: Paul
      non-dropping-particle: van
    - family: Contributors
      given: SciPy 1.0
  citation-key: virtanen_scipy_2020
  container-title: Nature Methods
  DOI: 10.1038/s41592-019-0686-2
  issue: '3'
  issued:
    - year: 2020
  license: All rights reserved
  page: 261--272
  source: arXiv.org
  title: SciPy 1.0--Fundamental Algorithms for Scientific Computing in Python
  type: article-journal
  volume: '17'

- id: vlachas_datadriven_2018
  abstract: >-
    We introduce a data-driven forecasting method for high-dimensional chaotic
    systems using long short-term memory (LSTM) recurrent neural networks. The
    proposed LSTM neural networks perform inference of high-dimensional
    dynamical systems in their reduced order space and are shown to be an
    effective set of nonlinear approximators of their attractor. We demonstrate
    the forecasting performance of the LSTM and compare it with Gaussian
    processes (GPs) in time series obtained from the Lorenz 96 system, the
    Kuramoto–Sivashinsky equation and a prototype climate model. The LSTM
    networks outperform the GPs in short-term forecasting accuracy in all
    applications considered. A hybrid architecture, extending the LSTM with a
    mean stochastic model (MSM–LSTM), is proposed to ensure convergence to the
    invariant measure. This novel hybrid method is fully data-driven and extends
    the forecasting capabilities of LSTM networks.
  accessed:
    - year: 2022
      month: 6
      day: 15
  author:
    - family: Vlachas
      given: Pantelis R.
    - family: Byeon
      given: Wonmin
    - family: Wan
      given: Zhong Y.
    - family: Sapsis
      given: Themistoklis P.
    - family: Koumoutsakos
      given: Petros
  citation-key: vlachas_datadriven_2018
  container-title: >-
    Proceedings of the Royal Society A: Mathematical, Physical and Engineering
    Sciences
  DOI: 10.1098/rspa.2017.0844
  issue: '2213'
  issued:
    - year: 2018
      month: 5
      day: 31
  page: '20170844'
  publisher: Royal Society
  source: royalsocietypublishing.org (Atypon)
  title: >-
    Data-driven forecasting of high-dimensional chaotic systems with long
    short-term memory networks
  type: article-journal
  URL: https://royalsocietypublishing.org/doi/full/10.1098/rspa.2017.0844
  volume: '474'

- id: voglis_rectangular_2004
  author:
    - family: Voglis
      given: C
    - family: Lagaris
      given: IE
  citation-key: voglis_rectangular_2004
  container-title: WSEAS Conference
  issued:
    - year: 2004
  note: '00000'
  page: 17–19
  title: >-
    A Rectangular Trust Region Dogleg Approach for Unconstrained and Bound
    Constrained Nonlinear Optimization
  type: paper-conference

- id: vonbachmann_ecgbased_2022
  abstract: >-
    Objective: Imbalances of the electrolyte concentration levels in the body
    can lead to catastrophic consequences, but accurate and accessible
    measurements could improve patient outcomes. While blood tests provide
    accurate measurements, they are invasive and the laboratory analysis can be
    slow or inaccessible. In contrast, an electrocardiogram (ECG) is a widely
    adopted tool which is quick and simple to acquire. However, the problem of
    estimating continuous electrolyte concentrations directly from ECGs is not
    well-studied. We therefore investigate if regression methods can be used for
    accurate ECG-based prediction of electrolyte concentrations. Methods: We
    explore the use of deep neural networks (DNNs) for this task. We analyze the
    regression performance across four electrolytes, utilizing a novel dataset
    containing over 290000 ECGs. For improved understanding, we also study the
    full spectrum from continuous predictions to binary classification of
    extreme concentration levels. To enhance clinical usefulness, we finally
    extend to a probabilistic regression approach and evaluate different
    uncertainty estimates. Results: We find that the performance varies
    significantly between different electrolytes, which is clinically justified
    in the interplay of electrolytes and their manifestation in the ECG. We also
    compare the regression accuracy with that of traditional machine learning
    models, demonstrating superior performance of DNNs. Conclusion:
    Discretization can lead to good classification performance, but does not
    help solve the original problem of predicting continuous concentration
    levels. While probabilistic regression demonstrates potential practical
    usefulness, the uncertainty estimates are not particularly well-calibrated.
    Significance: Our study is a first step towards accurate and reliable
    ECG-based prediction of electrolyte concentration levels.
  accessed:
    - year: 2023
      month: 1
      day: 18
  author:
    - family: Von Bachmann
      given: Philipp
    - family: Gedon
      given: Daniel
    - family: Gustafsson
      given: Fredrik K.
    - family: Ribeiro
      given: Antônio H.
    - family: Lampa
      given: Erik
    - family: Gustafsson
      given: Stefan
    - family: Sundström
      given: Johan
    - family: Schön
      given: Thomas B.
  citation-key: vonbachmann_ecgbased_2022
  container-title: arXiv:2212.13890
  DOI: 10.48550/arXiv.2212.13890
  issued:
    - year: 2022
      month: 12
      day: 21
  license: All rights reserved
  source: arXiv.org
  title: >-
    ECG-Based Electrolyte Prediction: Evaluating Regression and Probabilistic
    Methods
  title-short: ECG-Based Electrolyte Prediction
  type: article-journal
  URL: http://arxiv.org/abs/2212.13890

- id: vorontsov_orthogonality_2017
  abstract: >-
    It is well known that it is challenging to train deep neural networks and
    recurrent neural networks for tasks that exhibit long term dependencies. The
    vanishing or exploding gradient problem is a well known issue associated
    with these challenges. One approach to addressing vanishing and exploding
    gradients is to use either soft or hard constraints on weight matrices so as
    to encourage or enforce orthogonality. Orthogonal matrices preserve gradient
    norm during backpropagation and may therefore be a desirable property. This
    paper explores issues with optimization convergence, speed and gradient
    stability when encouraging or enforcing orthogonality. To perform this
    analysis, we propose a weight matrix factorization and parameterization
    strategy through which we can bound matrix norms and therein control the
    degree of expansivity induced during backpropagation. We find that hard
    constraints on orthogonality can negatively affect the speed of convergence
    and model performance.
  accessed:
    - year: 2019
      month: 7
      day: 27
  author:
    - family: Vorontsov
      given: Eugene
    - family: Trabelsi
      given: Chiheb
    - family: Kadoury
      given: Samuel
    - family: Pal
      given: Chris
  citation-key: vorontsov_orthogonality_2017
  container-title: arXiv:1702.00071 [cs]
  issued:
    - year: 2017
      month: 1
      day: 31
  source: arXiv.org
  title: On orthogonality and learning recurrent networks with long term dependencies
  type: article-journal
  URL: http://arxiv.org/abs/1702.00071

- id: voros_iterative_2015
  author:
    - family: Vörös
      given: Jozef
  citation-key: voros_iterative_2015
  container-title: Nonlinear Dynamics
  container-title-short: Nonlinear Dynamics
  DOI: 10.1007/s11071-014-1804-4
  ISSN: 0924-090X
  issue: '3'
  issued:
    - year: 2015
  note: '00000'
  page: 2187-2195
  title: >-
    Iterative identification of nonlinear dynamic systems with output backlash
    using three-block cascade models
  type: article-journal
  volume: '79'

- id: voss_nonlinear_2004
  author:
    - family: Voss
      given: Henning U
    - family: Timmer
      given: Jens
    - family: Kurths
      given: Jürgen
  citation-key: voss_nonlinear_2004
  container-title: International Journal of Bifurcation and Chaos
  DOI: 10.1142/S0218127404010345
  issue: '06'
  issued:
    - year: 2004
  note: '00000'
  page: 1905–1933
  title: >-
    Nonlinear dynamical system identification from uncertain and indirect
    measurements
  type: article-journal
  volume: '14'

- id: vukosavic_digital_2007
  author:
    - family: Vukosavic
      given: S.N.
  citation-key: vukosavic_digital_2007
  collection-title: Power Electronics and Power Systems
  ISBN: 978-0-387-48598-0
  issued:
    - year: 2007
  note: '00093'
  publisher: Springer US
  title: Digital Control of Electrical Drives
  type: book
  URL: https://books.google.com.br/books?id=lmvb2gzE0OEC

- id: vuorio_multimodal_2019
  author:
    - family: Vuorio
      given: Risto
    - family: Sun
      given: Shao-Hua
    - family: Hu
      given: Hexiang
    - family: Lim
      given: Joseph J
  citation-key: vuorio_multimodal_2019
  container-title: Advances in neural information processing systems 32
  editor:
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Beygelzimer
      given: A.
    - family: Buc
      given: F.
      non-dropping-particle: dAlché-
    - family: Fox
      given: E.
    - family: Garnett
      given: R.
  issued:
    - year: 2019
  page: 1-12
  publisher: Curran Associates, Inc.
  title: Multimodal model-agnostic meta-learning via task-aware modulation
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/8296-multimodal-model-agnostic-meta-learning-via-task-aware-modulation.pdf

- id: wachter_implementation_2006
  accessed:
    - year: 2017
      month: 8
      day: 20
  author:
    - family: Wächter
      given: Andreas
    - family: Biegler
      given: Lorenz T.
  citation-key: wachter_implementation_2006
  container-title: Mathematical Programming
  DOI: 10.1007/s10107-004-0559-y
  ISSN: 0025-5610, 1436-4646
  issue: '1'
  issued:
    - year: 2006
      month: 3
  language: en
  note: '04288'
  page: 25-57
  source: CrossRef
  title: >-
    On the implementation of an interior-point filter line-search algorithm for
    large-scale nonlinear programming
  type: article-journal
  URL: http://link.springer.com/10.1007/s10107-004-0559-y
  volume: '106'

- id: wagberg_regularized_2017
  abstract: >-
    Parametric prediction error methods constitute a classical approach to the
    identification of linear dynamic systems with excellent large-sample
    properties. A more recent regularized approach, inspired by machine learning
    and Bayesian methods, has also gained attention. Methods based on this
    approach estimate the system impulse response with excellent small-sample
    properties. In several applications, however, it is desirable to obtain a
    compact representation of the system in the form of a parametric model. By
    viewing the identification of such models as a decision, we develop a
    decision-theoretic formulation of the parametric system identification
    problem that bridges the gap between the classical and regularized
    approaches above. Using the output-error model class as an illustration, we
    show that this decision-theoretic approach leads to a regularized method
    that is robust to small sample-sizes as well as overparameterization.
  author:
    - family: Wågberg
      given: Johan
    - family: Zachariah
      given: Dave
    - family: Schön
      given: Thomas B.
  citation-key: wagberg_regularized_2017
  container-title: arXiv:1710.04009 [cs]
  issued:
    - year: 2017
      month: 10
      day: 11
  note: '00000'
  source: arXiv.org
  title: >-
    Regularized parametric system identification: a decision-theoretic
    formulation
  title-short: Regularized parametric system identification
  type: article-journal
  URL: http://arxiv.org/abs/1710.04009

- id: wagner_ptbxl_2020
  abstract: >-
    Electrocardiography (ECG) is a key non-invasive diagnostic tool for
    cardiovascular diseases which is increasingly supported by algorithms based
    on machine learning. Major obstacles for the development of automatic ECG
    interpretation algorithms are both the lack of public datasets and
    well-defined benchmarking procedures to allow comparison s of different
    algorithms. To address these issues, we put forward PTB-XL, the to-date
    largest freely accessible clinical 12-lead ECG-waveform dataset comprising
    21837 records from 18885 patients of 10 seconds length. The ECG-waveform
    data was annotated by up to two cardiologists as a multi-label dataset,
    where diagnostic labels were further aggregated into super and subclasses.
    The dataset covers a broad range of diagnostic classes including, in
    particular, a large fraction of healthy records. The combination with
    additional metadata on demographics, additional diagnostic statements,
    diagnosis likelihoods, manually annotated signal properties as well as
    suggested folds for splitting training and test sets turns the dataset into
    a rich resource for the development and the evaluation of automatic ECG
    interpretation algorithms.
  accessed:
    - year: 2020
      month: 7
      day: 14
  author:
    - family: Wagner
      given: Patrick
    - family: Strodthoff
      given: Nils
    - family: Bousseljot
      given: Ralf-Dieter
    - family: Kreiseler
      given: Dieter
    - family: Lunze
      given: Fatima I.
    - family: Samek
      given: Wojciech
    - family: Schaeffter
      given: Tobias
  citation-key: wagner_ptbxl_2020
  container-title: Scientific Data
  DOI: 10.1038/s41597-020-0495-6
  ISSN: 2052-4463
  issue: '1'
  issued:
    - year: 2020
      month: 5
      day: 25
  language: en
  license: 2020 The Author(s)
  number: '1'
  page: '154'
  publisher: Nature Publishing Group
  source: www.nature.com
  title: PTB-XL, a large publicly available electrocardiography dataset
  type: article-journal
  URL: https://www.nature.com/articles/s41597-020-0495-6
  volume: '7'

- id: wainwright_highdimensional_2019
  author:
    - family: Wainwright
      given: Martin J
  citation-key: wainwright_highdimensional_2019
  collection-title: Cambridge series on statistical and probabilistic mathematics 48
  ISBN: 978-1-108-62777-1 1-108-62777-3 978-1-108-49802-9
  issued:
    - year: 2019
  publisher: Cambridge University Press
  title: 'High-Simensional Statistics: a non-Asymptotic Viewpoint'
  type: book
  URL: http://gen.lib.rus.ec/book/index.php?md5=b80e7471e94d9e9a29b8b10221f70feb

- id: waltz_interior_2006
  accessed:
    - year: 2017
      month: 8
      day: 20
  author:
    - family: Waltz
      given: Richard A.
    - family: Morales
      given: José Luis
    - family: Nocedal
      given: Jorge
    - family: Orban
      given: Dominique
  citation-key: waltz_interior_2006
  container-title: Mathematical programming
  DOI: 10.1007/s10107-004-0560-5
  issue: '3'
  issued:
    - year: 2006
  note: '00643'
  page: 391–408
  source: Google Scholar
  title: >-
    An interior algorithm for nonlinear optimization that combines line search
    and trust region steps
  type: article-journal
  URL: http://www.springerlink.com/index/7735777P133W3221.pdf
  volume: '107'

- id: waltz_interior_2006a
  accessed:
    - year: 2017
      month: 8
      day: 20
  author:
    - family: Waltz
      given: Richard A.
    - family: Morales
      given: José Luis
    - family: Nocedal
      given: Jorge
    - family: Orban
      given: Dominique
  citation-key: waltz_interior_2006a
  container-title: Mathematical programming
  DOI: 10/dp848r
  issue: '3'
  issued:
    - year: 2006
  page: 391-408
  title: >-
    An Interior Algorithm for Nonlinear Optimization That Combines Line Search
    and Trust Region Steps
  type: article-journal
  volume: '107'

- id: wang_generalized_2015
  abstract: >-
    In this paper, traditional single-hidden layer feedforward network (SLFN) is
    extended to novel generalized SLFN (GSLFN) by employing polynomial functions
    of inputs as output weights connecting randomly generated hidden units with
    corresponding output nodes. The significant contributions of this paper are
    as follows: 1) a primal GSLFN (P-GSLFN) is implemented using randomly
    generated hidden nodes and polynomial output weights whereby the regression
    matrix is augmented by full or partial input variables and only polynomial
    coefficients are to be estimated; 2) a simplified GSLFN (S-GSLFN) is
    realized by decomposing the polynomial output weights of the P-GSLFN into
    randomly generated polynomial nodes and tunable output weights; 3) both P-
    and S-GSLFN are able to achieve universal approximation if the output
    weights are tuned by ridge regression estimators; and 4) by virtue of the
    developed batch and online sequential ridge ELM (BR-ELM and OSR-ELM)
    learning algorithms, high performance of the proposed GSLFNs in terms of
    generalization and learning speed is guaranteed. Comprehensive simulation
    studies and comparisons with standard SLFNs are carried out on real-world
    regression benchmark data sets. Simulation results demonstrate that the
    innovative GSLFNs using BR-ELM and OSR-ELM are superior to standard SLFNs in
    terms of accuracy, training speed, and structure compactness.
  author:
    - family: Wang
      given: N.
    - family: Er
      given: M. J.
    - family: Han
      given: M.
  citation-key: wang_generalized_2015
  container-title: IEEE Transactions on Neural Networks and Learning Systems
  DOI: 10.1109/TNNLS.2014.2334366
  ISSN: 2162-237X
  issue: '6'
  issued:
    - year: 2015
      month: 6
  note: '00073'
  page: 1161-1176
  source: IEEE Xplore
  title: Generalized Single-Hidden Layer Feedforward Networks for Regression Problems
  type: article-journal
  volume: '26'

- id: wang_hybrid_2016
  abstract: >-
    In this paper, a hybrid recursive least squares (HRLS) algorithm for online
    identification using sequential chunk-by-chunk observations is proposed. By
    employing the optimization-based least squares (O-LS), the HRLS can be
    initialized with any chunk of data samples and works successively in two
    recursive procedures for updating the inverse matrix with minimal dimension
    and least rank-deficiency, and thereby contributing to fast and stable
    online identification. Since norms of the output weight and training errors
    are minimized simultaneously, the HRLS achieves high accuracy in terms of
    both generalization and approximation. Simulation studies and comprehensive
    comparisons demonstrate that the HRLS is numerically more stable and
    superior to other algorithms in terms of accuracy and speed.
  author:
    - family: Wang
      given: Ning
    - family: Sun
      given: Jing-Chao
    - family: Er
      given: Meng Joo
    - family: Liu
      given: Yan-Cheng
  citation-key: wang_hybrid_2016
  container-title: Neurocomputing
  container-title-short: Neurocomputing
  DOI: 10.1016/j.neucom.2015.09.090
  ISSN: 0925-2312
  issued:
    - year: 2016
      month: 1
      day: 22
  note: '00014'
  page: 651-660
  source: ScienceDirect
  title: >-
    Hybrid recursive least squares algorithm for online sequential
    identification using data chunks
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S092523121501423X
  volume: '174'

- id: wang_new_2002
  author:
    - family: Wang
      given: J. Y.
  citation-key: wang_new_2002
  container-title: Computers in Cardiology, 2002
  issued:
    - year: 2002
  note: '00031'
  page: 85–88
  publisher: IEEE
  source: Google Scholar
  title: >-
    A new method for evaluating ECG signal quality for multi-lead arrhythmia
    analysis
  type: paper-conference

- id: wang_new_2017
  abstract: >-
    Recently, Recurrent Neural Network becomes a very popular research topic in
    machine learning field. Many new ideas and RNN structures have been
    generated by different authors, including long short term memory (LSTM) RNN
    and Gated Recurrent United (GRU) RNN ([1],[2]), a number of applications
    have also been developed among various research labs or industrial companies
    ([3]-[5]). Most of these schemes, however, are only applicable to machine
    learning problems, or static systems in control field. In this paper, a new
    concept of applying one of the most popular RNN approach - LSTM to identify
    and control dynamic system is to be investigated. Both identification (or
    learning) dynamic system and design of controller based on identification
    are going to be discussed. Also, a new concept of using a convex-based LSTM
    networks for fast learning purpose will be explained in detail. Simulation
    studies will be presented to demonstrated the new LSTM structure performs
    much better than conventional RNN and even single LSTM network.
  author:
    - family: Wang
      given: Yu
  citation-key: wang_new_2017
  container-title: 2017 American Control Conference (ACC)
  DOI: 10.23919/ACC.2017.7963782
  event-title: 2017 American Control Conference (ACC)
  issued:
    - year: 2017
      month: 5
  note: '00011'
  page: 5324-5329
  source: IEEE Xplore
  title: A new concept using LSTM Neural Networks for dynamic system identification
  type: paper-conference

- id: wang_parsimonious_2014
  abstract: >-
    Novel constructive and destructive parsimonious extreme learning machines
    (CP- and DP-ELM) are proposed in this paper. By virtue of the proposed ELMs,
    parsimonious structure and excellent generalization of
    multiinput-multioutput single hidden-layer feedforward networks (SLFNs) are
    obtained. The proposed ELMs are developed by innovative decomposition of the
    recursive orthogonal least squares procedure into sequential partial
    orthogonalization (SPO). The salient features of the proposed approaches are
    as follows: 1) Initial hidden nodes are randomly generated by the ELM
    methodology and recursively orthogonalized into an upper triangular matrix
    with dramatic reduction in matrix size; 2) the constructive SPO in the
    CP-ELM focuses on the partial matrix with the subcolumn of the selected
    regressor including nonzeros as the first column while the destructive SPO
    in the DP-ELM operates on the partial matrix including elements determined
    by the removed regressor; 3) termination criteria for CP- and DP-ELM are
    simplified by the additional residual error reduction method; and 4) the
    output weights of the SLFN need not be solved in the model selection
    procedure and is derived from the final upper triangular equation by
    backward substitution. Both single- and multi-output real-world regression
    data sets are used to verify the effectiveness and superiority of the CP-
    and DP-ELM in terms of parsimonious architecture and generalization
    accuracy. Innovative applications to nonlinear time-series modeling
    demonstrate superior identification results.
  author:
    - family: Wang
      given: N.
    - family: Er
      given: M. J.
    - family: Han
      given: M.
  citation-key: wang_parsimonious_2014
  container-title: IEEE Transactions on Neural Networks and Learning Systems
  DOI: 10.1109/TNNLS.2013.2296048
  ISSN: 2162-237X
  issue: '10'
  issued:
    - year: 2014
      month: 10
  note: '00083'
  page: 1828-1841
  source: IEEE Xplore
  title: >-
    Parsimonious Extreme Learning Machine Using Recursive Orthogonal Least
    Squares
  type: article-journal
  volume: '25'

- id: wang_recovery_2011
  author:
    - family: Wang
      given: Yanfei
    - family: Cao
      given: Jingjie
    - family: Yang
      given: Changchun
  citation-key: wang_recovery_2011
  container-title: Geophysical Journal International
  DOI: 10.1111/j.1365-246X.2011.05130.x
  issue: '1'
  issued:
    - year: 2011
  note: '00000'
  page: 199–213
  title: >-
    Recovery of seismic wavefields based on compressive sensing by an l1-norm
    constrained trust region method and the piecewise random subsampling
  type: article-journal
  volume: '187'

- id: wang_recovery_2011a
  author:
    - family: Wang
      given: Yanfei
    - family: Cao
      given: Jingjie
    - family: Yang
      given: Changchun
  citation-key: wang_recovery_2011a
  container-title: Geophysical Journal International
  issue: '1'
  issued:
    - year: 2011
  note: '00000'
  page: 199-213
  title: >-
    Recovery of Seismic Wavefields Based on Compressive Sensing by an L1-Norm
    Constrained Trust Region Method and the Piecewise Random Subsampling
  type: article-journal
  volume: '187'

- id: wang_regression_2007
  abstract: >-
    The least absolute shrinkage and selection operator ('lasso') has been
    widely used in regression shrinkage and selection. We extend its application
    to the regression model with autoregressive errors. Two types of lasso
    estimators are carefully studied. The first is similar to the traditional
    lasso estimator with only two tuning parameters (one for regression
    coefficients and the other for autoregression coeffi). These tuning
    parameters can be easily calculated via a data-driven method, but the
    resulting lasso estimator may not be fully efficient. To overcome this
    limitation, we propose a second lasso estimator which uses different tuning
    parameters for each coefficient. We show that this modified lasso can
    produce the estimator as efficiently as the oracle. Moreover, we propose an
    algorithm for tuning parameter estimates to obtain the modified lasso
    estimator. Simulation studies demonstrate that the modified estimator is
    superior to the traditional estimator. One empirical example is also
    presented to illustrate the usefulness of lasso estimators. The extension of
    the lasso to the autoregression with exogenous variables model is briefly
    discussed.
  author:
    - family: Wang
      given: Hansheng
    - family: Li
      given: Guodong
    - family: Tsai
      given: Chih-Ling
  citation-key: wang_regression_2007
  container-title: Journal of the Royal Statistical Society. Series B (Statistical Methodology)
  DOI: 10.1111/j.1467-9868.2007.00577.x
  ISSN: 1369-7412
  issue: '1'
  issued:
    - year: 2007
  note: '00225'
  page: 63-78
  source: JSTOR
  title: >-
    Regression Coefficient and Autoregressive Order Shrinkage and Selection Via
    the Lasso
  type: article-journal
  URL: http://www.jstor.org/stable/4623254
  volume: '69'

- id: wang_tight_2022
  abstract: >-
    We provide matching upper and lower bounds of order $\sigma^2/\log(d/n)$ for
    the prediction error of the minimum $\ell_1$-norm interpolator, a.k.a. basis
    pursuit. Our result is tight up to negligible terms when $d \gg n$, and is
    the first to imply asymptotic consistency of noisy minimum-norm
    interpolation for isotropic features and sparse ground truths. Our work
    complements the literature on "benign overfitting" for minimum $\ell_2$-norm
    interpolation, where asymptotic consistency can be achieved only when the
    features are effectively low-dimensional.
  accessed:
    - year: 2022
      month: 5
      day: 6
  author:
    - family: Wang
      given: Guillaume
    - family: Donhauser
      given: Konstantin
    - family: Yang
      given: Fanny
  citation-key: wang_tight_2022
  container-title: arXiv:2111.05987
  issued:
    - year: 2022
      month: 3
      day: 7
  source: arXiv.org
  title: Tight bounds for minimum L1-norm interpolation of noisy data
  type: article-journal
  URL: http://arxiv.org/abs/2111.05987

- id: wang_understanding_2018
  accessed:
    - year: 2018
      month: 12
      day: 6
  author:
    - family: Wang
      given: Liwei
    - family: Hu
      given: Lunjia
    - family: Gu
      given: Jiayuan
    - family: Wu
      given: Yue
    - family: Hu
      given: Zhiqiang
    - family: He
      given: Kun
    - family: Hopcroft
      given: John
  citation-key: wang_understanding_2018
  issued:
    - year: 2018
      month: 10
      day: 28
  language: en
  source: arxiv.org
  title: >-
    Towards Understanding Learning Representations: To What Extent Do Different
    Neural Networks Learn the Same Representation
  title-short: Towards Understanding Learning Representations
  type: article-journal
  URL: https://arxiv.org/abs/1810.11750

- id: warwick_introduction_1996
  author:
    - family: Warwick
      given: K
    - family: Craddock
      given: R
  citation-key: warwick_introduction_1996
  container-title: Decision and Control, 1996., Proceedings of the 35th IEEE Conference on
  issued:
    - year: 1996
  note: '00038'
  page: 464–469
  publisher: IEEE
  title: >-
    An introduction to radial basis functions for system identification. A
    comparison with other neural network methods
  type: paper-conference
  volume: '1'

- id: wasserman_all_2013
  author:
    - family: Wasserman
      given: Larry
  citation-key: wasserman_all_2013
  issued:
    - year: 2013
  note: '03141'
  publisher: Springer Science & Business Media
  source: Google Scholar
  title: 'All of statistics: a concise course in statistical inference'
  title-short: All of statistics
  type: book

- id: wehrmann_hierarchical_
  abstract: >-
    One of the most challenging machine learning problems is a particular case
    of data classiﬁcation in which classes are hierarchically structured and
    objects can be assigned to multiple paths of the class hierarchy at the same
    time. This task is known as hierarchical multi-label classiﬁcation (HMC),
    with applications in text classiﬁcation, image annotation, and in
    bioinformatics problems such as protein function prediction. In this paper,
    we propose novel neural network architectures for HMC called HMCN, capable
    of simultaneously optimizing local and global loss functions for discovering
    local hierarchical class-relationships and global information from the
    entire class hierarchy while penalizing hierarchical violations. We evaluate
    its performance in 21 datasets from four distinct domains, and we compare it
    against the current HMC state-of-the-art approaches. Results show that HMCN
    substantially outperforms all baselines with statistical signiﬁcance,
    arising as the novel state-of-the-art for HMC.
  author:
    - family: Wehrmann
      given: Jônatas
    - family: Cerri
      given: Ricardo
    - family: Barros
      given: Rodrigo C
  citation-key: wehrmann_hierarchical_
  language: en
  page: '10'
  source: Zotero
  title: Hierarchical Multi-Label Classification Networks
  type: article-journal

- id: wen_method_1976
  author:
    - family: Wen
      given: Yi-Kwei
  citation-key: wen_method_1976
  container-title: Journal of the engineering mechanics division
  container-title-short: Journal of the engineering mechanics division
  ISSN: 0044-7951
  issue: '2'
  issued:
    - year: 1976
  page: 249-263
  publisher: American Society of Civil Engineers
  title: Method for random vibration of hysteretic systems
  type: article-journal
  volume: '102'

- id: wen_sharpness_2023
  abstract: >-
    Despite extensive studies, the underlying reason as to why overparameterized
    neural networks can generalize remains elusive. Existing theory shows that
    common stochastic optimizers prefer flatter minimizers of the training loss,
    and thus a natural potential explanation is that flatness implies
    generalization. This work critically examines this explanation. Through
    theoretical and empirical investigation, we identify the following three
    scenarios for two-layer ReLU networks: (1) flatness provably implies
    generalization; (2) there exist non-generalizing flattest models and
    sharpness minimization algorithms fail to generalize poorly, and (3) perhaps
    most strikingly, there exist non-generalizing flattest models, but sharpness
    minimization algorithms still generalize. Our results suggest that the
    relationship between sharpness and generalization subtly depends on the data
    distributions and the model architectures and sharpness minimization
    algorithms do not only minimize sharpness to achieve better generalization.
    This calls for the search for other explanations for the generalization of
    over-parameterized neural networks
  accessed:
    - year: 2023
      month: 12
      day: 12
  author:
    - family: Wen
      given: Kaiyue
    - family: Li
      given: Zhiyuan
    - family: Ma
      given: Tengyu
  citation-key: wen_sharpness_2023
  event-title: NeurIPS
  issued:
    - year: 2023
      month: 11
      day: 2
  language: en
  source: openreview.net
  title: >-
    Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve
    Better Generalization
  type: paper-conference
  URL: https://openreview.net/forum?id=Dkmpa6wCIx

- id: wesely_dry_1979
  accessed:
    - year: 2017
      month: 9
      day: 11
  author:
    - family: Wesely
      given: M. L.
    - family: Hicks
      given: B. B.
  citation-key: wesely_dry_1979
  issued:
    - year: 1979
  publisher: Argonne National Lab., IL (USA)
  source: Google Scholar
  title: Dry deposition and emission of small particles at the surface of the earth
  type: report
  URL: https://www.osti.gov/scitech/biblio/6231439

- id: weste_cmos_2011
  author:
    - family: Weste
      given: Neil H. E.
    - family: Harris
      given: David Money
  call-number: TK7874 .W45 2011
  citation-key: weste_cmos_2011
  edition: 4th ed
  event-place: Boston
  ISBN: 978-0-321-54774-3
  issued:
    - year: 2011
  note: 'OCLC: ocn473447233'
  number-of-pages: '383'
  publisher: Addison Wesley
  publisher-place: Boston
  source: Library of Congress ISBN
  title: 'CMOS VLSI design: a circuits and systems perspective'
  title-short: CMOS VLSI design
  type: book

- id: weston_learning_2003
  abstract: >-
    We consider the problem of reconstructing patterns from a feature map.
    Learning algorithms using kernels to operate in a reproducing kernel Hilbert
    space (RKHS) express their solutions in terms of input points mapped into
    the RKHS. We introduce a technique based on kernel principal component
    analysis and regression to reconstruct corresponding patterns in the input
    space (aka pre-images) and review its performance in several applications
    requiring the construction of pre-images. The introduced technique avoids
    difﬁcult and/or unstable numerical optimization, is easy to implement and,
    unlike previous methods, permits the computation of pre-images in discrete
    input spaces.
  author:
    - family: Weston
      given: Jason
    - family: Schölkopf
      given: Bernhard
    - family: Bakir
      given: Gökhan H
  citation-key: weston_learning_2003
  container-title: Advances in Neural Information Processing Systems 16 (NIPS)
  issued:
    - year: 2003
  language: en
  source: Zotero
  title: Learning to Find Pre-Images
  type: article-journal

- id: widrow_adaptive_1960
  author:
    - family: Widrow
      given: Bernard
    - family: Hoff
      given: Marcian E
  citation-key: widrow_adaptive_1960
  issued:
    - year: 1960
  publisher: Stanford Univ Ca Stanford Electronics Labs
  title: Adaptive switching circuits
  type: report

- id: wigren_coupled_
  abstract: >-
    The following report provides a description of the CE8 coupled electric
    drives laboratory process. A first set of continuous time model structures
    that are suitable to describe the nonlinear dynamics are presented. The data
    sets, which are available in .mat and .csv file formats, are then described
    in detail. The available data sets are short, which constitute a challenge
    when performing identification. In support of future work, Wiener models are
    identified with a recursive algorithm that is parameterized in continuous
    time. This approach reduces the number of parameters to four for
    identification of third order dynamics.
  author:
    - family: Wigren
      given: Torbjörn
    - family: Schoukens
      given: Maarten
  citation-key: wigren_coupled_
  language: en
  page: '11'
  source: Zotero
  title: Coupled Electric Drives Data Set and Reference Models
  type: article-journal

- id: wigren_three_2013
  abstract: >-
    System identification is a fundamentally experimental field of science in
    that it deals with modeling of system dynamics using measured data. Despite
    this fact many algorithms and theoretical results are only tested with
    simulations at the time of publication. One reason for this may be a lack of
    easily available live data. This paper therefore presents three sets of
    data, suitable for development, testing and benchmarking of system
    identification algorithms for nonlinear systems. The data sets are collected
    from laboratory processes that can be described by block - oriented dynamic
    models, and by more general nonlinear difference and differential equation
    models. All data sets are available for free download.
  author:
    - family: Wigren
      given: T.
    - family: Schoukens
      given: J.
  citation-key: wigren_three_2013
  container-title: 2013 European Control Conference (ECC)
  DOI: 10.23919/ECC.2013.6669201
  event-title: 2013 European Control Conference (ECC)
  issued:
    - year: 2013
      month: 7
  page: 2933-2938
  source: IEEE Xplore
  title: >-
    Three free data sets for development and benchmarking in nonlinear system
    identification
  type: paper-conference

- id: wiklicky_nonexistence_1994
  accessed:
    - year: 2021
      month: 5
      day: 28
  author:
    - family: Wiklicky
      given: Herbert
  citation-key: wiklicky_nonexistence_1994
  container-title: Advances in Neural Information Processing Systems
  editor:
    - family: Cowan
      given: J.
    - family: Tesauro
      given: G.
    - family: Alspector
      given: J.
  issued:
    - year: 1994
  publisher: Morgan-Kaufmann
  source: Neural Information Processing Systems
  title: >-
    On the Non-Existence of a Universal Learning Algorithm for Recurrent Neural
    Networks
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/1993/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf
  volume: '6'

- id: willems_diagnostic_1991
  abstract: >-
    BACKGROUND: Computer programs for the interpretation of electrocardiograms
    (ECGs) are now widely used. However, a systematic assessment of various
    computer programs for the interpretation of ECGs has not been performed.

    METHODS: We undertook a large international study to compare the performance
    of nine electrocardiographic computer programs with that of eight
    cardiologists in interpreting ECGs in 1220 clinically validated cases of
    various cardiac disorders. ECGs from the following groups were included in
    the sample: control patients (n = 382); patients with left ventricular
    hypertrophy (n = 183), right ventricular hypertrophy (n = 55), or
    biventricular hypertrophy (n = 53); patients with anterior myocardial
    infarction (n = 170), inferior myocardial infarction (n = 273), or combined
    myocardial infarction (n = 73); and patients with combined infarction and
    hypertrophy (n = 31). The interpretations of the computer programs and the
    cardiologists were compared with the clinical diagnoses made independently
    of the ECGs, and the computer interpretations were compared with those of
    the cardiologists.

    RESULTS: The percentage of ECGs correctly classified by the computer
    programs (median, 91.3 percent) was lower than that of the cardiologists
    (median, 96.0 percent; P less than 0.01). The median sensitivity of the
    computer programs was also significantly lower than that of the
    cardiologists in diagnosing left ventricular hypertrophy (56.6 percent vs.
    63.9 percent, P less than 0.02), right ventricular hypertrophy (31.8 percent
    vs. 46.6 percent, P less than 0.01), anterior myocardial infarction (77.1
    percent vs. 84.9 percent, P less than 0.001), and inferior myocardial
    infarction (58.8 percent vs. 71.7 percent, P less than 0.0001). The median
    total accuracy level (the percentage of correct classifications) was 6.6
    percent lower for the computer programs (69.7 percent) than for the
    cardiologists (76.3 percent; P less than 0.001). However, the performance of
    the best programs nearly matched that of the most accurate cardiologists.

    CONCLUSIONS: Our study shows that some but not all computer programs for the
    interpretation of ECGs perform almost as well as cardiologists in
    identifying seven major cardiac disorders.
  author:
    - family: Willems
      given: J. L.
    - family: Abreu-Lima
      given: C.
    - family: Arnaud
      given: P.
    - family: Bemmel
      given: J. H.
      non-dropping-particle: van
    - family: Brohet
      given: C.
    - family: Degani
      given: R.
    - family: Denis
      given: B.
    - family: Gehring
      given: J.
    - family: Graham
      given: I.
    - family: Herpen
      given: G.
      non-dropping-particle: van
  citation-key: willems_diagnostic_1991
  container-title: The New England Journal of Medicine
  container-title-short: N. Engl. J. Med.
  DOI: 10.1056/NEJM199112193252503
  ISSN: 0028-4793
  issue: '25'
  issued:
    - year: 1991
      month: 12
      day: 19
  language: eng
  page: 1767-1773
  PMID: '1834940'
  source: PubMed
  title: >-
    The diagnostic performance of computer programs for the interpretation of
    electrocardiograms
  type: article-journal
  volume: '325'

- id: willems_testing_1987
  abstract: >-
    In an international project investigators from 21 institutes are trying to
    establish a common reference library and evaluation methods for testing the
    diagnostic performance of various ECG computer programs using ECG
    independent clinical information. Preliminary results indicate that the
    classification accuracy of different programs varies widely.
  author:
    - family: Willems
      given: J. L.
    - family: Abreu-Lima
      given: C.
    - family: Arnaud
      given: P.
    - family: Bemmel
      given: J. H.
      non-dropping-particle: van
    - family: Brohet
      given: C.
    - family: Degani
      given: R.
    - family: Denis
      given: B.
    - family: Graham
      given: I.
    - family: Herpen
      given: G.
      non-dropping-particle: van
    - family: Macfarlane
      given: P. W.
  citation-key: willems_testing_1987
  container-title: Journal of Electrocardiology
  container-title-short: J Electrocardiol
  ISSN: 0022-0736
  issued:
    - year: 1987
      month: 10
  language: eng
  page: 73-77
  source: PubMed
  title: >-
    Testing the performance of ECG computer programs: the CSE diagnostic pilot
    study
  title-short: Testing the performance of ECG computer programs
  type: article-journal
  volume: 20 Suppl

- id: williams_experimental_1989
  accessed:
    - year: 2017
      month: 9
      day: 10
  author:
    - family: Williams
      given: Ronald J.
    - family: Zipser
      given: David
  citation-key: williams_experimental_1989
  container-title: Connection Science
  DOI: 10.1080/09540098908915631
  issue: '1'
  issued:
    - year: 1989
  note: '00369'
  page: 87–111
  source: Google Scholar
  title: Experimental analysis of the real-time recurrent learning algorithm
  type: article-journal
  URL: http://www.tandfonline.com/doi/abs/10.1080/09540098908915631
  volume: '1'

- id: williams_using_2001
  abstract: >-
    A major problem for kernel-based predictors (such as Support Vector Machines
    and Gaussian processes) is that the amount of computation required to find
    the solution scales as O(n ), where n is the number of training examples. We
    show that an approximation to the eigendecomposition of the Gram matrix can
    be computed by the Nyström method (which is used for the numerical solution
    of eigenproblems). This is achieved by carrying out an eigendecomposition on
    a smaller system of size m ¡ n, and then expanding the results back up to n
    dimensions. The computational complexity of a predictor using this
    approximation is O(m n). We report experiments on the USPS and abalone data
    sets and show that we can set m n without any significant decrease in the
    accuracy of the solution.
  author:
    - family: Williams
      given: Christopher K. I.
    - family: Seeger
      given: Matthias
  citation-key: williams_using_2001
  container-title: Advances in neural information processing systems 13 (NIPS 2000)
  editor:
    - family: Leen
      given: T.K.
    - family: Dietterich
      given: T.G.
    - family: Tresp
      given: V.
  issued:
    - year: 2001
  language: English
  page: 682–688
  publisher: MIT Press
  title: Using the nyström method to speed up kernel machines
  type: paper-conference

- id: williams_wavelet_2018
  abstract: >-
    Convolutional Neural Networks continuously advance the progress of 2D and 3D
    image and object classiﬁcation. The steadfast usage of this algorithm
    requires constant evaluation and upgrading of foundational concepts to
    maintain progress. Network regularization techniques typically focus on
    convolutional layer operations, while leaving pooling layer operations
    without suitable options. We introduce Wavelet Pooling as another
    alternative to traditional neighborhood pooling. This method decomposes
    features into a second level decomposition, and discards the ﬁrst-level
    subbands to reduce feature dimensions. This method addresses the overﬁtting
    problem encountered by max pooling, while reducing features in a more
    structurally compact manner than pooling via neighborhood regions.
    Experimental results on four benchmark classiﬁcation datasets demonstrate
    our proposed method outperforms or performs comparatively with methods like
    max, mean, mixed, and stochastic pooling.
  author:
    - family: Williams
      given: Travis
    - family: Li
      given: Robert
  citation-key: williams_wavelet_2018
  issued:
    - year: 2018
  language: en
  page: '12'
  source: Zotero
  title: WAVELET POOLING FOR CONVOLUTIONAL NEURAL NETWORKS
  type: article-journal

- id: wills_stochastic_2018
  abstract: >-
    We provide a numerically robust and fast method capable of exploiting the
    local geometry when solving large-scale stochastic optimisation problems.
    Our key innovation is an auxiliary variable construction coupled with an
    inverse Hessian approximation computed using a receding history of iterates
    and gradients. It is the Markov chain nature of the classic stochastic
    gradient algorithm that enables this development. The construction offers a
    mechanism for stochastic line search adapting the step length. We
    numerically evaluate and compare against current state-of-the-art with
    encouraging performance on real-world benchmark problems where the number of
    observations and unknowns is in the order of millions.
  author:
    - family: Wills
      given: Adrian
    - family: Schön
      given: Thomas
  citation-key: wills_stochastic_2018
  container-title: arXiv:1802.04310 [cs, stat]
  issued:
    - year: 2018
      month: 2
      day: 12
  note: '00001'
  source: arXiv.org
  title: Stochastic quasi-Newton with adaptive step lengths for large-scale problems
  type: article-journal
  URL: http://arxiv.org/abs/1802.04310

- id: wilson_multidecadal_2017
  abstract: "Soil carbon sequestration in agroecosystems could play a key role in climate change mitigation but will require accurate predictions of soil organic carbon (SOC) stocks over spatial scales relevant to land management. Spatial variation in underlying drivers of SOC, such as plant productivity and soil mineralogy, complicates these predictions. Recent advances in the availability of remotely sensed data make it practical to generate multidecadal time series of vegetation indices with high spatial resolution and coverage. However, the utility of such data largely is unknown, only having been tested with shorter (e.g., 1–2\_yr) data summaries. Across a 2,000\_ha subtropical grassland, we found that a long time series (28\_yr) of a vegetation index (Enhanced Vegetation Index; EVI) derived from the Landsat 5 satellite significantly enhanced prediction of spatially varying SOC pools, while a short summary (2\_yr) was an ineffective predictor. EVI was the best predictor for surface SOC (0–5\_cm depth) and total measured SOC stocks (0–15\_cm). The optimum models for SOC in the upper soil layer combined EVI records with elevation and calcium concentration, while deeper SOC was more strongly associated with calcium availability. We demonstrate how data from the open access Landsat archive can predict SOC stocks, a key ecosystem metric, and illustrate the rich variety of analytical approaches that can be applied to long time series of remotely sensed greenness. Overall, our results showed that SOC pools were closely coupled to EVI in this ecosystem, demonstrating that maintenance of higher average green leaf area is correlated with higher SOC. The strong associations of vegetation greenness and calcium concentration with SOC suggest that the ability to sequester additional SOC likely will rely on strategic management of pasture vegetation and soil fertility."
  author:
    - family: Wilson
      given: Chris H.
    - family: Caughlin
      given: T. Trevor
    - family: Rifai
      given: Sami W.
    - family: Boughton
      given: Elizabeth H.
    - family: Mack
      given: Michelle C.
    - family: Flory
      given: S. Luke
  citation-key: wilson_multidecadal_2017
  container-title: Ecological Applications
  container-title-short: Ecol Appl
  DOI: 10.1002/eap.1557
  ISSN: 1939-5582
  issue: '5'
  issued:
    - year: 2017
      month: 7
      day: 1
  language: en
  note: '00007'
  page: 1646-1656
  source: Wiley Online Library
  title: >-
    Multi-decadal time series of remotely sensed vegetation improves prediction
    of soil carbon in a subtropical grassland
  type: article-journal
  URL: http://onlinelibrary.wiley.com/doi/10.1002/eap.1557/abstract
  volume: '27'

- id: winkler_association_2019
  abstract: >-
    <h3>Importance</h3><p>Deep learning convolutional neural networks (CNNs)
    have shown a performance at the level of dermatologists in the diagnosis of
    melanoma. Accordingly, further exploring the potential limitations of CNN
    technology before broadly applying it is of special
    interest.</p><h3>Objective</h3><p>To investigate the association between
    gentian violet surgical skin markings in dermoscopic images and the
    diagnostic performance of a CNN approved for use as a medical device in the
    European market.</p><h3>Design and Setting</h3><p>A cross-sectional analysis
    was conducted from August 1, 2018, to November 30, 2018, using a CNN
    architecture trained with more than 120 000 dermoscopic images of skin
    neoplasms and corresponding diagnoses. The association of gentian violet
    skin markings in dermoscopic images with the performance of the CNN was
    investigated in 3 image sets of 130 melanocytic lesions each (107 benign
    nevi, 23 melanomas).</p><h3>Exposures</h3><p>The same lesions were
    sequentially imaged with and without the application of a gentian violet
    surgical skin marker and then evaluated by the CNN for their probability of
    being a melanoma. In addition, the markings were removed by manually
    cropping the dermoscopic images to focus on the melanocytic
    lesion.</p><h3>Main Outcomes and Measures</h3><p>Sensitivity, specificity,
    and area under the curve (AUC) of the receiver operating characteristic
    (ROC) curve for the CNN’s diagnostic classification in unmarked, marked, and
    cropped images.</p><h3>Results</h3><p>In all, 130 melanocytic lesions (107
    benign nevi and 23 melanomas) were imaged. In unmarked lesions, the CNN
    achieved a sensitivity of 95.7% (95% CI, 79%-99.2%) and a specificity of
    84.1% (95% CI, 76.0%-89.8%). The ROC AUC was 0.969. In marked lesions, an
    increase in melanoma probability scores was observed that resulted in a
    sensitivity of 100% (95% CI, 85.7%-100%) and a significantly reduced
    specificity of 45.8% (95% CI, 36.7%-55.2%,*P* &lt; .001). The ROC AUC was
    0.922. Cropping images led to the highest sensitivity of 100% (95% CI,
    85.7%-100%), specificity of 97.2% (95% CI, 92.1%-99.0%), and ROC AUC of
    0.993. Heat maps created by vanilla gradient descent backpropagation
    indicated that the blue markings were associated with the increased
    false-positive rate.</p><h3>Conclusions and Relevance</h3><p>This study’s
    findings suggest that skin markings significantly interfered with the CNN’s
    correct diagnosis of nevi by increasing the melanoma probability scores and
    consequently the false-positive rate. A predominance of skin markings in
    melanoma training images may have induced the CNN’s association of markings
    with a melanoma diagnosis. Accordingly, these findings suggest that skin
    markings should be avoided in dermoscopic images intended for analysis by a
    CNN.</p><h3>Trial Registration</h3><p>German Clinical Trial Register (DRKS)
    Identifier:DRKS00013570</p>
  accessed:
    - year: 2019
      month: 8
      day: 30
  author:
    - family: Winkler
      given: Julia K.
    - family: Fink
      given: Christine
    - family: Toberer
      given: Ferdinand
    - family: Enk
      given: Alexander
    - family: Deinlein
      given: Teresa
    - family: Hofmann-Wellenhof
      given: Rainer
    - family: Thomas
      given: Luc
    - family: Lallas
      given: Aimilios
    - family: Blum
      given: Andreas
    - family: Stolz
      given: Wilhelm
    - family: Haenssle
      given: Holger A.
  citation-key: winkler_association_2019
  container-title: JAMA Dermatology
  container-title-short: JAMA Dermatol
  DOI: 10/gf6894
  issued:
    - year: 2019
      month: 8
      day: 14
  language: en
  source: jamanetwork.com
  title: >-
    Association Between Surgical Skin Markings in Dermoscopic Images and
    Diagnostic Performance of a Deep Learning Convolutional Neural Network for
    Melanoma Recognition
  type: article-journal
  URL: https://jamanetwork.com/journals/jamadermatology/fullarticle/2740808

- id: wisdom_fullcapacity_2016
  accessed:
    - year: 2019
      month: 9
      day: 19
  author:
    - family: Wisdom
      given: Scott
    - family: Powers
      given: Thomas
    - family: Hershey
      given: John
    - family: Le Roux
      given: Jonathan
    - family: Atlas
      given: Les
  citation-key: wisdom_fullcapacity_2016
  container-title: Advances in Neural Information Processing Systems 29
  editor:
    - family: Lee
      given: D. D.
    - family: Sugiyama
      given: M.
    - family: Luxburg
      given: U. V.
    - family: Guyon
      given: I.
    - family: Garnett
      given: R.
  issued:
    - year: 2016
  page: 4880–4888
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Full-Capacity Unitary Recurrent Neural Networks
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/6327-full-capacity-unitary-recurrent-neural-networks.pdf

- id: wolfe_convergence_1969
  author:
    - family: Wolfe
      given: Philip
  citation-key: wolfe_convergence_1969
  container-title: SIAM review
  issue: '2'
  issued:
    - year: 1969
  note: '00926'
  page: 226–235
  title: Convergence conditions for ascent methods
  type: article-journal
  volume: '11'

- id: wong_lasso_2016
  abstract: >-
    Many theoretical results on estimation of high dimensional time series
    require specifying an underlying data generating model (DGM). Instead, along
    the footsteps of~\cite{wong2017lasso}, this paper relies only on (strict)
    stationarity and $ \beta $-mixing condition to establish consistency of
    lasso when data comes from a $\beta$-mixing process with marginals having
    subgaussian tails. Because of the general assumptions, the data can come
    from DGMs different than standard time series models such as VAR or ARCH.
    When the true DGM is not VAR, the lasso estimates correspond to those of the
    best linear predictors using the past observations. We establish
    non-asymptotic inequalities for estimation and prediction errors of the
    lasso estimates. Together with~\cite{wong2017lasso}, we provide lasso
    guarantees that cover full spectrum of the parameters in specifications of $
    \beta $-mixing subgaussian time series. Applications of these results
    potentially extend to non-Gaussian, non-Markovian and non-linear times
    series models as the examples we provide demonstrate. In order to prove our
    results, we derive a novel Hanson-Wright type concentration inequality for
    $\beta$-mixing subgaussian random vectors that may be of independent
    interest.
  author:
    - family: Wong
      given: Kam Chung
    - family: Li
      given: Zifan
    - family: Tewari
      given: Ambuj
  citation-key: wong_lasso_2016
  container-title: arXiv:1602.04265 [cs, stat]
  issued:
    - year: 2016
      month: 2
      day: 12
  note: '00000'
  source: arXiv.org
  title: >-
    Lasso Guarantees for Time Series Estimation Under Subgaussian Tails and $
    \beta $-Mixing
  type: article-journal
  URL: http://arxiv.org/abs/1602.04265

- id: wong_learning_2020
  abstract: >-
    Although much progress has been made towards robust deep learning, a
    significant gap in robustness remains between real-world perturbations and
    more narrowly defined sets typically studied in adversarial defenses. In
    this paper, we aim to bridge this gap by learning perturbation sets from
    data, in order to characterize real-world effects for robust training and
    evaluation. Specifically, we use a conditional generator that defines the
    perturbation set over a constrained region of the latent space. We formulate
    desirable properties that measure the quality of a learned perturbation set,
    and theoretically prove that a conditional variational autoencoder naturally
    satisfies these criteria. Using this framework, our approach can generate a
    variety of perturbations at different complexities and scales, ranging from
    baseline digit transformations, through common image corruptions, to
    lighting variations. We measure the quality of our learned perturbation sets
    both quantitatively and qualitatively, finding that our models are capable
    of producing a diverse set of meaningful perturbations beyond the limited
    data seen during training. Finally, we leverage our learned perturbation
    sets to learn models which have improved generalization performance and are
    empirically and certifiably robust to adversarial image corruptions and
    adversarial lighting variations. All code and configuration files for
    reproducing the experiments as well as pretrained model weights can be found
    at https://github.com/locuslab/perturbation_learning.
  accessed:
    - year: 2020
      month: 7
      day: 27
  author:
    - family: Wong
      given: Eric
    - family: Kolter
      given: J. Zico
  citation-key: wong_learning_2020
  container-title: arXiv:2007.08450 [cs, stat]
  issued:
    - year: 2020
      month: 7
      day: 16
  source: arXiv.org
  title: Learning perturbation sets for robust machine learning
  type: article-journal
  URL: http://arxiv.org/abs/2007.08450

- id: worldhealthorganization_chagas_2015
  author:
    - literal: World Health Organization
  citation-key: worldhealthorganization_chagas_2015
  container-title: Weekly Epidemiological Record= Relevé épidémiologique hebdomadaire
  container-title-short: Weekly Epidemiological Record= Relevé épidémiologique hebdomadaire
  issue: '06'
  issued:
    - year: 2015
  page: 33-44
  title: >-
    Chagas disease in Latin America: an epidemiological update based on 2010
    estimates
  type: article-journal
  volume: '90'

- id: worldhealthorganization_global_2014
  author:
    - literal: World Health Organization
  citation-key: worldhealthorganization_global_2014
  event-place: Geneva
  ISBN: 978-92-4-156485-4
  issued:
    - year: 2014
  language: en
  note: 'OCLC: 907517003'
  publisher: World Health Organization
  publisher-place: Geneva
  source: Open WorldCat
  title: >-
    Global status report on noncommunicable diseases 2014: attaining the nine
    global noncommunicable diseases targets; a shared responsibility.
  title-short: Global status report on noncommunicable diseases 2014
  type: book

- id: wray_basic_2021
  author:
    - family: Wray
      given: Naomi R
    - family: Lin
      given: Tian
    - family: Austin
      given: Jehannine
    - family: McGrath
      given: John J
    - family: Hickie
      given: Ian B
    - family: Murray
      given: Graham K
    - family: Visscher
      given: Peter M
  citation-key: wray_basic_2021
  container-title: JAMA psychiatry
  container-title-short: JAMA psychiatry
  ISSN: 2168-622X
  issue: '1'
  issued:
    - year: 2021
  page: 101-109
  publisher: American Medical Association
  title: >-
    From basic science to clinical application of polygenic risk scores: a
    primer
  type: article-journal
  volume: '78'

- id: wright_direct_1996
  author:
    - family: Wright
      given: Margaret H
  citation-key: wright_direct_1996
  container-title: Pitman Research Notes in Mathematics Series
  container-title-short: Pitman Research Notes in Mathematics Series
  ISSN: 0269-3674
  issued:
    - year: 1996
  note: '00398'
  page: 191-208
  title: 'Direct search methods: Once scorned, now respectable'
  type: article-journal

- id: wu_coordinate_2008
  abstract: >-
    Imposition of a lasso penalty shrinks parameter estimates toward zero and
    performs continuous model selection. Lasso penalized regression is capable
    of handling linear regression problems where the number of predictors far
    exceeds the number of cases. This paper tests two exceptionally fast
    algorithms for estimating regression coefficients with a lasso penalty. The
    previously known ℓ₂ algorithm is based on cyclic coordinate descent. Our new
    ℓ₁ algorithm is based on greedy coordinate descent and Edgeworth's algorithm
    for ordinary ℓ₁ regression. Each algorithm relies on a tuning constant that
    can be chosen by cross-validation. In some regression problems it is natural
    to group parameters and penalize parameters group by group rather than
    separately. If the group penalty is proportional to the Euclidean norm of
    the parameters of the group, then it is possible to majorize the norm and
    reduce parameter estimation to ℓ₂ regression with a lasso penalty. Thus, the
    existing algorithm can be extended to novel settings. Each of the algorithms
    discussed is tested via either simulated or real data or both. The Appendix
    proves that a greedy form of the ℓ₂ algorithm converges to the minimum value
    of the objective function.
  author:
    - family: Wu
      given: Tong Tong
    - family: Lange
      given: Kenneth
  citation-key: wu_coordinate_2008
  container-title: The Annals of Applied Statistics
  DOI: 10.1214/07-AOAS147
  ISSN: 1932-6157
  issue: '1'
  issued:
    - year: 2008
  note: '00622'
  page: 224-244
  source: JSTOR
  title: Coordinate Descent Algorithms for Lasso Penalized Regression
  type: article-journal
  URL: http://www.jstor.org/stable/30244184
  volume: '2'

- id: wu_how_2018
  accessed:
    - year: 2018
      month: 12
      day: 13
  author:
    - family: Wu
      given: Lei
    - family: Ma
      given: Chao
    - family: E
      given: Weinan
  citation-key: wu_how_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 8289–8298
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: >-
    How SGD Selects the Global Minima in Over-parameterized Learning: A
    Dynamical Stability Perspective
  title-short: How SGD Selects the Global Minima in Over-parameterized Learning
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective.pdf

- id: wu_learning_2019
  abstract: >-
    Linear encoding of sparse vectors is widely popular, but is commonly
    data-independent -- missing any possible extra (but a priori unknown)
    structure beyond sparsity. In this paper we present a new method to learn
    linear encoders that adapt to data, while still performing well with the
    widely used $\ell_1$ decoder. The convex $\ell_1$ decoder prevents gradient
    propagation as needed in standard gradient-based training. Our method is
    based on the insight that unrolling the convex decoder into $T$ projected
    subgradient steps can address this issue. Our method can be seen as a
    data-driven way to learn a compressed sensing measurement matrix. We compare
    the empirical performance of 10 algorithms over 6 sparse datasets (3
    synthetic and 3 real). Our experiments show that there is indeed additional
    structure beyond sparsity in the real datasets; our method is able to
    discover it and exploit it to create excellent reconstructions with fewer
    measurements (by a factor of 1.1-3x) compared to the previous
    state-of-the-art methods. We illustrate an application of our method in
    learning label embeddings for extreme multi-label classification, and
    empirically show that our method is able to match or outperform the
    precision scores of SLEEC, which is one of the state-of-the-art
    embedding-based approaches.
  accessed:
    - year: 2020
      month: 7
      day: 20
  author:
    - family: Wu
      given: Shanshan
    - family: Dimakis
      given: Alexandros G.
    - family: Sanghavi
      given: Sujay
    - family: Yu
      given: Felix X.
    - family: Holtmann-Rice
      given: Daniel
    - family: Storcheus
      given: Dmitry
    - family: Rostamizadeh
      given: Afshin
    - family: Kumar
      given: Sanjiv
  citation-key: wu_learning_2019
  container-title: arXiv:1806.10175 [cs, math, stat]
  issued:
    - year: 2019
      month: 7
      day: 2
  source: arXiv.org
  title: Learning a Compressed Sensing Measurement Matrix via Gradient Unrolling
  type: article-journal
  URL: http://arxiv.org/abs/1806.10175

- id: wu_optical_2007
  author:
    - family: Wu
      given: Tao T
    - family: Qu
      given: Jianan Y
  citation-key: wu_optical_2007
  container-title: Optics express
  DOI: 10.1364/OE.15.010421
  issue: '16'
  issued:
    - year: 2007
  note: '00025'
  page: 10421–10426
  title: >-
    Optical imaging for medical diagnosis based on active stereo vision and
    motion tracking
  type: article-journal
  volume: '15'

- id: xia_internet_2012
  author:
    - family: Xia
      given: Feng
    - family: Yang
      given: Laurence T
    - family: Wang
      given: Lizhe
    - family: Vinel
      given: Alexey
  citation-key: xia_internet_2012
  container-title: International journal of communication systems
  container-title-short: International journal of communication systems
  ISSN: 1074-5351
  issue: '9'
  issued:
    - year: 2012
  page: '1101'
  title: Internet of things
  type: article-journal
  volume: '25'

- id: xie_adversarial_2020
  author:
    - family: Xie
      given: Cihang
    - family: Tan
      given: Mingxing
    - family: Gong
      given: Boqing
    - family: Wang
      given: Jiang
    - family: Yuille
      given: Alan L.
    - family: Le
      given: Quoc V.
  citation-key: xie_adversarial_2020
  container-title: >-
    Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition (CVPR)
  issued:
    - year: 2020
      month: 6
  title: Adversarial examples improve image recognition
  type: paper-conference

- id: xie_identification_2013
  abstract: >-
    An identification method is developed for nonlinear hysteretic systems by
    use of artificial neural network in the paper. Employing the Bouc–Wen
    differential model widely used for memory-type nonlinear hysteretic systems,
    the approach sets up a Bouc–Wen model-based neural network. The weights of
    the designed specifically network correspond to the Bouc–Wen model
    parameters and are thus physical ones. Taking advantage of powerful function
    approximation capability of neural network, the nonlinear hysteretic systems
    can be identified with the proposed approach by network training. The
    identification scheme is validated by a simulated case and thereafter
    applied to modeling of a wire cable vibration isolation experimental system.
    The results show that the presented identification method can identify the
    nonlinear hysteretic systems with high accuracy.
  author:
    - family: Xie
      given: S.L.
    - family: Zhang
      given: Y.H.
    - family: Chen
      given: C.H.
    - family: Zhang
      given: X.N.
  citation-key: xie_identification_2013
  container-title: Mechanical Systems and Signal Processing
  container-title-short: Mechanical Systems and Signal Processing
  DOI: 10/f4kjsq
  ISSN: 0888-3270
  issue: '1'
  issued:
    - year: 2013
      month: 1
      day: 1
  page: 76-87
  title: Identification of nonlinear hysteretic systems by artificial neural network
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0888327012003032
  volume: '34'

- id: xie_selftraining_2020
  abstract: >-
    We present a simple self-training method that achieves 88.4% top-1 accuracy
    on ImageNet, which is 2.0% better than the state-of-the-art model that
    requires 3.5B weakly labeled Instagram images. On robustness test sets, it
    improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C
    mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean ﬂip
    rate from 27.8 to 12.2.
  accessed:
    - year: 2020
      month: 10
      day: 16
  author:
    - family: Xie
      given: Qizhe
    - family: Luong
      given: Minh-Thang
    - family: Hovy
      given: Eduard
    - family: Le
      given: Quoc V.
  citation-key: xie_selftraining_2020
  container-title: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
  DOI: 10.1109/CVPR42600.2020.01070
  event-place: Seattle, WA, USA
  event-title: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
  ISBN: 978-1-72817-168-5
  issued:
    - year: 2020
      month: 6
  language: en
  page: 10684-10695
  publisher: IEEE
  publisher-place: Seattle, WA, USA
  source: DOI.org (Crossref)
  title: Self-Training With Noisy Student Improves ImageNet Classification
  type: paper-conference
  URL: https://ieeexplore.ieee.org/document/9156610/

- id: xie_smooth_2020
  abstract: >-
    It is commonly believed that networks cannot be both accurate and robust,
    that gaining robustness means losing accuracy. It is also generally believed
    that, unless making networks larger, network architectural elements would
    otherwise matter little in improving adversarial robustness. Here we present
    evidence to challenge these common beliefs by a careful study about
    adversarial training. Our key observation is that the widely-used ReLU
    activation function significantly weakens adversarial training due to its
    non-smooth nature. Hence we propose smooth adversarial training (SAT), in
    which we replace ReLU with its smooth approximations to strengthen
    adversarial training. The purpose of smooth activation functions in SAT is
    to allow it to find harder adversarial examples and compute better gradient
    updates during adversarial training. Compared to standard adversarial
    training, SAT improves adversarial robustness for "free", i.e., no drop in
    accuracy and no increase in computational cost. For example, without
    introducing additional computations, SAT significantly enhances ResNet-50's
    robustness from 33.0% to 42.3%, while also improving accuracy by 0.9% on
    ImageNet. SAT also works well with larger networks: it helps EfficientNet-L1
    to achieve 82.2% accuracy and 58.6% robustness on ImageNet, outperforming
    the previous state-of-the-art defense by 9.5% for accuracy and 11.6% for
    robustness.
  accessed:
    - year: 2020
      month: 7
      day: 7
  author:
    - family: Xie
      given: Cihang
    - family: Tan
      given: Mingxing
    - family: Gong
      given: Boqing
    - family: Yuille
      given: Alan
    - family: Le
      given: Quoc V.
  citation-key: xie_smooth_2020
  container-title: arXiv:2006.14536 [cs]
  issued:
    - year: 2020
      month: 6
      day: 25
  source: arXiv.org
  title: Smooth Adversarial Training
  type: article-journal
  URL: http://arxiv.org/abs/2006.14536

- id: xie_squareroot_2018
  accessed:
    - year: 2018
      month: 2
      day: 22
  author:
    - family: Xie
      given: Fang
    - family: Xiao
      given: Zhijie
  citation-key: xie_squareroot_2018
  container-title: Journal of Time Series Analysis
  DOI: 10.1111/jtsa.12278
  ISSN: '01439782'
  issue: '2'
  issued:
    - year: 2018
      month: 3
  language: en
  note: '00000'
  page: 212-238
  source: CrossRef
  title: >-
    Square-Root LASSO for High-Dimensional Sparse Linear Systems with Weakly
    Dependent Errors: Square-root LASSO with time series errors
  title-short: >-
    Square-Root LASSO for High-Dimensional Sparse Linear Systems with Weakly
    Dependent Errors
  type: article-journal
  URL: http://doi.wiley.com/10.1111/jtsa.12278
  volume: '39'

- id: xing_adversarially_2021
  abstract: >-
    Adversarial robust learning aims to design algorithms that are robust to
    small adversarial perturbations on input variables. Beyond the existing
    studies on the predictive performance to adversarial samples, our goal is to
    understand statistical properties of adversarial robust estimates and
    analyze adversarial risk in the setup of linear regression models. By
    discovering the statistical minimax rate of convergence of adversarial
    robust estimators, we emphasize the importance of incorporating model
    information, e.g., sparsity, in adversarial robust learning. Further, we
    reveal an explicit connection of adversarial and standard estimates, and
    propose a straightforward two-stage adversarial training framework, which
    facilitates to utilize model structure information to improve adversarial
    robustness. In theory, the consistency of the adversarial robust estimator
    is proven and its Bahadur representation is also developed for the
    statistical inference purpose. The proposed estimator converges in a sharp
    rate under either low-dimensional or sparse scenario. Moreover, our theory
    confirms two phenomena in adversarial robust learning: adversarial
    robustness hurts generalization, and unlabeled data help improve the
    generalization. In the end, we conduct numerical simulations to verify our
    theory.
  author:
    - family: Xing
      given: Yue
    - family: Zhang
      given: Ruizhi
    - family: Cheng
      given: Guang
  citation-key: xing_adversarially_2021
  container-title: >-
    Proceedings of the International Conference on Artificial Intelligence and
    Statistics (AISTATS)
  editor:
    - family: Banerjee
      given: Arindam
    - family: Fukumizu
      given: Kenji
  issued:
    - year: 2021
      month: 4
      day: 13
    - year: 2021
      month: 4
      day: 15
  page: 514–522
  title: Adversarially robust estimate and risk analysis in linear regression
  type: paper-conference
  URL: https://proceedings.mlr.press/v130/xing21c.html
  volume: '130'

- id: xing_generalization_2021
  abstract: >-
    Modern machine learning and deep learning models are shown to be vulnerable
    when testing data are slightly perturbed. Theoretical studies of adversarial
    training algorithms mostly focus on their adversarial training losses or
    local convergence properties. In contrast, this paper studies the
    generalization performance of a generic adversarial training algorithm.
    Specifically, we consider linear regression models and two-layer neural
    networks (with lazy training) using squared loss under low-dimensional
    regime and high-dimensional regime. In the former regime, after overcoming
    the non-smoothness of adversarial training, the adversarial risk of the
    trained models will converge to the minimal adversarial risk. In the latter
    regime, we discover that data interpolation prevents the adversarial robust
    estimator from being consistent (i.e. converge in probability). Therefore,
    inspired by successes of the least absolute shrinkage and selection operator
    (LASSO), we incorporate the 1L1\mathcal{L}_1 penalty in the high
    dimensional adversarial learning, and show that it leads to consistent
    adversarial robust estimation. A series of numerical studies are conducted
    to demonstrate that how the smoothness and 1L1\mathcal{L}_1 penalization
    help to improve the adversarial robustness of DNN models.
  accessed:
    - year: 2022
      month: 8
      day: 15
  author:
    - family: Xing
      given: Yue
    - family: Song
      given: Qifan
    - family: Cheng
      given: Guang
  citation-key: xing_generalization_2021
  container-title: >-
    Proceedings of the International Conference on Artificial Intelligence and
    Statistics
  event-title: International Conference on Artificial Intelligence and Statistics
  ISSN: 2640-3498
  issued:
    - year: 2021
      month: 3
      day: 18
  language: en
  page: 505-513
  source: proceedings.mlr.press
  title: On the Generalization Properties of Adversarial Training
  type: paper-conference
  URL: https://proceedings.mlr.press/v130/xing21b.html

- id: xu_robust_2008
  author:
    - family: Xu
      given: Huan
    - family: Caramanis
      given: Constantine
    - family: Mannor
      given: Shie
  citation-key: xu_robust_2008
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2008
  title: Robust regression and lasso
  type: article-journal
  volume: '21'

- id: xu_robustness_2009
  author:
    - family: Xu
      given: Huan
    - family: Caramanis
      given: Constantine
    - family: Mannor
      given: Shie
  citation-key: xu_robustness_2009
  container-title: Journal of Machine Learning Research
  issue: '51'
  issued:
    - year: 2009
  page: 1485–1510
  title: Robustness and regularization of support vector machines
  type: article-journal
  URL: http://jmlr.org/papers/v10/xu09b.html
  volume: '10'

- id: xu_robustness_2012
  abstract: >-
    We derive generalization bounds for learning algorithms based on their
    robustness: the property that if a testing sample is “similar” to a training
    sample, then the testing error is close to the training error. This provides
    a novel approach, different from complexity or stability arguments, to study
    generalization of learning algorithms. One advantage of the robustness
    approach, compared to previous methods, is the geometric intuition it
    conveys. Consequently, robustness-based analysis is easy to extend to
    learning in non-standard setups such as Markovian samples or quantile loss.
    We further show that a weak notion of robustness is both sufﬁcient and
    necessary for generalizability, which implies that robustness is a
    fundamental property that is required for learning algorithms to work.
  accessed:
    - year: 2020
      month: 7
      day: 14
  author:
    - family: Xu
      given: Huan
    - family: Mannor
      given: Shie
  citation-key: xu_robustness_2012
  container-title: Machine Learning
  container-title-short: Mach Learn
  DOI: 10.1007/s10994-011-5268-1
  ISSN: 0885-6125, 1573-0565
  issue: '3'
  issued:
    - year: 2012
      month: 3
  language: en
  page: 391-423
  source: DOI.org (Crossref)
  title: Robustness and generalization
  type: article-journal
  URL: http://link.springer.com/10.1007/s10994-011-5268-1
  volume: '86'

- id: yan_narmax_2015
  author:
    - family: Yan
      given: Jinyao
    - family: Deller
      given: JR
  citation-key: yan_narmax_2015
  container-title: Signal Processing
  issued:
    - year: 2015
  note: '00009'
  title: NARMAX model identification using a set-theoretic evolutionary approach
  type: article-journal

- id: yang_feedback_2013
  abstract: >-
    The feedback particle filter introduced in this paper is a new approach to
    approximate nonlinear filtering, motivated by techniques from mean-field
    game theory. The filter is defined by an ensemble of controlled stochastic
    systems (the particles). Each particle evolves under feedback control based
    on its own state, and features of the empirical distribution of the
    ensemble. The feedback control law is obtained as the solution to an optimal
    control problem, in which the optimization criterion is the Kullback-Leibler
    divergence between the actual posterior, and the common posterior of any
    particle. The following conclusions are obtained for diffusions with
    continuous observations: 1) The optimal control solution is exact: The two
    posteriors match exactly, provided they are initialized with identical
    priors. 2) The optimal filter admits an innovation error-based gain feedback
    structure. 3) The optimal feedback gain is obtained via a solution of an
    Euler-Lagrange boundary value problem; the feedback gain equals the Kalman
    gain in the linear Gaussian case. Numerical algorithms are introduced and
    implemented in two general examples, and a neuroscience application
    involving coupled oscillators. In some cases it is found that the filter
    exhibits significantly lower variance when compared to the bootstrap
    particle filter.
  author:
    - family: Yang
      given: T.
    - family: Mehta
      given: P. G.
    - family: Meyn
      given: S. P.
  citation-key: yang_feedback_2013
  container-title: IEEE Transactions on Automatic Control
  DOI: 10/f5bvxs
  ISSN: 0018-9286
  issue: '10'
  issued:
    - year: 2013
      month: 10
  page: 2465-2480
  source: IEEE Xplore
  title: Feedback Particle Filter
  type: article-journal
  volume: '58'

- id: yang_nystrom_2012
  accessed:
    - year: 2021
      month: 1
      day: 29
  author:
    - family: Yang
      given: Tianbao
    - family: Li
      given: Yu-feng
    - family: Mahdavi
      given: Mehrdad
    - family: Jin
      given: Rong
    - family: Zhou
      given: Zhi-Hua
  citation-key: yang_nystrom_2012
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2012
  language: en
  page: 476-484
  source: proceedings.neurips.cc
  title: >-
    Nyström Method vs Random Fourier Features: A Theoretical and Empirical
    Comparison
  title-short: Nyström Method vs Random Fourier Features
  type: article-journal
  URL: >-
    https://proceedings.neurips.cc/paper/2012/hash/621bf66ddb7c962aa0d22ac97d69b793-Abstract.html
  volume: '25'

- id: yang_tensor_2020
  abstract: >-
    We prove that a randomly initialized neural network of any architecture has
    its Tangent Kernel (NTK) converge to a deterministic limit, as the network
    widths tend to inﬁnity. We demonstrate how to calculate this limit. In prior
    literature, the heuristic study of neural network gradients often assumes
    every weight matrix used in forward propagation is independent from its
    transpose used in backpropagation [58]. This is known as the gradient
    independence assumption (GIA). We identify a commonly satisﬁed condition,
    which we call Simple GIA Check, such that the NTK limit calculation based on
    GIA is correct. Conversely, when Simple GIA Check fails, we show GIA can
    result in wrong answers. Our material here presents the NTK results of Yang
    [63] in a friendly manner and showcases the tensor programs technique for
    understanding wide neural networks. We provide reference implementations of
    inﬁnite-width NTKs of recurrent neural network, transformer, and batch
    normalization at https://github.com/thegregyang/NTK4A.
  accessed:
    - year: 2022
      month: 7
      day: 27
  author:
    - family: Yang
      given: Greg
  citation-key: yang_tensor_2020
  issued:
    - year: 2020
      month: 11
      day: 29
  language: en
  number: arXiv:2006.14548
  publisher: arXiv
  source: arXiv.org
  title: 'Tensor Programs II: Neural Tangent Kernel for Any Architecture'
  title-short: Tensor Programs II
  type: article
  URL: http://arxiv.org/abs/2006.14548

- id: yang_wide_2019
  abstract: >-
    Wide neural networks with random weights and biases are Gaussian processes,
    as observed by Neal (1995) for shallow networks, and more recently by Lee et
    al.~(2018) and Matthews et al.~(2018) for deep fully-connected networks, as
    well as by Novak et al.~(2019) and Garriga-Alonso et al.~(2019) for deep
    convolutional networks.

    We show that this Neural Network-Gaussian Process correspondence
    surprisingly extends to all modern feedforward or recurrent neural networks
    composed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph)
    convolution, pooling, skip connection, attention, batch normalization,
    and/or layer normalization.

    More generally, we introduce a language for expressing neural network
    computations, and our result encompasses all such expressible neural
    networks.

    This work serves as a tutorial on the \emph{tensor programs} technique
    formulated in Yang (2019) and elucidates the Gaussian Process results
    obtained there.

    We provide open-source implementations of the Gaussian Process kernels of
    simple RNN, GRU, transformer, and batchnorm+ReLU network at 
    github.com/thegregyang/GP4A.

    Please see our arxiv version for the complete and up-to-date version of this
    paper.
  accessed:
    - year: 2022
      month: 7
      day: 26
  author:
    - family: Yang
      given: Greg
  citation-key: yang_wide_2019
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2019
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: >-
    Wide Feedforward or Recurrent Neural Networks of Any Architecture are
    Gaussian Processes
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2019/hash/5e69fda38cda2060819766569fd93aa5-Abstract.html
  volume: '32'

- id: yansongwang_discrete_2011
  author:
    - literal: Yansong Wang
    - literal: Gongqi Shen
    - literal: Qiang Zhu
    - literal: Weiwei Wu
  citation-key: yansongwang_discrete_2011
  ISBN: 978-953-307-185-5
  issued:
    - year: 2011
  language: en.
  note: |-
    00006 
    OCLC: 884041491
  publisher: INTECH Open Access Publisher
  source: Open WorldCat
  title: Discrete Wavelet Transfom for Nonstationary Signal Processing.
  type: book

- id: yeh_knowledge_2009
  abstract: >-
    The objective of this paper is to introduce a comprehensive methodology to
    discover the knowledge for selecting targets for direct marketing from a
    database. This study expanded RFM model by including two parameters, time
    since first purchase and churn probability. Using Bernoulli sequence in
    probability theory, we derive out the formula that can estimate the
    probability that one customer will buy at the next time, and the expected
    value of the total number of times that the customer will buy in the future.
    This study also proposed the methodology to estimate the unknown parameters
    in the formula. This methodology leads to more efficient and accurate
    selection procedures than the existing ones. In the empirical part we
    examine a case study, blood transfusion service, to show that our
    methodology has greater predictive accuracy than traditional RFM approaches.
  accessed:
    - year: 2024
      month: 5
      day: 21
  author:
    - family: Yeh
      given: I-Cheng
    - family: Yang
      given: King-Jang
    - family: Ting
      given: Tao-Ming
  citation-key: yeh_knowledge_2009
  container-title: Expert Systems with Applications
  container-title-short: Expert Systems with Applications
  DOI: 10.1016/j.eswa.2008.07.018
  ISSN: 0957-4174
  issue: 3, Part 2
  issued:
    - year: 2009
      month: 4
      day: 1
  page: 5866-5871
  source: ScienceDirect
  title: Knowledge discovery on RFM model using Bernoulli sequence
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0957417408004508
  volume: '36'

- id: yildiz_revisiting_2012
  abstract: >-
    An echo state network (ESN) consists of a large, randomly connected neural
    network, the reservoir, which is driven by an input signal and projects to
    output units. During training, only the connections from the reservoir to
    these output units are learned. A key requisite for output-only training is
    the echo state property (ESP), which means that the effect of initial
    conditions should vanish as time passes. In this paper, we use analytical
    examples to show that a widely used criterion for the ESP, the spectral
    radius of the weight matrix being smaller than unity, is not sufficient to
    satisfy the echo state property. We obtain these examples by investigating
    local bifurcation properties of the standard ESNs. Moreover, we provide new
    sufficient conditions for the echo state property of standard sigmoid and
    leaky integrator ESNs. We furthermore suggest an improved technical
    definition of the echo state property, and discuss what practicians should
    (and should not) observe when they optimize their reservoirs for specific
    tasks.
  accessed:
    - year: 2019
      month: 10
      day: 28
  author:
    - family: Yildiz
      given: Izzet B.
    - family: Jaeger
      given: Herbert
    - family: Kiebel
      given: Stefan J.
  citation-key: yildiz_revisiting_2012
  container-title: Neural Networks
  container-title-short: Neural Networks
  DOI: 10.1016/j.neunet.2012.07.005
  ISSN: 0893-6080
  issued:
    - year: 2012
      month: 11
      day: 1
  page: 1-9
  source: ScienceDirect
  title: Re-visiting the echo state property
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0893608012001852
  volume: '35'

- id: yin_fourier_2019
  abstract: >-
    Achieving robustness to distributional shift is a longstanding and
    challenging goal of computer vision. Data augmentation is a commonly used
    approach for improving robustness, however robustness gains are typically
    not uniform across corruption types. Indeed increasing performance in the
    presence of random noise is often met with reduced performance on other
    corruptions such as contrast change. Understanding when and why these sorts
    of trade-offs occur is a crucial step towards mitigating them. Towards this
    end, we investigate recently observed trade-offs caused by Gaussian data
    augmentation and adversarial training. We find that both methods improve
    robustness to corruptions that are concentrated in the high frequency domain
    while reducing robustness to corruptions that are concentrated in the low
    frequency domain. This suggests that one way to mitigate these trade-offs
    via data augmentation is to use a more diverse set of augmentations. Towards
    this end we observe that AutoAugment, a recently proposed data augmentation
    policy optimized for clean accuracy, achieves state-of-the-art robustness on
    the CIFAR-10-C benchmark.
  accessed:
    - year: 2020
      month: 7
      day: 25
  author:
    - family: Yin
      given: Dong
    - family: Lopes
      given: Raphael Gontijo
    - family: Shlens
      given: Jonathon
    - family: Cubuk
      given: Ekin D.
    - family: Gilmer
      given: Justin
  citation-key: yin_fourier_2019
  container-title: arXiv:1906.08988 [cs, stat]
  issued:
    - year: 2019
      month: 10
      day: 29
  source: arXiv.org
  title: A Fourier Perspective on Model Robustness in Computer Vision
  type: article-journal
  URL: http://arxiv.org/abs/1906.08988

- id: yin_rademacher_2019
  abstract: >-
    Many machine learning models are vulnerable to adversarial attacks; for
    example, adding adversarial perturbations that are imperceptible to humans
    can often make machine learning models produce wrong predictions with high
    confidence; moreover, although we may obtain robust models on the training
    dataset via adversarial training, in some problems the learned models cannot
    generalize well to the test data. In this paper, we focus on ℓ∞ℓ∞\ell_\infty
    attacks, and study the adversarially robust generalization problem through
    the lens of Rademacher complexity. For binary linear classifiers, we prove
    tight bounds for the adversarial Rademacher complexity, and show that the
    adversarial Rademacher complexity is never smaller than its natural
    counterpart, and it has an unavoidable dimension dependence, unless the
    weight vector has bounded ℓ1ℓ1\ell_1 norm, and our results also extend to
    multi-class linear classifiers; in addition, for (nonlinear) neural
    networks, we show that the dimension dependence in the adversarial
    Rademacher complexity also exists. We further consider a surrogate
    adversarial loss for one-hidden layer ReLU network and prove margin bounds
    for this setting. Our results indicate that having ℓ1ℓ1\ell_1 norm
    constraints on the weight matrices might be a potential way to improve
    generalization in the adversarial setting. We demonstrate experimental
    results that validate our theoretical findings.
  accessed:
    - year: 2022
      month: 4
      day: 12
  author:
    - family: Yin
      given: Dong
    - family: Kannan
      given: Ramchandran
    - family: Bartlett
      given: Peter
  citation-key: yin_rademacher_2019
  container-title: Proceeding of the International Conference on Machine Learning
  event-title: International Conference on Machine Learning
  issued:
    - year: 2019
  page: 7085-7094
  source: proceedings.mlr.press
  title: Rademacher Complexity for Adversarially Robust Generalization
  type: paper-conference
  URL: https://proceedings.mlr.press/v97/yin19b.html

- id: yoon_bayesian_2018
  accessed:
    - year: 2018
      month: 12
      day: 4
  author:
    - family: Yoon
      given: Jaesik
    - family: Kim
      given: Taesup
    - family: Dia
      given: Ousmane
    - family: Kim
      given: Sungwoong
    - family: Bengio
      given: Yoshua
    - family: Ahn
      given: Sungjin
  citation-key: yoon_bayesian_2018
  container-title: Advances in Neural Information Processing Systems 31
  editor:
    - family: Bengio
      given: S.
    - family: Wallach
      given: H.
    - family: Larochelle
      given: H.
    - family: Grauman
      given: K.
    - family: Cesa-Bianchi
      given: N.
    - family: Garnett
      given: R.
  issued:
    - year: 2018
  page: 7342–7352
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Bayesian Model-Agnostic Meta-Learning
  type: chapter
  URL: http://papers.nips.cc/paper/7963-bayesian-model-agnostic-meta-learning.pdf

- id: yoon_penalized_2013
  abstract: >-
    Penalized regression methods have recently gained enormous attention in
    statistics and the field of machine learning due to their ability of
    reducing the prediction error and identifying important variables at the
    same time. Numerous studies have been conducted for penalized regression,
    but most of them are limited to the case when the data are independently
    observed. In this paper, we study a variable selection problem in penalized
    regression models with autoregressive (AR) error terms. We consider three
    estimators, adaptive least absolute shrinkage and selection operator,
    bridge, and smoothly clipped absolute deviation, and propose a computational
    algorithm that enables us to select a relevant set of variables and also the
    order of AR error terms simultaneously. In addition, we provide their
    asymptotic properties such as consistency, selection consistency, and
    asymptotic normality. The performances of the three estimators are compared
    with one another using simulated and real examples.
  author:
    - family: Yoon
      given: Young Joo
    - family: Park
      given: Cheolwoo
    - family: Lee
      given: Taewook
  citation-key: yoon_penalized_2013
  container-title: Journal of Statistical Computation and Simulation
  DOI: 10.1080/00949655.2012.669383
  ISSN: 0094-9655
  issue: '9'
  issued:
    - year: 2013
      month: 9
      day: 1
  note: '00022'
  page: 1756-1772
  source: Taylor and Francis+NEJM
  title: Penalized regression models with autoregressive error terms
  type: article-journal
  URL: https://doi.org/10.1080/00949655.2012.669383
  volume: '83'

- id: yosinski_how_2014
  accessed:
    - year: 2020
      month: 9
      day: 1
  author:
    - family: Yosinski
      given: Jason
    - family: Clune
      given: Jeff
    - family: Bengio
      given: Yoshua
    - family: Lipson
      given: Hod
  citation-key: yosinski_how_2014
  container-title: Advances in Neural Information Processing Systems 27
  editor:
    - family: Ghahramani
      given: Z.
    - family: Welling
      given: M.
    - family: Cortes
      given: C.
    - family: Lawrence
      given: N. D.
    - family: Weinberger
      given: K. Q.
  issued:
    - year: 2014
  page: 3320–3328
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: How transferable are features in deep neural networks?
  type: chapter
  URL: >-
    http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf

- id: you_drawing_2020
  abstract: >-
    (Frankle & Carbin, 2019) shows that there exist winning tickets (small but
    critical subnetworks) for dense, randomly initialized networks, that can be
    trained alone to achieve comparable accuracies to the latter in a similar
    number of iterations. However, the identification of these winning tickets
    still requires the costly train-prune-retrain process, limiting their
    practical benefits. In this paper, we discover for the first time that the
    winning tickets can be identified at the very early training stage, which we
    term as early-bird (EB) tickets, via low-cost training schemes (e.g., early
    stopping and low-precision training) at large learning rates. Our finding of
    EB tickets is consistent with recently reported observations that the key
    connectivity patterns of neural networks emerge early. Furthermore, we
    propose a mask distance metric that can be used to identify EB tickets with
    low computational overhead, without needing to know the true winning tickets
    that emerge after the full training. Finally, we leverage the existence of
    EB tickets and the proposed mask distance to develop efficient training
    methods, which are achieved by first identifying EB tickets via low-cost
    schemes, and then continuing to train merely the EB tickets towards the
    target accuracy. Experiments based on various deep networks and datasets
    validate: 1) the existence of EB tickets, and the effectiveness of mask
    distance in efficiently identifying them; and 2) that the proposed efficient
    training via EB tickets can achieve up to 4.7x energy savings while
    maintaining comparable or even better accuracy, demonstrating a promising
    and easily adopted method for tackling cost-prohibitive deep network
    training. Code available at https://github.com/RICE-EIC/Early-Bird-Tickets.
  accessed:
    - year: 2020
      month: 7
      day: 5
  author:
    - family: You
      given: Haoran
    - family: Li
      given: Chaojian
    - family: Xu
      given: Pengfei
    - family: Fu
      given: Yonggan
    - family: Wang
      given: Yue
    - family: Chen
      given: Xiaohan
    - family: Baraniuk
      given: Richard G.
    - family: Wang
      given: Zhangyang
    - family: Lin
      given: Yingyan
  citation-key: you_drawing_2020
  container-title: arXiv:1909.11957 [cs, stat]
  issued:
    - year: 2020
      month: 2
      day: 18
  source: arXiv.org
  title: 'Drawing early-bird tickets: Towards more efficient training of deep networks'
  title-short: Drawing early-bird tickets
  type: article-journal
  URL: http://arxiv.org/abs/1909.11957

- id: young_entropy_
  abstract: >-
    Both hµ and htop measure the exponential rates of growth of n-orbits: – hµ
    counts the number of typical n-orbits, while – htop counts all
    distinguishable n-orbits.
  author:
    - family: Young
      given: Lai-Sang
  citation-key: young_entropy_
  DOI: 10/gfz8cs
  language: en
  page: '13'
  source: Zotero
  title: Entropy in Dynamical Systems
  type: article-journal

- id: young_instrumental_1970
  author:
    - family: Young
      given: Peter C
  citation-key: young_instrumental_1970
  container-title: Automatica
  DOI: 10.1016/0005-1098(70)90098-1
  issue: '2'
  issued:
    - year: 1970
  note: '00477'
  page: 271–287
  title: >-
    An instrumental variable method for real-time identification of a noisy
    process
  type: article-journal
  volume: '6'

- id: yu_kernelbased_2013
  author:
    - family: Yu
      given: Shi
    - family: Tranchevent
      given: Léon-Charles
    - family: De Moor
      given: Bart
    - family: Moreau
      given: Yves
  citation-key: yu_kernelbased_2013
  issued:
    - year: 2013
  note: '00045'
  publisher: Springer
  title: Kernel-based data fusion for machine learning
  type: book

- id: yuan_adversarial_2019
  author:
    - family: Yuan
      given: Xiaoyong
    - family: He
      given: Pan
    - family: Zhu
      given: Qile
    - family: Li
      given: Xiaolin
  citation-key: yuan_adversarial_2019
  container-title: IEEE Transactions on Neural Networks and Learning Systems
  container-title-short: IEEE transactions on neural networks and learning systems
  ISSN: 2162-237X
  issue: '9'
  issued:
    - year: 2019
  page: 2805-2824
  title: 'Adversarial examples: Attacks and defenses for deep learning'
  type: article-journal
  volume: '30'

- id: yuan_model_2006
  abstract: >-
    We consider the problem of selecting grouped variables (factors) for
    accurate prediction in regression. Such a problem arises naturally in many
    practical situations with the multi-factor analysis-of-variance problem as
    the most important and well-known example. Instead of selecting factors by
    stepwise backward elimination, we focus on the accuracy of estimation and
    consider extensions of the lasso, the LARS algorithm and the non-negative
    garrotte for factor selection. The lasso, the LARS algorithm and the
    non-negative garrotte are recently proposed regression methods that can be
    used to select individual variables. We study and propose efficient
    algorithms for the extensions of these methods for factor selection and show
    that these extensions give superior performance to the traditional stepwise
    backward elimination method in factor selection problems. We study the
    similarities and the differences between these methods. Simulations and real
    examples are used to illustrate the methods.
  author:
    - family: Yuan
      given: Ming
    - family: Lin
      given: Yi
  citation-key: yuan_model_2006
  container-title: Journal of the Royal Statistical Society. Series B (Statistical Methodology)
  DOI: 10.1111/j.1467-9868.2005.00532.x
  ISSN: 1369-7412
  issue: '1'
  issued:
    - year: 2006
  note: '04899'
  page: 49-67
  source: JSTOR
  title: Model Selection and Estimation in Regression with Grouped Variables
  type: article-journal
  URL: http://www.jstor.org/stable/3647556
  volume: '68'

- id: yuan_review_2000
  accessed:
    - year: 2017
      month: 9
      day: 11
  author:
    - family: Yuan
      given: Ya-xiang
  citation-key: yuan_review_2000
  container-title: ICIAM
  issued:
    - year: 2000
  note: '00182'
  page: 271–282
  source: Google Scholar
  title: A review of trust region algorithms for optimization
  type: paper-conference
  URL: ftp://ftp.cc.ac.cn/pub/yyx/papers/p995.pdf
  volume: '99'

- id: yuwang_new_2017
  author:
    - literal: Yu Wang
  citation-key: yuwang_new_2017
  container-title: 2017 American Control Conference (ACC)
  container-title-short: 2017 American Control Conference (ACC)
  DOI: 10.23919/ACC.2017.7963782
  event-title: 2017 American Control Conference (ACC)
  ISBN: 2378-5861
  issued:
    - year: 2017
      month: 5
      day: 24
    - year: 2017
      month: 5
      day: 26
  page: 5324-5329
  title: A new concept using LSTM Neural Networks for dynamic system identification
  type: paper-conference

- id: zabihi_detection_2017
  accessed:
    - year: 2019
      month: 5
      day: 27
  author:
    - family: Zabihi
      given: Morteza
    - family: Rad
      given: Ali Bahrami
    - family: Katsaggelos
      given: Aggelos K.
    - family: Kiranyaz
      given: Serkan
    - family: Narkilahti
      given: Susanna
    - family: Gabbouj
      given: Moncef
  citation-key: zabihi_detection_2017
  container-title: Computing in Cardiology
  DOI: 10/gf2597
  event-title: 44th Computing in Cardiology Conference, CinC 2017
  issued:
    - year: 2017
      month: 1
      day: 1
  language: English (US)
  page: 1-4
  publisher: IEEE Computer Society
  source: www.scholars.northwestern.edu
  title: >-
    Detection of atrial fibrillation in ECG hand-held devices using a random
    forest classifier
  type: paper-conference
  URL: >-
    https://www.scholars.northwestern.edu/en/publications/detection-of-atrial-fibrillation-in-ecg-hand-held-devices-using-a
  volume: '44'

- id: zaki_data_2014
  author:
    - family: Zaki
      given: Mohammed J.
    - family: Meira
      given: Wagner
  call-number: QA76.9.D343 Z36 2014
  citation-key: zaki_data_2014
  event-place: New York, NY
  ISBN: 978-0-521-76633-3
  issued:
    - year: 2014
  number-of-pages: '593'
  publisher: Cambridge University Press
  publisher-place: New York, NY
  source: Library of Congress ISBN
  title: 'Data mining and analysis: fundamental concepts and algorithms'
  title-short: Data mining and analysis
  type: book

- id: zech_variable_2018
  abstract: >-
    Background There is interest in using convolutional neural networks (CNNs)
    to analyze medical imaging to provide computer-aided diagnosis (CAD). Recent
    work has suggested that image classification CNNs may not generalize to new
    data as well as previously believed. We assessed how well CNNs generalized
    across three hospital systems for a simulated pneumonia screening task.
    Methods and findings A cross-sectional design with multiple model training
    cohorts was used to evaluate model generalizability to external sites using
    split-sample validation. A total of 158,323 chest radiographs were drawn
    from three institutions: National Institutes of Health Clinical Center (NIH;
    112,120 from 30,805 patients), Mount Sinai Hospital (MSH; 42,396 from 12,904
    patients), and Indiana University Network for Patient Care (IU; 3,807 from
    3,683 patients). These patient populations had an age mean (SD) of 46.9
    years (16.6), 63.2 years (16.5), and 49.6 years (17) with a female
    percentage of 43.5%, 44.8%, and 57.3%, respectively. We assessed individual
    models using the area under the receiver operating characteristic curve
    (AUC) for radiographic findings consistent with pneumonia and compared
    performance on different test sets with DeLong’s test. The prevalence of
    pneumonia was high enough at MSH (34.2%) relative to NIH and IU (1.2% and
    1.0%) that merely sorting by hospital system achieved an AUC of 0.861 (95%
    CI 0.855–0.866) on the joint MSH–NIH dataset. Models trained on data from
    either NIH or MSH had equivalent performance on IU (P values 0.580 and
    0.273, respectively) and inferior performance on data from each other
    relative to an internal test set (i.e., new data from within the hospital
    system used for training data; P values both <0.001). The highest internal
    performance was achieved by combining training and test data from MSH and
    NIH (AUC 0.931, 95% CI 0.927–0.936), but this model demonstrated
    significantly lower external performance at IU (AUC 0.815, 95% CI
    0.745–0.885, P = 0.001). To test the effect of pooling data from sites with
    disparate pneumonia prevalence, we used stratified subsampling to generate
    MSH–NIH cohorts that only differed in disease prevalence between training
    data sites. When both training data sites had the same pneumonia prevalence,
    the model performed consistently on external IU data (P = 0.88). When a
    10-fold difference in pneumonia rate was introduced between sites, internal
    test performance improved compared to the balanced model (10× MSH risk P <
    0.001; 10× NIH P = 0.002), but this outperformance failed to generalize to
    IU (MSH 10× P < 0.001; NIH 10× P = 0.027). CNNs were able to directly detect
    hospital system of a radiograph for 99.95% NIH (22,050/22,062) and 99.98%
    MSH (8,386/8,388) radiographs. The primary limitation of our approach and
    the available public data is that we cannot fully assess what other factors
    might be contributing to hospital system–specific biases. Conclusion
    Pneumonia-screening CNNs achieved better internal than external performance
    in 3 out of 5 natural comparisons. When models were trained on pooled data
    from sites with different pneumonia prevalence, they performed better on new
    pooled data from these sites but not on external data. CNNs robustly
    identified hospital system and department within a hospital, which can have
    large differences in disease burden and may confound predictions.
  accessed:
    - year: 2018
      month: 11
      day: 19
  author:
    - family: Zech
      given: John R.
    - family: Badgeley
      given: Marcus A.
    - family: Liu
      given: Manway
    - family: Costa
      given: Anthony B.
    - family: Titano
      given: Joseph J.
    - family: Oermann
      given: Eric Karl
  citation-key: zech_variable_2018
  container-title: PLOS Medicine
  container-title-short: PLOS Medicine
  DOI: 10/gfj53h
  ISSN: 1549-1676
  issue: '11'
  issued:
    - year: 2018
      month: 6
      day: 11
  language: en
  page: e1002683
  source: PLoS Journals
  title: >-
    Variable generalization performance of a deep learning model to detect
    pneumonia in chest radiographs: A cross-sectional study
  title-short: >-
    Variable generalization performance of a deep learning model to detect
    pneumonia in chest radiographs
  type: article-journal
  URL: >-
    https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002683
  volume: '15'

- id: zeiler_rectified_2013
  author:
    - family: Zeiler
      given: M. D.
    - family: Ranzato
      given: M.
    - family: Monga
      given: R.
    - family: Mao
      given: M.
    - family: Yang
      given: K.
    - literal: Q. V. Le
    - literal: P. Nguyen
    - literal: A. Senior
    - literal: V. Vanhoucke
    - literal: J. Dean
    - literal: G. E. Hinton
  citation-key: zeiler_rectified_2013
  container-title: >-
    2013 IEEE International Conference on Acoustics, Speech and Signal
    Processing
  DOI: 10/gfv3hc
  event-title: >-
    2013 IEEE International Conference on Acoustics, Speech and Signal
    Processing
  ISBN: 1520-6149
  issued:
    - year: 2013
      month: 5
      day: 26
    - year: 2013
      month: 5
      day: 31
  page: 3517-3521
  title: On rectified linear units for speech processing
  type: paper-conference

- id: zeiler_visualizing_2013
  abstract: >-
    Large Convolutional Network models have recently demonstrated impressive
    classification performance on the ImageNet benchmark. However there is no
    clear understanding of why they perform so well, or how they might be
    improved. In this paper we address both issues. We introduce a novel
    visualization technique that gives insight into the function of intermediate
    feature layers and the operation of the classifier. We also perform an
    ablation study to discover the performance contribution from different model
    layers. This enables us to find model architectures that outperform
    Krizhevsky \etal on the ImageNet classification benchmark. We show our
    ImageNet model generalizes well to other datasets: when the softmax
    classifier is retrained, it convincingly beats the current state-of-the-art
    results on Caltech-101 and Caltech-256 datasets.
  author:
    - family: Zeiler
      given: Matthew D.
    - family: Fergus
      given: Rob
  citation-key: zeiler_visualizing_2013
  container-title: arXiv:1311.2901 [cs]
  issued:
    - year: 2013
      month: 11
      day: 12
  source: arXiv.org
  title: Visualizing and Understanding Convolutional Networks
  type: article-journal
  URL: http://arxiv.org/abs/1311.2901

- id: zeiler_visualizing_2013a
  abstract: >-
    Large Convolutional Network models have recently demonstrated impressive
    classification performance on the ImageNet benchmark. However there is no
    clear understanding of why they perform so well, or how they might be
    improved. In this paper we address both issues. We introduce a novel
    visualization technique that gives insight into the function of intermediate
    feature layers and the operation of the classifier. We also perform an
    ablation study to discover the performance contribution from different model
    layers. This enables us to find model architectures that outperform
    Krizhevsky \etal on the ImageNet classification benchmark. We show our
    ImageNet model generalizes well to other datasets: when the softmax
    classifier is retrained, it convincingly beats the current state-of-the-art
    results on Caltech-101 and Caltech-256 datasets.
  author:
    - family: Zeiler
      given: Matthew D.
    - family: Fergus
      given: Rob
  citation-key: zeiler_visualizing_2013a
  container-title: arXiv:1311.2901 [cs]
  issued:
    - year: 2013
      month: 11
      day: 12
  note: '04774'
  source: arXiv.org
  title: Visualizing and Understanding Convolutional Networks
  type: article-journal
  URL: http://arxiv.org/abs/1311.2901

- id: zeiler_visualizing_2013b
  abstract: >-
    Large Convolutional Network models have recently demonstrated impressive
    classification performance on the ImageNet benchmark. However there is no
    clear understanding of why they perform so well, or how they might be
    improved. In this paper we address both issues. We introduce a novel
    visualization technique that gives insight into the function of intermediate
    feature layers and the operation of the classifier. We also perform an
    ablation study to discover the performance contribution from different model
    layers. This enables us to find model architectures that outperform
    Krizhevsky \etal on the ImageNet classification benchmark. We show our
    ImageNet model generalizes well to other datasets: when the softmax
    classifier is retrained, it convincingly beats the current state-of-the-art
    results on Caltech-101 and Caltech-256 datasets.
  accessed:
    - year: 2020
      month: 5
      day: 6
  author:
    - family: Zeiler
      given: Matthew D.
    - family: Fergus
      given: Rob
  citation-key: zeiler_visualizing_2013b
  container-title: arXiv:1311.2901 [cs]
  issued:
    - year: 2013
      month: 11
      day: 28
  source: arXiv.org
  title: Visualizing and Understanding Convolutional Networks
  type: article-journal
  URL: http://arxiv.org/abs/1311.2901

- id: zeiler_visualizing_2014
  abstract: >-
    Large Convolutional Network models have recently demonstrated impressive
    classification performance on the ImageNet benchmark Krizhevsky et al. [18].
    However there is no clear understanding of why they perform so well, or how
    they might be improved. In this paper we explore both issues. We introduce a
    novel visualization technique that gives insight into the function of
    intermediate feature layers and the operation of the classifier. Used in a
    diagnostic role, these visualizations allow us to find model architectures
    that outperform Krizhevsky et al on the ImageNet classification benchmark.
    We also perform an ablation study to discover the performance contribution
    from different model layers. We show our ImageNet model generalizes well to
    other datasets: when the softmax classifier is retrained, it convincingly
    beats the current state-of-the-art results on Caltech-101 and Caltech-256
    datasets.
  accessed:
    - year: 2018
      month: 1
      day: 27
  author:
    - family: Zeiler
      given: Matthew D.
    - family: Fergus
      given: Rob
  citation-key: zeiler_visualizing_2014
  collection-title: Lecture Notes in Computer Science
  container-title: Computer Vision – ECCV 2014
  DOI: 10.1007/978-3-319-10590-1_53
  event-title: European Conference on Computer Vision
  ISBN: 978-3-319-10589-5 978-3-319-10590-1
  issued:
    - year: 2014
      month: 9
      day: 6
  language: en
  note: '04774'
  page: 818-833
  publisher: Springer, Cham
  source: link.springer.com
  title: Visualizing and Understanding Convolutional Networks
  type: paper-conference
  URL: https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53

- id: zhang_association_2023
  author:
    - family: Zhang
      given: Cuili
    - family: Miao
      given: Xiao
    - family: Wang
      given: Biqi
    - family: Ribeiro
      given: Antônio H
    - family: Brant
      given: Luisa
    - family: Ribeiro
      given: Antonio L P
    - family: Lin
      given: Honghuang
  citation-key: zhang_association_2023
  container-title: Frontiers in Cardiovascular Medicine
  DOI: 10.3389/fcvm.2023.1160091
  issued:
    - year: 2023
  license: All rights reserved
  title: Association of lifestyle with deep-learning based ECG-age
  type: article-journal
  volume: '10'

- id: zhang_causal_2020
  abstract: >-
    We present a causal view on the robustness of neural networks against input
    manipulations, which applies not only to traditional classification tasks
    but also to general measurement data. Based on this view, we design a deep
    causal manipulation augmented model (deep CAMA) which explicitly models
    possible manipulations on certain causes leading to changes in the observed
    effect.  We further develop data augmentation and test-time fine-tuning
    methods to improve deep CAMA's robustness. When compared with discriminative
    deep neural networks, our proposed model shows superior robustness against
    unseen manipulations. As a by-product, our model achieves disentangled
    representation which separates the representation of manipulations from
    those of other latent causes.
  accessed:
    - year: 2023
      month: 4
      day: 14
  author:
    - family: Zhang
      given: Cheng
    - family: Zhang
      given: Kun
    - family: Li
      given: Yingzhen
  citation-key: zhang_causal_2020
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2020
  page: 289–301
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: A Causal View on Robustness of Neural Networks
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2020/hash/02ed812220b0705fabb868ddbf17ea20-Abstract.html
  volume: '33'

- id: zhang_depth_2020
  abstract: >-
    We show that for any convex differentiable loss, a deep linear network has
    no spurious local minima as long as it is true for the two layer case. This
    reduction greatly simplifies the study on the existence of spurious local
    minima in deep linear networks. When applied to the quadratic loss, our
    result immediately implies the powerful result in [Kawaguchi 2016]. Further,
    with the work in [Zhou and Liang 2018], we can remove all the assumptions in
    [Kawaguchi 2016]. This property holds for more general "multi-tower" linear
    networks too. Our proof builds on [Laurent and von Brecht 2018] and develops
    a new perturbation argument to show that any spurious local minimum must
    have full rank, a structural property which can be useful more generally.
  accessed:
    - year: 2020
      month: 8
      day: 27
  author:
    - family: Zhang
      given: Li
  citation-key: zhang_depth_2020
  container-title: arXiv:1901.09827 [cs, stat]
  issued:
    - year: 2020
      month: 1
      day: 5
  source: arXiv.org
  title: Depth creates no more spurious local minima
  type: article-journal
  URL: http://arxiv.org/abs/1901.09827

- id: zhang_flexible_2000
  author:
    - family: Zhang
      given: Zhengyou
  citation-key: zhang_flexible_2000
  container-title: Pattern Analysis and Machine Intelligence, IEEE Transactions on
  DOI: 10.1109/34.888718
  issue: '11'
  issued:
    - year: 2000
  note: '11804'
  page: 1330–1334
  title: A flexible new technique for camera calibration
  type: article-journal
  volume: '22'

- id: zhang_forward_2015
  abstract: >-
    A forward and backward least angle regression (LAR) algorithm is proposed to
    construct the nonlinear autoregressive model with exogenous inputs (NARX)
    that is widely used to describe a large class of nonlinear dynamic systems.
    The main objective of this paper is to improve model sparsity and
    generalization performance of the original forward LAR algorithm. This is
    achieved by introducing a replacement scheme using an additional backward
    LAR stage. The backward stage replaces insignificant model terms selected by
    forward LAR with more significant ones, leading to an improved model in
    terms of the model compactness and performance. A numerical example to
    construct four types of NARX models, namely polynomials, radial basis
    function (RBF) networks, neuro fuzzy and wavelet networks, is presented to
    illustrate the effectiveness of the proposed technique in comparison with
    some popular methods.
  author:
    - family: Zhang
      given: Long
    - family: Li
      given: Kang
  citation-key: zhang_forward_2015
  container-title: Automatica
  container-title-short: Automatica
  DOI: 10.1016/j.automatica.2014.12.010
  ISSN: 0005-1098
  issued:
    - year: 2015
      month: 3
      day: 1
  note: '00013'
  page: 94-102
  source: ScienceDirect
  title: >-
    Forward and backward least angle regression for nonlinear system
    identification
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0005109814005895
  volume: '53'

- id: zhang_generalized_2018
  abstract: >-
    Deep neural networks (DNNs) have achieved tremendous success in a variety of
    applications across many disciplines. Yet, their superior performance comes
    with the expensive cost of requiring correctly annotated large-scale
    datasets. Moreover, due to DNNs' rich capacity, errors in training labels
    can hamper performance. To combat this problem, mean absolute error (MAE)
    has recently been proposed as a noise-robust alternative to the
    commonly-used categorical cross entropy (CCE) loss. However, as we show in
    this paper, MAE can perform poorly with DNNs and challenging datasets. Here,
    we present a theoretically grounded set of noise-robust loss functions that
    can be seen as a generalization of MAE and CCE. Proposed loss functions can
    be readily applied with any existing DNN architecture and algorithm, while
    yielding good performance in a wide range of noisy label scenarios. We
    report results from experiments conducted with CIFAR-10, CIFAR-100 and
    FASHION-MNIST datasets and synthetically generated noisy labels.
  accessed:
    - year: 2018
      month: 12
      day: 6
  author:
    - family: Zhang
      given: Zhilu
    - family: Sabuncu
      given: Mert R.
  citation-key: zhang_generalized_2018
  container-title: arXiv:1805.07836 [cs, stat]
  issued:
    - year: 2018
      month: 5
      day: 20
  source: arXiv.org
  title: >-
    Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy
    Labels
  type: article-journal
  URL: http://arxiv.org/abs/1805.07836

- id: zhang_identification_2017
  accessed:
    - year: 2017
      month: 8
      day: 23
  author:
    - family: Zhang
      given: Erliang
    - family: Pintelon
      given: Rik
  citation-key: zhang_identification_2017
  container-title: Automatica
  DOI: 10.1016/j.automatica.2017.04.031
  ISSN: '00051098'
  issued:
    - year: 2017
      month: 8
  language: en
  note: '00006'
  page: 69-78
  source: CrossRef
  title: >-
    Identification of multivariable dynamic errors-in-variables system with
    arbitrary inputs
  type: article-journal
  URL: http://linkinghub.elsevier.com/retrieve/pii/S000510981730225X
  volume: '82'

- id: zhang_making_2019
  abstract: >-
    Modern convolutional networks are not shift-invariant, as small input shifts
    or translations can cause drastic changes in the output. Commonly used
    downsampling methods, such as max-pooling, strided-convolution, and
    average-pooling, ignore the sampling theorem. The well-known signal
    processing fix is anti-aliasing by low-pass filtering before downsampling.
    However, simply inserting this module into deep networks degrades
    performance; as a result, it is seldomly used today. We show that when
    integrated correctly, it is compatible with existing architectural
    components, such as max-pooling and strided-convolution. We observe
    \textit{increased accuracy} in ImageNet classification, across several
    commonly-used architectures, such as ResNet, DenseNet, and MobileNet,
    indicating effective regularization. Furthermore, we observe \textit{better
    generalization}, in terms of stability and robustness to input corruptions.
    Our results demonstrate that this classical signal processing technique has
    been undeservingly overlooked in modern deep networks. Code and anti-aliased
    versions of popular networks are available at
    https://richzhang.github.io/antialiased-cnns/ .
  accessed:
    - year: 2020
      month: 3
      day: 23
  author:
    - family: Zhang
      given: Richard
  citation-key: zhang_making_2019
  container-title: Proceedings of the 36th International Conference on Machine Learning (ICML)
  issued:
    - year: 2019
      month: 6
      day: 8
  source: arXiv.org
  title: Making Convolutional Networks Shift-Invariant Again
  type: paper-conference
  URL: http://arxiv.org/abs/1904.11486

- id: zhang_modeling_2006
  author:
    - family: Zhang
      given: Dong-yan
    - family: Sun
      given: Li-ping
    - family: Cao
      given: Jun
  citation-key: zhang_modeling_2006
  container-title: Journal of Forestry Research
  DOI: 10.1007/s11676-006-0033-1
  issue: '2'
  issued:
    - year: 2006
  note: '00028'
  page: 141–144
  title: >-
    Modeling of temperature-humidity for wood drying based on time-delay neural
    network
  type: article-journal
  volume: '17'

- id: zhang_new_2014
  author:
    - family: Zhang
      given: Cheng
    - family: Li
      given: Kang
    - family: Yang
      given: Zhile
    - family: Pei
      given: Lei
    - family: Zhu
      given: Chunbo
  citation-key: zhang_new_2014
  container-title: 2014 IEEE PES General Meeting
  issued:
    - year: 2014
  title: A new battery modelling method based on simulation error minimization
  type: paper-conference

- id: zhang_stabilizing_2018
  abstract: >-
    Vanishing and exploding gradients are two of the main obstacles in training
    deep neural networks, especially in capturing long range dependencies in
    recurrent neural networks (RNNs). In this paper, we present an efﬁcient
    parametrization of the transition matrix of an RNN that allows us to
    stabilize the gradients that arise in its training. Speciﬁcally, we
    parameterize the transition matrix by its singular value decomposition
    (SVD), which allows us to explicitly track and control its singular values.
    We attain efﬁciency by using tools that are common in numerical linear
    algebra, namely Householder reﬂectors for representing the orthogonal
    matrices that arise in the SVD. By explicitly controlling the singular
    values, our proposed Spectral-RNN method allows us to provably solve the
    exploding gradient problem and we observe that it empirically solves the
    vanishing gradient issue to a large extent. We note that the SVD
    parameterization can be used for any rectangular weight matrix, hence it can
    be easily extended to any deep neural network, such as a multi-layer
    perceptron. Theoretically, we demonstrate that our parameterization does not
    lose any expressive power, and show how it controls generalization of RNN
    for the classiﬁcation task. Our extensive experimental results also
    demonstrate that the proposed framework converges faster, and has good
    generalization, especially in capturing long range dependencies, as shown on
    the synthetic addition and copy tasks, as well as on the MNIST and Penn Tree
    Bank data sets.
  author:
    - family: Zhang
      given: Jiong
    - family: Lei
      given: Qi
    - family: Dhillon
      given: Inderjit S
  citation-key: zhang_stabilizing_2018
  container-title: Proceedings of the 35 th International Conference on Machine Learning
  issued:
    - year: 2018
  language: en
  page: '9'
  source: Zotero
  title: >-
    Stabilizing Gradients for Deep Neural Networks via Efﬁcient SVD
    Parameterization
  type: article-journal

- id: zhang_trust_2010
  author:
    - family: Zhang
      given: Bo
    - family: Yang
      given: Xin
    - family: Qin
      given: Chenghu
    - family: Liu
      given: Dan
    - family: Zhu
      given: Shouping
    - family: Feng
      given: Jinchao
    - family: Sun
      given: Li
    - family: Liu
      given: Kai
    - family: Han
      given: Dong
    - family: Ma
      given: Xibo
    - literal: others
  citation-key: zhang_trust_2010
  container-title: Optics express
  DOI: 10.1364/OE.18.006477
  issue: '7'
  issued:
    - year: 2010
  note: '00029'
  page: 6477–6491
  title: >-
    A trust region method in adaptive finite element framework for
    bioluminescence tomography
  type: article-journal
  volume: '18'

- id: zhang_understanding_2017
  abstract: >-
    Despite their massive size, successful deep artificial neural networks can
    exhibit a remarkably small difference between training and test performance.
    Conventional wisdom attributes small generalization error either to
    properties of the model family, or to the regularization techniques used
    during training. Through extensive systematic experiments, we show how these
    traditional approaches fail to explain why large neural networks generalize
    well in practice. Specifically, our experiments establish that
    state-of-the-art convolutional networks for image classification trained
    with stochastic gradient methods easily fit a random labeling of the
    training data. This phenomenon is qualitatively unaffected by explicit
    regularization, and occurs even if we replace the true images by completely
    unstructured random noise. We corroborate these experimental findings with a
    theoretical construction showing that simple depth two neural networks
    already have perfect finite sample expressivity as soon as the number of
    parameters exceeds the number of data points as it usually does in practice.
    We interpret our experimental findings by comparison with traditional
    models.
  author:
    - family: Zhang
      given: Chiyuan
    - family: Bengio
      given: Samy
    - family: Hardt
      given: Moritz
    - family: Recht
      given: Benjamin
    - family: Vinyals
      given: Oriol
  citation-key: zhang_understanding_2017
  container-title: International Conference on Learning Representations
  event-title: International Conference on Learning Representations
  issued:
    - year: 2017
  title: Understanding deep learning requires rethinking generalization
  type: paper-conference

- id: zhangjeffrey_fully_2018
  abstract: >-
    Background:Automated cardiac image interpretation has the potential to
    transform clinical practice in multiple ways, including enabling serial
    assessment of cardiac function by nonexperts in primary care and rural
    settings. We hypothesized that advances in computer vision could enable
    building a fully automated, scalable analysis pipeline for echocardiogram
    interpretation, including (1) view identification, (2) image segmentation,
    (3) quantification of structure and function, and (4) disease
    detection.Methods:Using 14 035 echocardiograms spanning a 10-year period, we
    trained and evaluated convolutional neural network models for multiple
    tasks, including automated identification of 23 viewpoints and segmentation
    of cardiac chambers across 5 common views. The segmentation output was used
    to quantify chamber volumes and left ventricular mass, determine ejection
    fraction, and facilitate automated determination of longitudinal strain
    through speckle tracking. Results were evaluated through comparison to
    manual segmentation and measurements from 8666 echocardiograms obtained
    during the routine clinical workflow. Finally, we developed models to detect
    3 diseases: hypertrophic cardiomyopathy, cardiac amyloid, and pulmonary
    arterial hypertension.Results:Convolutional neural networks accurately
    identified views (eg, 96% for parasternal long axis), including flagging
    partially obscured cardiac chambers, and enabled the segmentation of
    individual cardiac chambers. The resulting cardiac structure measurements
    agreed with study report values (eg, median absolute deviations of 15% to
    17% of observed values for left ventricular mass, left ventricular diastolic
    volume, and left atrial volume). In terms of function, we computed automated
    ejection fraction and longitudinal strain measurements (within 2 cohorts),
    which agreed with commercial software-derived values (for ejection fraction,
    median absolute deviation=9.7% of observed, N=6407 studies; for strain,
    median absolute deviation=7.5%, n=419, and 9.0%, n=110) and demonstrated
    applicability to serial monitoring of patients with breast cancer for
    trastuzumab cardiotoxicity. Overall, we found automated measurements to be
    comparable or superior to manual measurements across 11 internal consistency
    metrics (eg, the correlation of left atrial and ventricular volumes).
    Finally, we trained convolutional neural networks to detect hypertrophic
    cardiomyopathy, cardiac amyloidosis, and pulmonary arterial hypertension
    with C statistics of 0.93, 0.87, and 0.85, respectively.Conclusions:Our
    pipeline lays the groundwork for using automated interpretation to support
    serial patient tracking and scalable analysis of millions of echocardiograms
    archived within healthcare systems.
  accessed:
    - year: 2018
      month: 11
      day: 27
  author:
    - literal: Zhang Jeffrey
    - literal: Gajjala Sravani
    - literal: Agrawal Pulkit
    - literal: Tison Geoffrey H.
    - literal: Hallock Laura A.
    - literal: Beussink-Nelson Lauren
    - literal: Lassen Mats H.
    - literal: Fan Eugene
    - literal: Aras Mandar A.
    - literal: Jordan ChaRandle
    - literal: Fleischmann Kirsten E.
    - literal: Melisko Michelle
    - literal: Qasim Atif
    - literal: Shah Sanjiv J.
    - literal: Bajcsy Ruzena
    - literal: Deo Rahul C.
  citation-key: zhangjeffrey_fully_2018
  container-title: Circulation
  container-title-short: Circulation
  DOI: 10/gfg9fn
  issue: '16'
  issued:
    - year: 2018
      month: 10
      day: 16
  page: 1623-1635
  source: ahajournals.org (Atypon)
  title: Fully Automated Echocardiogram Interpretation in Clinical Practice
  type: article-journal
  URL: https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.118.034338
  volume: '138'

- id: zhao_automated_2017
  abstract: >-
    The prediction of combined sewer overflow (CSO) operation in urban
    environments presents a challenging task for water utilities. The operation
    of CSOs (most often in heavy rainfall conditions) prevents houses and
    businesses from flooding. However, sometimes, CSOs do not operate as they
    should, potentially bringing environmental pollution risks. Therefore, CSOs
    should be appropriately managed by water utilities, highlighting the need
    for adapted decision support systems. This paper proposes an automated CSO
    predictive model construction methodology using field monitoring data, as a
    substitute for the commonly established hydrological-hydraulic modeling
    approach for time-series prediction of CSO statuses. It is a systematic
    methodology factoring in all monitored field variables to construct
    time-series dependencies for CSO statuses. The model construction process is
    largely automated with little human intervention, and the pertinent
    variables together with their associated time lags for every CSO are
    holistically and automatically generated. A fast least absolute shrinkage
    and selection operator solution generating scheme is proposed to expedite
    the model construction process, where matrix inversions are effectively
    eliminated. The whole algorithm works in a stepwise manner, invoking either
    an incremental or decremental movement for including or excluding one model
    regressor into, or from, the predictive model at every step. The
    computational complexity is thereby analyzed with the pseudo code provided.
    Actual experimental results from both single-step ahead (i.e., 15 min) and
    multistep ahead predictions are finally produced and analyzed on a U.K.
    pilot area with various types of monitoring data made available,
    demonstrating the efficiency and effectiveness of the proposed approach.
  author:
    - family: Zhao
      given: W.
    - family: Beach
      given: T. H.
    - family: Rezgui
      given: Y.
  citation-key: zhao_automated_2017
  container-title: 'IEEE Transactions on Systems, Man, and Cybernetics: Systems'
  DOI: 10.1109/TSMC.2017.2724440
  ISSN: 2168-2216
  issue: '99'
  issued:
    - year: 2017
  note: '00003'
  page: 1-16
  source: IEEE Xplore
  title: >-
    Automated Model Construction for Combined Sewer Overflow Prediction Based on
    Efficient LASSO Algorithm
  type: article-journal
  volume: PP

- id: zhao_identification_2014
  abstract: >-
    This work studies k-step-ahead prediction error model identification and its
    relationship to MPC control. The use of error criteria in parameter
    estimation will be discussed, where the identified model is used in model
    predictive control (MPC). Assume that the model error is dominated by the
    variance part, it can be shown that a k-step-ahead prediction error model is
    not optimal for k-step-ahead prediction. A normal one-step-ahead prediction
    error criterion will be optimal for k-step-ahead prediction. Then it is
    argued that even when some bias exists, the result could still hold true.
    Therefore, for MPC identification of linear processes, one-step-ahead
    prediction error models fever k-step-ahead prediction models. Simulations
    and industrial testing data will be used to illustrate the idea.
  accessed:
    - year: 2019
      month: 4
      day: 4
  author:
    - family: Zhao
      given: Jun
    - family: Zhu
      given: Yucai
    - family: Patwardhan
      given: Rohit
  citation-key: zhao_identification_2014
  container-title: Journal of Process Control
  container-title-short: Journal of Process Control
  DOI: 10/f5v69w
  ISSN: 0959-1524
  issue: '1'
  issued:
    - year: 2014
      month: 1
      day: 1
  page: 48-56
  source: ScienceDirect
  title: Identification of k-step-ahead prediction error model and MPC control
  type: article-journal
  URL: http://www.sciencedirect.com/science/article/pii/S0959152413002187
  volume: '24'

- id: zheng_state_2017
  abstract: >-
    Long Short-Term Memory (LSTM) is one of the most powerful sequence models.
    Despite the strong performance, however, it lacks the nice interpretability
    as in state space models. In this paper, we present a way to combine the
    best of both worlds by introducing State Space LSTM (SSL) models that
    generalizes the earlier work \cite{zaheer2017latent} of combining topic
    models with LSTM. However, unlike \cite{zaheer2017latent}, we do not make
    any factorization assumptions in our inference algorithm. We present an
    efficient sampler based on sequential Monte Carlo (SMC) method that draws
    from the joint posterior directly. Experimental results confirms the
    superiority and stability of this SMC inference algorithm on a variety of
    domains.
  accessed:
    - year: 2018
      month: 11
      day: 26
  author:
    - family: Zheng
      given: Xun
    - family: Zaheer
      given: Manzil
    - family: Ahmed
      given: Amr
    - family: Wang
      given: Yuan
    - family: Xing
      given: Eric P.
    - family: Smola
      given: Alexander J.
  citation-key: zheng_state_2017
  container-title: arXiv:1711.11179 [cs, stat]
  issued:
    - year: 2017
      month: 11
      day: 29
  source: arXiv.org
  title: State Space LSTM Models with Particle MCMC Inference
  type: article-journal
  URL: http://arxiv.org/abs/1711.11179

- id: zhou_deconstructing_2020
  abstract: >-
    The recent "Lottery Ticket Hypothesis" paper by Frankle & Carbin showed that
    a simple approach to creating sparse networks (keeping the large weights)
    results in models that are trainable from scratch, but only when starting
    from the same initial weights. The performance of these networks often
    exceeds the performance of the non-sparse base model, but for reasons that
    were not well understood. In this paper we study the three critical
    components of the Lottery Ticket (LT) algorithm, showing that each may be
    varied significantly without impacting the overall results. Ablating these
    factors leads to new insights for why LT networks perform as well as they
    do. We show why setting weights to zero is important, how signs are all you
    need to make the reinitialized network train, and why masking behaves like
    training. Finally, we discover the existence of Supermasks, masks that can
    be applied to an untrained, randomly initialized network to produce a model
    with performance far better than chance (86% on MNIST, 41% on CIFAR-10).
  accessed:
    - year: 2020
      month: 6
      day: 29
  author:
    - family: Zhou
      given: Hattie
    - family: Lan
      given: Janice
    - family: Liu
      given: Rosanne
    - family: Yosinski
      given: Jason
  citation-key: zhou_deconstructing_2020
  container-title: arXiv:1905.01067 [cs, stat]
  issued:
    - year: 2020
      month: 3
      day: 3
  source: arXiv.org
  title: 'Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask'
  title-short: Deconstructing Lottery Tickets
  type: article-journal
  URL: http://arxiv.org/abs/1905.01067

- id: zhou_essentials_1998
  author:
    - family: Zhou
      given: Kemin
    - family: Doyle
      given: John Comstock
  citation-key: zhou_essentials_1998
  issued:
    - year: 1998
  publisher: Prentice Hall
  title: Essentials of robust control
  type: book
  volume: '104'

- id: zhou_state_2017
  author:
    - family: Zhou
      given: Zupeng
    - family: Tan
      given: Yonghong
    - family: Xie
      given: Yangqiu
    - family: Dong
      given: Ruili
  citation-key: zhou_state_2017
  container-title: Mechanical Systems and Signal Processing
  container-title-short: Mechanical Systems and Signal Processing
  DOI: 10.1016/j.ymssp.2016.06.023
  ISSN: 0888-3270
  issued:
    - year: 2017
  note: '00007'
  page: 439-449
  title: >-
    State estimation of a compound non-smooth sandwich system with backlash and
    dead zone
  type: article-journal
  volume: '83'

- id: zhou_uniform_2021
  abstract: >-
    We consider an underdetermined noisy linear regression model where the
    minimum-norm interpolating predictor is known to be consistent, and ask: can
    uniform convergence in a norm ball, or at least (following Nagarajan and
    Kolter) the subset of a norm ball that the algorithm selects on a typical
    input set, explain this success? We show that uniformly bounding the
    difference between empirical and population errors cannot show any learning
    in the norm ball, and cannot show consistency for any set, even one
    depending on the exact algorithm and distribution. But we argue we can
    explain the consistency of the minimal-norm interpolator with a slightly
    weaker, yet standard, notion: uniform convergence of zero-error predictors
    in a norm ball. We use this to bound the generalization error of low- (but
    not minimal-) norm interpolating predictors.
  accessed:
    - year: 2022
      month: 12
      day: 6
  author:
    - family: Zhou
      given: Lijia
    - family: Sutherland
      given: Danica J.
    - family: Srebro
      given: Nathan
  citation-key: zhou_uniform_2021
  issued:
    - year: 2021
      month: 1
      day: 13
  language: en
  number: arXiv:2006.05942
  publisher: arXiv
  source: arXiv.org
  title: On Uniform Convergence and Low-Norm Interpolation Learning
  type: article
  URL: http://arxiv.org/abs/2006.05942

- id: zoph_learning_2017
  abstract: >-
    Developing neural network image classification models often requires
    significant architecture engineering. In this paper, we study a method to
    learn the model architectures directly on the dataset of interest. As this
    approach is expensive when the dataset is large, we propose to search for an
    architectural building block on a small dataset and then transfer the block
    to a larger dataset. The key contribution of this work is the design of a
    new search space (the "NASNet search space") which enables transferability.
    In our experiments, we search for the best convolutional layer (or "cell")
    on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by
    stacking together more copies of this cell, each with their own parameters
    to design a convolutional architecture, named "NASNet architecture". We also
    introduce a new regularization technique called ScheduledDropPath that
    significantly improves generalization in the NASNet models. On CIFAR-10
    itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On
    ImageNet, NASNet achieves, among the published works, state-of-the-art
    accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2%
    better in top-1 accuracy than the best human-invented architectures while
    having 9 billion fewer FLOPS - a reduction of 28% in computational demand
    from the previous state-of-the-art model. When evaluated at different levels
    of computational cost, accuracies of NASNets exceed those of the
    state-of-the-art human-designed models. For instance, a small version of
    NASNet also achieves 74% top-1 accuracy, which is 3.1% better than
    equivalently-sized, state-of-the-art models for mobile platforms. Finally,
    the learned features by NASNet used with the Faster-RCNN framework surpass
    state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.
  author:
    - family: Zoph
      given: Barret
    - family: Vasudevan
      given: Vijay
    - family: Shlens
      given: Jonathon
    - family: Le
      given: Quoc V.
  citation-key: zoph_learning_2017
  container-title: arXiv:1707.07012 [cs, stat]
  issued:
    - year: 2017
      month: 7
      day: 21
  note: '00171'
  source: arXiv.org
  title: Learning Transferable Architectures for Scalable Image Recognition
  type: article-journal
  URL: http://arxiv.org/abs/1707.07012

- id: zou_delving_2020
  author:
    - family: Zou
      given: Xueyan
  citation-key: zou_delving_2020
  container-title: Proceedings of the 31st British Machine Vision Virtual Conference (BMVC)
  issued:
    - year: 2020
  language: en
  source: Zotero
  title: Delving Deeper into Anti-aliasing in ConvNets
  type: paper-conference

- id: zou_regularization_2005
  abstract: >-
    We propose the elastic net, a new regularization and variable selection
    method. Real world data and a simulation study show that the elastic net
    often outperforms the lasso, while enjoying a similar sparsity of
    representation. In addition, the elastic net encourages a grouping effect,
    where strongly correlated predictors tend to be in or out of the model
    together. The elastic net is particularly useful when the number of
    predictors (p) is much bigger than the number of observations (n). By
    contrast, the lasso is not a very satisfactory variable selection method in
    the p ≫ n case. An algorithm called LARS-EN is proposed for computing
    elastic net regularization paths efficiently, much like algorithm LARS does
    for the lasso.
  author:
    - family: Zou
      given: Hui
    - family: Hastie
      given: Trevor
  citation-key: zou_regularization_2005
  container-title: Journal of the Royal Statistical Society. Series B (Statistical Methodology)
  DOI: 10.1111/j.1467-9868.2005.00503.x
  ISSN: 1369-7412
  issue: '2'
  issued:
    - year: 2005
  note: '08250'
  page: 301-320
  source: JSTOR
  title: Regularization and Variable Selection via the Elastic Net
  type: article-journal
  URL: http://www.jstor.org/stable/3647580
  volume: '67'

- id: zou_stochastic_2018
  abstract: >-
    We study the problem of training deep neural networks with Rectified Linear
    Unit (ReLU) activation function using gradient descent and stochastic
    gradient descent. In particular, we study the binary classification problem
    and show that for a broad family of loss functions, with proper random
    weight initialization, both gradient descent and stochastic gradient descent
    can find the global minima of the training loss for an over-parameterized
    deep ReLU network, under mild assumption on the training data. The key idea
    of our proof is that Gaussian random initialization followed by (stochastic)
    gradient descent produces a sequence of iterates that stay inside a small
    perturbation region centering around the initial weights, in which the
    empirical loss function of deep ReLU networks enjoys nice local curvature
    properties that ensure the global convergence of (stochastic) gradient
    descent. Our theoretical results shed light on understanding the
    optimization for deep learning, and pave the way for studying the
    optimization dynamics of training modern deep neural networks.
  accessed:
    - year: 2020
      month: 8
      day: 10
  author:
    - family: Zou
      given: Difan
    - family: Cao
      given: Yuan
    - family: Zhou
      given: Dongruo
    - family: Gu
      given: Quanquan
  citation-key: zou_stochastic_2018
  container-title: arXiv:1811.08888 [cs, math, stat]
  issued:
    - year: 2018
      month: 12
      day: 27
  source: arXiv.org
  title: Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks
  type: article-journal
  URL: http://arxiv.org/abs/1811.08888

- id: zurawski_industrial_2005
  call-number: TK5105.5 .I48 2005
  citation-key: zurawski_industrial_2005
  collection-number: '1'
  collection-title: The industrial information technology series
  editor:
    - family: Zurawski
      given: Richard
  event-place: Boca Raton, Fla
  ISBN: 978-0-8493-3077-3
  issued:
    - year: 2005
  number-of-pages: '1'
  publisher: Taylor & Francis
  publisher-place: Boca Raton, Fla
  source: Library of Congress ISBN
  title: The industrial communication technology handbook
  type: book

- id: zvuloni_merging_2023
  abstract: >-
    Objective: Over the past few years, deep learning (DL) has been used
    extensively in research for 12-lead electrocardiogram (ECG) analysis.
    However, it is unclear whether the explicit or implicit claims made on DL
    superiority to the more classical feature engineering (FE) approaches, based
    on domain knowledge, hold. In addition, it remains unclear whether combining
    DL with FE may improve performance over a single modality. Methods: To
    address these research gaps and in-line with recent major experiments, we
    revisited three tasks: cardiac arrhythmia diagnosis (multiclass-multilabel
    classification), atrial fibrillation risk prediction (binary
    classification), and age estimation (regression). We used an overall dataset
    of 2.3M 12-lead ECG recordings to train the following models for each task:
    i) a random forest taking FE as input; ii) an end-to-end DL model; and iii)
    a merged model of FE+DL. Results: FE yielded comparable results to DL while
    necessitating significantly less data for the two classification tasks. DL
    outperformed FE for the regression task. For all tasks, merging FE with DL
    did not improve performance over DL alone. These findings were confirmed on
    the additional PTB-XL dataset. Conclusion: We found that for traditional
    12-lead ECG based diagnosis tasks, DL did not yield a meaningful improvement
    over FE, while it improved significantly the nontraditional regression task.
    We also found that combining FE with DL did not improve over DL alone, which
    suggests that the FE were redundant with the features learned by DL.
    Significance: Our findings provides important recommendations on 12-lead ECG
    based machine learning strategy and data regime to choose for a given task.
    When looking at maximizing performance as the end goal, if the task is
    nontraditional and a large dataset is available then DL is preferable. If
    the task is a classical one and/or a small dataset is available then a FE
    approach may be the better choice.
  accessed:
    - year: 2022
      month: 7
      day: 22
  author:
    - family: Zvuloni
      given: Eran
    - family: Read
      given: Jesse
    - family: Ribeiro
      given: Antônio H.
    - family: Ribeiro
      given: Antonio Luiz P.
    - family: Behar
      given: Joachim A.
  citation-key: zvuloni_merging_2023
  container-title: IEEE Transactions on Biomedical Engineering
  DOI: 10.1109/TBME.2023.3239527
  issued:
    - year: 2023
      month: 1
      day: 25
  license: All rights reserved
  source: arXiv.org
  title: >-
    On Merging Feature Engineering and Deep Learning for Diagnosis,
    Risk-Prediction and Age Estimation Based on the 12-Lead ECG
  type: article-journal
  URL: http://arxiv.org/abs/2207.06096