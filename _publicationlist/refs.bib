
@article{ribeiro_efficient_2025,
	title = {Efficient {Optimization} {Algorithms} for {Linear} {Adversarial} {Training}},
	abstract = {Adversarial training can be used to learn models that are robust against perturbations. For linear models, it can be formulated as a convex optimization problem. Compared to methods proposed in the context of deep learning, leveraging the optimization structure allows significantly faster convergence rates. Still, the use of generic convex solvers can be inefficient for large-scale problems. Here, we propose tailored optimization algorithms for the adversarial training of linear models, which render large-scale regression and classification problems more tractable. For regression problems, we propose a family of solvers based on iterative ridge regression and, for classification, a family of solvers based on projected gradient descent. The methods are based on extended variable reformulations of the original problem. We illustrate their efficiency in numerical examples.},
	urldate = {2025-02-04},
	journal = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
	author = {RIbeiro, Antônio H. and Schön, Thomas B. and Zahariah, Dave and Bach, Francis},
	year = {2025},
	note = {arXiv:2410.12677 [stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{hempel_explainable_2025,
	title = {Explainable {AI} associates {ECG} aging effects with increased cardiovascular risk in a longitudinal population study},
	volume = {8},
	issn = {2398-6352},
	url = {https://doi.org/10.1038/s41746-024-01428-7},
	doi = {10.1038/s41746-024-01428-7},
	abstract = {Aging affects the 12-lead electrocardiogram (ECG) and correlates with cardiovascular disease (CVD). AI-ECG models estimate aging effects as a novel biomarker but have only been evaluated on single ECGs—without utilizing longitudinal data. We validated an AI-ECG model, originally trained on Brazilian data, using a German cohort with over 20 years of follow-up, demonstrating similar performance (r2 = 0.70) to the original study (0.71). Incorporating longitudinal ECGs revealed a stronger association with cardiovascular risk, increasing the hazard ratio for mortality from 1.43 to 1.65. Moreover, aging effects were associated with higher odds ratios for atrial fibrillation, heart failure, and mortality. Using explainable AI methods revealed that the model aligns with clinical knowledge by focusing on ECG features known to reflect aging. Our study suggests that aging effects in longitudinal ECGs can be applied on population level as a novel biomarker to identify patients at risk early.},
	number = {1},
	journal = {npj Digital Medicine},
	author = {Hempel, Philip and Ribeiro, Antônio H. and Vollmer, Marcus and Bender, Theresa and Dörr, Marcus and Krefting, Dagmar and Spicher, Nicolai},
	month = jan,
	year = {2025},
	pages = {25},
}

@article{ribeiro_ai-ecg_2024,
	title = {{AI}-{ECG} and prediction of new atrial fibrillation: when the heart tells the age},
	issn = {0195-668X},
	url = {https://doi.org/10.1093/eurheartj/ehae809},
	doi = {10.1093/eurheartj/ehae809},
	abstract = {This editorial refers to ‘Artificial intelligence–derived electrocardiographic aging and risk of atrial fibrillation: a multi-national study’, by S. Cho et al., https://doi.org/10.1093/eurheartj/ehae790.Ageing is a complex, multifaceted process encompassing biological, physiological, and psychosocial changes occurring over time. At the cellular and molecular levels, ageing is characterized by accumulating damage in cells and tissues, leading to a progressive decline in the body’s ability to maintain homeostasis and repair itself. This biological deterioration is not uniform across individuals or even within different organs of the same individual. While some organs may exhibit accelerated ageing, others might remain relatively preserved, highlighting the heterogeneity of the ageing process across the human body.1 The concept of ‘biological age’ vs. ‘chronological age’ is pivotal in understanding ageing. Chronological age is a simple measure of the years a person has lived, whereas biological age reflects the functional status and health of an individual’s organs and systems. Individuals of the same chronological age may have vastly different biological ages due to genetic predisposition, lifestyle factors, and environmental exposures. This variation can significantly impact disease risk and mortality. Understanding and measuring organ-specific ageing can open up avenues for targeted interventions that could decelerate ageing in specific organs, thereby improving overall health and extending a healthy life span.2},
	urldate = {2025-01-11},
	journal = {European Heart Journal},
	author = {Ribeiro, Antonio H and Ribeiro, Antonio Luiz P},
	month = dec,
	year = {2024},
	pages = {ehae809},
}

@article{rahimian_distributionally_2022,
	title = {Distributionally {Robust} {Optimization}: {A} {Review}},
	volume = {3},
	issn = {2777-5860},
	shorttitle = {Distributionally {Robust} {Optimization}},
	url = {http://arxiv.org/abs/1908.05659},
	doi = {10.5802/ojmo.15},
	abstract = {The concepts of risk-aversion, chance-constrained optimization, and robust optimization have developed significantly over the last decade. Statistical learning community has also witnessed a rapid theoretical and applied growth by relying on these concepts. A modeling framework, called distributionally robust optimization (DRO), has recently received significant attention in both the operations research and statistical learning communities. This paper surveys main concepts and contributions to DRO, and its relationships with robust optimization, risk-aversion, chance-constrained optimization, and function regularization.},
	urldate = {2024-12-17},
	journal = {Open Journal of Mathematical Optimization},
	author = {Rahimian, Hamed and Mehrotra, Sanjay},
	month = jul,
	year = {2022},
	note = {arXiv:1908.05659 [math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	pages = {1--85},
}

@article{rajabi_amplified_2024,
	title = {Amplified {Early} {Stopping} {Bias}: {Overestimated} {Performance} with {Deep} {Learning}},
	shorttitle = {Amplified {Early} {Stopping} {Bias}},
	url = {https://openreview.net/forum?id=wbUh4GL3c2#discussion},
	abstract = {Cross-validation is commonly used to estimate machine learning model performance on new samples. However, using it for both hyperparameter selection and error estimation can lead to overestimating model performance, especially with extensive hyperparameter searches that overly tailor models to validation data. We demonstrate that deep learning further amplifies this bias, with even minor model adjustments causing significant overestimation. Our extensive experiments on simulated and real data focus on the bias from early stopping during cross-validation. We find that overestimation intensifies with network depth and is especially severe in small datasets, which are common in physiological signal processing applications. Selecting the early stopping point during cross-validation can result in ROC-AUC estimates exceeding 90{\textbackslash}\% on random data, and this effect persists across various sample sizes, architectures, and network sizes.},
	language = {en},
	urldate = {2024-12-01},
	journal = {NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning},
	author = {Rajabi, Nona and Ribeiro, Antonio H. and Vasco, Miguel and Kragic, Danica},
	month = nov,
	year = {2024},
}

@article{von_bachmann_evaluating_2024,
	title = {Evaluating regression and probabilistic methods for {ECG}-based electrolyte prediction},
	volume = {14},
	copyright = {All rights reserved},
	doi = {10.1038/s41598-024-65223-w},
	abstract = {Imbalances in electrolyte concentrations can have severe consequences, but accurate and accessible measurements could improve patient outcomes. The current measurement method based on blood tests is accurate but invasive and time-consuming and is often unavailable for example in remote locations or an ambulance setting. In this paper, we explore the use of deep neural networks (DNNs) for regression tasks to accurately predict continuous electrolyte concentrations from electrocardiograms (ECGs), a quick and widely adopted tool. We analyze our DNN models on a novel dataset of over 290,000 ECGs across four major electrolytes and compare their performance with traditional machine learning models. For improved understanding, we also study the full spectrum from continuous predictions to a binary classification of extreme concentration levels. Finally, we investigate probabilistic regression approaches and explore uncertainty estimates for enhanced clinical usefulness. Our results show that DNNs outperform traditional models but model performance varies significantly across different electrolytes. While discretization leads to good classification performance, it does not address the original problem of continuous concentration level prediction. Probabilistic regression has practical potential, but our uncertainty estimates are not perfectly calibrated. Our study is therefore a first step towards developing an accurate and reliable ECG-based method for electrolyte concentration level prediction—a method with high potential impact within multiple clinical scenarios.},
	number = {15273},
	journal = {Scientific Reports},
	author = {Von Bachmann, Philipp and Gedon, Daniel and Gustafsson, Fredrik K. and Ribeiro, Antônio H. and Lampa, Erik and Gustafsson, Stefan and Sundström, Johan and Schön, Thomas B.},
	year = {2024},
	note = {arXiv:2212.13890},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
}

@article{sau_comparison_2024,
	title = {A comparison of artificial intelligence-enhanced electrocardiography approaches for prediction of time-to-mortality using electrocardiogram images},
	issn = {2634-3916},
	url = {https://doi.org/10.1093/ehjdh/ztae090},
	doi = {10.1093/ehjdh/ztae090},
	abstract = {Most artificial intelligence-enhanced ECG (AI-ECG) models used to predict adverse events including death require that the ECGs be stored digitally. However, the majority of clinical facilities worldwide store ECGs as images.1,163,401 ECGs (189,539 patients) from a secondary care dataset were available as both natively digital traces and PDF images. A digitisation pipeline extracted signals from PDFs. Separate 1D convolutional neural network (CNN) models were trained on natively digital or digitised ECGs, with a discrete-time survival loss function to predict time-to-mortality. A 2D CNN model was trained on 310x868 pixel ECG images. External validation was performed in 958,954 ECGs (645,373 patients) from a Brazilian primary care cohort and 1022 ECGs (1022 patients) from a Chagas disease cohort.The image 2D CNN model and digitised 1D CNN model performed comparably to natively digital 1D CNN model in internal (C-index 0.780 (0.779-0.781), 0.772 (0.771-0.774) and 0.775 (0.774-0.776 respectively) and external validation. Models trained on natively digital 1D ECGs had comparable performance when applied to digitised 1D ECGs (C-index 0.773 (0.771-0.774).Both the image 2D CNN and digitised 1D CNN enable mortality prediction from ECG images, with comparable performance to natively digital 1D CNN. Models trained on natively digital 1D ECGs can also be applied to digitised 1D ECGs, without any significant loss in performance. This works allows AI-ECG mortality prediction to be applied in diverse global settings lacking digital ECG infrastructure.},
	urldate = {2024-12-01},
	journal = {European Heart Journal - Digital Health},
	author = {Sau, Arunashis and Zeidaabadi, Boroumand and Patlatzoglou, Konstantinos and Pastika, Libor and Ribeiro, Antônio H and Sabino, Ester and Peters, Nicholas S and Ribeiro, Antonio Luiz P and Kramer, Daniel B and Waks, Jonathan W and Ng, Fu Siong},
	month = nov,
	year = {2024},
	pages = {ztae090},
}

@article{mcgurk_genetic_2024,
	title = {Genetic and phenotypic architecture of human myocardial trabeculation},
	issn = {2731-0590},
	url = {https://doi.org/10.1038/s44161-024-00564-3},
	doi = {10.1038/s44161-024-00564-3},
	abstract = {Cardiac trabeculae form a network of muscular strands that line the inner surfaces of the heart. Their development depends on multiscale morphogenetic processes and, while highly conserved across vertebrate evolution, their role in the pathophysiology of the mature heart is not fully understood. Here we report variant associations across the allele frequency spectrum for trabecular morphology in 47,803 participants of the UK Biobank using fractal dimension analysis of cardiac imaging. We identified an association between trabeculation and rare variants in 56 genes that regulate myocardial contractility and ventricular development. Genome-wide association studies identified 68 loci in pathways that regulate sarcomeric function, differentiation of the conduction system and cell fate determination. We found that trabeculation-associated variants were modifiers of cardiomyopathy phenotypes with opposing effects in hypertrophic and dilated cardiomyopathy. Together, these data provide insights into mechanisms that regulate trabecular development and plasticity, and identify a potential role in modifying monogenic disease expression.},
	journal = {Nature Cardiovascular Research},
	author = {McGurk, Kathryn A. and Qiao, Mengyun and Zheng, Sean L. and Sau, Arunashis and Henry, Albert and Ribeiro, Antonio Luiz P. and Ribeiro, Antônio H. and Ng, Fu Siong and Lumbers, R. Thomas and Bai, Wenjia and Ware, James S. and O’Regan, Declan P.},
	month = nov,
	year = {2024},
}

@article{sau_prognostic_2024,
	title = {Prognostic {Significance} and {Associations} of {Neural} {Network}–{Derived} {Electrocardiographic} {Features}},
	volume = {0},
	url = {https://doi.org/10.1161/CIRCOUTCOMES.123.010602},
	doi = {10.1161/CIRCOUTCOMES.123.010602},
	number = {0},
	urldate = {2024-12-01},
	journal = {Circulation: Cardiovascular Quality and Outcomes},
	author = {Sau, Arunashis and Ribeiro, Antônio H. and McGurk, Kathryn A. and Pastika, Libor and Bajaj, Nikesh and Gurnani, Mehak and Sieliwonczyk, Ewa and Patlatzoglou, Konstantinos and Ardissino, Maddalena and Chen, Jun Yu and Wu, Huiyi and Shi, Xili and Hnatkova, Katerina and Zheng, Sean L. and Britton, Annie and Shipley, Martin and Andršová, Irena and Novotný, Tomáš and Sabino, Ester C. and Giatti, Luana and Barreto, Sandhi M. and Waks, Jonathan W. and Kramer, Daniel B. and Mandic, Danilo and Peters, Nicholas S. and O’Regan, Declan P. and Malik, Marek and Ware, James S. and Ribeiro, Antonio Luiz P. and Ng, Fu Siong},
	month = nov,
	year = {2024},
	note = {Publisher: American Heart Association},
	pages = {e010602},
}

@article{JMLR:v9:rakotomamonjy08a,
	title = {{SimpleMKL}},
	volume = {9},
	url = {http://jmlr.org/papers/v9/rakotomamonjy08a.html},
	number = {83},
	journal = {Journal of Machine Learning Research},
	author = {Rakotomamonjy, Alain and Bach, Francis R. and Canu, Stéphane and Grandvalet, Yves},
	year = {2008},
	pages = {2491--2521},
}

@article{arlot_data-driven_nodate,
	title = {Data-driven calibration of linear estimators with minimal penalties},
	abstract = {This paper tackles the problem of selecting among several linear estimators in non-parametric regression; this includes model selection for linear regression, the choice of a regularization parameter in kernel ridge regression or spline smoothing, and the choice of a kernel in multiple kernel learning. We propose a new algorithm which ﬁrst estimates consistently the variance of the noise, based upon the concept of minimal penalty which was previously introduced in the context of model selection. Then, plugging our variance estimate in Mallows’ CL penalty is proved to lead to an algorithm satisfying an oracle inequality. Simulation experiments with kernel ridge regression and multiple kernel learning show that the proposed algorithm often improves signiﬁcantly existing calibration procedures such as 10-fold cross-validation or generalized cross-validation.},
	language = {en},
	author = {Arlot, Sylvain and Bach, Francis R},
}

@article{sau_artificial_2024,
	title = {Artificial intelligence-enabled electrocardiogram for mortality and cardiovascular risk estimation: a model development and validation study},
	volume = {6},
	issn = {2589-7500},
	url = {https://www.sciencedirect.com/science/article/pii/S2589750024001729},
	doi = {10.1016/S2589-7500(24)00172-9},
	abstract = {Summary
Background
Artificial intelligence (AI)-enabled electrocardiography (ECG) can be used to predict risk of future disease and mortality but has not yet been adopted into clinical practice. Existing model predictions do not have actionability at an individual patient level, explainability, or biological plausibi. We sought to address these limitations of previous AI-ECG approaches by developing the AI-ECG risk estimator (AIRE) platform.
Methods
The AIRE platform was developed in a secondary care dataset (Beth Israel Deaconess Medical Center [BIDMC]) of 1 163 401 ECGs from 189 539 patients with deep learning and a discrete-time survival model to create a patient-specific survival curve with a single ECG. Therefore, AIRE predicts not only risk of mortality, but also time-to-mortality. AIRE was validated in five diverse, transnational cohorts from the USA, Brazil, and the UK (UK Biobank [UKB]), including volunteers, primary care patients, and secondary care patients.
Findings
AIRE accurately predicts risk of all-cause mortality (BIDMC C-index 0·775, 95\% CI 0·773–0·776; C-indices on external validation datasets 0·638–0·773), future ventricular arrhythmia (BIDMC C-index 0·760, 95\% CI 0·756–0·763; UKB C-index 0·719, 95\% CI 0·635–0·803), future atherosclerotic cardiovascular disease (0·696, 0·694–0·698; 0·643, 0·624–0·662), and future heart failure (0·787, 0·785–0·789; 0·768, 0·733–0·802). Through phenome-wide and genome-wide association studies, we identified candidate biological pathways for the prediction of increased risk, including changes in cardiac structure and function, and genes associated with cardiac structure, biological ageing, and metabolic syndrome.
Interpretation
AIRE is an actionable, explainable, and biologically plausible AI-ECG risk estimation platform that has the potential for use worldwide across a wide range of clinical contexts for short-term and long-term risk estimation.
Funding
British Heart Foundation, National Institute for Health and Care Research, and Medical Research Council.},
	number = {11},
	journal = {The Lancet Digital Health},
	author = {Sau, Arunashis and Pastika, Libor and Sieliwonczyk, Ewa and Patlatzoglou, Konstantinos and Ribeiro, Antoônio H and McGurk, Kathryn A and Zeidaabadi, Boroumand and Zhang, Henry and Macierzanka, Krzysztof and Mandic, Danilo and Sabino, Ester and Giatti, Luana and Barreto, Sandhi M and Camelo, Lidyane do Valle and Tzoulaki, Ioanna and O'Regan, Declan P and Peters, Nicholas S and Ware, James S and Ribeiro, Antonio Luiz P and Kramer, Daniel B and Waks, Jonathan W and Ng, Fu Siong},
	month = nov,
	year = {2024},
	pages = {e791--e802},
}

@article{taleb_can_2024,
	title = {Can {Transformers} {Smell} {Like} {Humans}?},
	url = {https://nips.cc/virtual/2024/poster/96729},
	urldate = {2024-10-21},
	journal = {Advances in Neural Information Processing Systems},
	author = {Taleb, Farzaneh and Vasco, Miguel and Ribeiro, Antonio H. and Björkman, Mårten and Kragic, Danica},
	year = {2024},
}

@article{pillonetto_deep_2025,
	title = {Deep networks for system identification: a {Survey}},
	doi = {10.1016/j.automatica.2024.111907},
	journal = {Automatica},
	author = {Pillonetto, Gianluigi and Aravkin, Aleksandr and Gedon, Daniel and Ljung, Lennart and Ribeiro, Antonio H. and Schön, Thomas Bo},
	year = {2025},
}

@misc{klaassen_doublemldeep_2024,
	title = {{DoubleMLDeep}: {Estimation} of {Causal} {Effects} with {Multimodal} {Data}},
	shorttitle = {{DoubleMLDeep}},
	url = {http://arxiv.org/abs/2402.01785},
	abstract = {This paper explores the use of unstructured, multimodal data, namely text and images, in causal inference and treatment effect estimation. We propose a neural network architecture that is adapted to the double machine learning (DML) framework, specifically the partially linear model. An additional contribution of our paper is a new method to generate a semi-synthetic dataset which can be used to evaluate the performance of causal effect estimation in the presence of text and images as confounders. The proposed methods and architectures are evaluated on the semi-synthetic dataset and compared to standard approaches, highlighting the potential benefit of using text and images directly in causal studies. Our findings have implications for researchers and practitioners in economics, marketing, finance, medicine and data science in general who are interested in estimating causal quantities using non-traditional data.},
	language = {en},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Klaassen, Sven and Teichert-Kluge, Jan and Bach, Philipp and Chernozhukov, Victor and Spindler, Martin and Vijaykumar, Suhas},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01785 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Economics - Econometrics, Statistics - Machine Learning, Statistics - Methodology},
}

@article{belloni_inference_2014,
	title = {Inference on {Treatment} {Effects} after {Selection} among {High}-{Dimensional} {Controls}†},
	volume = {81},
	issn = {0034-6527},
	url = {https://doi.org/10.1093/restud/rdt044},
	doi = {10.1093/restud/rdt044},
	abstract = {We propose robust methods for inference about the effect of a treatment variable on a scalar outcome in the presence of very many regressors in a model with possibly non-Gaussian and heteroscedastic disturbances. We allow for the number of regressors to be larger than the sample size. To make informative inference feasible, we require the model to be approximately sparse; that is, we require that the effect of confounding factors can be controlled for up to a small approximation error by including a relatively small number of variables whose identities are unknown. The latter condition makes it possible to estimate the treatment effect by selecting approximately the right set of regressors. We develop a novel estimation and uniformly valid inference method for the treatment effect in this setting, called the “post-double-selection” method. The main attractive feature of our method is that it allows for imperfect selection of the controls and provides confidence intervals that are valid uniformly across a large class of models. In contrast, standard post-model selection estimators fail to provide uniform inference even in simple cases with a small, fixed number of controls. Thus, our method resolves the problem of uniform inference after model selection for a large, interesting class of models. We also present a generalization of our method to a fully heterogeneous model with a binary treatment variable. We illustrate the use of the developed methods with numerical simulations and an application that considers the effect of abortion on crime rates.},
	number = {2},
	urldate = {2024-10-14},
	journal = {The Review of Economic Studies},
	author = {Belloni, Alexandre and Chernozhukov, Victor and Hansen, Christian},
	month = apr,
	year = {2014},
	pages = {608--650},
}

@misc{curth_classical_2024,
	title = {Classical {Statistical} ({In}-{Sample}) {Intuitions} {Don}'t {Generalize} {Well}: {A} {Note} on {Bias}-{Variance} {Tradeoffs}, {Overfitting} and {Moving} from {Fixed} to {Random} {Designs}},
	shorttitle = {Classical {Statistical} ({In}-{Sample}) {Intuitions} {Don}'t {Generalize} {Well}},
	url = {http://arxiv.org/abs/2409.18842},
	doi = {10.48550/arXiv.2409.18842},
	abstract = {The sudden appearance of modern machine learning (ML) phenomena like double descent and benign overfitting may leave many classically trained statisticians feeling uneasy -- these phenomena appear to go against the very core of statistical intuitions conveyed in any introductory class on learning from data. The historical lack of earlier observation of such phenomena is usually attributed to today's reliance on more complex ML methods, overparameterization, interpolation and/or higher data dimensionality. In this note, we show that there is another reason why we observe behaviors today that appear at odds with intuitions taught in classical statistics textbooks, which is much simpler to understand yet rarely discussed explicitly. In particular, many intuitions originate in fixed design settings, in which in-sample prediction error (under resampling of noisy outcomes) is of interest, while modern ML evaluates its predictions in terms of generalization error, i.e. out-of-sample prediction error in random designs. Here, we highlight that this simple move from fixed to random designs has (perhaps surprisingly) far-reaching consequences on textbook intuitions relating to the bias-variance tradeoff, and comment on the resulting (im)possibility of observing double descent and benign overfitting in fixed versus random designs.},
	urldate = {2024-10-11},
	publisher = {arXiv},
	author = {Curth, Alicia},
	month = sep,
	year = {2024},
	note = {arXiv:2409.18842},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{bach_scaling_2024,
	title = {Scaling laws of optimization – {Machine} {Learning} {Research} {Blog}},
	url = {https://francisbach.com/scaling-laws-of-optimization/},
	language = {en-US},
	urldate = {2024-10-11},
	author = {Bach, Francis},
	month = oct,
	year = {2024},
}

@article{condat_fast_2016,
	title = {Fast projection onto the simplex and the l1 ball},
	volume = {158},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-015-0946-6},
	doi = {10.1007/s10107-015-0946-6},
	abstract = {A new algorithm is proposed to project, exactly and in finite time, a vector of arbitrary size onto a simplex or an \$\$l\_1\$\$-norm ball. It can be viewed as a Gauss–Seidel-like variant of Michelot’s variable fixing algorithm; that is, the threshold used to fix the variables is updated after each element is read, instead of waiting for a full reading pass over the list of non-fixed elements. This algorithm is empirically demonstrated to be faster than existing methods.},
	language = {en},
	number = {1},
	urldate = {2024-05-22},
	journal = {Mathematical Programming},
	author = {Condat, Laurent},
	month = jul,
	year = {2016},
	keywords = {49M30, 65C60, 65K05, 90C25, Large-scale optimization, Simplex, l\_1-Norm ball},
	pages = {575--585},
}

@article{xie_high-dimensional_2024,
	title = {High-dimensional ({Group}) {Adversarial} {Training} in {Linear} {Regression}},
	url = {http://arxiv.org/abs/2405.13940},
	abstract = {Adversarial training can achieve robustness against adversarial perturbations and has been widely used in machine learning models. This paper delivers a non-asymptotic consistency analysis of the adversarial training procedure under ℓ∞-perturbation in high-dimensional linear regression. It will be shown that the associated convergence rate of prediction error can achieve the minimax rate up to a logarithmic factor in the high-dimensional linear regression on the class of sparse parameters. Additionally, the group adversarial training procedure is analyzed. Compared with classic adversarial training, it will be proved that the group adversarial training procedure enjoys a better prediction error upper bound under certain group-sparsity patterns.},
	language = {en},
	urldate = {2024-07-15},
	journal = {Avances in Neural Information Processing Sys- tems (NeurIPS)},
	author = {Xie, Yiling and Huo, Xiaoming},
	year = {2024},
	note = {arXiv:2405.13940},
	keywords = {Mathematics - Statistics Theory},
}

@article{boyd_distributed_2011,
	title = {Distributed {Optimization} and {Statistical} {Learning} via the {Alternating} {Direction} {Method} of {Multipliers}},
	volume = {3},
	issn = {1935-8237, 1935-8245},
	url = {https://www.nowpublishers.com/article/Details/MAL-016},
	doi = {10.1561/2200000016},
	abstract = {Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers},
	language = {English},
	number = {1},
	urldate = {2024-10-09},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
	month = jul,
	year = {2011},
	note = {Publisher: Now Publishers, Inc.},
	pages = {1--122},
}

@book{buhlmann_statistics_2011,
	address = {Berlin, Heidelberg},
	series = {Springer {Series} in {Statistics}},
	title = {Statistics for {High}-{Dimensional} {Data}: {Methods}, {Theory} and {Applications}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-642-20191-2 978-3-642-20192-9},
	shorttitle = {Statistics for {High}-{Dimensional} {Data}},
	url = {https://link.springer.com/10.1007/978-3-642-20192-9},
	language = {en},
	urldate = {2024-10-09},
	publisher = {Springer Berlin Heidelberg},
	author = {Bühlmann, Peter and Van De Geer, Sara},
	year = {2011},
	doi = {10.1007/978-3-642-20192-9},
}

@article{burgess_robust_2020,
	title = {A robust and efficient method for {Mendelian} randomization with hundreds of genetic variants},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-14156-4},
	doi = {10.1038/s41467-019-14156-4},
	abstract = {Mendelian randomization (MR) is an epidemiological technique that uses genetic variants to distinguish correlation from causation in observational data. The reliability of a MR investigation depends on the validity of the genetic variants as instrumental variables (IVs). We develop the contamination mixture method, a method for MR with two modalities. First, it identifies groups of genetic variants with similar causal estimates, which may represent distinct mechanisms by which the risk factor influences the outcome. Second, it performs MR robustly and efficiently in the presence of invalid IVs. Compared to other robust methods, it has the lowest mean squared error across a range of realistic scenarios. The method identifies 11 variants associated with increased high-density lipoprotein-cholesterol, decreased triglyceride levels, and decreased coronary heart disease risk that have the same directions of associations with various blood cell traits, suggesting a shared mechanism linking lipids and coronary heart disease risk mediated via platelet aggregation.},
	language = {en},
	number = {1},
	urldate = {2024-10-09},
	journal = {Nature Communications},
	author = {Burgess, Stephen and Foley, Christopher N. and Allara, Elias and Staley, James R. and Howson, Joanna M. M.},
	month = jan,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cardiovascular genetics, Epidemiology, Genetic association study, Prognostic markers},
	pages = {376},
}

@article{safo_derivation_2023,
	title = {Derivation of a {Protein} {Risk} {Score} for {Cardiovascular} {Disease} {Among} a {Multiracial} and {Multiethnic} {HIV}+ {Cohort}},
	volume = {12},
	url = {https://www.ahajournals.org/doi/full/10.1161/JAHA.122.027273},
	doi = {10.1161/JAHA.122.027273},
	abstract = {BackgroundCardiovascular disease risk prediction models underestimate CVD risk in people living with HIV (PLWH). Our goal is to derive a risk score based on protein biomarkers that could be used to predict CVD in PLWH.Methods and ResultsIn a matched case–control study, we analyzed normalized protein expression data for participants enrolled in 1 of 4 trials conducted by INSIGHT (International Network for Strategic Initiatives in Global HIV Trials). We used dimension reduction, variable selection and resampling methods, and multivariable conditional logistic regression models to determine candidate protein biomarkers and to generate a protein score for predicting CVD in PLWH. We internally validated our findings using bootstrap. A protein score that was derived from 8 proteins (including HGF [hepatocyte growth factor] and interleukin‐6) was found to be associated with an increased risk of CVD after adjustment for CVD and HIV factors (odds ratio: 2.17 [95\% CI: 1.58–2.99]). The protein score improved CVD prediction when compared with predicting CVD risk using the individual proteins that comprised the protein score. Individuals with a protein score above the median score were 3.10 (95\% CI, 1.83–5.41) times more likely to develop CVD than those with a protein score below the median score.ConclusionsA panel of blood biomarkers may help identify PLWH at a high risk for developing CVD. If validated, such a score could be used in conjunction with established factors to identify CVD at‐risk individuals who might benefit from aggressive risk reduction, ultimately shedding light on CVD pathogenesis in PLWH.},
	number = {13},
	urldate = {2024-10-09},
	journal = {Journal of the American Heart Association},
	author = {Safo, Sandra E. and Haine, Lillian and Baker, Jason and Reilly, Cavan and Duprez, Daniel and Neaton, James D. and Jain, Mamta K. and Arenas‐Pinto, Alejandro and Polizzotto, Mark and Staub, Therese and {for the ESPRIT, INSIGHT FIRST, SMART and START study groups}},
	month = jul,
	year = {2023},
	note = {Publisher: Wiley},
	pages = {e027273},
}

@article{carlini_towards_2017,
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classiﬁcation t, it is possible to ﬁnd a new input x′ that is similar to x but classiﬁed as t. This makes it difﬁcult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks’ ability to ﬁnd adversarial examples from 95\% to 0.5\%.},
	language = {en},
	urldate = {2024-10-07},
	journal = {2017 IEEE Symposium on Security and Privacy (SP)},
	author = {Carlini, Nicholas and Wagner, David},
	month = mar,
	year = {2017},
	note = {arXiv:1608.04644 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@inproceedings{moosavi-dezfooli_deepfool_2016,
	address = {Las Vegas, NV, USA},
	title = {{DeepFool}: {A} {Simple} and {Accurate} {Method} to {Fool} {Deep} {Neural} {Networks}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {{DeepFool}},
	url = {http://ieeexplore.ieee.org/document/7780651/},
	doi = {10.1109/CVPR.2016.282},
	language = {en},
	urldate = {2024-10-07},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
	month = jun,
	year = {2016},
	pages = {2574--2582},
}

@misc{noauthor_krav_nodate,
	title = {krav och funktionsschema - {Google} {Search}},
	url = {https://www.google.com/search?q=krav+och+funktionsschema&sca_esv=7aa1caabb4121f2c&sca_upv=1&rlz=1C5GCCM_en&sxsrf=ADLYWIJPrVq6r6cITJdIx78fqQkRUX2IGA%3A1727712250447&ei=-sv6Zq_kGrqowPAPnpGDqAg&oq=Krav+och+funktionsschema&gs_lp=Egxnd3Mtd2l6LXNlcnAiGEtyYXYgb2NoIGZ1bmt0aW9uc3NjaGVtYSoCCAAyBxAAGLADGB4yBxAAGLADGB4yCxAAGIAEGLADGKIEMgsQABiABBiwAxiiBDILEAAYgAQYsAMYogQyCxAAGIAEGLADGKIESKEMUABYAHABeACQAQCYAQCgAQCqAQC4AQHIAQCYAgGgAgOYAwCIBgGQBgaSBwExoAcA&sclient=gws-wiz-serp},
	urldate = {2024-09-30},
}

@misc{sun_deep_2016,
	title = {Deep {CORAL}: {Correlation} {Alignment} for {Deep} {Domain} {Adaptation}},
	shorttitle = {Deep {CORAL}},
	url = {http://arxiv.org/abs/1607.01719},
	abstract = {Deep neural networks are able to learn powerful representations from large quantities of labeled input data, however they cannot always generalize well across changes in input distributions. Domain adaptation algorithms have been proposed to compensate for the degradation in performance due to domain shift. In this paper, we address the case when the target domain is unlabeled, requiring unsupervised adaptation. CORAL[1] is a “frustratingly easy” unsupervised domain adaptation method that aligns the second-order statistics of the source and target distributions with a linear transformation. Here, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (Deep CORAL). Experiments on standard benchmark datasets show state-of-the-art performance.},
	language = {en},
	urldate = {2024-08-28},
	publisher = {arXiv},
	author = {Sun, Baochen and Saenko, Kate},
	month = jul,
	year = {2016},
	note = {arXiv:1607.01719 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{ganin_unsupervised_2015,
	title = {Unsupervised {Domain} {Adaptation} by {Backpropagation}},
	url = {http://arxiv.org/abs/1409.7495},
	abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled targetdomain data is necessary).},
	urldate = {2024-08-28},
	journal = {International Conference on Machine Learning (ICML)},
	author = {Ganin, Yaroslav and Lempitsky, Victor},
	year = {2015},
	note = {arXiv:1409.7495 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	booktitle = {Neural {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	year = {2022},
}

@article{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	abstract = {SOTA computer vision systems are trained to predict a ﬁxed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efﬁcient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study performance on over 30 different computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of ﬁne-grained object classiﬁcation. The model transfers nontrivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset speciﬁc training. For instance, we match the accuracy of the original ResNet50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	language = {en},
	journal = {International Conference on Machine Learning (ICML)},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	year = {2021},
}

@article{alonso_simple_2013,
	title = {Simple {Risk} {Model} {Predicts} {Incidence} of {Atrial} {Fibrillation} in a {Racially} and {Geographically} {Diverse} {Population}: the {CHARGE} {AF} {Consortium}},
	volume = {2},
	shorttitle = {Simple {Risk} {Model} {Predicts} {Incidence} of {Atrial} {Fibrillation} in a {Racially} and {Geographically} {Diverse} {Population}},
	url = {https://www.ahajournals.org/doi/10.1161/JAHA.112.000102},
	doi = {10.1161/JAHA.112.000102},
	abstract = {BackgroundTools for the prediction of atrial fibrillation (AF) may identify high‐risk individuals more likely to benefit from preventive interventions and serve as a benchmark to test novel putative risk factors.Methods and ResultsIndividual‐level data from 3 large cohorts in the United States (Atherosclerosis Risk in Communities [ARIC] study, the Cardiovascular Health Study [CHS], and the Framingham Heart Study [FHS]), including 18 556 men and women aged 46 to 94 years (19\% African Americans, 81\% whites) were pooled to derive predictive models for AF using clinical variables. Validation of the derived models was performed in 7672 participants from the Age, Gene and Environment—Reykjavik study (AGES) and the Rotterdam Study (RS). The analysis included 1186 incident AF cases in the derivation cohorts and 585 in the validation cohorts. A simple 5‐year predictive model including the variables age, race, height, weight, systolic and diastolic blood pressure, current smoking, use of antihypertensive medication, diabetes, and history of myocardial infarction and heart failure had good discrimination (C‐statistic, 0.765; 95\% CI, 0.748 to 0.781). Addition of variables from the electrocardiogram did not improve the overall model discrimination (C‐statistic, 0.767; 95\% CI, 0.750 to 0.783; categorical net reclassification improvement, −0.0032; 95\% CI, −0.0178 to 0.0113). In the validation cohorts, discrimination was acceptable (AGES C‐statistic, 0.664; 95\% CI, 0.632 to 0.697 and RS C‐statistic, 0.705; 95\% CI, 0.664 to 0.747) and calibration was adequate.ConclusionA risk model including variables readily available in primary care settings adequately predicted AF in diverse populations from the United States and Europe.},
	number = {2},
	urldate = {2024-06-10},
	journal = {Journal of the American Heart Association},
	author = {Alonso, Alvaro and Krijthe, Bouwe P. and Aspelund, Thor and Stepas, Katherine A. and Pencina, Michael J. and Moser, Carlee B. and Sinner, Moritz F. and Sotoodehnia, Nona and Fontes, João D. and Janssens, A. Cecile J. W. and Kronmal, Richard A. and Magnani, Jared W. and Witteman, Jacqueline C. and Chamberlain, Alanna M. and Lubitz, Steven A. and Schnabel, Renate B. and Agarwal, Sunil K. and McManus, David D. and Ellinor, Patrick T. and Larson, Martin G. and Burke, Gregory L. and Launer, Lenore J. and Hofman, Albert and Levy, Daniel and Gottdiener, John S. and Kääb, Stefan and Couper, David and Harris, Tamara B. and Soliman, Elsayed Z. and Stricker, Bruno H. C. and Gudnason, Vilmundur and Heckbert, Susan R. and Benjamin, Emelia J.},
	year = {2013},
	note = {Publisher: Wiley},
	keywords = {atrial fibrillation, epidemiology, risk factors},
	pages = {e000102},
}

@inproceedings{ribeiro_automatic_2020,
	title = {Automatic 12-lead {ECG} classiﬁcation using a convolutional network ensemble},
	copyright = {All rights reserved},
	doi = {10.22489/CinC.2020.130},
	abstract = {The 12-lead electrocardiogram (ECG) is a major diagnostic test for cardiovascular diseases and enhanced automated analysis tools might lead to more reliable diagnosis and improved clinical practice. Deep neural networks are models composed of stacked transformations that learn tasks by examples. Inspired by the success of these models in computer vision, we propose an end-to-end approach for the task at hand. We trained deep convolutional neural network models in the heterogeneous dataset provided in the Physionet 2020 Challenge and used an ensemble of seven of these convolutional models for the classiﬁcation of abnormalities present in the ECG records. Ensembles use the output of multiple models to generate a combined prediction and are known to improve performance and generalization when compared to the individual models. In our submission, we use an ensemble of neural networks with the architecture similar to the one described in Nat Commun 11, 1760 (2020) for 12-lead ECGs classiﬁcation. On the partially hidden test dataset from the challenge, the best-scored entry for our team (the “Code Team”) had a performance of 0.657, which place us in the 7-th place team-wise in the challenge leaderboard.},
	booktitle = {Computing in {Cardiology} ({CinC})},
	author = {Ribeiro, Antonio H and Gedon, Daniel and Teixeira, Daniel Martins and Ribeiro, Manoel Horta and Ribeiro, Antonio L Pinho and Schon, Thomas B and Jr, Wagner Meira},
	year = {2020},
}

@article{candes_conformalized_2023,
	title = {Conformalized {Survival} {Analysis}},
	volume = {85},
	url = {http://arxiv.org/abs/2103.09763},
	doi = {10.1093/jrsssb/qkac004},
	abstract = {Existing survival analysis techniques heavily rely on strong modelling assumptions and are, therefore, prone to model misspecification errors. In this paper, we develop an inferential method based on ideas from conformal prediction, which can wrap around any survival prediction algorithm to produce calibrated, covariate-dependent lower predictive bounds on survival times. In the Type I right-censoring setting, when the censoring times are completely exogenous, the lower predictive bounds have guaranteed coverage in finite samples without any assumptions other than that of operating on independent and identically distributed data points. Under a more general conditionally independent censoring assumption, the bounds satisfy a doubly robust property which states the following: marginal coverage is approximately guaranteed if either the censoring mechanism or the conditional survival function is estimated well. Further, we demonstrate that the lower predictive bounds remain valid and informative for other types of censoring. The validity and efficiency of our procedure are demonstrated on synthetic data and real COVID-19 data from the UK Biobank.},
	number = {1},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Candès, Emmanuel J. and Lei, Lihua and Ren, Zhimei},
	year = {2023},
	note = {arXiv:2103.09763},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, \_tablet},
	pages = {24--45},
}

@book{angelopoulos_conformal_2023,
	title = {Conformal {Prediction}: {A} {Gentle} {Introduction}},
	url = {http://arxiv.org/abs/2107.07511},
	abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantiﬁcation to avoid consequential model failures. Conformal prediction (a.k.a. conformal inference) is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-speciﬁed probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the ﬁelds of computer vision, natural language processing, deep reinforcement learning, and so on.},
	language = {en},
	urldate = {2023-09-07},
	publisher = {Now Foundations and Trends},
	author = {Angelopoulos, Anastasios N. and Bates, Stephen},
	year = {2023},
	note = {arXiv:2107.07511 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
}

@article{bai_recent_2021,
	title = {Recent {Advances} in {Adversarial} {Training} for {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/2102.01356},
	abstract = {Adversarial training is one of the most effective approaches to defending deep learning models against adversarial examples. Unlike other defense strategies, adversarial training aims to enhance the robustness of models intrinsically. During the last few years, adversarial training has been studied and discussed from various aspects. A variety of improvements and developments of adversarial training are proposed, which were, however, neglected in existing surveys. For the ﬁrst time in this survey, we systematically review the recent progress on adversarial training for adversarial robustness with a novel taxonomy. Then we discuss the generalization problems in adversarial training from three perspectives and highlight the challenges which are not fully tackled. Finally, we present potential future directions.},
	journal = {International Joint Conference on Artificial Intelligence (IJCAI)},
	author = {Bai, Tao and Luo, Jinqi and Zhao, Jun and Wen, Bihan and Wang, Qian},
	month = apr,
	year = {2021},
	note = {arXiv: 2102.01356},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{pastika_artificial_2024,
	title = {Artificial intelligence–enabled electrocardiogram for mortality and cardiovascular risk estimation: {An} actionable, explainable and biologically plausible platform},
	volume = {7},
	doi = {https://doi.org/10.1038/s41746-024-01170-0},
	number = {167},
	journal = {npj Digital Medicine},
	author = {Pastika, Libor and Sau, Arunashis and Patlatzoglou, Konstantinos and Sieliwonczyk, Ewa and Ribeiro, Antonio H. and McGurk, Kathryn A. and Scott, William R and Ware, James S. and Ribeiro, Antonio Luiz P. and Kramer, Daniel B. and Waks, Jonathan W. and Ng, Fu Siong},
	year = {2024},
}

@article{muehlematter_fda-cleared_2023,
	title = {{FDA}-cleared artificial intelligence and machine learning-based medical devices and their 510(k) predicate networks},
	volume = {5},
	issn = {2589-7500},
	url = {https://www.thelancet.com/journals/landig/article/PIIS2589-7500(23)00126-7/fulltext},
	doi = {10.1016/S2589-7500(23)00126-7},
	language = {English},
	number = {9},
	urldate = {2024-08-19},
	journal = {The Lancet Digital Health},
	author = {Muehlematter, Urs J. and Bluethgen, Christian and Vokinger, Kerstin N.},
	month = sep,
	year = {2023},
	pmid = {37625896},
	note = {Publisher: Elsevier},
	pages = {e618--e626},
}

@article{martinez-millana_artificial_2022,
	title = {Artificial intelligence and its impact on the domains of universal health coverage, health emergencies and health promotion: {An} overview of systematic reviews},
	volume = {166},
	issn = {1386-5056},
	shorttitle = {Artificial intelligence and its impact on the domains of universal health coverage, health emergencies and health promotion},
	url = {https://www.sciencedirect.com/science/article/pii/S1386505622001691},
	doi = {10.1016/j.ijmedinf.2022.104855},
	abstract = {Background
Artificial intelligence is fueling a new revolution in medicine and in the healthcare sector. Despite the growing evidence on the benefits of artificial intelligence there are several aspects that limit the measure of its impact in people’s health. It is necessary to assess the current status on the application of AI towards the improvement of people’s health in the domains defined by WHO’s Thirteenth General Programme of Work (GPW13) and the European Programme of Work (EPW), to inform about trends, gaps, opportunities, and challenges.
Objective
To perform a systematic overview of systematic reviews on the application of artificial intelligence in the people’s health domains as defined in the GPW13 and provide a comprehensive and updated map on the application specialties of artificial intelligence in terms of methodologies, algorithms, data sources, outcomes, predictors, performance, and methodological quality.
Methods
A systematic search in MEDLINE, EMBASE, Cochrane and IEEEXplore was conducted between January 2015 and June 2021 to collect systematic reviews using a combination of keywords related to the domains of universal health coverage, health emergencies protection, and better health and wellbeing as defined by the WHO’s PGW13 and EPW. Eligibility criteria was based on methodological quality and the inclusion of practical implementation of artificial intelligence. Records were classified and labeled using ICD-11 categories into the domains of the GPW13. Descriptors related to the area of implementation, type of modeling, data entities, outcomes and implementation on care delivery were extracted using a structured form and methodological aspects of the included reviews studies was assessed using the AMSTAR checklist.
Results
The search strategy resulted in the screening of 815 systematic reviews from which 203 were assessed for eligibility and 129 were included in the review. The most predominant domain for artificial intelligence applications was Universal Health Coverage (N = 98) followed by Health Emergencies (N = 16) and Better Health and Wellbeing (N = 15). Neoplasms area on Universal Health Coverage was the disease area featuring most of the applications (21.7 \%, N = 28). The reviews featured analytics primarily over both public and private data sources (67.44 \%, N = 87). The most used type of data was medical imaging (31.8 \%, N = 41) and predictors based on regions of interest and clinical data. The most prominent subdomain of Artificial Intelligence was Machine Learning (43.4 \%, N = 56), in which Support Vector Machine method was predominant (20.9 \%, N = 27). Regarding the purpose, the application of Artificial Intelligence I is focused on the prediction of the diseases (36.4 \%, N = 47). With respect to the validation, more than a half of the reviews (54.3 \%, N = 70) did not report a validation procedure and, whenever available, the main performance indicator was the accuracy (28.7 \%, N = 37). According to the methodological quality assessment, a third of the reviews (34.9 \%, N = 45) implemented methods for analysis the risk of bias and the overall AMSTAR score below was 5 (4.01 ± 1.93) on all the included systematic reviews.
Conclusion
Artificial intelligence is being used for disease modelling, diagnose, classification and prediction in the three domains of GPW13. However, the evidence is often limited to laboratory and the level of adoption is largely unbalanced between ICD-11 categoriesand diseases. Data availability is a determinant factor on the developmental stage of artificial intelligence applications. Most of the reviewed studies show a poor methodological quality and are at high risk of bias, which limits the reproducibility of the results and the reliability of translating these applications to real clinical scenarios. The analyzed papers show results only in laboratory and testing scenarios and not in clinical trials nor case studies, limiting the supporting evidence to transfer artificial intelligence to actual care delivery.},
	urldate = {2024-08-19},
	journal = {International Journal of Medical Informatics},
	author = {Martinez-Millana, Antonio and Saez-Saez, Aida and Tornero-Costa, Roberto and Azzopardi-Muscat, Natasha and Traver, Vicente and Novillo-Ortiz, David},
	month = oct,
	year = {2022},
	keywords = {European region, Health and well-being, Health emergencies, Machine learning, Universal health coverage},
	pages = {104855},
}

@article{kashef_legacy_2016,
	title = {Legacy effect of statins: 20-year follow up of the {West} of {Scotland} {Coronary} {Prevention} {Study} ({WOSCOPS})},
	volume = {2016},
	copyright = {Copyright (c) 2017 Mohammed Amin Kashef, Gregory Giugliano},
	issn = {2305-7823},
	shorttitle = {Legacy effect of statins},
	url = {https://globalcardiologyscienceandpractice.com/index.php/gcsp/article/view/81},
	doi = {10.21542/gcsp.2016.35},
	abstract = {The West of Scotland Coronary Prevention Study (WOSCOPS) was a randomized, placebo- controlled, primary prevention trial of pravastatin in men aged 45 to 64 (mean age of 55 years) with no history of myocardial infarction at randomization. A total of 6,595 men, with a mean (SD) plasma cholesterol level of 272 (23) mg/dL and mean (SD) low density lipoprotein cholesterol (LDL-C) of 192 (17) mg/dL were randomly assigned to receive pravastatin 40 mg daily or placebo for five years. The primary outcome was a composite of death from coronary heart disease (CHD) and nonfatal myocardial infarction. There was a 31\% relative reduction in the primary outcome with pravastatin. There was similar reduction in risk of nonfatal myocardial infarction, death from CHD and death from all cardiovascular causes with no increased risk of death from non-cardiovascular causes nor an increase in incident cancers.},
	language = {en},
	number = {4},
	urldate = {2024-08-19},
	journal = {Global Cardiology Science and Practice},
	author = {Kashef, Mohammed Amin and Giugliano, Gregory},
	year = {2016},
	note = {Number: 4},
}

@article{von_bachmann_evaluating_2024-1,
	title = {Evaluating regression and probabilistic methods for {ECG}-based electrolyte prediction},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-65223-w},
	doi = {10.1038/s41598-024-65223-w},
	abstract = {Imbalances in electrolyte concentrations can have severe consequences, but accurate and accessible measurements could improve patient outcomes. The current measurement method based on blood tests is accurate but invasive and time-consuming and is often unavailable for example in remote locations or an ambulance setting. In this paper, we explore the use of deep neural networks (DNNs) for regression tasks to accurately predict continuous electrolyte concentrations from electrocardiograms (ECGs), a quick and widely adopted tool. We analyze our DNN models on a novel dataset of over 290,000 ECGs across four major electrolytes and compare their performance with traditional machine learning models. For improved understanding, we also study the full spectrum from continuous predictions to a binary classification of extreme concentration levels. Finally, we investigate probabilistic regression approaches and explore uncertainty estimates for enhanced clinical usefulness. Our results show that DNNs outperform traditional models but model performance varies significantly across different electrolytes. While discretization leads to good classification performance, it does not address the original problem of continuous concentration level prediction. Probabilistic regression has practical potential, but our uncertainty estimates are not perfectly calibrated. Our study is therefore a first step towards developing an accurate and reliable ECG-based method for electrolyte concentration level prediction—a method with high potential impact within multiple clinical scenarios.},
	language = {en},
	number = {1},
	urldate = {2024-08-19},
	journal = {Scientific Reports},
	author = {von Bachmann, Philipp and Gedon, Daniel and Gustafsson, Fredrik K. and Ribeiro, Antônio H. and Lampa, Erik and Gustafsson, Stefan and Sundström, Johan and Schön, Thomas B.},
	month = jul,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biomedical engineering, Cardiology, Computer science},
	pages = {15273},
}

@article{eriksson_transferability_2024,
	title = {Transferability and {Adversarial} {Training} in {Automatic} {Classification} of the {Electrocardiogram} with {Deep} {Learning}},
	copyright = {All rights reserved},
	journal = {Computers in Cardiology (CinC)},
	author = {Eriksson, Arvid and Schön, Thomas B and RIbeiro, Antonio H},
	year = {2024},
}

@article{berrevoets_causal_2024,
	title = {Causal {Deep} {Learning}: {Encouraging} {Impact} on {Real}-world {Problems} {Through} {Causality}},
	volume = {18},
	issn = {1932-8346, 1932-8354},
	shorttitle = {Causal {Deep} {Learning}},
	url = {https://www.nowpublishers.com/article/Details/SIG-123},
	doi = {10.1561/2000000123},
	abstract = {Causal Deep Learning: Encouraging Impact on Real-world Problems Through Causality},
	language = {English},
	number = {3},
	urldate = {2024-08-18},
	journal = {Foundations and Trends® in Signal Processing},
	author = {Berrevoets, Jeroen and Kacprzyk, Krzysztof and Qian, Zhaozhi and Schaar, Mihaela van der},
	month = jul,
	year = {2024},
	note = {Publisher: Now Publishers, Inc.},
	pages = {200--309},
}

@article{garfield_brief_2024,
	title = {A brief comparison of polygenic risk scores and {Mendelian} randomisation},
	volume = {17},
	issn = {1755-8794},
	url = {https://doi.org/10.1186/s12920-023-01769-4},
	doi = {10.1186/s12920-023-01769-4},
	abstract = {Mendelian randomisation and polygenic risk score analysis have become increasingly popular in the last decade due to the advent of large-scale genome-wide association studies. Each approach has valuable applications, some of which are overlapping, yet there are important differences which we describe here.},
	number = {1},
	urldate = {2024-07-30},
	journal = {BMC Medical Genomics},
	author = {Garfield, Victoria and Anderson, Emma L.},
	month = jan,
	year = {2024},
	keywords = {Genome-wide association studies, Horizontal pleiotropy, Mendelian randomisation, Polygenic risk scores},
	pages = {10},
}

@misc{noauthor_folkbokforing_nodate,
	title = {Folkbokföring {\textbar} {Mina} {Sidor}},
	url = {https://sso.skatteverket.se/ms/ms_web/page.do#/privat/folkbokforing},
	urldate = {2024-07-29},
}

@misc{noauthor_folkbokforing_nodate-1,
	title = {Folkbokföring {\textbar} {Mina} {Sidor}},
	url = {https://sso.skatteverket.se/ms/ms_web/page.do#/privat/folkbokforing},
	urldate = {2024-07-29},
}

@article{funk_doubly_2011,
	title = {Doubly {Robust} {Estimation} of {Causal} {Effects}},
	volume = {173},
	issn = {0002-9262},
	url = {https://doi.org/10.1093/aje/kwq439},
	doi = {10.1093/aje/kwq439},
	abstract = {Doubly robust estimation combines a form of outcome regression with a model for the exposure (i.e., the propensity score) to estimate the causal effect of an exposure on an outcome. When used individually to estimate a causal effect, both outcome regression and propensity score methods are unbiased only if the statistical model is correctly specified. The doubly robust estimator combines these 2 approaches such that only 1 of the 2 models need be correctly specified to obtain an unbiased effect estimator. In this introduction to doubly robust estimators, the authors present a conceptual overview of doubly robust estimation, a simple worked example, results from a simulation study examining performance of estimated and bootstrapped standard errors, and a discussion of the potential advantages and limitations of this method. The supplementary material for this paper, which is posted on the Journal's Web site (http://aje.oupjournals.org/), includes a demonstration of the doubly robust property (Web Appendix 1) and a description of a SAS macro (SAS Institute, Inc., Cary, North Carolina) for doubly robust estimation, available for download at http://www.unc.edu/∼mfunk/dr/.},
	number = {7},
	urldate = {2024-07-16},
	journal = {American Journal of Epidemiology},
	author = {Funk, Michele Jonsson and Westreich, Daniel and Wiesen, Chris and Stürmer, Til and Brookhart, M. Alan and Davidian, Marie},
	month = apr,
	year = {2011},
	pages = {761--767},
}

@article{van_de_geer_asymptotically_2014,
	title = {On asymptotically optimal confidence regions and tests for high-dimensional models},
	volume = {42},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1303.0518},
	doi = {10.1214/14-AOS1221},
	abstract = {We propose a general method for constructing confidence intervals and statistical tests for single or low-dimensional components of a large parameter vector in a high-dimensional model. It can be easily adjusted for multiplicity taking dependence among tests into account. For linear models, our method is essentially the same as in Zhang and Zhang [J. R. Stat. Soc. Ser. B Stat. Methodol. 76 (2014) 217-242]: we analyze its asymptotic properties and establish its asymptotic optimality in terms of semiparametric efficiency. Our method naturally extends to generalized linear models with convex loss functions. We develop the corresponding theory which includes a careful analysis for Gaussian, sub-Gaussian and bounded correlated designs.},
	language = {en},
	number = {3},
	urldate = {2024-07-10},
	journal = {The Annals of Statistics},
	author = {van de Geer, Sara and Bühlmann, Peter and Ritov, Ya'acov and Dezeure, Ruben},
	month = jun,
	year = {2014},
	note = {arXiv:1303.0518 [math, stat]},
	keywords = {Mathematics - Statistics Theory},
}

@article{diamant_patient_2022,
	title = {Patient contrastive learning: {A} performant, expressive, and practical approach to electrocardiogram modeling},
	volume = {18},
	issn = {1553-7358},
	shorttitle = {Patient contrastive learning},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1009862},
	doi = {10.1371/journal.pcbi.1009862},
	abstract = {Supervised machine learning applications in health care are often limited due to a scarcity of labeled training data. To mitigate the effect of small sample size, we introduce a pre-training approach, Patient Contrastive Learning of Representations (PCLR), which creates latent representations of electrocardiograms (ECGs) from a large number of unlabeled examples using contrastive learning. The resulting representations are expressive, performant, and practical across a wide spectrum of clinical tasks. We develop PCLR using a large health care system with over 3.2 million 12-lead ECGs and demonstrate that training linear models on PCLR representations achieves a 51\% performance increase, on average, over six training set sizes and four tasks (sex classification, age regression, and the detection of left ventricular hypertrophy and atrial fibrillation), relative to training neural network models from scratch. We also compared PCLR to three other ECG pre-training approaches (supervised pre-training, unsupervised pre-training with an autoencoder, and pre-training using a contrastive multi ECG-segment approach), and show significant performance benefits in three out of four tasks. We found an average performance benefit of 47\% over the other models and an average of a 9\% performance benefit compared to best model for each task. We release PCLR to enable others to extract ECG representations at https://github.com/ broadinstitute/ml4h/tree/master/model\_zoo/PCLR.},
	language = {en},
	number = {2},
	urldate = {2024-07-10},
	journal = {PLOS Computational Biology},
	author = {Diamant, Nathaniel and Reinertsen, Erik and Song, Steven and Aguirre, Aaron D. and Stultz, Collin M. and Batra, Puneet},
	editor = {Kouyos, Roger Dimitri},
	month = feb,
	year = {2022},
	pages = {e1009862},
}

@misc{noauthor_hundra_nodate,
	type = {text},
	title = {Hundra möjligheter att rekrytera utan att diskriminera},
	url = {https://www.do.se/kunskap-stod-och-vagledning/stodmaterial-forebygga-diskriminering/arbetslivet/hundra-mojligheter-att-rekrytera-utan-att-diskriminera},
	abstract = {Du får vägledning under rekryteringen med frågor och checklistor - 100 tips och råd för att undvika fallgropar som kan leda till diskriminering.},
	language = {sv},
	urldate = {2024-06-18},
}

@article{huang_generalization_2024,
	title = {Generalization challenges in electrocardiogram deep learning: insights from dataset characteristics and attention mechanism},
	volume = {0},
	copyright = {All rights reserved},
	issn = {1479-6678},
	shorttitle = {Generalization challenges in electrocardiogram deep learning},
	url = {https://doi.org/10.1080/14796678.2024.2354082},
	doi = {10.1080/14796678.2024.2354082},
	abstract = {Aim: Deep learning’s widespread use prompts heightened scrutiny, particularly in the biomedical fields, with a specific focus on model generalizability. This study delves into the influence of training data characteristics on the generalization performance of models, specifically in cardiac abnormality detection. Materials \& methods: Leveraging diverse electrocardiogram datasets, models are trained on subsets with varying characteristics and subsequently compared for performance. Additionally, the introduction of the attention mechanism aims to improve generalizability. Results: Experiments reveal that using a balanced dataset, just 1\% of a large dataset, leads to equal performance in generalization tasks, notably in detecting cardiology abnormalities. Conclusion: This balanced training data notably enhances model generalizability, while the integration of the attention mechanism further refines the model’s ability to generalize effectively. This study tackles a common problem for deep learning models: they often struggle when faced with new, unfamiliar data that they have not been trained on. This phenomenon is also known as performance drop in out-of-distribution generalization. This reduced performance on out-of-distribution generalization is a key focus of the research, aiming to improve the models’ ability to handle diverse data sets beyond their training data. The study examines how the characteristics of the dataset used to train deep learning models affect their ability to detect abnormal heart activities when applied to new, unseen data. Researchers trained these models using various sets of electrocardiogram (ECG) data and then evaluated their performance in identifying abnormalities. They also introduced an attention mechanism to enhance the models’ learning capabilities. The attention mechanism in deep learning is like a spotlight that helps the model focus on important information while ignoring less relevant details. The findings were particularly noteworthy. Despite being trained on a small, well-balanced subset of a larger dataset, the models excelled in detecting heart abnormalities in new, unfamiliar data. This training method significantly improved the models’ generalization and performance with unseen data. Furthermore, integrating the attention mechanism substantially enhanced the models’ ability to generalize effectively on new information. Investigate the impact of training data characteristics and attention mechanism on deep learning model generalizability in cardiac abnormality detection. Balanced dataset (1\% of the total) improves model performance in generalization tasks, especially in detecting cardiology abnormalities. The attention mechanism further enhances the model’s capacity to comprehend and utilize out-of-distribution data effectively. Utilized multiple electrocardiogram datasets for the study. Trained models on subsets with varying characteristics and evaluated performance. Added attention mechanism to enhance learning capabilities. Balanced training data significantly enhances model generalizability. Attention mechanism improves the model’s ability to generalize on out-of-distribution data. Lack of clinical user information in datasets due to privacy and ethical considerations. Future research may consider patient-specific models for improved generalization in biomedical machine learning. Balanced and curated datasets are crucial for training high-performing models in cardiac abnormality detection using deep learning. Attention mechanisms show promise in enhancing model accuracy and generalization.},
	number = {0},
	urldate = {2024-06-12},
	journal = {Future Cardiology},
	author = {Huang, Zhaojing and MacLachlan, Sarisha and Yu, Leping and Herbozo Contreras, Luis Fernando and Truong, Nhan Duy and Ribeiro, Antonio Horta and Kavehei, Omid},
	year = {2024},
	note = {https://www.medrxiv.org/content/10.1101/2023.07.05.23292238v2},
	keywords = {attention mechanism, cardiac abnormality detection, dataset characteristics, deep learning, electrocardiogram, generalization},
	pages = {1--12},
}

@book{barocas_fairness_2023,
	title = {Fairness and {Machine} {Learning}},
	language = {en},
	author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
	year = {2023},
}

@misc{noauthor_fairness_nodate,
	title = {Fairness and machine learning},
	url = {https://fairmlbook.org/},
	urldate = {2024-06-10},
}

@misc{noauthor_fairness_nodate-1,
	title = {Fairness and machine learning},
	url = {https://fairmlbook.org/},
	urldate = {2024-06-10},
}

@article{topol_medical_2024,
	title = {Medical forecasting},
	volume = {384},
	url = {https://www.science.org/doi/10.1126/science.adp7977},
	doi = {10.1126/science.adp7977},
	number = {6698},
	urldate = {2024-06-08},
	journal = {Science},
	author = {Topol, Eric J.},
	month = may,
	year = {2024},
	note = {Publisher: American Association for the Advancement of Science},
}

@article{goudis_charge-af_2022,
	title = {{CHARGE}-{AF}: {A} {Useful} {Score} {For} {Atrial} {Fibrillation} {Prediction}?},
	volume = {19},
	shorttitle = {{CHARGE}-{AF}},
	url = {https://www.eurekaselect.com/article/126070},
	doi = {10.2174/1573403X18666220901102557},
	abstract = {Atrial fibrillation (AF) is the commonest arrhythmia in clinical practice and is associated
with increased morbidity and mortality. Various predictive scores for new-onset AF have been
proposed, but so far, none have been widely used in clinical practice. CHARGE-AF score was developed
from a pooled diverse population from three large cohorts (Atherosclerosis Risk in Communities
study, Cardiovascular Health Study and Framingham Heart Study). A simple 5-year predictive
model includes the variables of age, race, height, weight, systolic and diastolic blood pressure,
current smoking, use of antihypertensive medication, diabetes mellitus, history of myocardial
infarction and heart failure. Recent studies report that the CHARGE-AF score has good discrimination
for incident AF and seems to be a promising prediction model for this arrhythmia. New
screening tools (smartphone apps, smartwatches) are rapidly developing for AF detection. Therefore,
the wide application of the CHARGE-AF score in clinical practice and the upcoming usage of
mobile health technologies and smartwatches may result in better AF prediction and adequate
stroke prevention, especially in high-risk patients.},
	language = {en},
	number = {2},
	urldate = {2024-06-10},
	journal = {Current Cardiology Reviews},
	author = {Goudis, Christos and Daios, Stylianos and Dimitriadis, Fotios and Liu, Tong},
	year = {2022},
	pages = {5--10},
}

@article{sau_artificial_2024-1,
	title = {Artificial intelligence–enabled electrocardiogram for mortality and cardiovascular risk estimation: {An} actionable, explainable and biologically plausible platform},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Artificial intelligence–enabled electrocardiogram for mortality and cardiovascular risk estimation},
	url = {https://www.medrxiv.org/content/10.1101/2024.01.13.24301267v1},
	doi = {10.1101/2024.01.13.24301267},
	abstract = {Background and Aims Artificial intelligence-enhanced electrocardiograms (AI-ECG) can be used to predict risk of future disease and mortality but has not yet been adopted into clinical practice. Existing model predictions lack actionability at an individual patient level, explainability and biological plausibility. We sought to address these limitations of previous AI-ECG approaches by developing the AI-ECG risk estimator (AIRE) platform.
Methods and Results The AIRE platform was developed in a secondary care dataset of 1,163,401 ECGs from 189,539 patients, using deep learning with a discrete-time survival model to create a subject-specific survival curve using a single ECG. Therefore, AIRE predicts not only risk of mortality, but time-to-mortality. AIRE was validated in five diverse, transnational cohorts from the USA, Brazil and the UK, including volunteers, primary care and secondary care subjects. AIRE accurately predicts risk of all-cause mortality (C-index 0.775 (0.773-0.776)), cardiovascular (CV) death 0.832 (0.831-0.834), non-CV death (0.749 (0.747-0.751)), future ventricular arrhythmia (0.760 (0.756-0.763)), future atherosclerotic cardiovascular disease (0.696 (0.694-0.698)) and future heart failure (0.787 (0.785-0.889))). Through phenome- and genome-wide association studies, we identified candidate biological pathways for the prediction of increased risk, including changes in cardiac structure and function, and genes associated with cardiac structure, biological aging and metabolic syndrome.
Conclusion AIRE is an actionable, explainable and biologically plausible AI-ECG risk estimation platform that has the potential for use worldwide across a wide range of clinical contexts for short- and long-term risk estimation.
{\textless}img class="highwire-fragment fragment-image" alt="Figure" src="https://www.medrxiv.org/content/medrxiv/early/2024/01/15/2024.01.13.24301267/F1.medium.gif" width="440" height="373"/{\textgreater}Download figureOpen in new tab},
	language = {en},
	urldate = {2024-02-26},
	journal = {medXiv},
	author = {Sau, Arunashis and Pastika, Libor and Sieliwonczyk, Ewa and Patlatzoglou, Konstantinos and Ribeiro, Antonio H. and McGurk, Kathryn A. and Zeidaabadi, Boroumand and Zhang, Henry and Macierzanka, Krzysztof and Mandic, Danilo and Sabino, Ester and Giatti, Luana and Barreto, Sandhi M. and Camelo, Lidyane do Valle and Tzoulaki, Ioanna and O’Regan, Declan P. and Peters, Nicholas S. and Ware, James S. and Ribeiro, Antonio Luiz P. and Kramer, Daniel B. and Waks, Jonathan W. and Ng, Fu Siong},
	month = jan,
	year = {2024},
	pages = {2024.01.13.24301267},
}

@article{sau_neural_2023,
	title = {Neural network-derived electrocardiographic features have prognostic significance and important phenotypic and genotypic associations},
	copyright = {All rights reserved},
	journal = {medRxiv},
	author = {Sau, Arunashis and Ribeiro, Antônio H. and McGurk, Kathryn and Pastika, Libor and Bajaj, Nikesh and Ardissino, Maddalena and Chen, Jun Yu and Wu, Huiyi and Shi, Xili and Hnatkova, Katerina and Zheng, Sean and Britton, Annie and Shipley, Martin and Andršová, Irena and Novotný, Tomáš and Sabino, Ester and Giatti, Luana and Barreto, Sandhi and Waks, Jonathan and Kramer, Daniel and Mandic, Danilo and Peters, Nicholas and O'Regan, Declan and Malik, Marek and Ware, James and Ribeiro, Antonio L. P. and Ng, Fu Siong},
	year = {2023},
}

@article{gedon_no_2024,
	title = {No {Double} {Descent} in {Principal} {Component} {Regression}: {A} {High}-{Dimensional} {Analysis}},
	copyright = {All rights reserved},
	journal = {International Conference on Machine Learning (ICML)},
	author = {Gedon, Daniel and Ribeiro, Antonio H. and Schön, Thomas B.},
	year = {2024},
}

@article{meyers_comparison_2021,
	title = {Comparison of the {ST}-{Elevation} {Myocardial} {Infarction} ({STEMI}) vs. {NSTEMI} and {Occlusion} {MI} ({OMI}) vs. {NOMI} {Paradigms} of {Acute} {MI}},
	volume = {60},
	issn = {0736-4679},
	doi = {10.1016/j.jemermed.2020.10.026},
	abstract = {BACKGROUND: The current ST-elevation myocardial infarction (STEMI) vs. non-STEMI (NSTEMI) paradigm prevents some NSTEMI patients with acute coronary occlusion from receiving emergent reperfusion, in spite of their known increased mortality compared with NSTEMI without occlusion. We have proposed a new paradigm known as occlusion MI vs. nonocclusion MI (OMI vs. NOMI).
OBJECTIVE: We aimed to compare the two paradigms within a single population. We hypothesized that STEMI(-) OMI would have characteristics similar to STEMI(+) OMI but longer time to catheterization.
METHODS: We performed a retrospective review of a prospectively collected acute coronary syndrome population. OMI was defined as an acute culprit and either TIMI 0-2 flow or TIMI 3 flow plus peak troponin T {\textgreater} 1.0 ng/mL. We collected electrocardiograms, demographic characteristics, laboratory results, angiographic data, and outcomes.
RESULTS: Among 467 patients, there were 108 OMIs, with only 60\% (67 of 108) meeting STEMI criteria. Median peak troponin T for the STEMI(+) OMI, STEMI(-) OMI, and no occlusion groups were 3.78 (interquartile range [IQR] 2.18-7.63), 1.87 (IQR 1.12-5.48), and 0.00 (IQR 0.00-0.08). Median time from arrival to catheterization was 41 min (IQR 23-86 min) for STEMI(+) OMI compared with 437 min (IQR 85-1590 min) for STEMI(-) OMI (p {\textless} 0.001). STEMI(+) OMI was more likely than STEMI(-) OMI to undergo catheterization within 90 min (76\% vs. 28\%; p {\textless} 0.001).
CONCLUSIONS: STEMI(-) OMI patients had significant delays to catheterization but adverse outcomes more similar to STEMI(+) OMI than those with no occlusion. These data support the OMI/NOMI paradigm and the importance of further research into emergent reperfusion for STEMI(-) OMI.},
	language = {eng},
	number = {3},
	journal = {The Journal of Emergency Medicine},
	author = {Meyers, H. Pendell and Bracey, Alexander and Lee, Daniel and Lichtenheld, Andrew and Li, Wei J. and Singer, Daniel D. and Kane, Jesse A. and Dodd, Kenneth W. and Meyers, Kristen E. and Thode, Henry C. and Shroff, Gautam R. and Singer, Adam J. and Smith, Stephen W.},
	month = mar,
	year = {2021},
	pmid = {33308915},
	keywords = {Electrocardiography, Humans, Myocardial Infarction, Non-ST Elevated Myocardial Infarction, Retrospective Studies, ST Elevation Myocardial Infarction, ST-segment elevation myocardial infarction, acute coronary syndrome, acute myocardial infarction, electrocardiogram, occlusion myocardial infarction},
	pages = {273--284},
}

@article{lin_ai-enabled_2024,
	title = {{AI}-enabled electrocardiography alert intervention and all-cause mortality: a pragmatic randomized clinical trial},
	volume = {30},
	copyright = {2024 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	shorttitle = {{AI}-enabled electrocardiography alert intervention and all-cause mortality},
	url = {https://www.nature.com/articles/s41591-024-02961-4},
	doi = {10.1038/s41591-024-02961-4},
	abstract = {The early identification of vulnerable patients has the potential to improve outcomes but poses a substantial challenge in clinical practice. This study evaluated the ability of an artificial intelligence (AI)-enabled electrocardiogram (ECG) to identify hospitalized patients with a high risk of mortality in a multisite randomized controlled trial involving 39 physicians and 15,965 patients. The AI-ECG alert intervention included an AI report and warning messages delivered to the physicians, flagging patients predicted to be at high risk of mortality. The trial met its primary outcome, finding that implementation of the AI-ECG alert was associated with a significant reduction in all-cause mortality within 90 days: 3.6\% patients in the intervention group died within 90 days, compared to 4.3\% in the control group (4.3\%) (hazard ratio (HR) = 0.83, 95\% confidence interval (CI) = 0.70–0.99). A prespecified analysis showed that reduction in all-cause mortality associated with the AI-ECG alert was observed primarily in patients with high-risk ECGs (HR = 0.69, 95\% CI = 0.53–0.90). In analyses of secondary outcomes, patients in the intervention group with high-risk ECGs received increased levels of intensive care compared to the control group; for the high-risk ECG group of patients, implementation of the AI-ECG alert was associated with a significant reduction in the risk of cardiac death (0.2\% in the intervention arm versus 2.4\% in the control arm, HR = 0.07, 95\% CI = 0.01–0.56). While the precise means by which implementation of the AI-ECG alert led to decreased mortality are to be fully elucidated, these results indicate that such implementation assists in the detection of high-risk patients, prompting timely clinical care and reducing mortality. ClinicalTrials.gov registration: NCT05118035.},
	language = {en},
	number = {5},
	urldate = {2024-06-08},
	journal = {Nature Medicine},
	author = {Lin, Chin-Sheng and Liu, Wei-Ting and Tsai, Dung-Jang and Lou, Yu-Sheng and Chang, Chiao-Hsiang and Lee, Chiao-Chin and Fang, Wen-Hui and Wang, Chih-Chia and Chen, Yen-Yuan and Lin, Wei-Shiang and Cheng, Cheng-Chung and Lee, Chia-Cheng and Wang, Chih-Hung and Tsai, Chien-Sung and Lin, Shih-Hua and Lin, Chin},
	month = may,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Prognosis, Prognostic markers},
	pages = {1461--1470},
}

@article{sau_neural_2023-1,
	title = {Neural network-derived electrocardiographic features have prognostic significance and important phenotypic and genotypic associations},
	volume = {44},
	issn = {0195-668X},
	url = {https://doi.org/10.1093/eurheartj/ehad655.2921},
	doi = {10.1093/eurheartj/ehad655.2921},
	abstract = {Subtle, prognostically-meaningful ECG features may not be apparent to physicians. In the course of supervised machine learning (ML) training, many thousands of ECG features are identified. These are not limited to conventional ECG parameters and morphology.To investigate novel neural network (NN)-derived ECG features, that may have clinical, phenotypic and genotypic associations and prognostic significance.We extracted 5120 NN-derived ECG features from an AI-ECG model trained for six simple diagnoses and applied unsupervised machine learning to identify three phenogroups. The derivation set, the Clinical Outcomes in Digital Electrocardiography (CODE) cohort (n = 1,558,421), is a database of ECGs recorded in primary care in Brazil. There were four external validation cohorts. A cohort of British civil servants (WH II, n = 5,066). A longitudinal study of volunteers in the UK (UK Biobank, n = 42,386). A longitudinal cohort of Brazilian public servants (ELSA-Brasil, n = 13,739). Lastly, a cohort of patients with chronic Chagas cardiomyopathy (SaMi-Trop, n = 1,631) .In the derivation cohort (CODE), the three phenogroups had significantly different mortality profiles (Figure 1). After adjusting for known covariates, phenogroup B had a 1.2-fold increase in long-term mortality compared to phenogroup A (HR 1.20, 95\% CI 1.17-1.23, p \&lt; 0.0001). We externally validated our findings in four diverse cohorts. Phenogroup C was poorly represented in the volunteer cohorts and therefore was excluded from those analyses. We found phenogroup B had a significantly greater risk of mortality in all cohorts (Figure 1). We performed a phenome-wide association study (PheWAS) in the UK Biobank. We found ECG phenogroup significantly associated with cardiac and non-cardiac phenotypes, including cardiac chamber volumes and cardiac output (Figure 2A). A single-trait genome-wide association study (GWAS) was conducted. The GWAS yielded four loci (Figure 2B). SCN10A, SCN5A and CAV1 have well described roles in cardiac conduction and arrhythmia. ARHGAP24 has been previously associated with ECG parameters, however, our analysis has identified for the first time ARHGAP24 as a gene associated with a prognostically significant phenogroup. Mendelian randomisation demonstrated the higher risk ECG phenogroup was causally associated with higher odds of atrioventricular (AV) block but lower odds of atrial fibrillation and ischaemic heart disease.NN-derived ECG features have important applications beyond the original model from which they are derived and may be transferable and applicable for risk prediction in a wide range of settings, in addition to mortality prediction. We have shown the significant potential of NN-derived ECG features, as a highly transferable and potentially universal risk marker, that may be applied to a wide range of clinical contexts.},
	number = {Supplement\_2},
	urldate = {2024-06-04},
	journal = {European Heart Journal},
	author = {Sau, A and Ribeiro, A H and Mcgurk, K A and Pastika, L and Chen, J Y and Ardissino, M and Sabino, E and Giatti, L and Barreto, S M and Mandic, D and Peters, N S and Malik, M and Ware, J and Ribeiro, A L P and Ng, F S},
	month = nov,
	year = {2023},
	pages = {ehad655.2921},
}

@article{perry_proteomic_2024,
	title = {Proteomic analysis of cardiorespiratory fitness for prediction of mortality and multisystem disease risks},
	copyright = {2024 The Author(s)},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-024-03039-x},
	doi = {10.1038/s41591-024-03039-x},
	abstract = {Despite the wide effects of cardiorespiratory fitness (CRF) on metabolic, cardiovascular, pulmonary and neurological health, challenges in the feasibility and reproducibility of CRF measurements have impeded its use for clinical decision-making. Here we link proteomic profiles to CRF in 14,145 individuals across four international cohorts with diverse CRF ascertainment methods to establish, validate and characterize a proteomic CRF score. In a cohort of around 22,000 individuals in the UK Biobank, a proteomic CRF score was associated with a reduced risk of all-cause mortality (unadjusted hazard ratio 0.50 (95\% confidence interval 0.48–0.52) per 1 s.d. increase). The proteomic CRF score was also associated with multisystem disease risk and provided risk reclassification and discrimination beyond clinical risk factors, as well as modulating high polygenic risk of certain diseases. Finally, we observed dynamicity of the proteomic CRF score in individuals who undertook a 20-week exercise training program and an association of the score with the degree of the effect of training on CRF, suggesting potential use of the score for personalization of exercise recommendations. These results indicate that population-based proteomics provides biologically relevant molecular readouts of CRF that are additive to genetic risk, potentially modifiable and clinically translatable.},
	language = {en},
	urldate = {2024-06-04},
	journal = {Nature Medicine},
	author = {Perry, Andrew S. and Farber-Eger, Eric and Gonzales, Tomas and Tanaka, Toshiko and Robbins, Jeremy M. and Murthy, Venkatesh L. and Stolze, Lindsey K. and Zhao, Shilin and Huang, Shi and Colangelo, Laura A. and Deng, Shuliang and Hou, Lifang and Lloyd-Jones, Donald M. and Walker, Keenan A. and Ferrucci, Luigi and Watts, Eleanor L. and Barber, Jacob L. and Rao, Prashant and Mi, Michael Y. and Gabriel, Kelley Pettee and Hornikel, Bjoern and Sidney, Stephen and Houstis, Nicholas and Lewis, Gregory D. and Liu, Gabrielle Y. and Thyagarajan, Bharat and Khan, Sadiya S. and Choi, Bina and Washko, George and Kalhan, Ravi and Wareham, Nick and Bouchard, Claude and Sarzynski, Mark A. and Gerszten, Robert E. and Brage, Soren and Wells, Quinn S. and Nayor, Matthew and Shah, Ravi V.},
	month = jun,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Epidemiology, Prognostic markers},
	pages = {1--11},
}

@inproceedings{curth_u-turn_2023,
	title = {A {U}-turn on {Double} {Descent}: {Rethinking} {Parameter} {Counting} in {Statistical} {Learning}},
	shorttitle = {A {U}-turn on {Double} {Descent}},
	url = {https://openreview.net/forum?id=O0Lz8XZT2b},
	abstract = {Conventional statistical wisdom established a well-understood relationship between model complexity and prediction error, typically presented as a \_U-shaped curve\_ reflecting a transition between under- and overfitting regimes. However, motivated by the success of overparametrized neural networks, recent influential work has suggested this theory to be generally incomplete, introducing an additional regime that exhibits a second descent in test error as the parameter count \$p\$ grows past sample size \$n\$ -- a phenomenon dubbed \_double descent\_. While most attention has naturally been given to the deep-learning setting, double descent was shown to emerge more generally across non-neural models: known cases include \_linear regression, trees, and boosting\_. In this work, we take a closer look at the evidence surrounding these more classical statistical machine learning methods and challenge the claim that observed cases of double descent truly extend the limits of a traditional U-shaped complexity-generalization curve therein. We show that once careful consideration is given to \_what is being plotted\_ on the x-axes of their double descent plots, it becomes apparent that there are implicitly multiple, distinct complexity axes along which the parameter count grows. We demonstrate that the second descent appears exactly (and \_only\_) when and where the transition between these underlying axes occurs, and that its location is thus \_not\_ inherently tied to the interpolation threshold \$p=n\$. We then gain further insight by adopting a classical nonparametric statistics perspective. We interpret the investigated methods as \_smoothers\_ and propose a generalized measure for the \_effective\_ number of parameters they use \_on unseen examples\_, using which we find that their apparent double descent curves do indeed fold back into more traditional convex shapes -- providing a resolution to the ostensible tension between double descent and traditional statistical intuition.},
	language = {en},
	urldate = {2023-12-12},
	author = {Curth, Alicia and Jeffares, Alan and Schaar, Mihaela van der},
	month = nov,
	year = {2023},
	keywords = {\_tablet\_modified},
}

@article{yeh_knowledge_2009,
	title = {Knowledge discovery on {RFM} model using {Bernoulli} sequence},
	volume = {36},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417408004508},
	doi = {10.1016/j.eswa.2008.07.018},
	abstract = {The objective of this paper is to introduce a comprehensive methodology to discover the knowledge for selecting targets for direct marketing from a database. This study expanded RFM model by including two parameters, time since first purchase and churn probability. Using Bernoulli sequence in probability theory, we derive out the formula that can estimate the probability that one customer will buy at the next time, and the expected value of the total number of times that the customer will buy in the future. This study also proposed the methodology to estimate the unknown parameters in the formula. This methodology leads to more efficient and accurate selection procedures than the existing ones. In the empirical part we examine a case study, blood transfusion service, to show that our methodology has greater predictive accuracy than traditional RFM approaches.},
	number = {3, Part 2},
	urldate = {2024-05-21},
	journal = {Expert Systems with Applications},
	author = {Yeh, I-Cheng and Yang, King-Jang and Ting, Tao-Ming},
	month = apr,
	year = {2009},
	keywords = {Bernoulli sequence, Knowledge discovery, Marketing, RFM model},
	pages = {5866--5871},
}

@article{fisher_use_1936,
	title = {The use of multiple measurements in taxonomic problems},
	volume = {7},
	issn = {2050-1420},
	url = {https://doi.org/10.1111/j.1469-1809.1936.tb02137.x},
	doi = {10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925?1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	number = {2},
	urldate = {2024-05-21},
	journal = {Annals of Eugenics},
	author = {Fisher, R.A.},
	month = sep,
	year = {1936},
	note = {Publisher: John Wiley \& Sons, Ltd},
	pages = {179--188},
}

@misc{noauthor_use_nodate,
	title = {{THE} {USE} {OF} {MULTIPLE} {MEASUREMENTS} {IN} {TAXONOMIC} {PROBLEMS} - {FISHER} - 1936 - {Annals} of {Eugenics} - {Wiley} {Online} {Library}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
	urldate = {2024-05-21},
}

@article{unwin_iris_2021,
	title = {The {Iris} {Data} {Set}: {In} {Search} of the {Source} of {Virginica}},
	volume = {18},
	issn = {1740-9705},
	shorttitle = {The {Iris} {Data} {Set}},
	url = {https://doi.org/10.1111/1740-9713.01589},
	doi = {10.1111/1740-9713.01589},
	abstract = {The iris data set is one of the best-known and most widely used data sets in statistics and data science. But the origins of at least part of the data have been something of a mystery for decades. Antony Unwin and Kim Kleinman believe they have traced the source},
	number = {6},
	urldate = {2024-05-21},
	journal = {Significance},
	author = {Unwin, Antony and Kleinman, Kim},
	month = dec,
	year = {2021},
	pages = {26--29},
}

@article{redmond_data-driven_2002,
	title = {A data-driven software tool for enabling cooperative information sharing among police departments},
	volume = {141},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221701002648},
	doi = {10.1016/S0377-2217(01)00264-8},
	abstract = {Semantic Scholar extracted view of "A data-driven software tool for enabling cooperative information sharing among police departments" by Michael Redmond et al.},
	language = {en},
	number = {3},
	urldate = {2024-05-21},
	journal = {European Journal of Operational Research},
	author = {Redmond, Michael and Baveja, Alok},
	month = sep,
	year = {2002},
	pages = {660--678},
}

@article{vanschoren_openml_2014,
	title = {{OpenML}: networked science in machine learning},
	volume = {15},
	issn = {1931-0145},
	shorttitle = {{OpenML}},
	url = {https://dl.acm.org/doi/10.1145/2641190.2641198},
	doi = {10.1145/2641190.2641198},
	abstract = {Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.},
	number = {2},
	urldate = {2024-05-16},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Vanschoren, Joaquin and van Rijn, Jan N. and Bischl, Bernd and Torgo, Luis},
	month = jun,
	year = {2014},
	pages = {49--60},
}

@article{mcdonald_instabilities_1973,
	title = {Instabilities of {Regression} {Estimates} {Relating} {Air} {Pollution} to {Mortality}},
	volume = {15},
	issn = {0040-1706},
	url = {https://www.jstor.org/stable/1266852},
	doi = {10.2307/1266852},
	abstract = {The instability of ordinary least squares estimates of linear regression coefficients is demonstrated for mortality rates regressed around various socioeconomic, weather and pollution variables. A ridge regression technique presented by Hoerl and Kennard (Technometrics 12 (1970) 69-82) is employed to arrive at "stable" regression coefficients which, in some instances, differ considerably from the ordinary least squares estimates. In addition, two methods of variable elimination are compared-one based on total squared error and the other on a ridge trace analysis.},
	number = {3},
	urldate = {2024-05-16},
	journal = {Technometrics},
	author = {McDonald, Gary C. and Schwing, Richard C.},
	year = {1973},
	note = {Publisher: [Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
	pages = {463--481},
}

@article{cortez_modeling_2009,
	series = {Smart {Business} {Networks}: {Concepts} and {Empirical} {Evidence}},
	title = {Modeling wine preferences by data mining from physicochemical properties},
	volume = {47},
	issn = {0167-9236},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923609001377},
	doi = {10.1016/j.dss.2009.05.016},
	abstract = {We propose a data mining approach to predict human wine taste preferences that is based on easily available analytical tests at the certification step. A large dataset (when compared to other studies in this domain) is considered, with white and red vinho verde samples (from Portugal). Three regression techniques were applied, under a computationally efficient procedure that performs simultaneous variable and model selection. The support vector machine achieved promising results, outperforming the multiple regression and neural network methods. Such model is useful to support the oenologist wine tasting evaluations and improve wine production. Furthermore, similar techniques can help in target marketing by modeling consumer tastes from niche markets.},
	number = {4},
	urldate = {2024-05-16},
	journal = {Decision Support Systems},
	author = {Cortez, Paulo and Cerdeira, António and Almeida, Fernando and Matos, Telmo and Reis, José},
	month = nov,
	year = {2009},
	keywords = {Model selection, Neural networks, Regression, Sensory preferences, Support vector machines, Variable selection},
	pages = {547--553},
}

@article{chicco_machine_2020,
	title = {Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone},
	volume = {20},
	issn = {1472-6947},
	url = {https://doi.org/10.1186/s12911-020-1023-5},
	doi = {10.1186/s12911-020-1023-5},
	abstract = {Cardiovascular diseases kill approximately 17 million people globally every year, and they mainly exhibit as myocardial infarctions and heart failures. Heart failure (HF) occurs when the heart cannot pump enough blood to meet the needs of the body.Available electronic medical records of patients quantify symptoms, body features, and clinical laboratory test values, which can be used to perform biostatistics analysis aimed at highlighting patterns and correlations otherwise undetectable by medical doctors. Machine learning, in particular, can predict patients’ survival from their data and can individuate the most important features among those included in their medical records.},
	number = {1},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Chicco, Davide and Jurman, Giuseppe},
	month = feb,
	year = {2020},
	pages = {16},
}

@inproceedings{street_nuclear_1993,
	title = {Nuclear feature extraction for breast tumor diagnosis},
	volume = {1905},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/1905/0000/Nuclear-feature-extraction-for-breast-tumor-diagnosis/10.1117/12.148698.full},
	doi = {10.1117/12.148698},
	abstract = {Interactive image processing techniques, along with a linear-programming-based inductive classifier, have been used to create a highly accurate system for diagnosis of breast tumors. A small fraction of a fine needle aspirate slide is selected and digitized. With an interactive interface, the user initializes active contour models, known as snakes, near the boundaries of a set of cell nuclei. The customized snakes are deformed to the exact shape of the nuclei. This allows for precise, automated analysis of nuclear size, shape and texture. Ten such features are computed for each nucleus, and the mean value, largest (or 'worst') value and standard error of each feature are found over the range of isolated cells. After 569 images were analyzed in this fashion, different combinations of features were tested to find those which best separate benign from malignant samples. Ten-fold cross-validation accuracy of 97\% was achieved using a single separating plane on three of the thirty features: mean texture, worst area and worst smoothness. This represents an improvement over the best diagnostic results in the medical literature. The system is currently in use at the University of Wisconsin Hospitals. The same feature set has also been utilized in the much more difficult task of predicting distant recurrence of malignancy in patients, resulting in an accuracy of 86\%.},
	urldate = {2024-05-16},
	booktitle = {Biomedical {Image} {Processing} and {Biomedical} {Visualization}},
	publisher = {SPIE},
	author = {Street, W. Nick and Wolberg, W. H. and Mangasarian, O. L.},
	month = jul,
	year = {1993},
	pages = {861--870},
}

@article{deng_mnist_2012,
	title = {The {MNIST} {Database} of {Handwritten} {Digit} {Images} for {Machine} {Learning} {Research}},
	volume = {29},
	issn = {1558-0792},
	doi = {10.1109/MSP.2012.2211477},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Deng, L.},
	month = nov,
	year = {2012},
	pages = {141--142},
}

@article{ribeiro_regularization_2023,
	title = {Regularization properties of adversarially-trained linear regression},
	journal = {Advances in Neural Information Processing Systems (NeurIPS)},
	author = {Ribeiro, Antônio H and Zachariah, Dave and Bach, Francis and Schön, Thomas B.},
	year = {2023},
}

@book{nocedal_numerical_2006,
	address = {New York},
	edition = {2nd ed},
	series = {Springer series in operations research},
	title = {Numerical {Optimization}},
	isbn = {978-0-387-30303-1},
	publisher = {Springer},
	author = {Nocedal, Jorge and Wright, Stephen J.},
	year = {2006},
	note = {OCLC: ocm68629100},
	keywords = {Mathematical optimization},
}

@article{daubechies_iteratively_2010,
	title = {Iteratively reweighted least squares minimization for sparse recovery},
	volume = {63},
	issn = {0010-3640},
	number = {1},
	journal = {Communications on Pure and Applied Mathematics},
	author = {Daubechies, Ingrid and DeVore, Ronald and Fornasier, Massimo and Güntürk, C Sinan},
	year = {2010},
	note = {Publisher: Wiley Online Library},
	pages = {1--38},
}

@inproceedings{defazio_saga_2014,
	title = {{SAGA}: {A} fast incremental gradient method with support for non-strongly convex composite objectives},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K.Q.},
	year = {2014},
}

@article{bubeck_convex_2015,
	title = {Convex {Optimization}: {Algorithms} and {Complexity}},
	volume = {8},
	shorttitle = {Convex {Optimization}},
	url = {http://arxiv.org/abs/1405.4980},
	abstract = {This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.},
	number = {3-4},
	urldate = {2024-01-16},
	journal = {Foundations and Trends in Machine Learning},
	author = {Bubeck, Sébastien},
	year = {2015},
	note = {arXiv:1405.4980 [cs, math, stat]},
	keywords = {Computer Science - Computational Complexity, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Optimization and Control, Statistics - Machine Learning},
	pages = {231--357},
}

@article{bach_optimization_2011,
	title = {Optimization with {Sparsity}-{Inducing} {Penalties}},
	volume = {4},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/article/Details/MAL-015},
	doi = {10.1561/2200000015},
	abstract = {Sparse estimation methods are aimed at using or obtaining parsimonious representations of data or models. They were ﬁrst dedicated to linear variable selection but numerous extensions have now emerged such as structured sparsity or kernel selection. It turns out that many of the related estimation problems can be cast as convex optimization problems by regularizing the empirical risk with appropriate nonsmooth norms. The goal of this monograph is to present from a general perspective optimization tools and techniques dedicated to such sparsity-inducing penalties. We cover proximal methods, block-coordinate descent, reweighted 2-penalized techniques, workingset and homotopy methods, as well as non-convex formulations and extensions, and provide an extensive set of experiments to compare various algorithms from a computational point of view.},
	language = {en},
	number = {1},
	urldate = {2023-06-12},
	journal = {Foundations and Trends in Machine Learning},
	author = {Bach, Francis},
	year = {2011},
	keywords = {\_tablet},
	pages = {1--106},
}

@misc{bach_eta-trick_2019,
	title = {The “eta-trick” or the effectiveness of reweighted least-squares – {Machine} {Learning} {Research} {Blog}},
	url = {https://francisbach.com/the-%ce%b7-trick-or-the-effectiveness-of-reweighted-least-squares/},
	language = {en-US},
	urldate = {2023-06-02},
	author = {Bach, Francis},
	month = jul,
	year = {2019},
	keywords = {\_tablet},
}

@article{hernan_causal_2020,
	title = {Causal {Inference}: {What} {If}},
	language = {en},
	author = {Hernan, Miguel A and Robins, James M},
	year = {2020},
}

@misc{noauthor_jobtalk_nodate,
	title = {{JobTalk} {Uppsala} {Automatic} {Control}},
	url = {https://docs.google.com/presentation/d/1xOLtsI769Fr_7SG-k6KVABh3vDy3ea4y7pwRiEME5j8/edit?ouid=112177853345925573315&usp=slides_home&ths=true&usp=embed_facebook},
	abstract = {Data-driven ECG analysis Antônio Horta Ribeiro On this slide: Hello my name is Antonio and I will talk about my work on data-driven analysis of the Electrocardiogram, the ECG About how my research enabled new uses of this exam. And what challenges it creates for automatic control and machine lear...},
	language = {en-GB},
	urldate = {2024-03-12},
	journal = {Google Docs},
}

@article{lu_decoding_2024,
	title = {Decoding 2.3 {Million} {ECGs}: {Interpretable} {Deep} {Learning} for {Advancing} {Cardiovascular} {Diagnosis} and {Mortality} {Risk} {Stratification}},
	copyright = {All rights reserved},
	issn = {2634-3916},
	shorttitle = {Decoding 2.3 {Million} {ECGs}},
	url = {https://doi.org/10.1093/ehjdh/ztae014},
	doi = {10.1093/ehjdh/ztae014},
	abstract = {Electrocardiogram (ECG) is widely considered the primary test for evaluating cardiovascular diseases. However, the use of artificial intelligence to advance these medical practices and learn new clinical insights from ECGs remains largely unexplored. Utilising a dataset of 2,322,513 ECGs collected from 1,558,772 patients with 7 years of follow-up, we developed a deep learning model with state-of-the-art granularity for the interpretable diagnosis of cardiac abnormalities, gender identification, and hyper- tension screening solely from ECGs, which are then used to stratify the risk of mortality. The model achieved the area under the receiver operating characteristic curve (AUC) scores of 0.998 (95\% confidence interval (CI), 0.995-0.999), 0.964 (0.963-0.965), and 0.839 (0.837-0.841) for the three diagnostic tasks separately. Using ECG-predicted results, we find high risks of mortality for subjects with sinus tachycardia (adjusted hazard ratio (HR) of 2.24, 1.96-2.57), and atrial fibrillation (adjusted HR of 2.22, 1.99-2.48). We further use salient morphologies produced by the deep learning model to identify key ECG leads that achieved similar performance for the three diagnoses, and we find that the V1 ECG lead is important for hypertension screening and mortality risk stratification of hypertensive cohorts, with an AUC of 0.816 (0.814-0.818) and a univariate HR of 1.70 (1.61-1.79) for the two tasks separately. Using ECGs alone, our developed model showed cardiologist-level accuracy in interpretable cardiac diagnosis, and the advancement in mortality risk stratification; In addition, the potential to facilitate clinical knowledge discovery for gender and hypertension detection which are not readily available.},
	urldate = {2024-02-26},
	journal = {European Heart Journal - Digital Health},
	author = {Lu, Lei and Zhu, Tingting and Ribeiro, Antonio H and Clifton, Lei and Zhao, Erying and Zhou, Jiandong and Ribeiro, Antonio Luiz P and Zhang, Yuan-Ting and Clifton, David A},
	month = feb,
	year = {2024},
	pages = {ztae014},
}

@inproceedings{NIPS2014_ede7e2b6,
	title = {{SAGA}: {A} fast incremental gradient method with support for non-strongly convex composite objectives},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K.Q.},
	year = {2014},
}

@article{vilhjalmsson_modeling_2015,
	title = {Modeling {Linkage} {Disequilibrium} {Increases} {Accuracy} of {Polygenic} {Risk} {Scores}},
	volume = {97},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4596916/},
	doi = {10.1016/j.ajhg.2015.09.001},
	abstract = {Polygenic risk scores have shown great promise in predicting complex disease risk and will become more accurate as training sample sizes increase. The standard approach for calculating risk scores involves linkage disequilibrium (LD)-based marker pruning ...},
	language = {en},
	number = {4},
	urldate = {2024-01-24},
	journal = {American Journal of Human Genetics},
	author = {Vilhjálmsson, Bjarni J. and Yang, Jian and Finucane, Hilary K. and Gusev, Alexander and Lindström, Sara and Ripke, Stephan and Genovese, Giulio and Loh, Po-Ru and Bhatia, Gaurav and Do, Ron and Hayeck, Tristan and Won, Hong-Hee and Consortium, Schizophrenia Working Group of the Psychiatric Genomics and Discovery, Biology and Kathiresan, Sekar and Pato, Michele and Pato, Carlos and Tamimi, Rulla and Stahl, Eli and Zaitlen, Noah and Pasaniuc, Bogdan and Belbin, Gillian and Kenny, Eimear E. and Schierup, Mikkel H. and Jager, Philip De and Patsopoulos, Nikolaos A. and McCarroll, Steve and Daly, Mark and Purcell, Shaun and Chasman, Daniel and Neale, Benjamin and Goddard, Michael and Visscher, Peter M. and Kraft, Peter and Patterson, Nick and Price, Alkes L.},
	month = oct,
	year = {2015},
	pmid = {26430803},
	note = {Publisher: Elsevier},
	pages = {576},
}

@article{mak_polygenic_2017,
	title = {Polygenic scores via penalized regression on summary statistics},
	volume = {41},
	issn = {0741-0395},
	url = {https://doi.org/10.1002/gepi.22050},
	doi = {10.1002/gepi.22050},
	abstract = {ABSTRACT Polygenic scores (PGS) summarize the genetic contribution of a person's genotype to a disease or phenotype. They can be used to group participants into different risk categories for diseases, and are also used as covariates in epidemiological analyses. A number of possible ways of calculating PGS have been proposed, and recently there is much interest in methods that incorporate information available in published summary statistics. As there is no inherent information on linkage disequilibrium (LD) in summary statistics, a pertinent question is how we can use LD information available elsewhere to supplement such analyses. To answer this question, we propose a method for constructing PGS using summary statistics and a reference panel in a penalized regression framework, which we call lassosum. We also propose a general method for choosing the value of the tuning parameter in the absence of validation data. In our simulations, we showed that pseudovalidation often resulted in prediction accuracy that is comparable to using a dataset with validation phenotype and was clearly superior to the conservative option of setting the tuning parameter of lassosum to its lowest value. We also showed that lassosum achieved better prediction accuracy than simple clumping and P-value thresholding in almost all scenarios. It was also substantially faster and more accurate than the recently proposed LDpred.},
	number = {6},
	urldate = {2024-01-24},
	journal = {Genetic Epidemiology},
	author = {Mak, Timothy Shin Heng and Porsch, Robert Milan and Choi, Shing Wan and Zhou, Xueya and Sham, Pak Chung},
	month = sep,
	year = {2017},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {LASSO, elastic net, linkage disequilibrium, polygenic score, summary statistics},
	pages = {469--480},
}

@article{wray_basic_2021,
	title = {From basic science to clinical application of polygenic risk scores: a primer},
	volume = {78},
	issn = {2168-622X},
	number = {1},
	journal = {JAMA psychiatry},
	author = {Wray, Naomi R and Lin, Tian and Austin, Jehannine and McGrath, John J and Hickie, Ian B and Murray, Graham K and Visscher, Peter M},
	year = {2021},
	note = {Publisher: American Medical Association},
	pages = {101--109},
}

@article{lewis_polygenic_2020,
	title = {Polygenic risk scores: from research tools to clinical instruments},
	volume = {12},
	issn = {1756-994X},
	number = {1},
	journal = {Genome medicine},
	author = {Lewis, Cathryn M and Vassos, Evangelos},
	year = {2020},
	note = {Publisher: BioMed Central},
	pages = {1--11},
}

@article{torkamani_personal_2018,
	title = {The personal and clinical utility of polygenic risk scores},
	volume = {19},
	issn = {1471-0056},
	number = {9},
	journal = {Nature Reviews Genetics},
	author = {Torkamani, Ali and Wineinger, Nathan E and Topol, Eric J},
	year = {2018},
	note = {Publisher: Nature Publishing Group UK London},
	pages = {581--590},
}

@article{choi_tutorial_2020,
	title = {Tutorial: a guide to performing polygenic risk score analyses},
	volume = {15},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1750-2799},
	shorttitle = {Tutorial},
	url = {https://www.nature.com/articles/s41596-020-0353-1},
	doi = {10.1038/s41596-020-0353-1},
	abstract = {A polygenic score (PGS) or polygenic risk score (PRS) is an estimate of an individual’s genetic liability to a trait or disease, calculated according to their genotype profile and relevant genome-wide association study (GWAS) data. While present PRSs typically explain only a small fraction of trait variance, their correlation with the single largest contributor to phenotypic variation—genetic liability—has led to the routine application of PRSs across biomedical research. Among a range of applications, PRSs are exploited to assess shared etiology between phenotypes, to evaluate the clinical utility of genetic data for complex disease and as part of experimental studies in which, for example, experiments are performed that compare outcomes (e.g., gene expression and cellular response to treatment) between individuals with low and high PRS values. As GWAS sample sizes increase and PRSs become more powerful, PRSs are set to play a key role in research and stratified medicine. However, despite the importance and growing application of PRSs, there are limited guidelines for performing PRS analyses, which can lead to inconsistency between studies and misinterpretation of results. Here, we provide detailed guidelines for performing and interpreting PRS analyses. We outline standard quality control steps, discuss different methods for the calculation of PRSs, provide an introductory online tutorial, highlight common misconceptions relating to PRS results, offer recommendations for best practice and discuss future challenges.},
	language = {en},
	number = {9},
	urldate = {2024-01-24},
	journal = {Nature Protocols},
	author = {Choi, Shing Wan and Mak, Timothy Shin-Heng and O’Reilly, Paul F.},
	month = sep,
	year = {2020},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Genetic association study, Population genetics, Software},
	pages = {2759--2772},
}

@article{gower_variance-reduced_2020,
	title = {Variance-{Reduced} {Methods} for {Machine} {Learning}},
	volume = {108},
	issn = {1558-2256},
	doi = {10.1109/JPROC.2020.3028013},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Gower, R. M. and Schmidt, M. and Bach, F. and Richtárik, P.},
	month = nov,
	year = {2020},
	pages = {1968--1983},
}

@article{beck_fast_2009,
	title = {A {Fast} {Iterative} {Shrinkage}-{Thresholding} {Algorithm} for {Linear} {Inverse} {Problems}},
	volume = {2},
	issn = {1936-4954},
	url = {http://epubs.siam.org/doi/10.1137/080716542},
	doi = {10.1137/080716542},
	abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be signiﬁcantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.},
	language = {en},
	number = {1},
	urldate = {2024-01-16},
	journal = {SIAM Journal on Imaging Sciences},
	author = {Beck, Amir and Teboulle, Marc},
	month = jan,
	year = {2009},
	pages = {183--202},
}

@misc{noauthor_anstallningsavtal_nodate,
	title = {Anställningsavtal / {Employment} contract: {Antônio} {Horta} {Riberio}},
	shorttitle = {Anställningsavtal / {Employment} contract},
	url = {https://kthsign.eu1.documents.adobe.com/public/esign?tsid=CBFCIBAACBSCTBABDUAAABACAABAAlWFtvnB7wu1XlvxJX0ziEsw7Nw_l2I_JkmS-Zm07tKJ0ezVLCryFV0unMwB1AEAVpeoVU8i1YhyGDsuXnGWuq_sR90R5OQ1-AVa2_2ssCYjwzWLLnfj1reMtHFOkcf70&},
	abstract = {Share with your colleagues and friends the simplicity of using Adobe Acrobat Sign to sign documents electronically. Click on the buttons below to spread the word! Learn more at {\textless}a target='\_blank' href='https://acrobat.adobe.com'{\textgreater}https:{\textbackslash}/{\textbackslash}/acrobat.adobe.com{\textless}/a{\textgreater}},
	language = {en-GB},
	urldate = {2024-01-10},
	journal = {Acrobat Sign},
}

@misc{noauthor_researcher_nodate,
	title = {Researcher - {Swedish} {Migration} {Agency}},
	url = {https://www.migrationsverket.se/formengineweb/v2/b8361dcd-7856-4b84-869d-2bfb48c6377e/complete},
	urldate = {2024-01-09},
}

@misc{noauthor_ctan_nodate,
	title = {{CTAN}: /tex-archive/macros/latex/contrib/elsarticle/doc},
	url = {https://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle/doc},
	urldate = {2024-01-03},
}

@inproceedings{marcotte_abide_2023,
	title = {Abide by the law and follow the flow: conservation laws for gradient flows},
	shorttitle = {Abide by the law and follow the flow},
	url = {https://openreview.net/forum?id=kMueEV8Eyy},
	abstract = {Understanding the geometric properties of gradient descent dynamics is a key ingredient in deciphering the recent success of very large machine learning models. A striking observation is that trained over-parameterized models retain some properties of the optimization initialization. This "implicit bias" is believed to be responsible for some favorable properties of the trained models and could explain their good generalization properties. The purpose of this article is threefold. First, we rigorously expose the definition and basic properties of "conservation laws", that define quantities conserved during gradient flows of a given model (e.g. of a ReLU network with a given architecture) with any training data and any loss. Then we explain how to find the maximal number of independent conservation laws by performing finite-dimensional algebraic manipulations on the Lie algebra generated by the Jacobian of the model. Finally, we provide algorithms to: a) compute a family of polynomial laws; b) compute the maximal number of (not necessarily polynomial) independent conservation laws. We provide showcase examples that we fully work out theoretically. Besides, applying the two algorithms confirms for a number of ReLU network architectures that all known laws are recovered by the algorithm, and that there are no other independent laws. Such computational tools pave the way to understanding desirable properties of optimization initialization in large machine learning models.},
	language = {en},
	urldate = {2023-12-12},
	author = {Marcotte, Sibylle and Gribonval, Rémi and Peyré, Gabriel},
	month = nov,
	year = {2023},
}

@inproceedings{wen_sharpness_2023,
	title = {Sharpness {Minimization} {Algorithms} {Do} {Not} {Only} {Minimize} {Sharpness} {To} {Achieve} {Better} {Generalization}},
	url = {https://openreview.net/forum?id=Dkmpa6wCIx},
	abstract = {Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization. This work critically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize poorly, and (3) perhaps most strikingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize. Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve better generalization. This calls for the search for other explanations for the generalization of over-parameterized neural networks},
	language = {en},
	urldate = {2023-12-12},
	author = {Wen, Kaiyue and Li, Zhiyuan and Ma, Tengyu},
	month = nov,
	year = {2023},
}

@inproceedings{bartlett_spectrally-normalized_2017,
	title = {Spectrally-normalized margin bounds for neural networks},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/b22b257ad0519d4500539da3c8bcf4dd-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@misc{guimaraes_super_nodate,
	title = {Super {Partituras} - {Partitura} da música {Forró} no {Escuro} v.2 ({Luiz} {Gonzaga}).},
	url = {https://www.superpartituras.com.br/luiz-gonzaga/forro-no-escuro-v-2},
	abstract = {Página que contém a partitura da música Forró no Escuro v.2 (Luiz Gonzaga).},
	language = {pt-br},
	urldate = {2023-11-28},
	journal = {SuperPartituras},
	author = {Guimaraes, Jonatas},
}

@misc{arora_stronger_2018,
	title = {Stronger generalization bounds for deep nets via a compression approach},
	url = {http://arxiv.org/abs/1802.05296},
	abstract = {Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet result in sample complexity bounds better than naive parameter counting. The current paper shows generalization bounds that’re orders of magnitude better in practice. These rely upon new succinct reparametrizations of the trained net — a compression that is explicit and eﬃcient. These yield generalization bounds via a simple compression-based framework introduced here. Our results also provide some theoretical justiﬁcation for widespread empirical success in compressing deep nets.},
	language = {en},
	urldate = {2023-11-27},
	publisher = {arXiv},
	author = {Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
	month = nov,
	year = {2018},
	note = {arXiv:1802.05296 [cs]},
	keywords = {Computer Science - Machine Learning, \_tablet},
}

@misc{gurbuzbalaban_heavy-tail_2021,
	title = {The {Heavy}-{Tail} {Phenomenon} in {SGD}},
	url = {http://arxiv.org/abs/2006.04740},
	doi = {10.48550/arXiv.2006.04740},
	abstract = {In recent years, various notions of capacity and complexity have been proposed for characterizing the generalization properties of stochastic gradient descent (SGD) in deep learning. Some of the popular notions that correlate well with the performance on unseen data are (i) the `flatness' of the local minimum found by SGD, which is related to the eigenvalues of the Hessian, (ii) the ratio of the stepsize \${\textbackslash}eta\$ to the batch-size \$b\$, which essentially controls the magnitude of the stochastic gradient noise, and (iii) the `tail-index', which measures the heaviness of the tails of the network weights at convergence. In this paper, we argue that these three seemingly unrelated perspectives for generalization are deeply linked to each other. We claim that depending on the structure of the Hessian of the loss at the minimum, and the choices of the algorithm parameters \${\textbackslash}eta\$ and \$b\$, the SGD iterates will converge to a {\textbackslash}emph\{heavy-tailed\} stationary distribution. We rigorously prove this claim in the setting of quadratic optimization: we show that even in a simple linear regression problem with independent and identically distributed data whose distribution has finite moments of all order, the iterates can be heavy-tailed with infinite variance. We further characterize the behavior of the tails with respect to algorithm parameters, the dimension, and the curvature. We then translate our results into insights about the behavior of SGD in deep learning. We support our theory with experiments conducted on synthetic data, fully connected, and convolutional neural networks.},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Gurbuzbalaban, Mert and Şimşekli, Umut and Zhu, Lingjiong},
	month = jun,
	year = {2021},
	note = {arXiv:2006.04740 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Mathematics - Statistics Theory},
}

@article{raj_algorithmic_nodate,
	title = {Algorithmic {Stability} of {Heavy}-{Tailed} {Stochastic} {Gradient} {Descent} on {Least} {Squares}},
	abstract = {Recent studies have shown that heavy tails can emerge in stochastic optimization and that the heaviness of the tails have links to the generalization error. While these studies have shed light on interesting aspects of the generalization behavior in modern settings, they relied on strong topological and statistical regularity assumptions, which are hard to verify in practice. Furthermore, it has been empirically illustrated that the relation between heavy tails and generalization might not always be monotonic in practice, contrary to the conclusions of existing theory. In this study, we establish novel links between the tail behavior and generalization properties of stochastic gradient descent (SGD), through the lens of algorithmic stability. We consider a quadratic optimization problem and use a heavy-tailed stochastic differential equation (and its Euler discretization) as a proxy for modeling the heavy-tailed behavior emerging in SGD. We then prove uniform stability bounds, which reveal the following outcomes: (i) Without making any exotic assumptions, we show that SGD will not be stable if the stability is measured with the squared-loss x → x2, whereas it in turn becomes stable if the stability is instead measured with a surrogate loss x → {\textbar}x{\textbar}p with some p {\textless} 2. (ii) Depending on the variance of the data, there exists a ‘threshold of heavy-tailedness’ such that the generalization error decreases as the tails become heavier, as long as the tails are lighter than this threshold. This suggests that the relation between heavy tails and generalization is not globally monotonic. (iii) We prove matching lower-bounds on uniform stability, implying that our bounds are tight in terms of the heaviness of the tails. We support our theory with synthetic and real neural network experiments.},
	language = {en},
	author = {Raj, Anant},
}

@article{attiazachii._age_2019,
	title = {Age and {Sex} {Estimation} {Using} {Artificial} {Intelligence} {From} {Standard} 12-{Lead} {ECGs}},
	volume = {12},
	url = {https://www.ahajournals.org/doi/full/10.1161/CIRCEP.119.007284},
	doi = {10/gf7d9g},
	abstract = {Background:Sex and age have long been known to affect the ECG. Several biologic variables and anatomic factors may contribute to sex and age-related differences on the ECG. We hypothesized that a convolutional neural network (CNN) could be trained through a process called deep learning to predict a person’s age and self-reported sex using only 12-lead ECG signals. We further hypothesized that discrepancies between CNN-predicted age and chronological age may serve as a physiological measure of health.Methods:We trained CNNs using 10-second samples of 12-lead ECG signals from 499 727 patients to predict sex and age. The networks were tested on a separate cohort of 275 056 patients. Subsequently, 100 randomly selected patients with multiple ECGs over the course of decades were identified to assess within-individual accuracy of CNN age estimation.Results:Of 275 056 patients tested, 52\% were males and mean age was 58.6±16.2 years. For sex classification, the model obtained 90.4\% classification accuracy with an area under the curve of 0.97 in the independent test data. Age was estimated as a continuous variable with an average error of 6.9±5.6 years (R-squared =0.7). Among 100 patients with multiple ECGs over the course of at least 2 decades of life, most patients (51\%) had an average error between real age and CNN-predicted age of {\textless}7 years. Major factors seen among patients with a CNN-predicted age that exceeded chronologic age by {\textgreater}7 years included: low ejection fraction, hypertension, and coronary disease (P{\textless}0.01). In the 27\% of patients where correlation was {\textgreater}0.8 between CNN-predicted and chronologic age, no incident events occurred over follow-up (33±12 years).Conclusions:Applying artificial intelligence to the ECG allows prediction of patient sex and estimation of age. The ability of an artificial intelligence algorithm to determine physiological age, with further validation, may serve as a measure of overall health.},
	number = {9},
	urldate = {2019-09-02},
	journal = {Circulation: Arrhythmia and Electrophysiology},
	author = {{Attia Zachi I.} and A., Friedman Paul and {Noseworthy Peter A.} and {Lopez-Jimenez Francisco} and {Ladewig Dorothy J.} and {Satam Gaurav} and {Pellikka Patricia A.} and {Munger Thomas M.} and {Asirvatham Samuel J.} and {Scott Christopher G.} and {Carter Rickey E.} and {Kapa Suraj}},
	month = sep,
	year = {2019},
	pages = {e007284},
}

@misc{noauthor_inbox_nodate,
	title = {Inbox (2) - antonior92@gmail.com - {Gmail}},
	url = {https://mail.google.com/mail/u/0/#inbox},
	urldate = {2023-10-16},
}

@article{lima_deep_2021,
	title = {Deep neural network estimated electrocardiographic-age as a mortality predictor},
	volume = {12},
	doi = {10.1038/s41467-021-25351-7},
	abstract = {The electrocardiogram (ECG) is the most commonly used exam for the screening and evaluation of cardiovascular diseases. Here we propose that the age predicted by artificial intelligence (AI) from the raw ECG tracing (ECG-age) can be a measure of cardiovascular health and provide prognostic information. A deep convolutional neural network was trained to predict a patient9s age from the 12-lead ECG using data from patients that underwent an ECG from 2010 to 2017 - the CODE study cohort (n=1,558,415 patients). On the 15\% hold-out CODE test split, patients with ECG-age more than 8 years greater than chronological age had a higher mortality rate (hazard ratio (HR) 1.79, p\&lt;0.001) in a mean follow-up of 3.67 years, whereas those with ECG-age more than 8 years less than chronological age had a lower mortality rate (HR 0.78, p\&lt;0.001). Similar results were obtained in the external cohorts ELSA-Brasil (n=14,236) and SaMi-Trop (n=1,631). The ability to predict mortality from the ECG predicted age remains even when we adjust the model for cardiovascular risk factors. Moreover, even for apparent normal ECGs, having a predicted ECG-age 8 or more years greater than chronological age remained a statistically significant predictor of risk (HR 1.53, p\&lt;0.001 in CODE 15\% test split). These results show that AI-enabled analysis of the ECG can add prognostic information to the interpretation of the 12-lead ECGs.},
	journal = {Nature Communications},
	author = {Lima, Emilly M. and Ribeiro, Antônio H. and Paixão, Gabriela M M and Ribeiro, Manoel Horta and Filho, Marcelo M. Pinto and Gomes, Paulo R. and Oliveira, Derick M. and Sabino, Ester C. and Duncan, Bruce B. and Giatti, Luana and Barreto, Sandhi M. and Meira, Wagner and Schön, Thomas B. and Ribeiro, Antonio Luiz P.},
	year = {2021},
	note = {medRxiv doi: 10.1101/2021.02.19.21251232},
}

@inproceedings{dan_sharp_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Sharp statistical guaratees for adversarially robust {Gaussian} classification},
	volume = {119},
	url = {https://proceedings.mlr.press/v119/dan20b.html},
	abstract = {Adversarial robustness has become a fundamental requirement in modern machine learning applications. Yet, there has been surprisingly little statistical understanding so far. In this paper, we provide the first result of the \textit{optimal} minimax guarantees for the excess risk for adversarially robust classification, under Gaussian mixture model proposed by schmidt2018adversarially. The results are stated in terms of the \textit{Adversarial Signal-to-Noise Ratio (AdvSNR)}, which generalizes a similar notion for standard linear classification to the adversarial setting. For the Gaussian mixtures with AdvSNR value of r, we prove an excess risk lower bound of order Θ(e$^{\textrm{-(½+o(1)) r²}}$ ᵈ⁄ₙ) and design a computationally efficient estimator that achieves this optimal rate. Our results built upon minimal assumptions while cover a wide spectrum of adversarial perturbations including ₚ balls for any p 1.},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Dan, Chen and Wei, Yuting and Ravikumar, Pradeep},
	editor = {III, Hal Daumé and Singh, Aarti},
	month = jul,
	year = {2020},
	pages = {2345--2355},
}

@article{dobriban_provable_2022,
	title = {Provable tradeoffs in adversarially robust classification},
	url = {http://arxiv.org/abs/2006.05161},
	doi = {10.48550/arXiv.2006.05161},
	abstract = {It is well known that machine learning methods can be vulnerable to adversarially-chosen perturbations of their inputs. Despite significant progress in the area, foundational open problems remain. In this paper, we address several key questions. We derive exact and approximate Bayes-optimal robust classifiers for the important setting of two- and three-class Gaussian classification problems with arbitrary imbalance, for \${\textbackslash}ell\_2\$ and \${\textbackslash}ell\_{\textbackslash}infty\$ adversaries. In contrast to classical Bayes-optimal classifiers, determining the optimal decisions here cannot be made pointwise and new theoretical approaches are needed. We develop and leverage new tools, including recent breakthroughs from probability theory on robust isoperimetry, which, to our knowledge, have not yet been used in the area. Our results reveal fundamental tradeoffs between standard and robust accuracy that grow when data is imbalanced. We also show further results, including an analysis of classification calibration for convex losses in certain models, and finite sample rates for the robust risk.},
	urldate = {2023-10-03},
	journal = {arXiv:2006.05161},
	author = {Dobriban, Edgar and Hassani, Hamed and Hong, David and Robey, Alexander},
	month = jan,
	year = {2022},
	note = {arXiv:2006.05161},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{xu_robustness_2009,
	title = {Robustness and regularization of support vector machines},
	volume = {10},
	url = {http://jmlr.org/papers/v10/xu09b.html},
	number = {51},
	journal = {Journal of Machine Learning Research},
	author = {Xu, Huan and Caramanis, Constantine and Mannor, Shie},
	year = {2009},
	pages = {1485--1510},
}

@inproceedings{jakubovitz_improving_2018,
	title = {Improving {DNN} robustness to adversarial attacks using jacobian regularization},
	url = {https://api.semanticscholar.org/CorpusID:4326223},
	booktitle = {European conference on computer vision},
	author = {Jakubovitz, Daniel and Giryes, Raja},
	year = {2018},
}

@article{habineza_end--end_2023,
	title = {End-to-end {Risk} {Prediction} of {Atrial} {Fibrillation} from the 12-{Lead} {ECG} by {Deep} {Neural} {Networks}},
	copyright = {All rights reserved},
	doi = {10.1016/j.jelectrocard.2023.09.011},
	journal = {Journal of Electrocardiology},
	author = {Habineza, Theogene and Ribeiro, Antônio H. and Gedon, Daniel and Behar, Joachim A. and Ribeiro, Antonio Luiz P. and Schön, Thomas B.},
	year = {2023},
}

@article{andersson_deep_2019,
	title = {Deep {Convolutional} {Networks} in {System} {Identification}},
	copyright = {All rights reserved},
	doi = {10.1109/CDC40024.2019.9030219},
	abstract = {Recent developments within deep learning are relevant for nonlinear system identification problems. In this paper, we establish connections between the deep learning and the system identification communities. It has recently been shown that convolutional architectures are at least as capable as recurrent architectures when it comes to sequence modeling tasks. Inspired by these results we explore the explicit relationships between the recently proposed temporal convolutional network (TCN) and two classic system identification model structures; Volterra series and block-oriented models. We end the paper with an experimental study where we provide results on two real-world problems, the well-known Silverbox dataset and a newer dataset originating from ground vibration experiments on an F-16 fighter aircraft.},
	journal = {IEEE Conference on Decision and Control (CDC)},
	author = {Andersson, Carl and Ribeiro, Antônio H. and Tiels, Koen and Wahlström, Niklas and Schön, Thomas B.},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.01730},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning},
	pages = {3670--3676},
}

@inproceedings{ribeiro_exploding_2020,
	title = {Beyond exploding and vanishing gradients: attractors and smoothness in the analysis of recurrent neural network training},
	volume = {108},
	copyright = {All rights reserved},
	url = {http://proceedings.mlr.press/v108/ribeiro20a.html},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {Ribeiro, Antônio H. and Tiels, Koen and Aguirre, Luis A. and Schön, Thomas B.},
	year = {2020},
	note = {arXiv:1906.08482},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Dynamical Systems, Statistics - Machine Learning},
	pages = {2370--2380},
}

@inproceedings{oliveira_explaining_2020a,
	address = {Ghent, Belgium},
	title = {Explaining end-to-end {ECG} automated diagnosis using contextual features},
	volume = {12461},
	doi = {10.1007/978-3-030-67670-4_13},
	booktitle = {European {Conference} on {Machine} {Learning} and {Principles} and {Practice} of {Knowledge} {Discovery} in {Databases} ({ECML}-{PKDD})},
	publisher = {Springer},
	author = {Oliveira, Derick M. and Ribeiro, Antônio H. and Pedrosa, João A. O. and Paixao, Gabriela M.M. and Ribeiro, Antonio Luiz P. and Meira Jr, Wagner},
	month = sep,
	year = {2020},
	pages = {204--219},
}

@inproceedings{ribeiro_occam_2021b,
	title = {Beyond {Occam}'s {Razor} in {System} {Identification}: {Double}-{Descent} when {Modeling} {Dynamics}},
	volume = {54},
	shorttitle = {Beyond {Occam}'s {Razor} in {System} {Identification}},
	doi = {10.1016/j.ifacol.2021.08.341},
	booktitle = {{IFAC} {Symposium} on {System} {Identification} ({SYSID})},
	author = {Ribeiro, Antônio H. and Hendriks, Johannes N. and Wills, Adrian G. and Schön, Thomas B.},
	year = {2021},
	note = {arXiv: 2012.06341},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning},
	pages = {97--102},
}

@article{ribeiro_shooting_2017,
	title = {Shooting {Methods} for {Parameter} {Estimation} of {Output} {Error} {Models}},
	volume = {50},
	copyright = {All rights reserved},
	issn = {2405-8963},
	doi = {10.1016/j.ifacol.2017.08.2421},
	abstract = {This paper studies parameter estimation of output error (OE) models. The commonly used approach of minimizing the free-run simulation error is called single shooting in contrast with the new multiple shooting approach proposed in this paper, for which the free-run simulation error of sub-datasets is minimized subject to equality constraints. The names “single shooting” and “multiple shooting” are used due to the similarities with techniques for estimating ODE (ordinary differential equation) parameters. Examples with nonlinear polynomial models illustrate the advantages of OE models as well as the capability of the multiple shooting approach to avoid undesirable local minima.},
	number = {1},
	journal = {IFAC World Congress},
	author = {Ribeiro, Antônio H. and Aguirre, Luis A.},
	month = jul,
	year = {2017},
	keywords = {Multiple shooting, nonlinear least-squares, output error models, simulation error minimization},
	pages = {13998--14003},
}

@article{ribeiro_selecting_2015,
	title = {Selecting transients automatically for the identification of models for an oil well},
	volume = {48},
	copyright = {All rights reserved},
	doi = {10.1016/j.ifacol.2015.08.024},
	number = {6},
	journal = {IFAC Workshop on Automatic Control in Offshore Oil and Gas Production},
	author = {Ribeiro, Antônio H. and Aguirre, Luis A.},
	year = {2015},
	keywords = {Automatic transient selection, intelligent oil fields, soft sensors, system identification},
	pages = {154--158},
}

@article{hendriks_deep_2021,
	title = {Deep {Energy}-{Based} {NARX} {Models}},
	volume = {54},
	copyright = {All rights reserved},
	doi = {10.1016/j.ifacol.2021.08.410},
	abstract = {This paper is directed towards the problem of learning nonlinear ARX models based on system input--output data. In particular, our interest is in learning a conditional distribution of the current output based on a finite window of past inputs and outputs. To achieve this, we consider the use of so-called energy-based models, which have been developed in allied fields for learning unknown distributions based on data. This energy-based model relies on a general function to describe the distribution, and here we consider a deep neural network for this purpose. The primary benefit of this approach is that it is capable of learning both simple and highly complex noise models, which we demonstrate on simulated and experimental data.},
	number = {7},
	journal = {IFAC Symposium on System Identification (SYSID)},
	author = {Hendriks, Johannes N. and Gustafsson, Fredrik K. and Ribeiro, Antônio H. and Wills, Adrian G. and Schön, Thomas B.},
	year = {2021},
	note = {arXiv: 2012.04136},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
	pages = {505--510},
}

@article{ek_off-policy_nodate,
	title = {Off-{Policy} {Evaluation} with {Out}-of-{Sample} {Guarantees}},
	abstract = {We consider the problem of evaluating the performance of a decision policy using past observational data. The outcome of a policy is measured in terms of a loss (aka. disutility or negative reward) and the main problem is making valid inferences about its out-of-sample loss when the past data was observed under a different and possibly unknown policy. Using a sample-splitting method, we show that it is possible to draw such inferences with finitesample coverage guarantees about the entire loss distribution, rather than just its mean. Importantly, the method takes into account model misspecifications of the past policy –including unmeasured confounding. The evaluation method can be used to certify the performance of a policy using observational data under a specified range of credible model assumptions.},
	language = {en},
	author = {Ek, Sofia and Zachariah, Dave and Johansson, Fredrik D and Stoica, Petre},
	keywords = {\_tablet},
}

@misc{manski_patient-centered_2022,
	title = {Patient-{Centered} {Appraisal} of {Race}-{Free} {Clinical} {Risk} {Assessment}},
	url = {http://arxiv.org/abs/2112.01639},
	doi = {10.48550/arXiv.2112.01639},
	abstract = {Until recently, there has been a consensus that clinicians should condition patient risk assessments on all observed patient covariates with predictive power. The broad idea is that knowing more about patients enables more accurate predictions of their health risks and, hence, better clinical decisions. This consensus has recently unraveled with respect to a specific covariate, namely race. There have been increasing calls for race-free risk assessment, arguing that using race to predict patient outcomes contributes to racial disparities and inequities in health care. Writers calling for race-free risk assessment have not studied how it would affect the quality of clinical decisions. Considering the matter from the patient-centered perspective of medical economics yields a disturbing conclusion: Race-free risk assessment would harm patients of all races.},
	urldate = {2023-09-04},
	publisher = {arXiv},
	author = {Manski, Charles F.},
	month = feb,
	year = {2022},
	note = {arXiv:2112.01639 [econ]},
	keywords = {Economics - Econometrics, \_tablet\_modified},
}

@article{de_vet_when_2006,
	title = {When to use agreement versus reliability measures},
	volume = {59},
	issn = {08954356},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435606000291},
	doi = {10.1016/j.jclinepi.2005.10.015},
	abstract = {Background: Reproducibility concerns the degree to which repeated measurements provide similar results. Agreement parameters assess how close the results of the repeated measurements are, by estimating the measurement error in repeated measurements. Reliability parameters assess whether study objects, often persons, can be distinguished from each other, despite measurement errors. In that case, the measurement error is related to the variability between persons. Consequently, reliability parameters are highly dependent on the heterogeneity of the study sample, while the agreement parameters, based on measurement error, are more a pure characteristic of the measurement instrument.
Methods and Results: Using an example of an interrater study, in which different physical therapists measure the range of motion of the arm in patients with shoulder complaints, the differences and relationships between reliability and agreement parameters for continuous variables are illustrated.
Conclusion: If the research question concerns the distinction of persons, reliability parameters are the most appropriate. But if the aim is to measure change in health status, which is often the case in clinical practice, parameters of agreement are preferred. Ó 2006 Elsevier Inc. All rights reserved.},
	language = {en},
	number = {10},
	urldate = {2023-08-30},
	journal = {Journal of Clinical Epidemiology},
	author = {De Vet, Henrica C.W. and Terwee, Caroline B. and Knol, Dirk L. and Bouter, Lex M.},
	month = oct,
	year = {2006},
	keywords = {\_tablet\_modified},
	pages = {1033--1039},
}

@article{brant_reproducibility_2013,
	title = {Reproducibility of peripheral arterial tonometry for the assessment of endothelial function in adults},
	volume = {31},
	issn = {0263-6352},
	url = {https://journals.lww.com/00004872-201310000-00011},
	doi = {10.1097/HJH.0b013e328362d913},
	abstract = {Objectives: Endothelial dysfunction is associated to cardiovascular risk factors and predicts cardiovascular events. Peripheral arterial tonometry (PAT) is a novel noninvasive method to assess endothelial function. However, there is a paucity of data about its reproducibility. The aim of this study was to assess the feasibility and reproducibility of PAT in adults.
Methods: PAT exams were performed twice in the same day in 123 participants of a cohort about the determinants of diabetes and cardiovascular diseases (Brazilian Longitudinal Study of Adult Health – ELSA-Brasil). The interval between the exams was 2–6 h (mean ¼ 4 h). Endothelial function in PAT method is measured by reactive hyperemia index (RHI), which evaluates arterial pulsatile volume changes in response to hyperemia. Agreement of RHI values was compared by Bland–Altman method, coefficient of variation and coefficient of repeatability. Reliability was assessed by intraclass correlation coefficient (ICC).
Results: Mean values of RHI did not differ significantly between the exams of each participant (1.92 Æ 0.56 vs. 1.96 Æ 0.58, P ¼ 0.48). There were no systematic errors between the exams (mean of differences ¼ À0.03 Æ 0.5). Measurement error was 0.35, coefficient of variation was 18.0\% and ICC was 0.61. Sex, age or the presence of obesity did not have a considerable influence on the reproducibility of PAT.
Conclusion: PAT exam is feasible and has acceptable reproducibility in adults when compared with other noninvasive methods for endothelial function assessment. This performance makes PAT a promising method for future clinical and epidemiological studies.},
	language = {en},
	number = {10},
	urldate = {2023-08-30},
	journal = {Journal of Hypertension},
	author = {Brant, Luisa C.C. and Barreto, Sandhi M. and Passos, Valéria M.A. and Ribeiro, Antônio L.P.},
	month = oct,
	year = {2013},
	keywords = {\_tablet},
	pages = {1984--1990},
}

@article{hlatky_criteria_2009,
	title = {Criteria for {Evaluation} of {Novel} {Markers} of {Cardiovascular} {Risk}: {A} {Scientific} {Statement} {From} the {American} {Heart} {Association}},
	volume = {119},
	issn = {0009-7322, 1524-4539},
	shorttitle = {Criteria for {Evaluation} of {Novel} {Markers} of {Cardiovascular} {Risk}},
	url = {https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.109.192278},
	doi = {10.1161/CIRCULATIONAHA.109.192278},
	abstract = {There is increasing interest in utilizing novel markers of cardiovascular disease risk, and consequently, there is a need to assess the value of their use. This scientific statement reviews current concepts of risk evaluation and proposes standards for the critical appraisal of risk assessment methods. An adequate evaluation of a novel risk marker requires a sound research design, a representative at-risk population, and an adequate number of outcome events. Studies of a novel marker should report the degree to which it adds to the prognostic information provided by standard risk markers. No single statistical measure provides all the information needed to assess a novel marker, so measures of both discrimination and accuracy should be reported. The clinical value of a marker should be assessed by its effect on patient management and outcomes. In general, a novel risk marker should be evaluated in several phases, including initial proof of concept, prospective validation in independent populations, documentation of incremental information when added to standard risk markers, assessment of effects on patient management and outcomes, and ultimately, cost-effectiveness.},
	language = {en},
	number = {17},
	urldate = {2023-08-30},
	journal = {Circulation},
	author = {Hlatky, Mark A. and Greenland, Philip and Arnett, Donna K. and Ballantyne, Christie M. and Criqui, Michael H. and Elkind, Mitchell S.V. and Go, Alan S. and Harrell, Frank E. and Hong, Yuling and Howard, Barbara V. and Howard, Virginia J. and Hsue, Priscilla Y. and Kramer, Christopher M. and McConnell, Joseph P. and Normand, Sharon-Lise T. and O'Donnell, Christopher J. and Smith, Sidney C. and Wilson, Peter W.F.},
	month = may,
	year = {2009},
	pages = {2408--2416},
}

@article{szumilas_explaining_2010,
	title = {Explaining {Odds} {Ratios}},
	volume = {19},
	issn = {1719-8429},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2938757/},
	number = {3},
	urldate = {2023-08-30},
	journal = {Journal of the Canadian Academy of Child and Adolescent Psychiatry},
	author = {Szumilas, Magdalena},
	month = aug,
	year = {2010},
	pmid = {20842279},
	pmcid = {PMC2938757},
	pages = {227--229},
}

@misc{chernozhukov_doubledebiased_2017,
	title = {Double/{Debiased} {Machine} {Learning} for {Treatment} and {Causal} {Parameters}},
	url = {http://arxiv.org/abs/1608.00060},
	abstract = {We revisit the classic semiparametric problem of inference on a low dimensional parameter θ0 in the presence of high-dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high-dimensional that the traditional assumptions, such as Donsker properties, that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods which are particularly well-suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading oﬀ regularization bias with overﬁtting in practice. However, both regularization bias and overﬁtting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N −1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overﬁtting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0, and (2) making use of cross-ﬁtting which provides an eﬃcient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in a N −1/2-neighborhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid conﬁdence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of DML applied to learn the main regression parameter in a partially linear regression model, DML applied to learn the coeﬃcient on an endogenous variable in a partially linear instrumental variables model, DML applied to learn the average treatment eﬀect and the average treatment eﬀect on the treated under unconfoundedness, and DML applied to learn the local average treatment eﬀect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
	language = {en},
	urldate = {2023-08-30},
	publisher = {arXiv},
	author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
	month = dec,
	year = {2017},
	note = {arXiv:1608.00060 [econ, stat]},
	keywords = {62G, Economics - Econometrics, Statistics - Machine Learning},
}

@article{tibshirani_lasso_2013,
	title = {The {Lasso} {Problem} and {Uniqueness}},
	volume = {7},
	abstract = {The lasso is a popular tool for sparse linear regression, especially for problems in which the number of variables p exceeds the number of observations n. But when p {\textgreater} n, the lasso criterion is not strictly convex, and hence it may not have a unique minimizer. An important question is: when is the lasso solution well-deﬁned (unique)? We review results from the literature, which show that if the predictor variables are drawn from a continuous probability distribution, then there is a unique lasso solution with probability one, regardless of the sizes of n and p. We also show that this result extends easily to 1 penalized minimization problems over a wide range of loss functions.},
	language = {en},
	journal = {Electronic Journal of Statistics},
	author = {Tibshirani, Ryan J},
	year = {2013},
	keywords = {\_tablet},
	pages = {1456--1490},
}

@misc{luo_understanding_2022,
	title = {Understanding {Diffusion} {Models}: {A} {Unified} {Perspective}},
	shorttitle = {Understanding {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2208.11970},
	doi = {10.48550/arXiv.2208.11970},
	abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
	urldate = {2023-07-27},
	publisher = {arXiv},
	author = {Luo, Calvin},
	month = aug,
	year = {2022},
	note = {arXiv:2208.11970 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{lindow_heart_2023,
	title = {Heart age gap by explainable advanced electrocardiography is associated with cardiovascular risk factors and survival},
	copyright = {All rights reserved},
	doi = {10.1093/ehjdh/ztad045},
	journal = {European Heart Journal - Digital Health},
	author = {Lindow, Thomas and Maanja, Maren and Schelbert, Erik B and Ribeiro, Antonio H and Ribeiro, Antonio Luiz P and Schlegel, Todd T and Ugander, Martin},
	year = {2023},
}

@article{sangha_detection_2023,
	title = {Detection of {Left} {Ventricular} {Systolic} {Dysfunction} from {Electrocardiographic} {Images}},
	doi = {10.1161/CIRCULATIONAHA.122.062646},
	abstract = {BACKGROUND:
Left ventricular (LV) systolic dysfunction is associated with a {\textgreater}8-fold increased risk of heart failure and a 2-fold risk of premature death. The use of ECG signals in screening for LV systolic dysfunction is limited by their availability to clinicians. We developed a novel deep learning–based approach that can use ECG images for the screening of LV systolic dysfunction.

METHODS:
Using 12-lead ECGs plotted in multiple different formats, and corresponding echocardiographic data recorded within 15 days from the Yale New Haven Hospital between 2015 and 2021, we developed a convolutional neural network algorithm to detect an LV ejection fraction {\textless}40\%. The model was validated within clinical settings at Yale New Haven Hospital and externally on ECG images from Cedars Sinai Medical Center in Los Angeles, CA; Lake Regional Hospital in Osage Beach, MO; Memorial Hermann Southeast Hospital in Houston, TX; and Methodist Cardiology Clinic of San Antonio, TX. In addition, it was validated in the prospective Brazilian Longitudinal Study of Adult Health. Gradient-weighted class activation mapping was used to localize class-discriminating signals in ECG images.

RESULTS:
Overall, 385 601 ECGs with paired echocardiograms were used for model development. The model demonstrated high discrimination power across various ECG image formats and calibrations in internal validation (area under receiving operation characteristics [AUROCs], 0.91; area under precision-recall curve [AUPRC], 0.55); and external sets of ECG images from Cedars Sinai (AUROC, 0.90 and AUPRC, 0.53), outpatient Yale New Haven Hospital clinics (AUROC, 0.94 and AUPRC, 0.77), Lake Regional Hospital (AUROC, 0.90 and AUPRC, 0.88), Memorial Hermann Southeast Hospital (AUROC, 0.91 and AUPRC 0.88), Methodist Cardiology Clinic (AUROC, 0.90 and AUPRC, 0.74), and Brazilian Longitudinal Study of Adult Health cohort (AUROC, 0.95 and AUPRC, 0.45). An ECG suggestive of LV systolic dysfunction portended a {\textgreater}27-fold higher odds of LV systolic dysfunction on transthoracic echocardiogram (odds ratio, 27.5 [95\% CI, 22.3–33.9] in the held-out set). Class-discriminative patterns localized to the anterior and anteroseptal leads (V2 to V3), corresponding to the left ventricle regardless of the ECG layout. A positive ECG screen in individuals with an LV ejection fraction ≥40\% at the time of initial assessment was associated with a 3.9-fold increased risk of developing incident LV systolic dysfunction in the future (hazard ratio, 3.9 [95\% CI, 3.3–4.7]; median follow-up, 3.2 years).

CONCLUSIONS:
We developed and externally validated a deep learning model that identifies LV systolic dysfunction from ECG images. This approach represents an automated and accessible screening strategy for LV systolic dysfunction, particularly in low-resource settings.},
	journal = {Circulation},
	author = {Sangha, Veer and Nargesi, Arash A. and Dhingra, Lovedeep S. and Mortazavi, Bobak J. and Ribeiro, Antônio H. and Brandt, Cynthia A. and Miller, Edward J. and Ribeiro, Antonio Luiz P. and Velazquez, Eric J. and Krumholz, Harlan M. and Khera, Rohan},
	year = {2023},
}

@misc{barsbey_heavy_2021,
	title = {Heavy {Tails} in {SGD} and {Compressibility} of {Overparametrized} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2106.03795},
	doi = {10.48550/arXiv.2106.03795},
	abstract = {Neural network compression techniques have become increasingly popular as they can drastically reduce the storage and computation requirements for very large networks. Recent empirical studies have illustrated that even simple pruning strategies can be surprisingly effective, and several theoretical studies have shown that compressible networks (in specific senses) should achieve a low generalization error. Yet, a theoretical characterization of the underlying cause that makes the networks amenable to such simple compression schemes is still missing. In this study, we address this fundamental question and reveal that the dynamics of the training algorithm has a key role in obtaining such compressible networks. Focusing our attention on stochastic gradient descent (SGD), our main contribution is to link compressibility to two recently established properties of SGD: (i) as the network size goes to infinity, the system can converge to a mean-field limit, where the network weights behave independently, (ii) for a large step-size/batch-size ratio, the SGD iterates can converge to a heavy-tailed stationary distribution. In the case where these two phenomena occur simultaneously, we prove that the networks are guaranteed to be '\${\textbackslash}ell\_p\$-compressible', and the compression errors of different pruning techniques (magnitude, singular value, or node pruning) become arbitrarily small as the network size increases. We further prove generalization bounds adapted to our theoretical framework, which indeed confirm that the generalization error will be lower for more compressible networks. Our theory and numerical study on various neural networks show that large step-size/batch-size ratios introduce heavy-tails, which, in combination with overparametrization, result in compressibility.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Barsbey, Melih and Sefidgaran, Milad and Erdogdu, Murat A. and Richard, Gaël and Şimşekli, Umut},
	month = jun,
	year = {2021},
	note = {arXiv:2106.03795 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, \_tablet\_modified},
}

@article{brant_electrocardiographic_2023,
	title = {Electrocardiographic {Age} {Predicts} {Cardiovascular} {Events} in {Community}: {The} {Framingham} {Heart} {Study}},
	copyright = {All rights reserved},
	doi = {10.1161/CIRCOUTCOMES.122.009821},
	journal = {Circulation: Cardiovascular Quality and Outcomes},
	author = {Brant, Luisa C C and Ribeiro, Antônio H and Pinto-Filho, Marcelo M and Kornej, Jelena and Preis, Sarah R. and Eromosele, Benjamin and Magnani, Jared W. and Murabito, Joanne M. and Larson, Martin G and Benjamin, Emelia J and Ribeiro, Antonio L P and Lin, Honghuang},
	year = {2023},
}

@article{jidling_screening_2023,
	title = {Screening for {Chagas} disease from the electrocardiogram using a deep neural network},
	volume = {17},
	doi = {10.1371/journal.pntd.0011118},
	number = {7},
	journal = {Plos Neglected Tropical Diseases},
	author = {Jidling, Carl and Gedon, Daniel and Schön, Thomas B. and Oliveira, Claudia Di Lorenzo and Cardos, Clareci Silva and Ferreira, Ariela Mota and Giatti, Luana and Barreto, Sandhi Maria and Sabino, Ester C. and Ribeiro, Antônio L. P. and Ribeiro, Antônio H.},
	year = {2023},
}

@article{katzman_deepsurv_2018,
	title = {{DeepSurv}: personalized treatment recommender system using a {Cox} proportional hazards deep neural network},
	volume = {18},
	issn = {1471-2288},
	shorttitle = {{DeepSurv}},
	url = {https://doi.org/10.1186/s12874-018-0482-1},
	doi = {10.1186/s12874-018-0482-1},
	abstract = {Medical practitioners use survival models to explore and understand the relationships between patients’ covariates (e.g. clinical and genetic features) and the effectiveness of various treatment options. Standard survival models like the linear Cox proportional hazards model require extensive feature engineering or prior medical knowledge to model treatment interaction at an individual level. While nonlinear survival methods, such as neural networks and survival forests, can inherently model these high-level interaction terms, they have yet to be shown as effective treatment recommender systems.},
	number = {1},
	urldate = {2023-07-06},
	journal = {BMC Medical Research Methodology},
	author = {Katzman, Jared L. and Shaham, Uri and Cloninger, Alexander and Bates, Jonathan and Jiang, Tingting and Kluger, Yuval},
	month = feb,
	year = {2018},
	keywords = {Deep learning, Survival analysis, Treatment recommendations},
	pages = {24},
}

@article{deng_comparison_2023,
	title = {Comparison of {State}-of-the-{Art} {Neural} {Network} {Survival} {Models} with the {Pooled} {Cohort} {Equations} for {Cardiovascular} {Disease} {Risk} {Prediction}},
	volume = {23},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-022-01829-w},
	doi = {10.1186/s12874-022-01829-w},
	abstract = {The Pooled Cohort Equations (PCEs) are race- and sex-specific Cox proportional hazards (PH)-based models used for 10-year atherosclerotic cardiovascular disease (ASCVD) risk prediction with acceptable discrimination. In recent years, neural network models have gained increasing popularity with their success in image recognition and text classification. Various survival neural network models have been proposed by combining survival analysis and neural network architecture to take advantage of the strengths from both. However, the performance of these survival neural network models compared to each other and to PCEs in ASCVD prediction is unknown.},
	number = {1},
	urldate = {2023-07-06},
	journal = {BMC Medical Research Methodology},
	author = {Deng, Yu and Liu, Lei and Jiang, Hongmei and Peng, Yifan and Wei, Yishu and Zhou, Zhiyang and Zhong, Yizhen and Zhao, Yun and Yang, Xiaoyun and Yu, Jingzhi and Lu, Zhiyong and Kho, Abel and Ning, Hongyan and Allen, Norrina B. and Wilkins, John T. and Liu, Kiang and Lloyd-Jones, Donald M. and Zhao, Lihui},
	month = jan,
	year = {2023},
	keywords = {Artificial intelligence, Cardiovascular disease, Cox regression, Deep learning, Machine learning, Neural network, Pooled Cohort Equations, Predictive modeling, Survival analysis},
	pages = {22},
}

@article{katzman_deepsurv_2018-1,
	title = {{DeepSurv}: {Personalized} {Treatment} {Recommender} {System} {Using} {A} {Cox} {Proportional} {Hazards} {Deep} {Neural} {Network}},
	volume = {18},
	issn = {1471-2288},
	shorttitle = {{DeepSurv}},
	url = {http://arxiv.org/abs/1606.00931},
	doi = {10.1186/s12874-018-0482-1},
	abstract = {Medical practitioners use survival models to explore and understand the relationships between patients' covariates (e.g. clinical and genetic features) and the effectiveness of various treatment options. Standard survival models like the linear Cox proportional hazards model require extensive feature engineering or prior medical knowledge to model treatment interaction at an individual level. While nonlinear survival methods, such as neural networks and survival forests, can inherently model these high-level interaction terms, they have yet to be shown as effective treatment recommender systems. We introduce DeepSurv, a Cox proportional hazards deep neural network and state-of-the-art survival method for modeling interactions between a patient's covariates and treatment effectiveness in order to provide personalized treatment recommendations. We perform a number of experiments training DeepSurv on simulated and real survival data. We demonstrate that DeepSurv performs as well as or better than other state-of-the-art survival models and validate that DeepSurv successfully models increasingly complex relationships between a patient's covariates and their risk of failure. We then show how DeepSurv models the relationship between a patient's features and effectiveness of different treatment options to show how DeepSurv can be used to provide individual treatment recommendations. Finally, we train DeepSurv on real clinical studies to demonstrate how it's personalized treatment recommendations would increase the survival time of a set of patients. The predictive and modeling capabilities of DeepSurv will enable medical researchers to use deep neural networks as a tool in their exploration, understanding, and prediction of the effects of a patient's characteristics on their risk of failure.},
	number = {1},
	urldate = {2023-07-06},
	journal = {BMC Medical Research Methodology},
	author = {Katzman, Jared and Shaham, Uri and Bates, Jonathan and Cloninger, Alexander and Jiang, Tingting and Kluger, Yuval},
	month = dec,
	year = {2018},
	note = {arXiv:1606.00931 [cs, stat]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	pages = {24},
}

@book{rockafellar_convex_1970,
	title = {Convex {Analysis}},
	author = {Rockafellar, R.Tyrell},
	year = {1970},
	keywords = {\_tablet\_modified},
}

@book{bauschke_convex_2011,
	address = {New York, NY},
	series = {{CMS} {Books} in {Mathematics}},
	title = {Convex {Analysis} and {Monotone} {Operator} {Theory} in {Hilbert} {Spaces}},
	isbn = {978-1-4419-9466-0 978-1-4419-9467-7},
	url = {https://link.springer.com/10.1007/978-1-4419-9467-7},
	language = {en},
	urldate = {2023-03-21},
	publisher = {Springer New York},
	author = {Bauschke, Heinz H. and Combettes, Patrick L.},
	year = {2011},
	doi = {10.1007/978-1-4419-9467-7},
}

@misc{bach_relationship_2023,
	title = {On the relationship between multivariate splines and infinitely-wide neural networks},
	url = {http://arxiv.org/abs/2302.03459},
	doi = {10.48550/arXiv.2302.03459},
	abstract = {We consider multivariate splines and show that they have a random feature expansion as infinitely wide neural networks with one-hidden layer and a homogeneous activation function which is the power of the rectified linear unit. We show that the associated function space is a Sobolev space on a Euclidean ball, with an explicit bound on the norms of derivatives. This link provides a new random feature expansion for multivariate splines that allow efficient algorithms. This random feature expansion is numerically better behaved than usual random Fourier features, both in theory and practice. In particular, in dimension one, we compare the associated leverage scores to compare the two random expansions and show a better scaling for the neural network expansion.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Bach, Francis},
	month = mar,
	year = {2023},
	note = {arXiv:2302.03459 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{noauthor_cite_nodate,
	title = {Cité des sciences et de l'industrie - {Accueil} - {Expositions}, conférences, cinémas, activités culturelles et sorties touristiques pour les enfants, les parents, les familles - {Paris}},
	url = {https://www.cite-sciences.fr/fr/accueil},
	abstract = {La cité des sciences et de l'industrie est un établissement public de diffusion de la culture scientifique, technique et industrielle située à Paris, La Villette. La Cité propose expositions, films, conférences et animations pour les enfants et leurs familles.},
	language = {fr-FR},
	urldate = {2023-05-28},
}

@article{bach_high-dimensional_2023,
	title = {High-dimensional analysis of double descent for linear regression with random projections},
	url = {http://arxiv.org/abs/2303.01372},
	abstract = {We consider linear regression problems with a varying number of random projections, where we provably exhibit a double descent curve for a fixed prediction problem, with a high-dimensional analysis based on random matrix theory. We first consider the ridge regression estimator and re-interpret earlier results using classical notions from non-parametric statistics, namely degrees of freedom, also known as effective dimensionality. In particular, we show that the random design performance of ridge regression with a specific regularization parameter matches the classical bias and variance expressions coming from the easier fixed design analysis but for another larger implicit regularization parameter. We then compute asymptotic equivalents of the generalization performance (in terms of bias and variance) of the minimum norm least-squares fit with random projections, providing simple expressions for the double descent phenomenon.},
	journal = {arXiv:2303.01372},
	author = {Bach, Francis},
	month = mar,
	year = {2023},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{wainwright_highdimensional_2019,
	series = {Cambridge series on statistical and probabilistic mathematics 48},
	title = {High-{Simensional} {Statistics}: a non-{Asymptotic} {Viewpoint}},
	isbn = {978-1-108-62777-1 1-108-62777-3 978-1-108-49802-9},
	url = {http://gen.lib.rus.ec/book/index.php?md5=b80e7471e94d9e9a29b8b10221f70feb},
	publisher = {Cambridge University Press},
	author = {Wainwright, Martin J},
	year = {2019},
}

@inproceedings{koehler_uniform_2021,
	title = {Uniform {Convergence} of {Interpolators}: {Gaussian} {Width}, {Norm} {Bounds} and {Benign} {Overfitting}},
	shorttitle = {Uniform {Convergence} of {Interpolators}},
	url = {https://openreview.net/forum?id=FyOhThdDBM},
	abstract = {Uniform convergence of interpolating predictors can explain consistency for high-dimensional linear regression.},
	language = {en},
	urldate = {2022-08-04},
	booktitle = {{NeurIPS}},
	author = {Koehler, Frederic and Zhou, Lijia and Sutherland, Danica J. and Srebro, Nathan},
	month = oct,
	year = {2021},
}

@inproceedings{zhang_understanding_2017,
	title = {Understanding deep learning requires rethinking generalization},
	abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year = {2017},
	keywords = {Computer Science - Machine Learning},
}

@article{tsipras_robustness_2019,
	title = {Robustness {May} {Be} {At} {Odds} with {Accuracy}},
	abstract = {We show that there exists an inherent tension between the goal of adversarial robustness and that of standard generalization. Speciﬁcally, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting. These ﬁndings also corroborate a similar phenomenon observed in practice. Further, we argue that this phenomenon is a consequence of robust classiﬁers learning fundamentally different feature representations than standard classiﬁers. These differences, in particular, seem to result in unexpected beneﬁts: the features learned by robust models tend to align better with salient data characteristics and human perception.},
	language = {en},
	journal = {International Conference for Learning Representations},
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Ma, Aleksander},
	year = {2019},
}

@inproceedings{kurakin_adversarial_2018,
	title = {Adversarial attacks and defences competition},
	isbn = {978-3-319-94042-7},
	abstract = {To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several of the top-placing teams.},
	booktitle = {The {NIPS} '17 competition: {Building} {Intelligent} {Systems}},
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy and Dong, Yinpeng and Liao, Fangzhou and Liang, Ming and Pang, Tianyu and Zhu, Jun and Hu, Xiaolin and Xie, Cihang and Wang, Jianyu and Zhang, Zhishuai and Ren, Zhou and Yuille, Alan and Huang, Sangxia and Zhao, Yao and Zhao, Yuzhe and Han, Zhonglin and Long, Junjiajia and Berdibekov, Yerkebulan and Akiba, Takuya and Tokui, Seiya and Abe, Motoki},
	editor = {Escalera, Sergio and Weimer, Markus},
	year = {2018},
	pages = {195--231},
}

@article{gedon_invertible_2023,
	title = {Invertible {Kernel} {PCA} with {Random} {Fourier} {Features}},
	doi = {10.1109/LSP.2023.3275499},
	abstract = {Kernel principal component analysis (kPCA) is a widely studied method to construct a low-dimensional data representation after a nonlinear transformation. The prevailing method to reconstruct the original input signal from kPCA—an important task for denoising—requires us to solve a supervised learning problem. In this paper, we present an alternative method where the reconstruction follows naturally from the compression step. We first approximate the kernel with random Fourier features. Then, we exploit the fact that the nonlinear transformation is invertible in a certain subdomain. Hence, the name invertible kernel PCA (ikPCA) . We experiment with different data modalities and show that ikPCA performs similarly to kPCA with supervised reconstruction on denoising tasks, making it a strong alternative.},
	journal = {IEEE Signal Processing Letters},
	author = {Gedon, Daniel and Ribeiro, Antônio H. and Wahlström, Niklas and Schön, Thomas B.},
	month = may,
	year = {2023},
	note = {arXiv:2303.05043},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
}

@article{javanmard_precise_2022,
	title = {Precise statistical analysis of classification accuracies for adversarial training},
	volume = {50},
	url = {https://doi.org/10.1214/22-AOS2180},
	doi = {10.1214/22-AOS2180},
	number = {4},
	journal = {The Annals of Statistics},
	author = {Javanmard, Adel and Soltanolkotabi, Mahdi},
	year = {2022},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Binary classification, Precise high-dimensional asymptotics, adversarial training},
	pages = {2127 -- 2156},
}

@article{zhang_association_2023,
	title = {Association of lifestyle with deep-learning based {ECG}-age},
	volume = {10},
	copyright = {All rights reserved},
	doi = {10.3389/fcvm.2023.1160091},
	journal = {Frontiers in Cardiovascular Medicine},
	author = {Zhang, Cuili and Miao, Xiao and Wang, Biqi and Ribeiro, Antônio H and Brant, Luisa and Ribeiro, Antonio L P and Lin, Honghuang},
	year = {2023},
}

@article{rojas-carulla_invariant_2018,
	title = {Invariant {Models} for {Causal} {Transfer} {Learning}},
	abstract = {Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the ﬁeld of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are suﬃciently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set.},
	language = {en},
	number = {19},
	journal = {Journal of Machine Learning Research},
	author = {Rojas-Carulla, Mateo and Scholkopf, Bernhard and Turner, Richard and Peters, Jonas},
	year = {2018},
	pages = {1--34},
}

@inproceedings{carlini_certified_2023,
	title = {({Certified}!!) {Adversarial} {Robustness} for {Free}!},
	url = {http://arxiv.org/abs/2206.10550},
	doi = {10.48550/arXiv.2206.10550},
	abstract = {In this paper we show how to achieve state-of-the-art certified adversarial robustness to 2-norm bounded perturbations by relying exclusively on off-the-shelf pretrained models. To do so, we instantiate the denoised smoothing approach of Salman et al. 2020 by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This allows us to certify 71\% accuracy on ImageNet under adversarial perturbations constrained to be within an 2-norm of 0.5, an improvement of 14 percentage points over the prior certified SoTA using any approach, or an improvement of 30 percentage points over denoised smoothing. We obtain these results using only pretrained diffusion models and image classifiers, without requiring any fine tuning or retraining of model parameters.},
	urldate = {2023-04-13},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Tramer, Florian and Dvijotham, Krishnamurthy Dj and Rice, Leslie and Sun, Mingjie and Kolter, J. Zico},
	month = mar,
	year = {2023},
	note = {arXiv:2206.10550 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{cohen_certified_2019,
	title = {Certified {Adversarial} {Robustness} via {Randomized} {Smoothing}},
	volume = {97},
	url = {http://arxiv.org/abs/1902.02918},
	doi = {10.48550/arXiv.1902.02918},
	abstract = {We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the \${\textbackslash}ell\_2\$ norm. This "randomized smoothing" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in \${\textbackslash}ell\_2\$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49\% under adversarial perturbations with \${\textbackslash}ell\_2\$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified \${\textbackslash}ell\_2\$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at http://github.com/locuslab/smoothing.},
	urldate = {2023-04-14},
	booktitle = {Proceedings of the {International} {Conference} on {Machine} {Learning}},
	publisher = {arXiv},
	author = {Cohen, Jeremy M. and Rosenfeld, Elan and Kolter, J. Zico},
	month = jun,
	year = {2019},
	note = {arXiv:1902.02918 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1310--1320},
}

@article{gevers_identification_2005,
	title = {Identification for control: {From} the early achievements to the revival of experiment design},
	volume = {11},
	shorttitle = {Identification for control},
	number = {4-5},
	journal = {European Journal of Control},
	author = {Gevers, Michel},
	year = {2005},
	note = {Publisher: Elsevier},
	pages = {335--352},
}

@inproceedings{croce_robustbench_2021,
	title = {{RobustBench}: a standardized adversarial robustness benchmark},
	shorttitle = {{RobustBench}},
	url = {http://arxiv.org/abs/2010.09670},
	doi = {10.48550/arXiv.2010.09670},
	abstract = {As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to robustness overestimation. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models. We evaluate adversarial robustness with AutoAttack, an ensemble of white- and black-box attacks, which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. To prevent overadaptation of new defenses to AutoAttack, we welcome external evaluations based on adaptive attacks, especially where AutoAttack flags a potential overestimation of robustness. Our leaderboard, hosted at https://robustbench.github.io/, contains evaluations of 120+ models and aims at reflecting the current state of the art in image classification on a set of well-defined tasks in \${\textbackslash}ell\_{\textbackslash}infty\$- and \${\textbackslash}ell\_2\$-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library https://github.com/RobustBench/robustbench that provides unified access to 80+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability.},
	urldate = {2023-04-13},
	booktitle = {{NeurIPS} {Datasets} and {Benchmarks} track},
	author = {Croce, Francesco and Andriushchenko, Maksym and Sehwag, Vikash and Debenedetti, Edoardo and Flammarion, Nicolas and Chiang, Mung and Mittal, Prateek and Hein, Matthias},
	month = oct,
	year = {2021},
	note = {arXiv:2010.09670 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{laurent_deep_2018,
	title = {Deep {Linear} {Networks} with {Arbitrary} {Loss}: {All} {Local} {Minima} {Are} {Global}},
	shorttitle = {Deep {Linear} {Networks} with {Arbitrary} {Loss}},
	url = {http://proceedings.mlr.press/v80/laurent18a.html},
	abstract = {We consider deep linear networks with arbitrary convex differentiable loss. We provide a short and elementary proof of the fact that all local minima are global minima if the hidden layers are eith...},
	language = {en},
	urldate = {2020-09-07},
	booktitle = {Proceedings of the {International} {Conference} on {Machine} {Learning}},
	author = {Laurent, Thomas and Brecht, James},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2902--2907},
}

@inproceedings{yin_rademacher_2019,
	title = {Rademacher {Complexity} for {Adversarially} {Robust} {Generalization}},
	url = {https://proceedings.mlr.press/v97/yin19b.html},
	abstract = {Many machine learning models are vulnerable to adversarial attacks; for example, adding adversarial perturbations that are imperceptible to humans can often make machine learning models produce wrong predictions with high confidence; moreover, although we may obtain robust models on the training dataset via adversarial training, in some problems the learned models cannot generalize well to the test data. In this paper, we focus on ℓ∞ℓ∞{\textbackslash}ell\_{\textbackslash}infty attacks, and study the adversarially robust generalization problem through the lens of Rademacher complexity. For binary linear classifiers, we prove tight bounds for the adversarial Rademacher complexity, and show that the adversarial Rademacher complexity is never smaller than its natural counterpart, and it has an unavoidable dimension dependence, unless the weight vector has bounded ℓ1ℓ1{\textbackslash}ell\_1 norm, and our results also extend to multi-class linear classifiers; in addition, for (nonlinear) neural networks, we show that the dimension dependence in the adversarial Rademacher complexity also exists. We further consider a surrogate adversarial loss for one-hidden layer ReLU network and prove margin bounds for this setting. Our results indicate that having ℓ1ℓ1{\textbackslash}ell\_1 norm constraints on the weight matrices might be a potential way to improve generalization in the adversarial setting. We demonstrate experimental results that validate our theoretical findings.},
	urldate = {2022-04-12},
	booktitle = {Proceeding of the {International} {Conference} on {Machine} {Learning}},
	author = {Yin, Dong and Kannan, Ramchandran and Bartlett, Peter},
	year = {2019},
	pages = {7085--7094},
}

@article{saxe_exact_2014,
	title = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
	url = {http://arxiv.org/abs/1312.6120},
	abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
	urldate = {2020-08-27},
	journal = {International Conference on Learning Representations (ICLR)},
	author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
	month = feb,
	year = {2014},
	note = {arXiv: 1312.6120},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{soderstrom_errors--variables_2007,
	title = {Errors-in-variables methods in system identification},
	volume = {43},
	issn = {00051098},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0005109807000714},
	doi = {10.1016/j.automatica.2006.11.025},
	language = {en},
	number = {6},
	urldate = {2017-08-23},
	journal = {Automatica},
	author = {Söderström, Torsten},
	year = {2007},
	pages = {939--958},
}

@article{gilmer_adversarial_2018,
	title = {Adversarial {Spheres}},
	url = {http://arxiv.org/abs/1801.02774},
	abstract = {Machine learning models with very low test error have been shown to be consistently vulnerable to small, adversarially chosen perturbations of the input. We hypothesize that this counterintuitive behavior is a result of the high-dimensional geometry of the data manifold, and explore this hypothesis on a simple highdimensional dataset. For this dataset we show a fundamental bound relating the classiﬁcation error rate to the average distance to the nearest misclassiﬁcation, which is independent of the model. We train different neural network architectures on this dataset and show their error sets approach this theoretical bound. As a result of the theory, the vulnerability of machine learning models to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this foundational synthetic case will point a way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.},
	language = {en},
	urldate = {2020-03-20},
	journal = {arXiv:1801.02774},
	author = {Gilmer, Justin and Metz, Luke and Faghri, Fartash and Schoenholz, Samuel S. and Raghu, Maithra and Wattenberg, Martin and Goodfellow, Ian},
	month = sep,
	year = {2018},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, I.2.6},
}

@misc{shen_towards_2021,
	title = {Towards {Out}-{Of}-{Distribution} {Generalization}: {A} {Survey}},
	shorttitle = {Towards {Out}-{Of}-{Distribution} {Generalization}},
	url = {http://arxiv.org/abs/2108.13624},
	doi = {10.48550/arXiv.2108.13624},
	abstract = {Classic machine learning methods are built on the \$i.i.d.\$ assumption that training and testing data are independent and identically distributed. However, in real scenarios, the \$i.i.d.\$ assumption can hardly be satisfied, rendering the sharp drop of classic machine learning algorithms' performances under distributional shifts, which indicates the significance of investigating the Out-of-Distribution generalization problem. Out-of-Distribution (OOD) generalization problem addresses the challenging setting where the testing distribution is unknown and different from the training. This paper serves as the first effort to systematically and comprehensively discuss the OOD generalization problem, from the definition, methodology, evaluation to the implications and future directions. Firstly, we provide the formal definition of the OOD generalization problem. Secondly, existing methods are categorized into three parts based on their positions in the whole learning pipeline, namely unsupervised representation learning, supervised model learning and optimization, and typical methods for each category are discussed in detail. We then demonstrate the theoretical connections of different categories, and introduce the commonly used datasets and evaluation metrics. Finally, we summarize the whole literature and raise some future directions for OOD generalization problem. The summary of OOD generalization methods reviewed in this survey can be found at http://out-of-distribution-generalization.com.},
	urldate = {2023-04-04},
	publisher = {arXiv},
	author = {Shen, Zheyan and Liu, Jiashuo and He, Yue and Zhang, Xingxuan and Xu, Renzhe and Yu, Han and Cui, Peng},
	month = aug,
	year = {2021},
	note = {arXiv:2108.13624},
	keywords = {Computer Science - Machine Learning},
}

@book{shalev-shwartz_understanding_2014,
	title = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
	isbn = {978-1-107-29801-9},
	shorttitle = {Understanding {Machine} {Learning}},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781107298019},
	language = {en},
	urldate = {2020-05-06},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	year = {2014},
	doi = {10.1017/CBO9781107298019},
}

@article{madry_towards_2018,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
	journal = {International Conference for Learning Representations (ICLR)},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{yuan_adversarial_2019,
	title = {Adversarial examples: {Attacks} and defenses for deep learning},
	volume = {30},
	issn = {2162-237X},
	number = {9},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Yuan, Xiaoyong and He, Pan and Zhu, Qile and Li, Xiaolin},
	year = {2019},
	pages = {2805--2824},
}

@inproceedings{bruna_intriguing_2014,
	title = {Intriguing properties of neural networks},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Bruna, Joan and Szegedy, Christian and Sutskever, Ilya and Goodfellow, Ian and Zaremba, Wojciech and Fergus, Rob and Erhan, Dumitru},
	year = {2014},
}

@book{soderstrom_system_1988,
	title = {System identification},
	publisher = {Prentice-Hall, Inc.},
	author = {Söderström, Torsten and Stoica, Petre},
	year = {1988},
}

@inproceedings{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high conﬁdence. Early attempts at explaining this phenomenon focused on nonlinearity and overﬁtting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the ﬁrst explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	year = {2015},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{belkin_reconciling_2019,
	title = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	doi = {10.1073/pnas.1903070116},
	abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
	number = {32},
	urldate = {2020-08-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	year = {2019},
	pages = {15849--15854},
}

@book{zhou_essentials_1998,
	title = {Essentials of robust control},
	volume = {104},
	publisher = {Prentice Hall},
	author = {Zhou, Kemin and Doyle, John Comstock},
	year = {1998},
}

@article{pillonetto_kernel_2014,
	title = {Kernel methods in system identification, machine learning and function estimation: {A} survey},
	volume = {50},
	shorttitle = {Kernel methods in system identification, machine learning and function estimation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000510981400020X},
	doi = {10/f236r8},
	abstract = {Most of the currently used techniques for linear system identification are based on classical estimation paradigms coming from mathematical statistics. In particular, maximum likelihood and prediction error methods represent the mainstream approaches to identification of linear dynamic systems, with a long history of theoretical and algorithmic contributions. Parallel to this, in the machine learning community alternative techniques have been developed. Until recently, there has been little contact between these two worlds. The first aim of this survey is to make accessible to the control community the key mathematical tools and concepts as well as the computational aspects underpinning these learning techniques. In particular, we focus on kernel-based regularization and its connections with reproducing kernel Hilbert spaces and Bayesian estimation of Gaussian processes. The second aim is to demonstrate that learning techniques tailored to the specific features of dynamic systems may outperform conventional parametric approaches for identification of stable linear systems.},
	number = {3},
	urldate = {2019-04-10},
	journal = {Automatica},
	author = {Pillonetto, Gianluigi and Dinuzzo, Francesco and Chen, Tianshi and De Nicolao, Giuseppe and Ljung, Lennart},
	year = {2014},
	pages = {657--682},
}

@inproceedings{zhang_causal_2020,
	title = {A {Causal} {View} on {Robustness} of {Neural} {Networks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/02ed812220b0705fabb868ddbf17ea20-Abstract.html},
	abstract = {We present a causal view on the robustness of neural networks against input manipulations, which applies not only to traditional classification tasks but also to general measurement data. Based on this view, we design a deep causal manipulation augmented model (deep CAMA) which explicitly models possible manipulations on certain causes leading to changes in the observed effect.  We further develop data augmentation and test-time fine-tuning methods to improve deep CAMA's robustness. When compared with discriminative deep neural networks, our proposed model shows superior robustness against unseen manipulations. As a by-product, our model achieves disentangled representation which separates the representation of manipulations from those of other latent causes.},
	urldate = {2023-04-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Cheng and Zhang, Kun and Li, Yingzhen},
	year = {2020},
	pages = {289--301},
}

@misc{hua_causal_2022,
	title = {Causal {Information} {Bottleneck} {Boosts} {Adversarial} {Robustness} of {Deep} {Neural} {Network}},
	url = {http://arxiv.org/abs/2210.14229},
	doi = {10.48550/arXiv.2210.14229},
	abstract = {The information bottleneck (IB) method is a feasible defense solution against adversarial attacks in deep learning. However, this method suffers from the spurious correlation, which leads to the limitation of its further improvement of adversarial robustness. In this paper, we incorporate the causal inference into the IB framework to alleviate such a problem. Specifically, we divide the features obtained by the IB method into robust features (content information) and non-robust features (style information) via the instrumental variables to estimate the causal effects. With the utilization of such a framework, the influence of non-robust features could be mitigated to strengthen the adversarial robustness. We make an analysis of the effectiveness of our proposed method. The extensive experiments in MNIST, FashionMNIST, and CIFAR-10 show that our method exhibits the considerable robustness against multiple adversarial attacks. Our code would be released.},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Hua, Huan and Yan, Jun and Fang, Xi and Huang, Weiquan and Yin, Huilin and Ge, Wancheng},
	month = oct,
	year = {2022},
	note = {arXiv:2210.14229 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{noseworthy_artificial_2022,
	title = {Artificial intelligence-guided screening for atrial fibrillation using electrocardiogram during sinus rhythm: a prospective non-randomised interventional trial},
	volume = {400},
	issn = {0140-6736},
	url = {https://www.sciencedirect.com/science/article/pii/S0140673622016373},
	doi = {https://doi.org/10.1016/S0140-6736(22)01637-3},
	abstract = {Summary Background Previous atrial fibrillation screening trials have highlighted the need for more targeted approaches. We did a pragmatic study to evaluate the effectiveness of an artificial intelligence (AI) algorithm-guided targeted screening approach for identifying previously unrecognised atrial fibrillation. Methods For this non-randomised interventional trial, we prospectively recruited patients with stroke risk factors but with no known atrial fibrillation who had an electrocardiogram (ECG) done in routine practice. Participants wore a continuous ambulatory heart rhythm monitor for up to 30 days, with the data transmitted in near real time through a cellular connection. The AI algorithm was applied to the ECGs to divide patients into high-risk or low-risk groups. The primary outcome was newly diagnosed atrial fibrillation. In a secondary analysis, trial participants were propensity-score matched (1:1) to individuals from the eligible but unenrolled population who served as real-world controls. This study is registered with ClinicalTrials.gov, NCT04208971. Findings 1003 patients with a mean age of 74 years (SD 8·8) from 40 US states completed the study. Over a mean 22·3 days of continuous monitoring, atrial fibrillation was detected in six (1·6\%) of 370 patients with low risk and 48 (7·6\%) of 633 with high risk (odds ratio 4·98, 95\% CI 2·11–11·75, p=0·0002). Compared with usual care, AI-guided screening was associated with increased detection of atrial fibrillation (high-risk group: 3·6\% [95\% CI 2·3–5·4] with usual care vs 10·6\% [8·3–13·2] with AI-guided screening, p¡0·0001; low-risk group: 0·9\% vs 2·4\%, p=0·12) over a median follow-up of 9·9 months (IQR 7·1–11·0). Interpretation An AI-guided targeted screening approach that leverages existing clinical data increased the yield for atrial fibrillation detection and could improve the effectiveness of atrial fibrillation screening. Funding Mayo Clinic Robert D and Patricia E Kern Center for the Science of Health Care Delivery.},
	number = {10359},
	journal = {The Lancet},
	author = {Noseworthy, Peter A and Attia, Zachi I and Behnken, Emma M and Giblon, Rachel E and Bews, Katherine A and Liu, Sijia and Gosse, Tara A and Linn, Zachery D and Deng, Yihong and Yin, Jun and Gersh, Bernard J and Graff-Radford, Jonathan and Rabinstein, Alejandro A and Siontis, Konstantinos C and Friedman, Paul A and Yao, Xiaoxi},
	year = {2022},
	pages = {1206--1212},
}

@article{poorthuis_utility_2021,
	title = {Utility of risk prediction models to detect atrial fibrillation in screened participants},
	volume = {28},
	issn = {2047-4881},
	doi = {10.1093/eurjpc/zwaa082},
	abstract = {AIMS: Atrial fibrillation (AF) is associated with higher risk of stroke. While the prevalence of AF is low in the general population, risk prediction models might identify individuals for selective screening of AF. We aimed to systematically identify and compare the utility of established models to predict prevalent AF.
METHODS AND RESULTS: Systematic search of PubMed and EMBASE for risk prediction models for AF. We adapted established risk prediction models and assessed their predictive performance using data from 2.5M individuals who attended vascular screening clinics in the USA and the UK and in the subset of 1.2M individuals with CHA2DS2-VASc ≥2. We assessed discrimination using area under the receiver operating characteristic (AUROC) curves and agreement between observed and predicted cases using calibration plots. After screening 6959 studies, 14 risk prediction models were identified. In our cohort, 10 464 (0.41\%) participants had AF. For discrimination, six prediction model had AUROC curves of 0.70 or above in all individuals and those with CHA2DS2-VASc ≥2. In these models, calibration plots showed very good concordance between predicted and observed risks of AF. The two models with the highest observed prevalence in the highest decile of predicted risk, CHARGE-AF and MHS, showed an observed prevalence of AF of 1.6\% with a number needed to screen of 63. Selective screening of the 10\% highest risk identified 39\% of cases with AF.
CONCLUSION: Prediction models can reliably identify individuals at high risk of AF. The best performing models showed an almost fourfold higher prevalence of AF by selective screening of individuals in the highest decile of risk compared with systematic screening of all cases.
REGISTRATION: This systematic review was registered (PROSPERO CRD42019123847).},
	language = {eng},
	number = {6},
	journal = {European Journal of Preventive Cardiology},
	author = {Poorthuis, Michiel H. F. and Jones, Nicholas R. and Sherliker, Paul and Clack, Rachel and de Borst, Gert J. and Clarke, Robert and Lewington, Sarah and Halliday, Alison and Bulbulia, Richard},
	month = may,
	year = {2021},
	pmid = {33624100},
	pmcid = {PMC8651014},
	keywords = {Atrial Fibrillation, Atrial fibrillation, Cohort Studies, External validation, Humans, Predictive Value of Tests, ROC Curve, Risk Assessment, Risk Factors, Risk prediction models, Selective screening, Stroke},
	pages = {586--595},
}

@article{10.1001/jamacardio.2017.3180,
	title = {Incidence of previously undiagnosed atrial fibrillation using insertable cardiac monitors in a high-risk population: {The} {REVEAL} {AF} study},
	volume = {2},
	issn = {2380-6583},
	url = {https://doi.org/10.1001/jamacardio.2017.3180},
	doi = {10.1001/jamacardio.2017.3180},
	abstract = {In approximately 20 \% of atrial fibrillation (AF)–related ischemic strokes, stroke is the first clinical manifestation of AF. Strategies are needed to identify and therapeutically address previously undetected AF.To quantify the incidence of AF in patients at high risk for but without previously known AF using an insertable cardiac monitor.This prospective, single-arm, multicenter study was conducted from November 2012 to January 2017. Visits took place at 57 centers in the United States and Europe. Patients with a CHADS2 score of 3 or greater (or 2 with at least 1 additional risk factor) were enrolled. Approximately 90 \% had nonspecific symptoms potentially compatible with AF, such as fatigue, dyspnea, and/or palpitations.Patients underwent monitoring with an insertable cardiac monitor for 18 to 30 months.The primary end point was adjudicated AF lasting 6 or more minutes and was assessed at 18 months. Other analyses included detection rates at points from 30 days to 30 months and among CHADS2 score subgroups. Median time from insertion to detection and the percentage of patients subsequently prescribed oral anticoagulation therapy was also determined.A total of 446 patients were enrolled; 233 (52.2 \%) were male, and the mean (SD) age was 71.5 (9.9) years. A total of 385 patients (86.3 \%) received an insertable cardiac monitor, met the primary analysis cohort definition, and were observed for a mean (SD) period of 22.5 (7.7) months. The detection rate of AF lasting 6 or more minutes at 18 months was 29.3 \%. Detection rates at 30 days and 6, 12, 24, and 30 months were 6.2 \%, 20.4 \%, 27.1 \%, 33.6 \%, and 40.0 \%, respectively. At 18 months, AF incidence was similar among patients with CHADS2 scores of 2 (24.7 \%; 95 \% CI, 17.3-31.4), 3 (32.7 \%; 95 \% CI, 23.8-40.7), and 4 or greater (31.7 \%; 95 \% CI, 22.0-40.3) (P = .23). Median (interquartile) time from device insertion to first AF episode detection was 123 (41-330) days. Of patients meeting the primary end point, 13 (10.2 \%) had 1 or more episodes lasting 24 hours or longer, and oral anticoagulation therapy was prescribed for 72 patients (56.3 \%).The incidence of previously undiagnosed AF may be substantial in patients with risk factors for AF and stroke. Atrial fibrillation would have gone undetected in most patients had monitoring been limited to 30 days. Further trials regarding the value of detecting subclinical AF and of prophylactic therapies are warranted.clinicaltrials.gov Identifier: NCT01727297},
	number = {10},
	journal = {JAMA Cardiology},
	author = {Reiffel, James A. and Verma, Atul and Kowey, Peter R. and Halperin, Jonathan L. and Gersh, Bernard J. and Wachter, Rolf and Pouliot, Erika and Ziegler, Paul D. and Investigators, for the REVEAL AF},
	month = oct,
	year = {2017},
	note = {tex.eprint: https://jamanetwork.com/journals/jamacardiology/articlepdf/2650790/jamacardiology{\textbackslash}\_reiffel{\textbackslash}\_2017{\textbackslash}\_oi{\textbackslash}\_170047.pdf},
	pages = {1120--1127},
}

@article{castro_causality_2020,
	title = {Causality matters in medical imaging},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-17478-w},
	doi = {10.1038/s41467-020-17478-w},
	abstract = {Causal reasoning can shed new light on the major challenges in machine learning for medical imaging: scarcity of high-quality annotated data and mismatch between the development dataset and the target environment. A causal perspective on these issues allows decisions about data collection, annotation, preprocessing, and learning strategies to be made and scrutinized more transparently, while providing a detailed categorisation of potential biases and mitigation techniques. Along with worked clinical examples, we highlight the importance of establishing the causal relationship between images and their annotations, and offer step-by-step recommendations for future studies.},
	language = {en},
	number = {1},
	urldate = {2023-04-04},
	journal = {Nature Communications},
	author = {Castro, Daniel C. and Walker, Ian and Glocker, Ben},
	month = jul,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational models, Data processing, Machine learning, Medical research, Predictive medicine},
	pages = {3673},
}

@article{hendrycks_benchmarking_2019,
	title = {Benchmarking neural network robustness to common corruptions and perturbations},
	journal = {Proceedings of the International Conference on Learning Representations},
	author = {Hendrycks, Dan and Dietterich, Thomas},
	year = {2019},
}

@misc{drenkow_systematic_2022,
	title = {A {Systematic} {Review} of {Robustness} in {Deep} {Learning} for {Computer} {Vision}: {Mind} the gap?},
	shorttitle = {A {Systematic} {Review} of {Robustness} in {Deep} {Learning} for {Computer} {Vision}},
	url = {http://arxiv.org/abs/2112.00639},
	doi = {10.48550/arXiv.2112.00639},
	abstract = {Deep neural networks for computer vision are deployed in increasingly safety-critical and socially-impactful applications, motivating the need to close the gap in model performance under varied, naturally occurring imaging conditions. Robustness, ambiguously used in multiple contexts including adversarial machine learning, refers here to preserving model performance under naturally-induced image corruptions or alterations. We perform a systematic review to identify, analyze, and summarize current definitions and progress towards non-adversarial robustness in deep learning for computer vision. We find this area of research has received disproportionately less attention relative to adversarial machine learning, yet a significant robustness gap exists that manifests in performance degradation similar in magnitude to adversarial conditions. Toward developing a more transparent definition of robustness, we provide a conceptual framework based on a structural causal model of the data generating process and interpret non-adversarial robustness as pertaining to a model's behavior on corrupted images corresponding to low-probability samples from the unaltered data distribution. We identify key architecture-, data augmentation-, and optimization tactics for improving neural network robustness. This robustness perspective reveals that common practices in the literature correspond to causal concepts. We offer perspectives on how future research may mind this evident and significant non-adversarial robustness gap.},
	urldate = {2023-04-04},
	publisher = {arXiv},
	author = {Drenkow, Nathan and Sani, Numair and Shpitser, Ilya and Unberath, Mathias},
	month = nov,
	year = {2022},
	note = {arXiv:2112.00639 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{belloni_square-root_2011,
	title = {Square-{Root} {Lasso}: {Pivotal} {Recovery} of {Sparse} {Signals} via {Conic} {Programming}},
	volume = {98},
	issn = {0006-3444, 1464-3510},
	shorttitle = {Square-{Root} {Lasso}},
	url = {http://arxiv.org/abs/1009.5689},
	doi = {10.1093/biomet/asr043},
	abstract = {We propose a pivotal method for estimating high-dimensional sparse linear regression models, where the overall number of regressors \$p\$ is large, possibly much larger than \$n\$, but only \$s\$ regressors are significant. The method is a modification of the lasso, called the square-root lasso. The method is pivotal in that it neither relies on the knowledge of the standard deviation \${\textbackslash}sigma\$ or nor does it need to pre-estimate \${\textbackslash}sigma\$. Moreover, the method does not rely on normality or sub-Gaussianity of noise. It achieves near-oracle performance, attaining the convergence rate \${\textbackslash}sigma {\textbackslash}\{(s/n){\textbackslash}log p{\textbackslash}\}{\textasciicircum}\{1/2\}\$ in the prediction norm, and thus matching the performance of the lasso with known \${\textbackslash}sigma\$. These performance results are valid for both Gaussian and non-Gaussian errors, under some mild moment restrictions. We formulate the square-root lasso as a solution to a convex conic programming problem, which allows us to implement the estimator using efficient algorithmic methods, such as interior-point and first-order methods.},
	number = {4},
	urldate = {2022-10-10},
	journal = {Biometrika},
	author = {Belloni, Alexandre and Chernozhukov, Victor and Wang, Lie},
	month = dec,
	year = {2011},
	note = {arXiv:1009.5689 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology},
	pages = {791--806},
}

@article{kassam_robust_1985,
	title = {Robust techniques for signal processing: {A} survey},
	volume = {73},
	number = {3},
	journal = {Proceedings of the IEEE},
	author = {Kassam, Saleem A and Poor, H Vincent},
	year = {1985},
	note = {Publisher: IEEE},
	pages = {433--481},
}

@book{huber_robust_1981,
	address = {New York},
	series = {Wiley series in probability and mathematical statistics},
	title = {Robust statistics},
	isbn = {978-0-471-41805-4},
	language = {en},
	publisher = {Wiley},
	author = {Huber, Peter J.},
	year = {1981},
	keywords = {Robust statistics},
}

@book{bach_learning_2023,
	title = {Learning {Theory} from {First} {Principles}},
	language = {en},
	author = {Bach, Francis},
	year = {2023},
	keywords = {\_tablet\_modified},
}

@inproceedings{xing_adversarially_2021,
	title = {Adversarially robust estimate and risk analysis in linear regression},
	volume = {130},
	url = {https://proceedings.mlr.press/v130/xing21c.html},
	abstract = {Adversarial robust learning aims to design algorithms that are robust to small adversarial perturbations on input variables. Beyond the existing studies on the predictive performance to adversarial samples, our goal is to understand statistical properties of adversarial robust estimates and analyze adversarial risk in the setup of linear regression models. By discovering the statistical minimax rate of convergence of adversarial robust estimators, we emphasize the importance of incorporating model information, e.g., sparsity, in adversarial robust learning. Further, we reveal an explicit connection of adversarial and standard estimates, and propose a straightforward two-stage adversarial training framework, which facilitates to utilize model structure information to improve adversarial robustness. In theory, the consistency of the adversarial robust estimator is proven and its Bahadur representation is also developed for the statistical inference purpose. The proposed estimator converges in a sharp rate under either low-dimensional or sparse scenario. Moreover, our theory confirms two phenomena in adversarial robust learning: adversarial robustness hurts generalization, and unlabeled data help improve the generalization. In the end, we conduct numerical simulations to verify our theory.},
	booktitle = {Proceedings of the {International} {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {Xing, Yue and Zhang, Ruizhi and Cheng, Guang},
	editor = {Banerjee, Arindam and Fukumizu, Kenji},
	month = apr,
	year = {2021},
	note = {tex.pdf: http://proceedings.mlr.press/v130/xing21c/xing21c.pdf},
	pages = {514--522},
}

@article{xu_robust_2008,
	title = {Robust regression and lasso},
	volume = {21},
	journal = {Advances in Neural Information Processing Systems},
	author = {Xu, Huan and Caramanis, Constantine and Mannor, Shie},
	year = {2008},
}

@inproceedings{xing_generalization_2021,
	title = {On the {Generalization} {Properties} of {Adversarial} {Training}},
	url = {https://proceedings.mlr.press/v130/xing21b.html},
	abstract = {Modern machine learning and deep learning models are shown to be vulnerable when testing data are slightly perturbed. Theoretical studies of adversarial training algorithms mostly focus on their adversarial training losses or local convergence properties. In contrast, this paper studies the generalization performance of a generic adversarial training algorithm. Specifically, we consider linear regression models and two-layer neural networks (with lazy training) using squared loss under low-dimensional regime and high-dimensional regime. In the former regime, after overcoming the non-smoothness of adversarial training, the adversarial risk of the trained models will converge to the minimal adversarial risk. In the latter regime, we discover that data interpolation prevents the adversarial robust estimator from being consistent (i.e. converge in probability). Therefore, inspired by successes of the least absolute shrinkage and selection operator (LASSO), we incorporate the 1L1{\textbackslash}mathcal\{L\}\_1 penalty in the high dimensional adversarial learning, and show that it leads to consistent adversarial robust estimation. A series of numerical studies are conducted to demonstrate that how the smoothness and 1L1{\textbackslash}mathcal\{L\}\_1 penalization help to improve the adversarial robustness of DNN models.},
	language = {en},
	urldate = {2022-08-15},
	booktitle = {Proceedings of the {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Xing, Yue and Song, Qifan and Cheng, Guang},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {505--513},
}

@book{clarke_optimization_1990,
	title = {Optimization and {Nonsmooth} {Analysis}},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611971309},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Clarke, Frank H.},
	year = {1990},
	doi = {10.1137/1.9781611971309},
	note = {tex.eprint: https://epubs.siam.org/doi/pdf/10.1137/1.9781611971309},
}

@book{bertsekas_convex_2003,
	title = {Convex {Analysis} and {Optimization}},
	author = {Bertsekas, Dimitri P and Nedi, Angelia and Ozdaglar, Asuman E},
	year = {2003},
}

@article{wang_tight_2022,
	title = {Tight bounds for minimum {L1}-norm interpolation of noisy data},
	url = {http://arxiv.org/abs/2111.05987},
	abstract = {We provide matching upper and lower bounds of order \${\textbackslash}sigma{\textasciicircum}2/{\textbackslash}log(d/n)\$ for the prediction error of the minimum \${\textbackslash}ell\_1\$-norm interpolator, a.k.a. basis pursuit. Our result is tight up to negligible terms when \$d {\textbackslash}gg n\$, and is the first to imply asymptotic consistency of noisy minimum-norm interpolation for isotropic features and sparse ground truths. Our work complements the literature on "benign overfitting" for minimum \${\textbackslash}ell\_2\$-norm interpolation, where asymptotic consistency can be achieved only when the features are effectively low-dimensional.},
	urldate = {2022-05-06},
	journal = {arXiv:2111.05987},
	author = {Wang, Guillaume and Donhauser, Konstantin and Yang, Fanny},
	month = mar,
	year = {2022},
	note = {arXiv: 2111.05987},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{min_curious_2021,
	title = {The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data} {Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}},
	volume = {161},
	shorttitle = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	url = {http://arxiv.org/abs/2002.11080},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classification problems. We first investigate the Gaussian mixture classification with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classification problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classification, support vector machines (SVMs), and linear regression.},
	journal = {Proceedings of the Conference on Uncertainty in Artificial Intelligence},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {129--139},
}

@inproceedings{javanmard_precise_2020,
	title = {Precise tradeoffs in adversarial training for linear regression},
	volume = {125},
	url = {http://proceedings.mlr.press/v125/javanmard20a.html},
	abstract = {Despite breakthrough performance, modern learning models are known to be highly vulnerable to small adversarial perturbations in their inputs. While a wide variety of recent \textit{adversarial training} methods have been effective at improving robustness to perturbed inputs (robust accuracy), often this benefit is accompanied by a decrease in accuracy on benign inputs (standard accuracy), leading to a tradeoff between often competing objectives. Complicating matters further, recent empirical evidence suggest that a variety of other factors (size and quality of training data, model size, etc.) affect this tradeoff in somewhat surprising ways. In this paper we provide a precise and comprehensive understanding of the role of adversarial training in the context of linear regression with Gaussian features. In particular, we characterize the fundamental tradeoff between the accuracies achievable by any algorithm regardless of computational power or size of the training data. Furthermore, we precisely characterize the standard/robust accuracy and the corresponding tradeoff achieved by a contemporary mini-max adversarial training approach in a high-dimensional regime where the number of data points and the parameters of the model grow in proportion to each other. Our theory for adversarial training algorithms also facilitates the rigorous study of how a variety of factors (size and quality of training data, model overparametrization etc.) affect the tradeoff between these two competing accuracies.},
	booktitle = {Proceedings of the {Conference} on {Learning} {Theory}},
	author = {Javanmard, Adel and Soltanolkotabi, Mahdi and Hassani, Hamed},
	month = jul,
	year = {2020},
	pages = {2034--2078},
}

@article{rackauckas_comparison_2018,
	title = {A {Comparison} of {Automatic} {Differentiation} and {Continuous} {Sensitivity} {Analysis} for {Derivatives} of {Differential} {Equation} {Solutions}},
	url = {http://arxiv.org/abs/1812.01892},
	abstract = {The derivatives of differential equation solutions are commonly used as model diagnostics and as part of parameter estimation routines. In this manuscript we investigate an implementation of Discrete local Sensitivity Analysis via Automatic Differentiation (DSAAD). A non-stiff Lotka-Volterra model, a discretization of the two dimensional (\$N {\textbackslash}times N\$) Brusselator stiff reaction-diffusion PDE, a stiff non-linear air pollution and a non-stiff pharmacokinetic/pharmacodynamic (PK/PD) model were used as prototype models for this investigation. Our benchmarks show that on sufficiently small ({\textless}100 parameters) stiff and non-stiff systems of ODEs, forward-mode DSAAD is more efficient than both reverse-mode DSAAD and continuous forward/adjoint sensitivity analysis. The scalability of continuous adjoint methods is shown to result in better efficiency for larger ODE systems such as PDE discretizations. In addition to testing efficiency, results on test equations demonstrate the applicability of DSAAD to differential-algebraic equations, delay differential equations, and hybrid differential equation systems where the event timing and effects are dependent on model parameters. Together, these results show that language-level automatic differentiation is an efficient method for calculating local sensitivities of a wide range of differential equation models.},
	urldate = {2018-12-14},
	journal = {arXiv:1812.01892 [cs]},
	author = {Rackauckas, Christopher and Ma, Yingbo and Dixit, Vaibhav and Guo, Xingjian and Innes, Mike and Revels, Jarrett and Nyberg, Joakim and Ivaturi, Vijay},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.01892},
	keywords = {Computer Science - Numerical Analysis},
}

@article{byrd_limited_1995,
	title = {A limited memory algorithm for bound constrained optimization},
	volume = {16},
	issn = {1064-8275},
	doi = {10.1137/0916069},
	number = {5},
	journal = {SIAM Journal on Scientific Computing},
	author = {Byrd, Richard H and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
	year = {1995},
	pages = {1190--1208},
}

@article{ribeiro_overparameterized_2023,
	title = {Overparameterized {Linear} {Regression} under {Adversarial} {Attacks}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2204.06274},
	doi = {10.1109/TSP.2023.3246228},
	abstract = {As machine learning models start to be used in critical applications, their vulnerabilities and brittleness become a pressing concern. Adversarial attacks are a popular framework for studying these vulnerabilities. In this work, we study the error of linear regression in the face of adversarial attacks. We provide bounds of the error in terms of the traditional risk and the parameter norm and show how these bounds can be leveraged and make it possible to use analysis from non-adversarial setups to study the adversarial risk. The usefulness of these results is illustrated by shedding light on whether or not overparameterized linear models can be adversarially robust. We show that adding features to linear models might be either a source of additional robustness or brittleness. We show that these differences appear due to scaling and how the \${\textbackslash}ell\_1\$ and \${\textbackslash}ell\_2\$ norms of random projections concentrate. We also show how the reformulation we propose allows for solving adversarial training as a convex optimization problem. This is then used as a tool to study how adversarial training and other regularization methods might affect the robustness of the estimated models.},
	urldate = {2022-04-22},
	journal = {IEEE Transactions on Signal Processing},
	author = {Ribeiro, Antônio H. and Schön, Thomas B.},
	year = {2023},
	note = {arXiv: 2204.06274},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@incollection{rahimi_random_2008,
	title = {Random {Features} for {Large}-{Scale} {Kernel} {Machines}},
	urldate = {2020-08-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 20},
	author = {Rahimi, Ali and Recht, Benjamin},
	year = {2008},
	pages = {1177--1184},
}

@article{taheri_asymptotic_2022,
	title = {Asymptotic {Behavior} of {Adversarial} {Training} in {Binary} {Classification}},
	volume = {127-132},
	url = {http://arxiv.org/abs/2010.13275},
	doi = {10.1109/ISIT50566.2022.9834717},
	abstract = {It has been consistently reported that many machine learning models are susceptible to adversarial attacks i.e., small additive adversarial perturbations applied to data points can cause misclassification. Adversarial training using empirical risk minimization is considered to be the state-of-the-art method for defense against adversarial attacks. Despite being successful in practice, several problems in understanding generalization performance of adversarial training remain open. In this paper, we derive precise theoretical predictions for the performance of adversarial training in binary classification. We consider the high-dimensional regime where the dimension of data grows with the size of the training data-set at a constant ratio. Our results provide exact asymptotics for standard and adversarial errors of the estimators obtained by adversarial training with \${\textbackslash}ell\_q\$-norm bounded perturbations (\$q {\textbackslash}ge 1\$) for both discriminative binary models and generative Gaussian mixture models. Furthermore, we use these sharp predictions to uncover several intriguing observations on the role of various parameters including the over-parameterization ratio, the data model, and the attack budget on the adversarial and standard errors.},
	urldate = {2021-05-16},
	journal = {IEEE International Symposium on Information Theory (ISIT)},
	author = {Taheri, Hossein and Pedarsani, Ramtin and Thrampoulidis, Christos},
	year = {2022},
	note = {arXiv: 2010.13275},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
}

@article{hastie_surprises_2022,
	title = {Surprises in high-dimensional ridgeless least squares interpolation},
	volume = {50},
	url = {https://doi.org/10.1214/21-AOS2133},
	doi = {10.1214/21-AOS2133},
	number = {2},
	journal = {The Annals of Statistics},
	author = {Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J.},
	year = {2022},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Random matrix theory, Ridge regression, interpolation, overparametrization, regression},
	pages = {949 -- 986},
}

@misc{scetbon_robust_2023,
	title = {Robust {Linear} {Regression}: {Gradient}-descent, {Early}-stopping, and {Beyond}},
	shorttitle = {Robust {Linear} {Regression}},
	url = {http://arxiv.org/abs/2301.13486},
	doi = {10.48550/arXiv.2301.13486},
	abstract = {In this work we study the robustness to adversarial attacks, of early-stopping strategies on gradient-descent (GD) methods for linear regression. More precisely, we show that early-stopped GD is optimally robust (up to an absolute constant) against Euclidean-norm adversarial attacks. However, we show that this strategy can be arbitrarily sub-optimal in the case of general Mahalanobis attacks. This observation is compatible with recent findings in the case of classification{\textasciitilde}{\textbackslash}cite\{Vardi2022GradientMP\} that show that GD provably converges to non-robust models. To alleviate this issue, we propose to apply instead a GD scheme on a transformation of the data adapted to the attack. This data transformation amounts to apply feature-depending learning rates and we show that this modified GD is able to handle any Mahalanobis attack, as well as more general attacks under some conditions. Unfortunately, choosing such adapted transformations can be hard for general attacks. To the rescue, we design a simple and tractable estimator whose adversarial risk is optimal up to within a multiplicative constant of 1.1124 in the population regime, and works for any norm.},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Scetbon, Meyer and Dohmatob, Elvis},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13486 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ribeiro_surprises_2022,
	title = {Surprises in adversarially-trained linear regression},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2205.12695},
	doi = {10.48550/arXiv.2205.12695},
	abstract = {State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is one of the most effective approaches to defend against such examples. We show that for linear regression problems, adversarial training can be formulated as a convex problem. This fact is then used to show that \${\textbackslash}ell\_{\textbackslash}infty\$-adversarial training produces sparse solutions and has many similarities to the lasso method. Similarly, \${\textbackslash}ell\_2\$-adversarial training has similarities with ridge regression. We use a robust regression framework to analyze and understand these similarities and also point to some differences. Finally, we show how adversarial training behaves differently from other regularization methods when estimating overparameterized models (i.e., models with more parameters than datapoints). It minimizes a sum of three terms which regularizes the solution, but unlike lasso and ridge regression, it can sharply transition into an interpolation mode. We show that for sufficiently many features or sufficiently small regularization parameters, the learned model perfectly interpolates the training data while still exhibiting good out-of-sample performance.},
	urldate = {2022-05-26},
	journal = {arXiv:2205.12695},
	author = {Ribeiro, Antônio H. and Zachariah, Dave and Schön, Thomas B.},
	month = may,
	year = {2022},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{stensrud_why_2020,
	title = {Why {Test} for {Proportional} {Hazards}?},
	volume = {323},
	issn = {1538-3598},
	doi = {10.1001/jama.2020.1267},
	language = {eng},
	number = {14},
	journal = {JAMA},
	author = {Stensrud, Mats J. and Hernán, Miguel A.},
	month = apr,
	year = {2020},
	pmid = {32167523},
	keywords = {Humans, Methods, Proportional Hazards Models, Research Design, Survival Analysis},
	pages = {1401--1402},
}

@article{lu_knowledge_2022,
	title = {Knowledge {Discovery} with {Electrocardiography} {Using} {Interpretable} {Deep} {Neural} {Networks}},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2022.11.01.22281722v1},
	doi = {10.1101/2022.11.01.22281722},
	abstract = {Despite the potentials of artificial intelligence (AI) in healthcare, very little work focuses on the extraction of clinical information or knowledge discovery from clinical measurements. Here we propose a novel deep learning model to extract characteristics in electrocardiogram (ECG) and explore its usage in knowledge discovery. Utilising a 12-lead ECG dataset (nECGs = 2,322,513) collected from unique subjects (nSubjects = 1,558,772) in primary care, we performed three independent medical tasks with the proposed model: (i) cardiac abnormality diagnosis, (ii) gender identification, and (iii) hypertension screening. We achieved an area under the curve (AUC) score of 0.998 (95\% confidence interval (CI), 0.995-0.999), 0.964 (95\% CI, 0.963-0.965), and 0.839 (95\% CI, 0.837-0.841) for each task, respectively; We provide interpretation of salient morphologies and further identified key ECG leads that achieve similar performance for the three tasks: (i) AVR and V1 leads (AUC=0.990 (95\% CI, 0.982-0.995); (ii) V5 lead (AUC=0.900 (95\% CI, 0.899-0.902)); and (iii) V1 lead (AUC=0.816 (95\% CI, 0.814-0.818)). Using ECGs, our model not only has demonstrated cardiologist-level accuracy in heart diagnosis with interpretability, but also shows its potentials in facilitating clinical knowledge discovery for gender and hypertension detection which are not readily available.},
	language = {en},
	urldate = {2022-11-17},
	journal = {Under review at Nature Comunications (preprint: medRxiv)},
	author = {Lu, Lei and Zhu, Tingting and Ribeiro, Antônio H. and Clifton, Lei and Zhao, Erying and Ribeiro, Antonio Luiz P. and Zhang, Yuan-Ting and Clifton, David A.},
	month = nov,
	year = {2022},
}

@article{zvuloni_merging_2023,
	title = {On {Merging} {Feature} {Engineering} and {Deep} {Learning} for {Diagnosis}, {Risk}-{Prediction} and {Age} {Estimation} {Based} on the 12-{Lead} {ECG}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2207.06096},
	doi = {10.1109/TBME.2023.3239527},
	abstract = {Objective: Over the past few years, deep learning (DL) has been used extensively in research for 12-lead electrocardiogram (ECG) analysis. However, it is unclear whether the explicit or implicit claims made on DL superiority to the more classical feature engineering (FE) approaches, based on domain knowledge, hold. In addition, it remains unclear whether combining DL with FE may improve performance over a single modality. Methods: To address these research gaps and in-line with recent major experiments, we revisited three tasks: cardiac arrhythmia diagnosis (multiclass-multilabel classification), atrial fibrillation risk prediction (binary classification), and age estimation (regression). We used an overall dataset of 2.3M 12-lead ECG recordings to train the following models for each task: i) a random forest taking FE as input; ii) an end-to-end DL model; and iii) a merged model of FE+DL. Results: FE yielded comparable results to DL while necessitating significantly less data for the two classification tasks. DL outperformed FE for the regression task. For all tasks, merging FE with DL did not improve performance over DL alone. These findings were confirmed on the additional PTB-XL dataset. Conclusion: We found that for traditional 12-lead ECG based diagnosis tasks, DL did not yield a meaningful improvement over FE, while it improved significantly the nontraditional regression task. We also found that combining FE with DL did not improve over DL alone, which suggests that the FE were redundant with the features learned by DL. Significance: Our findings provides important recommendations on 12-lead ECG based machine learning strategy and data regime to choose for a given task. When looking at maximizing performance as the end goal, if the task is nontraditional and a large dataset is available then DL is preferable. If the task is a classical one and/or a small dataset is available then a FE approach may be the better choice.},
	urldate = {2022-07-22},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Zvuloni, Eran and Read, Jesse and Ribeiro, Antônio H. and Ribeiro, Antonio Luiz P. and Behar, Joachim A.},
	month = jan,
	year = {2023},
	note = {arXiv:2207.06096 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, I.2.0, J.3},
}

@article{rajpurkar_ai_2022,
	title = {{AI} in health and medicine},
	volume = {28},
	copyright = {2022 Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-021-01614-0},
	doi = {10.1038/s41591-021-01614-0},
	abstract = {Artificial intelligence (AI) is poised to broadly reshape medicine, potentially improving the experiences of both clinicians and patients. We discuss key findings from a 2-year weekly effort to track and share key developments in medical AI. We cover prospective studies and advances in medical image analysis, which have reduced the gap between research and deployment. We also address several promising avenues for novel medical AI research, including non-image data sources, unconventional problem formulations and human–AI collaboration. Finally, we consider serious technical and ethical challenges in issues spanning from data scarcity to racial bias. As these challenges are addressed, AI’s potential may be realized, making healthcare more accurate, efficient and accessible for patients worldwide.},
	language = {en},
	urldate = {2022-01-25},
	journal = {Nature Medicine},
	author = {Rajpurkar, Pranav and Chen, Emma and Banerjee, Oishi and Topol, Eric J.},
	month = jan,
	year = {2022},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Computational biology and bioinformatics;Medical research
Subject\_term\_id: computational-biology-and-bioinformatics;medical-research},
	keywords = {Computational biology and bioinformatics, Medical research},
	pages = {31--38},
}

@article{johnstone_pca_2018,
	title = {{PCA} in {High} {Dimensions}: {An} {Orientation}},
	volume = {106},
	issn = {1558-2256},
	shorttitle = {{PCA} in {High} {Dimensions}},
	doi = {10.1109/JPROC.2018.2846730},
	abstract = {When the data are high dimensional, widely used multivariate statistical methods such as principal component analysis can behave in unexpected ways. In settings where the dimension of the observations is comparable to the sample size, upward bias in sample eigenvalues and inconsistency of sample eigenvectors are among the most notable phenomena that appear. These phenomena, and the limiting behavior of the rescaled extreme sample eigenvalues, have recently been investigated in detail under the spiked covariance model. The behavior of the bulk of the sample eigenvalues under weak distributional assumptions on the observations has been described. These results have been exploited to develop new estimation and hypothesis testing methods for the population covariance matrix. Furthermore, partly in response to these phenomena, alternative classes of estimation procedures have been developed by exploiting sparsity of the eigenvectors or the covariance matrix. This paper gives an orientation to these areas.},
	number = {8},
	journal = {Proceedings of the IEEE},
	author = {Johnstone, Iain M. and Paul, Debashis},
	month = aug,
	year = {2018},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Covariance matrices, Eigenvalues and eigenfunctions, Estimation, Marcenko–Pastur distribution, Matrix decomposition, Principal component analysis, Statistical analysis, Tracy–Widom law, phase transition phenomena, principal component analysis (PCA), random matrix theory, spiked covariance model},
	pages = {1277--1292},
}

@article{candes_modern_2006,
	title = {Modern statistical estimation via oracle inequalities},
	volume = {15},
	issn = {0962-4929, 1474-0508},
	url = {https://www.cambridge.org/core/product/identifier/S0962492906230010/type/journal_article},
	doi = {10.1017/S0962492906230010},
	abstract = {A number of fundamental results in modern statistical theory involve thresholding estimators. This survey paper aims at reconstructing the history of how thresholding rules came to be popular in statistics and describing, in a not overly technical way, the domain of their application. Two notions play a fundamental role in our narrative: sparsity and oracle inequalities. Sparsity is a property of the object to estimate, which seems to be characteristic of many modern problems, in statistics as well as applied mathematics and theoretical computer science, to name a few. ‘Oracle inequalities’ are a powerful decision-theoretic tool which has served to understand the optimality of thresholding rules, but which has many other potential applications, some of which we will discuss.
            Our story is also the story of the dialogue between statistics and applied harmonic analysis. Starting with the work of Wiener, we will see that certain representations emerge as being optimal for estimation. A leitmotif throughout our exposition is that efficient representations lead to efficient estimation.},
	language = {en},
	urldate = {2022-12-28},
	journal = {Acta Numerica},
	author = {Candès, Emmanuel J.},
	month = may,
	year = {2006},
	pages = {257--325},
}

@article{weston_learning_2003,
	title = {Learning to {Find} {Pre}-{Images}},
	abstract = {We consider the problem of reconstructing patterns from a feature map. Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS. We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images. The introduced technique avoids difﬁcult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces.},
	language = {en},
	journal = {Advances in Neural Information Processing Systems 16 (NIPS)},
	author = {Weston, Jason and Schölkopf, Bernhard and Bakir, Gökhan H},
	year = {2003},
}

@article{scholkopf_kernel_1997,
	title = {Kernel {Principal} {Component} {Analysis}},
	doi = {https://doi.org/10.1007/BFb0020217},
	abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can e ciently compute principal components in high dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
	journal = {Artificial Neural Networks — ICANN},
	author = {Scholkopf, Bernhard and Smola, Alexander and Muller, Klaus Robert},
	year = {1997},
	pages = {583--588},
}

@inproceedings{10.5555/3524938.3525611,
	series = {{ICML}'20},
	title = {In defense of uniform convergence: {Generalization} via derandomization with an application to interpolating predictors},
	abstract = {We propose to study the generalization error of a learned predictor undefined in terms of that of a surrogate (potentially randomized) predictor that is coupled to undefined and designed to trade empirical risk for control of generalization error. In the case where undefined interpolates the data, it is interesting to consider theoretical surrogate predictors that are partially derandomized or rerandomized, e.g., fit to the training data but with modified label noise. We also show that replacing undefined by its conditional distribution with respect to an arbitrary σ-field is a convenient way to derandomize. We study two examples, inspired by the work of Nagarajan and Kolter (2019) and Bartlett et al. (2020), where the learned predictor undefined interpolates the training data with high probability, has small risk, and, yet, does not belong to a nonrandom class with a tight uniform bound on two-sided generalization error. At the same time, we bound the risk of undefined in terms of surrogates constructed by conditioning and denoising, respectively, and shown to belong to nonrandom classes with uniformly small generalization error.},
	booktitle = {Proceedings of the 37th international conference on machine learning},
	publisher = {JMLR.org},
	author = {Negrea, Jeffrey and Dziugaite, Gintare Karolina and Roy, Daniel M.},
	year = {2020},
	note = {Number of pages: 10
tex.articleno: 673},
}

@misc{ardeshir_support_2021,
	title = {Support vector machines and linear regression coincide with very high-dimensional features},
	url = {http://arxiv.org/abs/2105.14084},
	doi = {10.48550/arXiv.2105.14084},
	abstract = {The support vector machine (SVM) and minimum Euclidean norm least squares regression are two fundamentally different approaches to fitting linear models, but they have recently been connected in models for very high-dimensional data through a phenomenon of support vector proliferation, where every training example used to fit an SVM becomes a support vector. In this paper, we explore the generality of this phenomenon and make the following contributions. First, we prove a super-linear lower bound on the dimension (in terms of sample size) required for support vector proliferation in independent feature models, matching the upper bounds from previous works. We further identify a sharp phase transition in Gaussian feature models, bound the width of this transition, and give experimental support for its universality. Finally, we hypothesize that this phase transition occurs only in much higher-dimensional settings in the \${\textbackslash}ell\_1\$ variant of the SVM, and we present a new geometric characterization of the problem that may elucidate this phenomenon for the general \${\textbackslash}ell\_p\$ case.},
	urldate = {2022-12-12},
	publisher = {arXiv},
	author = {Ardeshir, Navid and Sanford, Clayton and Hsu, Daniel},
	month = oct,
	year = {2021},
	note = {arXiv:2105.14084 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{zhou_uniform_2021,
	title = {On {Uniform} {Convergence} and {Low}-{Norm} {Interpolation} {Learning}},
	url = {http://arxiv.org/abs/2006.05942},
	abstract = {We consider an underdetermined noisy linear regression model where the minimum-norm interpolating predictor is known to be consistent, and ask: can uniform convergence in a norm ball, or at least (following Nagarajan and Kolter) the subset of a norm ball that the algorithm selects on a typical input set, explain this success? We show that uniformly bounding the difference between empirical and population errors cannot show any learning in the norm ball, and cannot show consistency for any set, even one depending on the exact algorithm and distribution. But we argue we can explain the consistency of the minimal-norm interpolator with a slightly weaker, yet standard, notion: uniform convergence of zero-error predictors in a norm ball. We use this to bound the generalization error of low- (but not minimal-) norm interpolating predictors.},
	language = {en},
	urldate = {2022-12-06},
	publisher = {arXiv},
	author = {Zhou, Lijia and Sutherland, Danica J. and Srebro, Nathan},
	month = jan,
	year = {2021},
	note = {arXiv:2006.05942 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_medarbetarportalen_nodate,
	title = {Medarbetarportalen - {Uppsala} {University}},
	url = {https://mp.uu.se/},
	urldate = {2022-11-25},
}

@misc{liu_double_2022,
	title = {On the {Double} {Descent} of {Random} {Features} {Models} {Trained} with {SGD}},
	url = {http://arxiv.org/abs/2110.06910},
	doi = {10.48550/arXiv.2110.06910},
	abstract = {We study generalization properties of random features (RF) regression in high dimensions optimized by stochastic gradient descent (SGD) in under-/over-parameterized regime. In this work, we derive precise non-asymptotic error bounds of RF regression under both constant and polynomial-decay step-size SGD setting, and observe the double descent phenomenon both theoretically and empirically. Our analysis shows how to cope with multiple randomness sources of initialization, label noise, and data sampling (as well as stochastic gradients) with no closed-form solution, and also goes beyond the commonly-used Gaussian/spherical data assumption. Our theoretical results demonstrate that, with SGD training, RF regression still generalizes well for interpolation learning, and is able to characterize the double descent behavior by the unimodality of variance and monotonic decrease of bias. Besides, we also prove that the constant step-size SGD setting incurs no loss in convergence rate when compared to the exact minimum-norm interpolator, as a theoretical justification of using SGD in practice.},
	urldate = {2022-11-25},
	publisher = {arXiv},
	author = {Liu, Fanghui and Suykens, Johan A. K. and Cevher, Volkan},
	month = oct,
	year = {2022},
	note = {arXiv:2110.06910 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{muthukumar_classification_2021,
	title = {Classification vs regression in overparameterized regimes: {Does} the loss function matter?},
	shorttitle = {Classification vs regression in overparameterized regimes},
	url = {http://arxiv.org/abs/2005.08054},
	doi = {10.48550/arXiv.2005.08054},
	abstract = {We compare classification and regression tasks in an overparameterized linear model with Gaussian features. On the one hand, we show that with sufficient overparameterization all training points are support vectors: solutions obtained by least-squares minimum-norm interpolation, typically used for regression, are identical to those produced by the hard-margin support vector machine (SVM) that minimizes the hinge loss, typically used for training classifiers. On the other hand, we show that there exist regimes where these interpolating solutions generalize well when evaluated by the 0-1 test loss function, but do not generalize if evaluated by the square loss function, i.e. they approach the null risk. Our results demonstrate the very different roles and properties of loss functions used at the training phase (optimization) and the testing phase (generalization).},
	urldate = {2022-11-23},
	publisher = {arXiv},
	author = {Muthukumar, Vidya and Narang, Adhyyan and Subramanian, Vignesh and Belkin, Mikhail and Hsu, Daniel and Sahai, Anant},
	month = oct,
	year = {2021},
	note = {arXiv:2005.08054 [cs, math, stat]},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{derezinski_improved_2020,
	title = {Improved guarantees and a multiple-descent curve for {Column} {Subset} {Selection} and the {Nystrom} method},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/342c472b95d00421be10e9512b532866-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Derezinski, Michal and Khanna, Rajiv and Mahoney, Michael W},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {4953--4964},
}

@inproceedings{d_ascoli_triple_2020,
	title = {Triple descent and the two kinds of overfitting: where \&amp; why do they appear?},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {d' Ascoli, Stéphane and Sagun, Levent and Biroli, Giulio},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {3058--3069},
}

@inproceedings{chen_multiple_2021,
	title = {Multiple {Descent}: {Design} {Your} {Own} {Generalization} {Curve}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/file/4ae67a7dd7e491f8fb6f9ea0cf25dfdb-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Lin and Min, Yifei and Belkin, Mikhail and Karbasi, Amin},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {8898--8912},
}

@article{gustafsson_development_2022,
	title = {Development and validation of deep learning {ECG}-based prediction of myocardial infarction in emergency department patients},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-24254-x},
	doi = {10.1038/s41598-022-24254-x},
	abstract = {Myocardial infarction diagnosis is a common challenge in the emergency department. In managed settings, deep learning-based models and especially convolutional deep models have shown promise in electrocardiogram (ECG) classification, but there is a lack of high-performing models for the diagnosis of myocardial infarction in real-world scenarios. We aimed to train and validate a deep learning model using ECGs to predict myocardial infarction in real-world emergency department patients. We studied emergency department patients in the Stockholm region between 2007 and 2016 that had an ECG obtained because of their presenting complaint. We developed a deep neural network based on convolutional layers similar to a residual network. Inputs to the model were ECG tracing, age, and sex; and outputs were the probabilities of three mutually exclusive classes: non-ST-elevation myocardial infarction (NSTEMI), ST-elevation myocardial infarction (STEMI), and control status, as registered in the SWEDEHEART and other registries. We used an ensemble of five models. Among 492,226 ECGs in 214,250 patients, 5,416 were recorded with an NSTEMI, 1,818 a STEMI, and 485,207 without a myocardial infarction. In a random test set, our model could discriminate STEMIs/NSTEMIs from controls with a C-statistic of 0.991/0.832 and had a Brier score of 0.001/0.008. The model obtained a similar performance in a temporally separated test set of the study sample, and achieved a C-statistic of 0.985 and a Brier score of 0.002 in discriminating STEMIs from controls in an external test set. We developed and validated a deep learning model with excellent performance in discriminating between control, STEMI, and NSTEMI on the presenting ECG of a real-world sample of the important population of all-comers to the emergency department. Hence, deep learning models for ECG decision support could be valuable in the emergency department.},
	number = {1},
	urldate = {2022-11-17},
	journal = {Scientific Reports},
	author = {Gustafsson, Stefan and Gedon, Daniel and Lampa, Erik and Ribeiro, Antônio H. and Holzmann, Martin J. and Schön, Thomas B. and Sundström, Johan},
	month = nov,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Cardiology, Cardiovascular diseases, Diseases, Health care, Machine learning, Medical research},
	pages = {19615},
}

@inproceedings{hsu_proliferation_2021,
	title = {On the proliferation of support vectors in high dimensions},
	url = {https://proceedings.mlr.press/v130/hsu21a.html},
	abstract = {The support vector machine (SVM) is a well-established classification method whose name refers to the particular training examples, called support vectors, that determine the maximum margin separating hyperplane. The SVM classifier is known to enjoy good generalization properties when the number of support vectors is small compared to the number of training examples. However, recent research has shown that in sufficiently high-dimensional linear classification problems, the SVM can generalize well despite a proliferation of support vectors where all training examples are support vectors. In this paper, we identify new deterministic equivalences for this phenomenon of support vector proliferation, and use them to (1) substantially broaden the conditions under which the phenomenon occurs in high-dimensional settings, and (2) prove a nearly matching converse result.},
	language = {en},
	urldate = {2022-11-16},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Hsu, Daniel and Muthukumar, Vidya and Xu, Ji},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {91--99},
}

@inproceedings{dohmatob_generalized_2019,
	series = {Proceedings of machine learning research},
	title = {Generalized no free lunch theorem for adversarial robustness},
	volume = {97},
	url = {https://proceedings.mlr.press/v97/dohmatob19a.html},
	abstract = {This manuscript presents some new impossibility results on adversarial robustness in machine learning, a very important yet largely open problem. We show that if conditioned on a class label the data distribution satisfies the W₂ Talagrand transportation-cost inequality (for example, this condition is satisfied if the conditional distribution has density which is log-concave; is the uniform measure on a compact Riemannian manifold with positive Ricci curvature, any classifier can be adversarially fooled with high probability once the perturbations are slightly greater than the natural noise level in the problem. We call this result The Strong "No Free Lunch" Theorem as some recent results (Tsipras et al. 2018, Fawzi et al. 2018, etc.) on the subject can be immediately recovered as very particular cases. Our theoretical bounds are demonstrated on both simulated and real data (MNIST). We conclude the manuscript with some speculation on possible future research directions.},
	booktitle = {Proceedings of the 36th international conference on machine learning},
	publisher = {PMLR},
	author = {Dohmatob, Elvis},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	note = {tex.pdf: http://proceedings.mlr.press/v97/dohmatob19a/dohmatob19a.pdf},
	pages = {1646--1654},
}

@misc{andriushchenko_sgd_2022,
	title = {{SGD} with large step sizes learns sparse features},
	url = {http://arxiv.org/abs/2210.05337},
	doi = {10.48550/arXiv.2210.05337},
	abstract = {We showcase important features of the dynamics of the Stochastic Gradient Descent (SGD) in the training of neural networks. We present empirical observations that commonly used large step sizes (i) lead the iterates to jump from one side of a valley to the other causing loss stabilization, and (ii) this stabilization induces a hidden stochastic dynamics orthogonal to the bouncing directions that biases it implicitly toward simple predictors. Furthermore, we show empirically that the longer large step sizes keep SGD high in the loss landscape valleys, the better the implicit regularization can operate and find sparse representations. Notably, no explicit regularization is used so that the regularization effect comes solely from the SGD training dynamics influenced by the step size schedule. Therefore, these observations unveil how, through the step size schedules, both gradient and noise drive together the SGD dynamics through the loss landscape of neural networks. We justify these findings theoretically through the study of simple neural network models as well as qualitative arguments inspired from stochastic processes. Finally, this analysis allows to shed a new light on some common practice and observed phenomena when training neural networks. The code of our experiments is available at https://github.com/tml-epfl/sgd-sparse-features.},
	urldate = {2022-10-26},
	publisher = {arXiv},
	author = {Andriushchenko, Maksym and Varre, Aditya and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
	month = oct,
	year = {2022},
	note = {arXiv:2210.05337 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{vershynin_high-dimensional_2018,
	series = {Cambridge series in statistical and probabilistic mathematics},
	title = {High-{Dimensional} {Probability}},
	isbn = {978-1-108-41519-4 1-108-41519-9 978-1-108-23159-6},
	url = {http://gen.lib.rus.ec/book/index.php?md5=e0ec86c023c4d37ad5c1d1428c69ca31},
	publisher = {Cambridge University Press},
	author = {Vershynin, Roman},
	year = {2018},
}

@article{hussain_autonomous_2019,
	title = {Autonomous {Cars}: {Research} {Results}, {Issues}, and {Future} {Challenges}},
	volume = {21},
	issn = {1553-877X},
	shorttitle = {Autonomous {Cars}},
	doi = {10.1109/COMST.2018.2869360},
	abstract = {Throughout the last century, the automobile industry achieved remarkable milestones in manufacturing reliable, safe, and affordable vehicles. Because of significant recent advances in computation and communication technologies, autonomous cars are becoming a reality. Already autonomous car prototype models have covered millions of miles in test driving. Leading technical companies and car manufacturers have invested a staggering amount of resources in autonomous car technology, as they prepare for autonomous cars' full commercialization in the coming years. However, to achieve this goal, several technical and nontechnical issues remain: software complexity, real-time data analytics, and testing and verification are among the greater technical challenges; and consumer stimulation, insurance management, and ethical/moral concerns rank high among the nontechnical issues. Tackling these challenges requires thoughtful solutions that satisfy consumers, industry, and governmental requirements, regulations, and policies. Thus, here we present a comprehensive review of state-of-the-art results for autonomous car technology. We discuss current issues that hinder autonomous cars' development and deployment on a large scale. We also highlight autonomous car applications that will benefit consumers and many other sectors. Finally, to enable cost-effective, safe, and efficient autonomous cars, we discuss several challenges that must be addressed (and provide helpful suggestions for adoption) by designers, implementers, policymakers, regulatory organizations, and car manufacturers.},
	number = {2},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Hussain, Rasheed and Zeadally, Sherali},
	year = {2019},
	note = {Conference Name: IEEE Communications Surveys \& Tutorials},
	keywords = {Automobiles, Autonomous automobiles, Autonomous cars, Companies, Industries, Roads, Tutorials, Vehicular ad hoc networks, connected cars, driverless cars, policy, privacy, security, simulation},
	pages = {1275--1313},
}

@misc{hendrycks_unsolved_2022,
	title = {Unsolved {Problems} in {ML} {Safety}},
	url = {http://arxiv.org/abs/2109.13916},
	doi = {10.48550/arXiv.2109.13916},
	abstract = {Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards ("Robustness"), identifying hazards ("Monitoring"), reducing inherent model hazards ("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout, we clarify each problem's motivation and provide concrete research directions.},
	urldate = {2022-10-24},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
	month = jun,
	year = {2022},
	note = {arXiv:2109.13916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@techreport{boyd_subgradients_2022,
	title = {Subgradients ({Lecture} {Notes})},
	number = {Notes for EE364b, Stanford University, Spring 2021-22},
	author = {Boyd, S. and Duchi, J. and Pilanci, M. and Vandenberghe, L.},
	month = apr,
	year = {2022},
}

@inproceedings{fahlman_cascade-correlation_1989,
	title = {The {Cascade}-{Correlation} {Learning} {Architecture}},
	volume = {2},
	url = {https://papers.nips.cc/paper/1989/hash/69adc1e107f7f7d035d7baf04342e1ca-Abstract.html},
	abstract = {Cascade-Correlation is a new architecture and supervised learning algo(cid:173) rithm for artificial neural networks.  Instead of just adjusting the weights  in a network of fixed topology. Cascade-Correlation begins with a min(cid:173) imal network,  then automatically trains  and adds new hidden  units  one  by  one,  creating a  multi-layer structure.  Once  a  new  hidden  unit  has  been added  to the network, its  input-side weights are frozen.  This  unit  then becomes a permanent feature-detector in the network, available for  producing  outputs  or for  creating other,  more complex  feature  detec(cid:173) tors.  The Cascade-Correlation architecture has  several advantages over  existing algorithms:  it  learns  very quickly,  the network . determines  its  own size and  topology, it retains  the structures  it  has  built even  if the  training set changes, and it requires no back-propagation of error signals  through  the connections of the network.},
	urldate = {2022-09-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {Fahlman, Scott and Lebiere, Christian},
	year = {1989},
}

@article{nunes_incidence_2021,
	title = {Incidence and {Predictors} of {Progression} to {Chagas} {Cardiomyopathy}: {Long}-{Term} {Follow}-{Up} of {Trypanosoma} cruzi-{Seropositive} {Individuals}},
	volume = {144},
	issn = {1524-4539},
	shorttitle = {Incidence and {Predictors} of {Progression} to {Chagas} {Cardiomyopathy}},
	doi = {10.1161/CIRCULATIONAHA.121.055112},
	abstract = {BACKGROUND: There are few contemporary cohorts of Trypanosoma cruzi-seropositive individuals, and the basic clinical epidemiology of Chagas disease is poorly understood. Herein, we report the incidence of cardiomyopathy and death associated with T. cruzi seropositivity.
METHODS: Participants were selected in blood banks at 2 Brazilian centers. Cases were defined as T. cruzi-seropositive blood donors. T. cruzi-seronegative controls were matched for age, sex, and period of donation. Patients with established Chagas cardiomyopathy were recruited from a tertiary outpatient service. Participants underwent medical examination, blood collection, ECG, and echocardiogram at enrollment (2008-2010) and at follow-up (2018-2019). The primary outcomes were all-cause mortality and development of cardiomyopathy, defined as the presence of a left ventricular ejection fraction {\textless}50\% or QRS complex duration ≥120 ms, or both. To handle loss to follow-up, a sensitivity analysis was performed using inverse probability weights for selection.
RESULTS: We enrolled 499 T. cruzi-seropositive donors (age 48±10 years, 52\% male), 488 T. cruzi-seronegative donors (age 49±10 years, 49\% male), and 101 patients with established Chagas cardiomyopathy (age 48±8 years, 59\% male). The mortality in patients with established cardiomyopathy was 80.9 deaths/1000 person-years (py) (54/101, 53\%) and 15.1 deaths/1000 py (17/114, 15\%) in T. cruzi-seropositive donors with cardiomyopathy at baseline. Among T. cruzi-seropositive donors without cardiomyopathy at baseline, mortality was 3.7 events/1000 py (15/385, 4\%), which was no different from T. cruzi-seronegative donors with 3.6 deaths/1000 py (17/488, 3\%). The incidence of cardiomyopathy in T. cruzi-seropositive donors was 13.8 (95\% CI, 9.5-19.6) events/1000 py (32/262, 12\%) compared with 4.6 (95\% CI, 2.3-8.3) events/1000 py (11/277, 4\%) in seronegative controls, with an absolute incidence difference associated with T. cruzi seropositivity of 9.2 (95\% CI, 3.6-15.0) events/1000 py. T. cruzi antibody level at baseline was associated with development of cardiomyopathy (adjusted odds ratio, 1.4 [95\% CI, 1.1-1.8]).
CONCLUSIONS: We present a comprehensive description of the natural history of T. cruzi seropositivity in a contemporary patient population. The results highlight the central importance of anti-T. cruzi antibody titer as a marker of Chagas disease activity and risk of progression.},
	language = {eng},
	number = {19},
	journal = {Circulation},
	author = {Nunes, Maria Carmo P. and Buss, Lewis F. and Silva, Jose Luiz P. and Martins, Larissa Natany A. and Oliveira, Claudia Di Lorenzo and Cardoso, Clareci Silva and Brito, Bruno Oliveira de Figueiredo and Ferreira, Ariela Mota and Oliveira, Lea Campos and Bierrenbach, Ana Luiza and Fernandes, Fabio and Busch, Michael P. and Hotta, Viviane Tiemi and Martinelli, Luiz Mario Baptista and Soeiro, Maria Carolina F. Almeida and Brentegani, Adriana and Salemi, Vera M. C. and Menezes, Marcia M. and Ribeiro, Antonio Luiz P. and Sabino, Ester Cerdeira},
	month = nov,
	year = {2021},
	pmid = {34565171},
	pmcid = {PMC8578457},
	keywords = {Chagas Cardiomyopathy, Chagas cardiomyopathy, Chagas disease, Disease Progression, Female, Humans, Incidence, Male, Middle Aged, Trypanosoma cruzi, disease progression, mortality, serology},
	pages = {1553--1566},
}

@inproceedings{thrampoulidis_regularized_2015,
	title = {Regularized {Linear} {Regression}: {A} {Precise} {Analysis} of the {Estimation} {Error}},
	shorttitle = {Regularized {Linear} {Regression}},
	url = {https://proceedings.mlr.press/v40/Thrampoulidis15.html},
	abstract = {Non-smooth regularized convex optimization procedures have emerged as a powerful tool to recover structured signals (sparse, low-rank, etc.) from (possibly compressed) noisy linear measurements. We focus on the problem of linear regression and consider a general class of optimization methods that minimize a loss function measuring the misfit of the model to the observations with an added structured-inducing regularization term. Celebrated instances include the LASSO, Group-LASSO, Least-Absolute Deviations method, etc.. We develop a quite general framework for how to determine precise prediction performance guaranties (e.g. mean-square-error) of such methods for the case of Gaussian measurement ensemble. The  machinery builds upon  Gordon’s Gaussian min-max theorem under additional convexity assumptions that arise in many practical applications. This theorem associates with a primary optimization (PO) problem a simplified auxiliary optimization  (AO) problem from which we can tightly infer properties of the original (PO), such as the optimal cost, the norm of the optimal solution, etc. Our theory applies to general loss functions and regularization and provides guidelines on how to optimally tune the regularizer coefficient when certain structural properties (such as sparsity level, rank, etc.) are known.},
	language = {en},
	urldate = {2022-08-04},
	booktitle = {Proceedings of {The} 28th {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Thrampoulidis, Christos and Oymak, Samet and Hassibi, Babak},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {1683--1709},
}

@book{neal_bayesian_1995,
	title = {Bayesian learning for neural networks},
	volume = {118},
	author = {Neal, Radford M},
	year = {1995},
}

@misc{yang_tensor_2020,
	title = {Tensor {Programs} {II}: {Neural} {Tangent} {Kernel} for {Any} {Architecture}},
	shorttitle = {Tensor {Programs} {II}},
	url = {http://arxiv.org/abs/2006.14548},
	abstract = {We prove that a randomly initialized neural network of any architecture has its Tangent Kernel (NTK) converge to a deterministic limit, as the network widths tend to inﬁnity. We demonstrate how to calculate this limit. In prior literature, the heuristic study of neural network gradients often assumes every weight matrix used in forward propagation is independent from its transpose used in backpropagation [58]. This is known as the gradient independence assumption (GIA). We identify a commonly satisﬁed condition, which we call Simple GIA Check, such that the NTK limit calculation based on GIA is correct. Conversely, when Simple GIA Check fails, we show GIA can result in wrong answers. Our material here presents the NTK results of Yang [63] in a friendly manner and showcases the tensor programs technique for understanding wide neural networks. We provide reference implementations of inﬁnite-width NTKs of recurrent neural network, transformer, and batch normalization at https://github.com/thegregyang/NTK4A.},
	language = {en},
	urldate = {2022-07-27},
	publisher = {arXiv},
	author = {Yang, Greg},
	month = nov,
	year = {2020},
	note = {arXiv:2006.14548 [cond-mat, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks, Statistics - Machine Learning},
}

@inproceedings{arora_exact_2019,
	title = {On exact computation with an infinitely wide neural net},
	booktitle = {Thirty-third conference on neural information processing systems},
	author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
	year = {2019},
}

@inproceedings{yang_wide_2019,
	title = {Wide {Feedforward} or {Recurrent} {Neural} {Networks} of {Any} {Architecture} are {Gaussian} {Processes}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/5e69fda38cda2060819766569fd93aa5-Abstract.html},
	abstract = {Wide neural networks with random weights and biases are Gaussian processes, as observed by Neal (1995) for shallow networks, and more recently by Lee et al.{\textasciitilde}(2018) and Matthews et al.{\textasciitilde}(2018) for deep fully-connected networks, as well as by Novak et al.{\textasciitilde}(2019) and Garriga-Alonso et al.{\textasciitilde}(2019) for deep convolutional networks.
We show that this Neural Network-Gaussian Process correspondence surprisingly extends to all modern feedforward or recurrent neural networks composed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph) convolution, pooling, skip connection, attention, batch normalization, and/or layer normalization.
More generally, we introduce a language for expressing neural network computations, and our result encompasses all such expressible neural networks.
This work serves as a tutorial on the {\textbackslash}emph\{tensor programs\} technique formulated in Yang (2019) and elucidates the Gaussian Process results obtained there.
We provide open-source implementations of the Gaussian Process kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at  github.com/thegregyang/GP4A.
Please see our arxiv version for the complete and up-to-date version of this paper.},
	urldate = {2022-07-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yang, Greg},
	year = {2019},
}

@misc{novak_fast_2022,
	title = {Fast {Finite} {Width} {Neural} {Tangent} {Kernel}},
	url = {http://arxiv.org/abs/2206.08720},
	abstract = {The Neural Tangent Kernel (NTK), defined as \${\textbackslash}Theta\_{\textbackslash}theta{\textasciicircum}f(x\_1, x\_2) = {\textbackslash}left[{\textbackslash}partial f({\textbackslash}theta, x\_1){\textbackslash}big/{\textbackslash}partial {\textbackslash}theta{\textbackslash}right] {\textbackslash}left[{\textbackslash}partial f({\textbackslash}theta, x\_2){\textbackslash}big/{\textbackslash}partial {\textbackslash}theta{\textbackslash}right]{\textasciicircum}T\$ where \${\textbackslash}left[{\textbackslash}partial f({\textbackslash}theta, {\textbackslash}cdot){\textbackslash}big/{\textbackslash}partial {\textbackslash}theta{\textbackslash}right]\$ is a neural network (NN) Jacobian, has emerged as a central object of study in deep learning. In the infinite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and generalization of NN architectures. At finite widths, the NTK is also used to better initialize NNs, compare the conditioning across models, perform architecture search, and do meta-learning. Unfortunately, the finite width NTK is notoriously expensive to compute, which severely limits its practical utility. We perform the first in-depth analysis of the compute and memory requirements for NTK computation in finite width networks. Leveraging the structure of neural networks, we further propose two novel algorithms that change the exponent of the compute and memory requirements of the finite width NTK, dramatically improving efficiency. Our algorithms can be applied in a black box fashion to any differentiable function, including those implementing neural networks. We open-source our implementations within the Neural Tangents package (arXiv:1912.02803) at https://github.com/google/neural-tangents.},
	urldate = {2022-07-26},
	publisher = {arXiv},
	author = {Novak, Roman and Sohl-Dickstein, Jascha and Schoenholz, Samuel S.},
	month = jun,
	year = {2022},
	note = {arXiv:2206.08720 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hu_infinitely_2020,
	title = {Infinitely {Wide} {Graph} {Convolutional} {Networks}: {Semi}-supervised {Learning} via {Gaussian} {Processes}},
	shorttitle = {Infinitely {Wide} {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/2002.12168},
	doi = {10.48550/arXiv.2002.12168},
	abstract = {Graph convolutional neural networks{\textasciitilde}(GCNs) have recently demonstrated promising results on graph-based semi-supervised classification, but little work has been done to explore their theoretical properties. Recently, several deep neural networks, e.g., fully connected and convolutional neural networks, with infinite hidden units have been proved to be equivalent to Gaussian processes{\textasciitilde}(GPs). To exploit both the powerful representational capacity of GCNs and the great expressive power of GPs, we investigate similar properties of infinitely wide GCNs. More specifically, we propose a GP regression model via GCNs{\textasciitilde}(GPGC) for graph-based semi-supervised learning. In the process, we formulate the kernel matrix computation of GPGC in an iterative analytical form. Finally, we derive a conditional distribution for the labels of unobserved nodes based on the graph structure, labels for the observed nodes, and the feature matrix of all the nodes. We conduct extensive experiments to evaluate the semi-supervised classification performance of GPGC and demonstrate that it outperforms other state-of-the-art methods by a clear margin on all the datasets while being efficient.},
	urldate = {2022-07-26},
	publisher = {arXiv},
	author = {Hu, Jilin and Shen, Jianbing and Yang, Bin and Shao, Ling},
	month = feb,
	year = {2020},
	note = {arXiv:2002.12168 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{lee_deep_2018,
	title = {Deep {Neural} {Networks} as {Gaussian} {Processes}},
	abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of inﬁnite network width. This correspondence enables exact Bayesian inference for inﬁnite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identiﬁed that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network.},
	booktitle = {{ICLR}},
	author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	year = {2018},
	note = {arXiv:1711.00165 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{loukas_how_2017,
	series = {Proceedings of machine learning research},
	title = {How close are the eigenvectors of the sample and actual covariance matrices?},
	volume = {70},
	url = {https://proceedings.mlr.press/v70/loukas17a.html},
	abstract = {How many samples are sufficient to guarantee that the eigenvectors of the sample covariance matrix are close to those of the actual covariance matrix? For a wide family of distributions, including distributions with finite second moment and sub-gaussian distributions supported in a centered Euclidean ball, we prove that the inner product between eigenvectors of the sample and actual covariance matrices decreases proportionally to the respective eigenvalue distance and the number of samples. Our findings imply ¡em¿non-asymptotic¡/em¿ concentration bounds for eigenvectors and eigenvalues and carry strong consequences for the non-asymptotic analysis of PCA and its applications. For instance, they provide conditions for separating components estimated from O(1) samples and show that even few samples can be sufficient to perform dimensionality reduction, especially for low-rank covariances.},
	booktitle = {Proceedings of the 34th international conference on machine learning},
	publisher = {PMLR},
	author = {Loukas, Andreas},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = aug,
	year = {2017},
	note = {tex.pdf: http://proceedings.mlr.press/v70/loukas17a/loukas17a.pdf},
	pages = {2228--2237},
}

@article{vlachas_data-driven_2018,
	title = {Data-driven forecasting of high-dimensional chaotic systems with long short-term memory networks},
	volume = {474},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rspa.2017.0844},
	doi = {10.1098/rspa.2017.0844},
	abstract = {We introduce a data-driven forecasting method for high-dimensional chaotic systems using long short-term memory (LSTM) recurrent neural networks. The proposed LSTM neural networks perform inference of high-dimensional dynamical systems in their reduced order space and are shown to be an effective set of nonlinear approximators of their attractor. We demonstrate the forecasting performance of the LSTM and compare it with Gaussian processes (GPs) in time series obtained from the Lorenz 96 system, the Kuramoto–Sivashinsky equation and a prototype climate model. The LSTM networks outperform the GPs in short-term forecasting accuracy in all applications considered. A hybrid architecture, extending the LSTM with a mean stochastic model (MSM–LSTM), is proposed to ensure convergence to the invariant measure. This novel hybrid method is fully data-driven and extends the forecasting capabilities of LSTM networks.},
	number = {2213},
	urldate = {2022-06-15},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Vlachas, Pantelis R. and Byeon, Wonmin and Wan, Zhong Y. and Sapsis, Themistoklis P. and Koumoutsakos, Petros},
	month = may,
	year = {2018},
	note = {Publisher: Royal Society},
	keywords = {Gaussian processes, Lorenz 96, T21 barotropic climate model, data-driven forecasting, long short-term memory},
	pages = {20170844},
}

@article{sangiorgio_robustness_2020,
	title = {Robustness of {LSTM} neural networks for multi-step forecasting of chaotic time series},
	volume = {139},
	issn = {0960-0779},
	url = {https://www.sciencedirect.com/science/article/pii/S0960077920304422},
	doi = {10.1016/j.chaos.2020.110045},
	abstract = {Recurrent neurons (and in particular LSTM cells) demonstrated to be efficient when used as basic blocks to build sequence to sequence architectures, which represent the state-of-the-art approach in many sequential tasks related to natural language processing. In this work, these architectures are proposed as general purposes, multi-step predictors for nonlinear time series. We analyze artificial, noise-free data generated by chaotic oscillators and compare LSTM nets with the benchmarks set by feed-forward, one-step-recursive and multi-output predictors. We focus on two different training methods for LSTM nets. The traditional one makes use of the so-called teacher forcing, i.e. the ground truth data are used as input for each time step ahead, rather than the outputs predicted for the previous steps. Conversely, the second feeds the previous predictions back into the recurrent neurons, as it happens when the network is used in forecasting. LSTM predictors robustly show the strengths of the two benchmark competitors, i.e., the good short-term performance of one-step-recursive predictors and greatly improved mid-long-term predictions with respect to feed-forward, multi-output predictors. Training LSTM predictors without teacher forcing is recommended to improve accuracy and robustness, and ensures a more uniform distribution of the predictive power within the chaotic attractor. We also show that LSTM architectures maintain good performances when the number of time lags included in the input differs from the actual embedding dimension of the dataset, a feature that is very important when working on real data.},
	language = {en},
	urldate = {2022-06-15},
	journal = {Chaos, Solitons \& Fractals},
	author = {Sangiorgio, Matteo and Dercole, Fabio},
	month = oct,
	year = {2020},
	keywords = {Deterministic chaos, Exposure bias, Multi-step prediction, Nonlinear time series, Recurrent neural networks, Teacher forcing},
	pages = {110045},
}

@inproceedings{rehmer_using_2019,
	title = {On {Using} {Gated} {Recurrent} {Units} for {Nonlinear} {System} {Identification}},
	doi = {10.23919/ECC.2019.8795631},
	abstract = {During recent years Deep Learning (DL) methods facilitated impressive progress on various fields of research: Deep Convolutional Neural Networks (CNN) enabled object classification with to this day unmatched precision, while state of the art results in speech recognition and natural language processing (NLP) were achieved via gated units such as the LSTM. Although recurrent neural network architectures are long established in the field of system identification as a realization of an internal dynamics approach [1] [2], little research has yet been dedicated towards gated units. The purpose of this paper is to evaluate the architectures of recurrent gated units from the viewpoint of system identification and test their performance on a nonlinear system identification task.},
	booktitle = {2019 18th {European} {Control} {Conference} ({ECC})},
	author = {Rehmer, Alexander and Kroll, Andreas},
	month = jun,
	year = {2019},
	pages = {2504--2509},
}

@article{gonzalez_non-linear_2018,
	series = {2nd {IFAC} {Conference} on {Modelling}, {Identification} and {Control} of {Nonlinear} {Systems} {MICNON} 2018},
	title = {Non-linear system modeling using {LSTM} neural networks},
	volume = {51},
	issn = {2405-8963},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896318310814},
	doi = {10.1016/j.ifacol.2018.07.326},
	abstract = {Long-Short Term Memory (LSTM) is a type of Recurrent Neural Networks (RNN). It takes sequences of information and uses recurrent mechanisms and gate techniques. LSTM has many advantages over other feedforward and recurrent NNs in modeling of time series, such as audio and video. However, in non-linear system modeling normal LSTM does not work well(Wang, 2017). In this paper, we combine LSTM with NN, and use the advantages. The novel neural model consists of hierarchical recurrent networks and one multilayer perceptron. We design a special learning algorithm which uses backpropagation, and backpropagation through time methods. We use two non-linear system examples to compare our neural modeling with other well known methods. The results show that for the simulation model (only the test input is used and the past test output is not used), the modified LSTM model proposed in this paper is much better than the other existing neural models.},
	language = {en},
	number = {13},
	urldate = {2022-06-09},
	journal = {IFAC-PapersOnLine},
	author = {Gonzalez, Jesús and Yu, Wen},
	month = jan,
	year = {2018},
	keywords = {Black-Box identification, LSTM, neural networks},
	pages = {485--489},
}

@inproceedings{yu_wang_new_2017,
	title = {A new concept using {LSTM} {Neural} {Networks} for dynamic system identification},
	isbn = {2378-5861},
	doi = {10.23919/ACC.2017.7963782},
	booktitle = {2017 {American} {Control} {Conference} ({ACC})},
	author = {{Yu Wang}},
	month = may,
	year = {2017},
	note = {Journal Abbreviation: 2017 American Control Conference (ACC)},
	pages = {5324--5329},
}

@article{efron_bootstrap_1979,
	title = {Bootstrap {Methods}: {Another} {Look} at the {Jackknife}},
	volume = {7},
	url = {https://doi.org/10.1214/aos/1176344552},
	doi = {10.1214/aos/1176344552},
	number = {1},
	journal = {The Annals of Statistics},
	author = {Efron, B.},
	month = jan,
	year = {1979},
	pages = {1--26},
}

@article{chatterji_foolish_2022,
	title = {Foolish {Crowds} {Support} {Benign} {Overfitting}},
	url = {http://arxiv.org/abs/2110.02914},
	abstract = {We prove a lower bound on the excess risk of sparse interpolating procedures for linear regression with Gaussian data in the overparameterized regime. We apply this result to obtain a lower bound for basis pursuit (the minimum \${\textbackslash}ell\_1\$-norm interpolant) that implies that its excess risk can converge at an exponentially slower rate than OLS (the minimum \${\textbackslash}ell\_2\$-norm interpolant), even when the ground truth is sparse. Our analysis exposes the benefit of an effect analogous to the "wisdom of the crowd", except here the harm arising from fitting the \${\textbackslash}textit\{noise\}\$ is ameliorated by spreading it among many directions -- the variance reduction arises from a \${\textbackslash}textit\{foolish\}\$ crowd.},
	urldate = {2022-05-18},
	journal = {arXiv:2110.02914},
	author = {Chatterji, Niladri S. and Long, Philip M.},
	month = mar,
	year = {2022},
	note = {arXiv: 2110.02914},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{bartlett_deep_2021,
	title = {Deep learning: a statistical viewpoint},
	shorttitle = {Deep learning},
	url = {http://arxiv.org/abs/2103.09177},
	abstract = {The remarkable practical success of deep learning has revealed some major surprises from a theoretical perspective. In particular, simple gradient methods easily find near-optimal solutions to non-convex optimization problems, and despite giving a near-perfect fit to training data without any explicit effort to control model complexity, these methods exhibit excellent predictive accuracy. We conjecture that specific principles underlie these phenomena: that overparametrization allows gradient methods to find interpolating solutions, that these methods implicitly impose regularization, and that overparametrization leads to benign overfitting. We survey recent theoretical progress that provides examples illustrating these principles in simpler settings. We first review classical uniform convergence results and why they fall short of explaining aspects of the behavior of deep learning methods. We give examples of implicit regularization in simple settings, where gradient methods lead to minimal norm functions that perfectly fit the training data. Then we review prediction methods that exhibit benign overfitting, focusing on regression problems with quadratic loss. For these methods, we can decompose the prediction rule into a simple component that is useful for prediction and a spiky component that is useful for overfitting but, in a favorable setting, does not harm prediction accuracy. We focus specifically on the linear regime for neural networks, where the network can be approximated by a linear model. In this regime, we demonstrate the success of gradient flow, and we consider benign overfitting with two-layer networks, giving an exact asymptotic analysis that precisely demonstrates the impact of overparametrization. We conclude by highlighting the key challenges that arise in extending these insights to realistic deep learning settings.},
	urldate = {2021-03-22},
	journal = {arXiv:2103.09177},
	author = {Bartlett, Peter L. and Montanari, Andrea and Rakhlin, Alexander},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.09177},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{huang_learning_2016,
	title = {Learning with a {Strong} {Adversary}},
	url = {http://arxiv.org/abs/1511.03034},
	abstract = {The robustness of neural networks to intended perturbations has recently attracted significant attention. In this paper, we propose a new method, {\textbackslash}emph\{learning with a strong adversary\}, that learns robust classifiers from supervised data. The proposed method takes finding adversarial examples as an intermediate step. A new and simple way of finding adversarial examples is presented and experimentally shown to be efficient. Experimental results demonstrate that resulting learning method greatly improves the robustness of the classification models produced.},
	urldate = {2022-04-29},
	journal = {arXiv:1511.03034},
	author = {Huang, Ruitong and Xu, Bing and Schuurmans, Dale and Szepesvari, Csaba},
	month = jan,
	year = {2016},
	note = {arXiv: 1511.03034},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{perdomo_performative_2020,
	series = {Proceedings of machine learning research},
	title = {Performative prediction},
	volume = {119},
	url = {https://proceedings.mlr.press/v119/perdomo20a.html},
	abstract = {When predictions support decisions they may influence the outcome they aim to predict. We call such predictions performative; the prediction influences the target. Performativity is a well-studied phenomenon in policy-making that has so far been neglected in supervised learning. When ignored, performativity surfaces as undesirable distribution shift, routinely addressed with retraining. We develop a risk minimization framework for performative prediction bringing together concepts from statistics, game theory, and causality. A conceptual novelty is an equilibrium notion we call performative stability. Performative stability implies that the predictions are calibrated not against past outcomes, but against the future outcomes that manifest from acting on the prediction. Our main results are necessary and sufficient conditions for the convergence of retraining to a performatively stable point of nearly minimal loss. In full generality, performative prediction strictly subsumes the setting known as strategic classification. We thus also give the first sufficient conditions for retraining to overcome strategic feedback effects.},
	booktitle = {Proceedings of the 37th international conference on machine learning},
	publisher = {PMLR},
	author = {Perdomo, Juan and Zrnic, Tijana and Mendler-Dünner, Celestine and Hardt, Moritz},
	editor = {III, Hal Daumé and Singh, Aarti},
	month = jul,
	year = {2020},
	note = {tex.pdf: http://proceedings.mlr.press/v119/perdomo20a/perdomo20a.pdf},
	pages = {7599--7609},
}

@incollection{cheng_errors_1998,
	address = {Heidelberg},
	title = {Errors in variables in econometrics},
	isbn = {978-3-642-47027-1},
	url = {https://doi.org/10.1007/978-3-642-47027-1_1},
	abstract = {This article discusses the use of instrumental variables and grouping methods in the linear errors-in-variables or measurement error model. Comparisons are made between these methods, standard measurement error model methods with side conditions, least squares methods, and replicated models. It is demonstrated that there are close relationships between these apparently diverse estimation techniques.},
	booktitle = {Econometrics in theory and practice: {Festschrift} for hans schneeweiß},
	publisher = {Physica-Verlag HD},
	author = {Cheng, Chi-Lun and Van Ness, John W.},
	editor = {Galata, Robert and Küchenhoff, Helmut},
	year = {1998},
	doi = {10.1007/978-3-642-47027-1_1},
	pages = {3--13},
}

@article{chen_atomic_1998,
	title = {Atomic decomposition by basis pursuit},
	volume = {20},
	url = {https://doi.org/10.1137/S1064827596304010},
	doi = {10.1137/S1064827596304010},
	abstract = {The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries — stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB).Basis Pursuit (BP) is a principle for decomposing a signal into an "optimal" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising.BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.},
	number = {1},
	journal = {SIAM Journal on Scientific Computing},
	author = {Chen, Scott Shaobing and Donoho, David L. and Saunders, Michael A.},
	year = {1998},
	note = {tex.eprint: https://doi.org/10.1137/S1064827596304010},
	pages = {33--61},
}

@misc{noauthor_151103034_nodate,
	title = {[1511.03034] {Learning} with a {Strong} {Adversary}},
	url = {https://arxiv.org/abs/1511.03034},
	urldate = {2022-04-29},
}

@article{shaham_understanding_2018,
	title = {Understanding adversarial training: {Increasing} local stability of supervised models through robust optimization},
	volume = {307},
	issn = {0925-2312},
	journal = {Neurocomputing},
	author = {Shaham, Uri and Yamada, Yutaro and Negahban, Sahand},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {195--204},
}

@article{fawzi_analysis_2018,
	title = {Analysis of classifiers’ robustness to adversarial perturbations},
	volume = {107},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-017-5663-3},
	doi = {10.1007/s10994-017-5663-3},
	abstract = {The goal of this paper is to analyze the intriguing instability of classifiers to adversarial perturbations (Szegedy et al., in: International conference on learning representations (ICLR), 2014). We provide a theoretical framework for analyzing the robustness of classifiers to adversarial perturbations, and show fundamental upper bounds on the robustness of classifiers. Specifically, we establish a general upper bound on the robustness of classifiers to adversarial perturbations, and then illustrate the obtained upper bound on two practical classes of classifiers, namely the linear and quadratic classifiers. In both cases, our upper bound depends on a distinguishability measure that captures the notion of difficulty of the classification task. Our results for both classes imply that in tasks involving small distinguishability, no classifier in the considered set will be robust to adversarial perturbations, even if a good accuracy is achieved. Our theoretical framework moreover suggests that the phenomenon of adversarial instability is due to the low flexibility of classifiers, compared to the difficulty of the classification task (captured mathematically by the distinguishability measure). We further show the existence of a clear distinction between the robustness of a classifier to random noise and its robustness to adversarial perturbations. Specifically, the former is shown to be larger than the latter by a factor that is proportional to \$\${\textbackslash}sqrt\{d\}\$\$(with d being the signal dimension) for linear classifiers. This result gives a theoretical explanation for the discrepancy between the two robustness properties in high dimensional problems, which was empirically observed by Szegedy et al. in the context of neural networks. We finally show experimental results on controlled and real-world data that confirm the theoretical analysis and extend its spirit to more complex classification schemes.},
	language = {en},
	number = {3},
	urldate = {2022-04-29},
	journal = {Machine Learning},
	author = {Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	month = mar,
	year = {2018},
	keywords = {Adversarial examples, Classification robustness, Deep networks, Instability, Random noise},
	pages = {481--508},
}

@article{sangha_automated_2022,
	title = {Automated multilabel diagnosis on electrocardiographic images and signals},
	volume = {13},
	doi = {10.1038/s41467-022-29153-3},
	journal = {Nature Communications},
	author = {Sangha, Veer and Mortazavi, Bobak J. and Haimovich, Adrian D. and Ribeiro, Antônio H. and Brandt, Cynthia A. and Jacoby, Daniel L. and Schulz, Wade L. and Krumholz, Harlan M. and Ribeiro, Antonio Luiz P. and Khera, Rohan},
	year = {2022},
	note = {https://www.medrxiv.org/content/10.1101/2021.09.22.21263926v1},
	pages = {1583},
}

@article{mei_generalization_2022,
	title = {The {Generalization} {Error} of {Random} {Features} {Regression}: {Precise} {Asymptotics} and the {Double} {Descent} {Curve}},
	volume = {75},
	shorttitle = {The generalization error of random features regression},
	url = {http://arxiv.org/abs/1908.05355},
	doi = {https://doi.org/10.1002/cpa.22008},
	abstract = {Deep learning methods operate in regimes that defy the traditional statistical mindset. The neural network architectures often contain more parameters than training samples, and are so rich that they can interpolate the observed labels, even if the latter are replaced by pure noise. Despite their huge complexity, the same architectures achieve small generalization error on real data. This phenomenon has been rationalized in terms of a so-called `double descent' curve. As the model complexity increases, the generalization error follows the usual U-shaped curve at the beginning, first decreasing and then peaking around the interpolation threshold (when the model achieves vanishing training error). However, it descends again as model complexity exceeds this threshold. The global minimum of the generalization error is found in this overparametrized regime, often when the number of parameters is much larger than the number of samples. Far from being a peculiar property of deep neural networks, elements of this behavior have been demonstrated in much simpler settings, including linear regression with random covariates. In this paper we consider the problem of learning an unknown function over the \$d\$-dimensional sphere \${\textbackslash}mathbb S{\textasciicircum}\{d-1\}\$, from \$n\$ i.i.d. samples \$({\textbackslash}boldsymbol x\_i, y\_i) {\textbackslash}in {\textbackslash}mathbb S{\textasciicircum}\{d-1\} {\textbackslash}times {\textbackslash}mathbb R\$, \$i {\textbackslash}le n\$. We perform ridge regression on \$N\$ random features of the form \${\textbackslash}sigma({\textbackslash}boldsymbol w\_a{\textasciicircum}\{{\textbackslash}mathsf T\}{\textbackslash}boldsymbol x)\$, \$a {\textbackslash}le N\$. This can be equivalently described as a two-layers neural network with random first-layer weights. We compute the precise asymptotics of the generalization error, in the limit \$N, n, d {\textbackslash}to {\textbackslash}infty\$ with \$N/d\$ and \$n/d\$ fixed. This provides the first analytically tractable model that captures all the features of the double descent phenomenon without assuming ad hoc misspecification structures.},
	number = {4},
	urldate = {2020-07-22},
	journal = {Communications on Pure and Applied Mathematics},
	author = {Mei, Song and Montanari, Andrea},
	year = {2022},
	note = {arXiv: 1908.05355},
	keywords = {62J99, Mathematics - Statistics Theory, Statistics - Machine Learning},
	pages = {667--766},
}

@inproceedings{daniely_most_2020,
	title = {Most {ReLU} {Networks} {Suffer} from ell 2 {Adversarial} {Perturbations}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/497476fe61816251905e8baafdf54c23-Abstract.html},
	urldate = {2022-04-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Daniely, Amit and Shacham, Hadas},
	year = {2020},
	pages = {6629--6636},
}

@article{bartlett_adversarial_2021,
	title = {Adversarial {Examples} in {Multi}-{Layer} {Random} {ReLU} {Networks}},
	abstract = {We consider the phenomenon of adversarial examples in ReLU networks with independent Gaussian parameters. For networks of constant depth and with a large range of widths (for instance, it sufﬁces if the width of each layer is polynomial in that of any other layer), small perturbations of input vectors lead to large changes of outputs. This generalizes results of Daniely and Schacham (2020) for networks of rapidly decreasing width and of Bubeck et al (2021) for two-layer networks. Our proof shows that adversarial examples arise in these networks because the functions they compute are locally very similar to random linear functions. Bottleneck layers play a key role: the minimal width up to some point in the network determines scales and sensitivities of mappings computed up to that point. The main result is for networks with constant depth, but we also show that some constraint on depth is necessary for a result of this kind, because there are suitably deep networks that, with constant probability, compute a function that is close to constant.},
	language = {en},
	journal = {Neural Information Processing Systems  (NeurIPS)},
	author = {Bartlett, Peter L and Bubeck, Sébastien and Cherapanamjeri, Yeshwanth},
	year = {2021},
}

@article{bubeck_law_2021,
	title = {A law of robustness for two-layers neural networks},
	volume = {134},
	abstract = {We initiate the study of the inherent tradeoffs between the size of a neural network and its robustness, as measured by its Lipschitz constant. We make a precise conjecture that, for any Lipschitz activation function and for most datasets, any two-layers neural network with \$k\$ neurons that perfectly fit the data must have its Lipschitz constant larger (up to a constant) than \${\textbackslash}sqrt\{n/k\}\$ where \$n\$ is the number of datapoints. In particular, this conjecture implies that overparametrization is necessary for robustness, since it means that one needs roughly one neuron per datapoint to ensure a \$O(1)\$-Lipschitz network, while mere data fitting of \$d\$-dimensional data requires only one neuron per \$d\$ datapoints. We prove a weaker version of this conjecture when the Lipschitz constant is replaced by an upper bound on it based on the spectral norm of the weight matrix. We also prove the conjecture in the high-dimensional regime \$n {\textbackslash}approx d\$ (which we also refer to as the undercomplete case, since only \$k {\textbackslash}leq d\$ is relevant here). Finally we prove the conjecture for polynomial activation functions of degree \$p\$ when \$n {\textbackslash}approx d{\textasciicircum}p\$. We complement these findings with experimental evidence supporting the conjecture.},
	journal = {134 of Proceedings of Machine Learning Research, Conference on Learning Theory (COLT)},
	author = {Bubeck, Sébastien and Li, Yuanzhi and Nagaraj, Dheeraj},
	year = {2021},
	note = {arXiv: 2009.14444},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {804--820},
}

@inproceedings{deng_model_2020,
	title = {A {Model} of {Double} {Descent} for {High}-{Dimensional} {Logistic} {Regression}},
	doi = {10.1109/ICASSP40776.2020.9053524},
	abstract = {We consider a model for logistic regression where only a subset of features of size p is used for training a linear classifier over n training samples. The classifier is obtained by running gradient-descent (GD) on the logistic-loss. For this model, we investigate the dependence of the classification error on the overparameterization ratio κ = p/n. First, building on known deterministic results on convergence properties of the GD, we uncover a phase-transition phenomenon for the case of Gaussian features: the classification error of GD is the same as that of the maximum-likelihood (ML) solution when κ {\textless}; κ*, and that of the max-margin (SVM) solution when κ {\textgreater} κ*. Next, using the convex Gaussian min-max theorem (CGMT), we sharply characterize the performance of both the ML and SVM solutions. Combining these results, we obtain curves that explicitly characterize the test error of GD for varying values of κ. The numerical results validate the theoretical predictions and unveil “double-descent” phenomena that complement similar recent observations in linear regression settings.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Deng, Zeyu and Kammoun, Abla and Thrampoulidis, Christos},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {Asymptotics, Binary Classification, Generalization error, Linear regression, Logistics, Max-margin, Numerical models, Overparameterization, Signal processing, Speech processing, Support vector machines, Training},
	pages = {4267--4271},
}

@inproceedings{globerson_nightmare_2006,
	title = {Nightmare at test time: robust learning by feature deletion},
	isbn = {978-1-59593-383-6},
	url = {http://portal.acm.org/citation.cfm?doid=1143844.1143889},
	doi = {10.1145/1143844.1143889},
	abstract = {When constructing a classiﬁer from labeled data, it is important not to assign too much weight to any single input feature, in order to increase the robustness of the classiﬁer. This is particularly important in domains with nonstationary feature distributions or with input sensor failures. A common approach to achieving such robustness is to introduce regularization which spreads the weight more evenly between the features. However, this strategy is very generic, and cannot induce robustness speciﬁcally tailored to the classiﬁcation task at hand. In this work, we introduce a new algorithm for avoiding single feature over-weighting by analyzing robustness using a game theoretic formalization. We develop classiﬁers which are optimally resilient to deletion of features in a minimax sense, and show how to construct such classiﬁers using quadratic programming. We illustrate the applicability of our methods on spam ﬁltering and handwritten digit recognition tasks, where feature deletion is indeed a realistic noise model.},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning  ({ICML})},
	author = {Globerson, Amir and Roweis, Sam},
	year = {2006},
	pages = {353--360},
}

@inproceedings{deng_imagenet_2009,
	title = {Imagenet: {A} large-scale hierarchical image database},
	booktitle = {2009 {IEEE} conference on computer vision and pattern recognition},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year = {2009},
	note = {tex.organization: Ieee},
	pages = {248--255},
}

@inproceedings{guo_sparse_2018,
	title = {Sparse {DNNs} with {Improved} {Adversarial} {Robustness}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/4c5bde74a8f110656874902f07378009-Abstract.html},
	urldate = {2022-02-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Guo, Yiwen and Zhang, Chao and Zhang, Changshui and Chen, Yurong},
	year = {2018},
}

@inproceedings{diochnos_adversarial_2018,
	title = {Adversarial risk and robustness: {General} definitions and implications for the uniform distribution},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/file/3483e5ec0489e5c394b028ec4e81f3e1-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Diochnos, Dimitrios and Mahloujifar, Saeed and Mahmoody, Mohammad},
	year = {2018},
}

@article{gopalakrishnan_combating_2018,
	title = {Combating {Adversarial} {Attacks} {Using} {Sparse} {Representations}},
	url = {http://arxiv.org/abs/1803.03880},
	abstract = {It is by now well-known that small adversarial perturbations can induce classification errors in deep neural networks (DNNs). In this paper, we make the case that sparse representations of the input data are a crucial tool for combating such attacks. For linear classifiers, we show that a sparsifying front end is provably effective against \${\textbackslash}ell\_\{{\textbackslash}infty\}\$-bounded attacks, reducing output distortion due to the attack by a factor of roughly \$K / N\$ where \$N\$ is the data dimension and \$K\$ is the sparsity level. We then extend this concept to DNNs, showing that a "locally linear" model can be used to develop a theoretical foundation for crafting attacks and defenses. Experimental results for the MNIST dataset show the efficacy of the proposed sparsifying front end.},
	urldate = {2022-02-03},
	journal = {arXiv:1803.03880},
	author = {Gopalakrishnan, Soorya and Marzi, Zhinus and Madhow, Upamanyu and Pedarsani, Ramtin},
	month = jul,
	year = {2018},
	note = {arXiv: 1803.03880},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{boyd_convex_2004,
	title = {Convex optimization},
	isbn = {978-0-521-83378-3},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen P. and Vandenberghe, Lieven},
	year = {2004},
	keywords = {Convex functions, Mathematical optimization},
}

@article{fazlyab_efficient_2019,
	title = {Efficient and {Accurate} {Estimation} of {Lipschitz} {Constants} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1906.04893},
	abstract = {Tight estimation of the Lipschitz constant for deep neural networks (DNNs) is useful in many applications ranging from robustness certification of classifiers to stability analysis of closed-loop systems with reinforcement learning controllers. Existing methods in the literature for estimating the Lipschitz constant suffer from either lack of accuracy or poor scalability. In this paper, we present a convex optimization framework to compute guaranteed upper bounds on the Lipschitz constant of DNNs both accurately and efficiently. Our main idea is to interpret activation functions as gradients of convex potential functions. Hence, they satisfy certain properties that can be described by quadratic constraints. This particular description allows us to pose the Lipschitz constant estimation problem as a semidefinite program (SDP). The resulting SDP can be adapted to increase either the estimation accuracy (by capturing the interaction between activation functions of different layers) or scalability (by decomposition and parallel implementation). We illustrate the utility of our approach with a variety of experiments on randomly generated networks and on classifiers trained on the MNIST and Iris datasets. In particular, we experimentally demonstrate that our Lipschitz bounds are the most accurate compared to those in the literature. We also study the impact of adversarial training methods on the Lipschitz bounds of the resulting classifiers and show that our bounds can be used to efficiently provide robustness guarantees.},
	urldate = {2020-07-08},
	journal = {Advances in Neural Information Processing Systems (NeurIPS)},
	author = {Fazlyab, Mahyar and Robey, Alexander and Hassani, Hamed and Morari, Manfred and Pappas, George J.},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.04893},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{hassani_curse_2022,
	title = {The curse of overparametrization in adversarial training: {Precise} analysis of robust generalization for random features regression},
	shorttitle = {The curse of overparametrization in adversarial training},
	url = {http://arxiv.org/abs/2201.05149},
	abstract = {Successful deep learning models often involve training neural network architectures that contain more parameters than the number of training samples. Such overparametrized models have been extensively studied in recent years, and the virtues of overparametrization have been established from both the statistical perspective, via the double-descent phenomenon, and the computational perspective via the structural properties of the optimization landscape. Despite the remarkable success of deep learning architectures in the overparametrized regime, it is also well known that these models are highly vulnerable to small adversarial perturbations in their inputs. Even when adversarially trained, their performance on perturbed inputs (robust generalization) is considerably worse than their best attainable performance on benign inputs (standard generalization). It is thus imperative to understand how overparametrization fundamentally affects robustness. In this paper, we will provide a precise characterization of the role of overparametrization on robustness by focusing on random features regression models (two-layer neural networks with random first layer weights). We consider a regime where the sample size, the input dimension and the number of parameters grow in proportion to each other, and derive an asymptotically exact formula for the robust generalization error when the model is adversarially trained. Our developed theory reveals the nontrivial effect of overparametrization on robustness and indicates that for adversarially trained random features models, high overparametrization can hurt robust generalization.},
	urldate = {2022-02-25},
	journal = {arXiv:2201.05149},
	author = {Hassani, Hamed and Javanmard, Adel},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.05149},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{bubeck_universal_2021,
	title = {A {Universal} {Law} of {Robustness} via {Isoperimetry}},
	url = {http://arxiv.org/abs/2105.12806},
	abstract = {Classically, data interpolation with a parametrized model class is possible as long as the number of parameters is larger than the number of equations to be satisfied. A puzzling phenomenon in deep learning is that models are trained with many more parameters than what this classical theory would suggest. We propose a theoretical explanation for this phenomenon. We prove that for a broad class of data distributions and model classes, overparametrization is necessary if one wants to interpolate the data smoothly. Namely we show that smooth interpolation requires \$d\$ times more parameters than mere interpolation, where \$d\$ is the ambient data dimension. We prove this universal law of robustness for any smoothly parametrized function class with polynomial size weights, and any covariate distribution verifying isoperimetry. In the case of two-layers neural networks and Gaussian covariates, this law was conjectured in prior work by Bubeck, Li and Nagaraj. We also give an interpretation of our result as an improved generalization bound for model classes consisting of smooth functions.},
	urldate = {2021-08-26},
	journal = {Advances in Neural Information Processing Systems},
	author = {Bubeck, Sébastien and Sellke, Mark},
	year = {2021},
	note = {arXiv: 2105.12806},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{dalvi_adversarial_2004,
	title = {Adversarial classification},
	isbn = {1-58113-888-1},
	doi = {10.1145/1014052.1014066},
	abstract = {Essentially all data mining algorithms assume that the data-generating process is independent of the data miner's activities. However, in many domains, including spam detection, intrusion detection, fraud detection, surveillance and counter-terrorism, this is far from the case: the data is actively manipulated by an adversary seeking to make the classifier produce false negatives. In these domains, the performance of a classifier can degrade rapidly after it is deployed, as the adversary learns to defeat it. Currently the only solution to this is repeated, manual, ad hoc reconstruction of the classifier. In this paper we develop a formal framework and algorithms for this problem. We view classification as a game between the classifier and the adversary, and produce a classifier that is optimal given the adversary's optimal strategy. Experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way, and (within the parameters of the problem) automatically adapt the classifier to the adversary's evolving manipulations.},
	booktitle = {Proceedings of the tenth {ACM} {SIGKDD} international conference on knowledge discovery and data mining},
	author = {Dalvi, Nilesh and Domingos, Pedro and {Mausam} and Sanghai, Sumit and Verma, Deepak},
	year = {2004},
	keywords = {cost-sensitive learning, game theory, integer linear programming, naive Bayes, spam detection},
}

@inproceedings{osama_inferring_2019,
	title = {Inferring {Heterogeneous} {Causal} {Effects} in {Presence} of {Spatial} {Confounding}},
	abstract = {We address the problem of inferring the causal effect of an exposure on an outcome across space, using observational data. The data is possibly subject to unmeasured confounding variables which, in a standard approach, must be adjusted for by estimating a nuisance function. Here we develop a method that eliminates the nuisance function, while mitigating the resulting errors-in-variables. The result is a robust and accurate inference method for spatially varying heterogeneous causal effects. The properties of the method are demonstrated on synthetic as well as real data from Germany and the US.},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning},},
	author = {Osama, Muhammad and Zachariah, Dave and Schön, Thomas B},
	year = {2019},
}

@book{peters_elements_2017,
	title = {Elements of causal inference: foundations and learning algorithms},
	shorttitle = {Elements of causal inference},
	url = {https://www.tandfonline.com/doi/full/10.1080/00949655.2018.1505197},
	language = {en},
	urldate = {2022-03-13},
	author = {Peters, Jonas and Janzing, Dominik and Schölkopf, Bernhard},
	year = {2017},
}

@article{scott_limited_2021,
	title = {Limited haplotype diversity underlies polygenic trait architecture across 70 years of wheat breeding},
	volume = {22},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/s13059-021-02354-7},
	doi = {10.1186/s13059-021-02354-7},
	abstract = {Selection has dramatically shaped genetic and phenotypic variation in bread wheat. We can assess the genomic basis of historical phenotypic changes, and the potential for future improvement, using experimental populations that attempt to undo selection through the randomizing effects of recombination.},
	number = {1},
	urldate = {2022-02-28},
	journal = {Genome Biology},
	author = {Scott, Michael F. and Fradgley, Nick and Bentley, Alison R. and Brabbs, Thomas and Corke, Fiona and Gardner, Keith A. and Horsnell, Richard and Howell, Phil and Ladejobi, Olufunmilayo and Mackay, Ian J. and Mott, Richard and Cockram, James},
	month = may,
	year = {2021},
	keywords = {GWAS, Genomic prediction, Imputation, Low-coverage whole-genome sequencing, MAGIC, Multi-parent population, Phenomics, Pleiotropy, Wheat},
	pages = {137},
}

@misc{noauthor_interpolators-under-attack_nodate,
	title = {interpolators-under-attack},
	url = {https://www.overleaf.com/project/5fabd1f02a42038b23abc59f},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2022-02-26},
}

@article{diamond_cvxpy_2016,
	title = {{CVXPY}: {A} {Python}-embedded modeling language for convex optimization},
	volume = {17},
	number = {83},
	journal = {Journal of Machine Learning Research},
	author = {Diamond, Steven and Boyd, Stephen},
	year = {2016},
	pages = {1--5},
}

@inproceedings{ribeiro_lasso_2018,
	title = {Lasso {Regularization} {Paths} for {NARMAX} {Models} via {Coordinate} {Descent}},
	copyright = {All rights reserved},
	isbn = {2378-5861},
	doi = {10.23919/ACC.2018.8430924},
	booktitle = {2018 {Annual} {American} {Control} {Conference} ({ACC})},
	author = {Ribeiro, Antonio H. and Aguirre, Luis A.},
	month = jun,
	year = {2018},
	keywords = {Approximation algorithms, Computational modeling, Data models, Estimation, L1-norm penalty, Lasso estimation, Lasso regularization paths, Mathematical model, Minimization, NARMAX models, Optimization, autoregressive moving average processes, entire regularization path, error regressors, linear combination, linear models, nonlinear regression problem, polynomial models, regression analysis, regressor matrix},
	pages = {5268--5273},
}

@article{ribeiro_parallel_2018,
	title = {''{Parallel} {Training} {Considered} {Harmful}?'': {Comparing} series-parallel and parallel feedforward network training},
	volume = {316},
	copyright = {All rights reserved},
	issn = {0925-2312},
	doi = {10.1016/j.neucom.2018.07.071},
	abstract = {Neural network models for dynamic systems can be trained either in parallel or in series-parallel configurations. Influenced by early arguments, several papers justify the choice of series-parallel rather than parallel configuration claiming it has a lower computational cost, better stability properties during training and provides more accurate results. Other published results, on the other hand, defend parallel training as being more robust and capable of yielding more accurate long-term predictions. The main contribution of this paper is to present a study comparing both methods under the same unified framework with special attention to three aspects: (i) robustness of the estimation in the presence of noise; (ii) computational cost; and, (iii) convergence. A unifying mathematical framework and simulation studies show situations where each training method provides superior validation results and suggest that parallel training is generally better in more realistic scenarios. An example using measured data seems to reinforce such a claim. Complexity analysis and numerical examples show that both methods have similar computational cost although series-parallel training is more amenable to parallelization. Some informal discussion about stability and convergence properties is presented and explored in the examples.},
	journal = {Neurocomputing},
	author = {Ribeiro, Antônio H. and Aguirre, Luis A.},
	month = nov,
	year = {2018},
	keywords = {Neural network, Output error models, Parallel training, Series-parallel training, System identification},
	pages = {222--231},
}

@article{paixao_evaluation_2019,
	title = {Evaluation of mortality in bundle branch block patients from an electronic cohort: {Clinical} {Outcomes} in {Digital} {Electrocardiography} ({CODE}) study},
	copyright = {All rights reserved},
	issn = {0022-0736},
	doi = {10.1016/j.jelectrocard.2019.09.004},
	journal = {Journal of Electrocardiology},
	author = {Paixão, Gabriela M. M. and Lima, Emilly M. and Gomes, Paulo R. and Ferreira, Milton P. and Oliveira, Derick M. and Ribeiro, Manoel Horta and Ribeiro, Antônio H. and Nascimento, Jamil and Canazart, Jéssica A. and Cardoso, Gustavo and Ribeiro, Leonardo B. and Ribeiro, Antonio Luiz P.},
	month = sep,
	year = {2019},
	keywords = {Artificial intelligence, Big-data, Electrocardiography, Telehealth},
}

@article{virtanen_scipy_2020,
	title = {{SciPy} 1.0--{Fundamental} {Algorithms} for {Scientific} {Computing} in {Python}},
	volume = {17},
	copyright = {All rights reserved},
	doi = {10.1038/s41592-019-0686-2},
	abstract = {SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.},
	number = {3},
	journal = {Nature Methods},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul and Contributors, SciPy 1.0},
	year = {2020},
	note = {arXiv: 1907.10121},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Mathematical Software, Computer Science - Software Engineering, Physics - Computational Physics},
	pages = {261--272},
}

@article{ribeiro_automatic_2020a,
	title = {Automatic diagnosis of the 12-lead {ECG} using a deep neural network},
	volume = {11},
	copyright = {All rights reserved},
	doi = {10.1038/s41467-020-15432-4},
	number = {1},
	journal = {Nature Communications},
	author = {Ribeiro, Antônio H. and Ribeiro, Manoel Horta and Paixão, Gabriela M. M. and Oliveira, Derick M. and Gomes, Paulo R. and Canazart, Jéssica A. and Ferreira, Milton P. S. and Andersson, Carl R. and Macfarlane, Peter W. and Meira Jr., Wagner and Schön, Thomas B. and Ribeiro, Antonio Luiz P.},
	year = {2020},
	note = {arxiv: 1904.01949},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
	pages = {1760},
}

@inproceedings{gedon_resnet-based_2021,
	title = {{ResNet}-based {ECG} {Diagnosis} of {Myocardial} {Infarction} in the {Emergency} {Department}},
	copyright = {All rights reserved},
	booktitle = {Machine learning from ground truth: {New} medical imaging datasets for unsolved medical problems {Workshop} at {NeurIPS}},
	author = {Gedon, Daniel and Gustafsson, Stefan and Lampa, Erik and Ribeiro, Antônio H. and Holzmann, Martin J. and Schön, Thomas B. and Sundström, Johan},
	year = {2021},
}

@inproceedings{gedon_first_2021,
	title = {First {Steps} {Towards} {Self}-{Supervised} {Pretraining} of the 12-{Lead} {ECG}},
	volume = {48},
	copyright = {All rights reserved},
	doi = {10.23919/CinC53138.2021.9662748},
	abstract = {Self-supervised learning is a paradigm that extracts general features which describe the input space by artificially generating labels from the input without the need for explicit annotations. The learned features can then be used by transfer learning to boost the performance on a downstream task. Such methods have recently produced state of the art results in natural language processing and computer vision. Here, we propose a self-supervised learning method for 12-lead electrocardiograms (ECGs). For pretraining the model we design a task to mask out subsegements of all channels of the input signals and try to predict the actual values. As the model architecture, we use a U-ResNet containing an encoder-decoder structure. We test our method by self-supervised pretraining on the CODE dataset and then transfer the learnt features by finetuning on the PTB-XL and CPSC benchmarks to evaluate the effect of our method in the classification of 12-leads ECGs. The method does provide modest improvements in performance when compared to not using pretraining. In future work we will make use of these ideas in smaller dataset, where we believe it can lead to larger performance gains.},
	booktitle = {Computing in {Cardiology} ({CinC})},
	author = {Gedon, Daniel and Ribeiro, Antônio H. and Wahlström, Niklas and Schön, Thomas B.},
	month = sep,
	year = {2021},
	note = {ISSN: 2325-887X},
	keywords = {Computational modeling, Computer vision, Electrocardiography, Learning systems, Performance gain, Predictive models, Transfer learning},
	pages = {1--4},
}

@article{hodgkin_quantitative_1952,
	title = {A quantitative description of membrane current and its application to conduction and excitation in nerve},
	volume = {117},
	issn = {1469-7793},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1952.sp004764},
	doi = {10.1113/jphysiol.1952.sp004764},
	language = {en},
	number = {4},
	urldate = {2022-01-12},
	journal = {The Journal of Physiology},
	author = {Hodgkin, A. L. and Huxley, A. F.},
	year = {1952},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1952.sp004764},
	pages = {500--544},
}

@article{world_health_organization_chagas_2015,
	title = {Chagas disease in {Latin} {America}: an epidemiological update based on 2010 estimates},
	volume = {90},
	number = {06},
	journal = {Weekly Epidemiological Record= Relevé épidémiologique hebdomadaire},
	author = {{World Health Organization}},
	year = {2015},
	pages = {33--44},
}

@article{rassi_american_2012,
	series = {Tropical {Diseases}},
	title = {American {Trypanosomiasis} ({Chagas} {Disease})},
	volume = {26},
	issn = {0891-5520},
	url = {https://www.sciencedirect.com/science/article/pii/S0891552012000116},
	doi = {10.1016/j.idc.2012.03.002},
	language = {en},
	number = {2},
	urldate = {2021-11-25},
	journal = {Infectious Disease Clinics of North America},
	author = {Rassi, Anis and Rassi, Anis and Marcondes de Rezende, Joffre},
	month = jun,
	year = {2012},
	keywords = {American trypanosomiasis, Chagas disease, Chagas heart disease, Epidemiology, Treatment},
	pages = {275--291},
}

@article{guarner_chagas_2019,
	series = {Emerging infections issue},
	title = {Chagas disease as example of a reemerging parasite},
	volume = {36},
	issn = {0740-2570},
	url = {https://www.sciencedirect.com/science/article/pii/S0740257019300401},
	doi = {10.1053/j.semdp.2019.04.008},
	abstract = {Trypanosoma cruzi, the protozoan that causes Chagas disease, is primarily transmitted by three main Triatomine vectors in endemic areas. However, the infection has become a potential emerging disease because the vector is found in non-endemic areas, there is migration of infected asymptomatic people that can infect the vector, become blood donors, or pass the disease vertically (congenital infections). Lastly, the disease can be acquired through contaminated food (oral transmission). This review will present the different transmission pathways, clinical manifestations, diagnostic modalities and treatment considerations of Chagas disease.},
	language = {en},
	number = {3},
	urldate = {2021-11-25},
	journal = {Seminars in Diagnostic Pathology},
	author = {Guarner, Jeannette},
	month = may,
	year = {2019},
	keywords = {Chagas disease, Trypanosoma cruzi},
	pages = {164--169},
}

@article{nunes_chagas_2013,
	title = {Chagas disease: an overview of clinical and epidemiological aspects},
	volume = {62},
	issn = {1558-3597},
	shorttitle = {Chagas disease},
	doi = {10.1016/j.jacc.2013.05.046},
	abstract = {Chagas disease, caused by the parasite Trypanosoma cruzi, is a serious health problem in Latin America and is an emerging disease in non-endemic countries. In recent decades, the epidemiological profile of the disease has changed due to new patterns of immigration and successful control in its transmission, leading to the urbanization and globalization of the disease. Dilated cardiomyopathy is the most important and severe manifestation of human chronic Chagas disease and is characterized by heart failure, ventricular arrhythmias, heart blocks, thromboembolic phenomena, and sudden death. This article will present an overview of the clinical and epidemiological aspects of Chagas disease. It will focus on several clinical aspects of the disease, such as chronic Chagas disease without detectable cardiac pathology, as well as dysautonomia, some specific features, and the principles of treatment of chronic cardiomyopathy.},
	language = {eng},
	number = {9},
	journal = {Journal of the American College of Cardiology},
	author = {Nunes, Maria Carmo Pereira and Dones, Wistremundo and Morillo, Carlos A. and Encina, Juan Justiniano and Ribeiro, Antônio Luiz and {Council on Chagas Disease of the Interamerican Society of Cardiology}},
	month = aug,
	year = {2013},
	pmid = {23770163},
	keywords = {ChD, Chagas Disease, Chagas disease, Death, Sudden, Cardiac, E/e', ECG, Humans, ICD, LV, Prognosis, VT, dilated cardiomyopathy, early transmitral flow velocity to the early diastolic velocity of the mitral annulus, electrocardiogram, heart failure, implantable cardioverter-defibrillator, left ventricle/ventricular, ventricular tachycardia},
	pages = {767--776},
}

@article{sabino_ten-year_2013,
	title = {Ten-{Year} {Incidence} of {Chagas} {Cardiomyopathy} {Among} {Asymptomatic} {Trypanosoma} cruzi–{Seropositive} {Former} {Blood} {Donors}},
	volume = {127},
	url = {https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.112.123612},
	doi = {10.1161/CIRCULATIONAHA.112.123612},
	abstract = {Background—

Very few studies have measured disease penetrance and prognostic factors of Chagas cardiomyopathy among asymptomatic Trypanosoma cruzi–infected persons.

Methods and Results—

We performed a retrospective cohort study among initially healthy blood donors with an index T cruzi–seropositive donation and age-, sex-, and period-matched seronegatives in 1996 to 2002 in the Brazilian cities of São Paulo and Montes Claros. In 2008 to 2010, all subjects underwent medical history, physical examination, ECGs, and echocardiograms. ECG and echocardiogram results were classified by blinded core laboratories, and records with abnormal results were reviewed by a blinded panel of 3 cardiologists who adjudicated the outcome of Chagas cardiomyopathy. Associations with Chagas cardiomyopathy were tested with multivariate logistic regression. Mean follow-up time between index donation and outcome assessment was 10.5 years for the seropositives and 11.1 years for the seronegatives. Among 499 T cruzi seropositives, 120 (24\%) had definite Chagas cardiomyopathy, and among 488 T cruzi seronegatives, 24 (5\%) had cardiomyopathy, for an incidence difference of 1.85 per 100 person-years attributable to T cruzi infection. Of the 120 seropositives classified as having Chagas cardiomyopathy, only 31 (26\%) presented with ejection fraction {\textless}50\%, and only 11 (9\%) were classified as New York Heart Association class II or higher. Chagas cardiomyopathy was associated (P{\textless}0.01) with male sex, a history of abnormal ECG, and the presence of an S3 heart sound.

Conclusions—

There is a substantial annual incidence of Chagas cardiomyopathy among initially asymptomatic T cruzi–seropositive blood donors, although disease was mild at diagnosis.},
	number = {10},
	urldate = {2021-11-25},
	journal = {Circulation},
	author = {Sabino, Ester C. and Ribeiro, Antonio L. and Salemi, Vera M.C. and Di Lorenzo Oliveira, Claudia and Antunes, Andre P. and Menezes, Marcia M. and Ianni, Barbara M. and Nastari, Luciano and Fernandes, Fabio and Patavino, Giuseppina M. and Sachdev, Vandana and Capuani, Ligia and de Almeida-Neto, Cesar and Carrick, Danielle M. and Wright, David and Kavounis, Katherine and Goncalez, Thelma T. and Carneiro-Proietti, Anna Barbara and Custer, Brian and Busch, Michael P. and Murphy, Edward L.},
	month = mar,
	year = {2013},
	note = {Publisher: American Heart Association},
	keywords = {Brazil, Chagas cardiomyopathy, Chagas disease, blood donors, incidence},
	pages = {1105--1115},
}

@article{james_global_2018,
	title = {Global, regional, and national incidence, prevalence, and years lived with disability for 354 diseases and injuries for 195 countries and territories, 1990–2017: a systematic analysis for the {Global} {Burden} of {Disease} {Study} 2017},
	volume = {392},
	issn = {0140-6736},
	shorttitle = {Global, regional, and national incidence, prevalence, and years lived with disability for 354 diseases and injuries for 195 countries and territories, 1990–2017},
	url = {https://www.sciencedirect.com/science/article/pii/S0140673618322797},
	doi = {10.1016/S0140-6736(18)32279-7},
	abstract = {Background
The Global Burden of Diseases, Injuries, and Risk Factors Study 2017 (GBD 2017) includes a comprehensive assessment of incidence, prevalence, and years lived with disability (YLDs) for 354 causes in 195 countries and territories from 1990 to 2017. Previous GBD studies have shown how the decline of mortality rates from 1990 to 2016 has led to an increase in life expectancy, an ageing global population, and an expansion of the non-fatal burden of disease and injury. These studies have also shown how a substantial portion of the world's population experiences non-fatal health loss with considerable heterogeneity among different causes, locations, ages, and sexes. Ongoing objectives of the GBD study include increasing the level of estimation detail, improving analytical strategies, and increasing the amount of high-quality data.
Methods
We estimated incidence and prevalence for 354 diseases and injuries and 3484 sequelae. We used an updated and extensive body of literature studies, survey data, surveillance data, inpatient admission records, outpatient visit records, and health insurance claims, and additionally used results from cause of death models to inform estimates using a total of 68 781 data sources. Newly available clinical data from India, Iran, Japan, Jordan, Nepal, China, Brazil, Norway, and Italy were incorporated, as well as updated claims data from the USA and new claims data from Taiwan (province of China) and Singapore. We used DisMod-MR 2.1, a Bayesian meta-regression tool, as the main method of estimation, ensuring consistency between rates of incidence, prevalence, remission, and cause of death for each condition. YLDs were estimated as the product of a prevalence estimate and a disability weight for health states of each mutually exclusive sequela, adjusted for comorbidity. We updated the Socio-demographic Index (SDI), a summary development indicator of income per capita, years of schooling, and total fertility rate. Additionally, we calculated differences between male and female YLDs to identify divergent trends across sexes. GBD 2017 complies with the Guidelines for Accurate and Transparent Health Estimates Reporting.
Findings
Globally, for females, the causes with the greatest age-standardised prevalence were oral disorders, headache disorders, and haemoglobinopathies and haemolytic anaemias in both 1990 and 2017. For males, the causes with the greatest age-standardised prevalence were oral disorders, headache disorders, and tuberculosis including latent tuberculosis infection in both 1990 and 2017. In terms of YLDs, low back pain, headache disorders, and dietary iron deficiency were the leading Level 3 causes of YLD counts in 1990, whereas low back pain, headache disorders, and depressive disorders were the leading causes in 2017 for both sexes combined. All-cause age-standardised YLD rates decreased by 3·9\% (95\% uncertainty interval [UI] 3·1–4·6) from 1990 to 2017; however, the all-age YLD rate increased by 7·2\% (6·0–8·4) while the total sum of global YLDs increased from 562 million (421–723) to 853 million (642–1100). The increases for males and females were similar, with increases in all-age YLD rates of 7·9\% (6·6–9·2) for males and 6·5\% (5·4–7·7) for females. We found significant differences between males and females in terms of age-standardised prevalence estimates for multiple causes. The causes with the greatest relative differences between sexes in 2017 included substance use disorders (3018 cases [95\% UI 2782–3252] per 100 000 in males vs s1400 [1279–1524] per 100 000 in females), transport injuries (3322 [3082–3583] vs 2336 [2154–2535]), and self-harm and interpersonal violence (3265 [2943–3630] vs 5643 [5057–6302]).
Interpretation
Global all-cause age-standardised YLD rates have improved only slightly over a period spanning nearly three decades. However, the magnitude of the non-fatal disease burden has expanded globally, with increasing numbers of people who have a wide spectrum of conditions. A subset of conditions has remained globally pervasive since 1990, whereas other conditions have displayed more dynamic trends, with different ages, sexes, and geographies across the globe experiencing varying burdens and trends of health loss. This study emphasises how global improvements in premature mortality for select conditions have led to older populations with complex and potentially expensive diseases, yet also highlights global achievements in certain domains of disease and injury.
Funding
Bill \& Melinda Gates Foundation.},
	language = {en},
	number = {10159},
	urldate = {2021-11-25},
	journal = {The Lancet},
	author = {James, Spencer L and Abate, Degu and Abate, Kalkidan Hassen and Abay, Solomon M and Abbafati, Cristiana and Abbasi, Nooshin and Abbastabar, Hedayat and Abd-Allah, Foad and Abdela, Jemal and Abdelalim, Ahmed and Abdollahpour, Ibrahim and Abdulkader, Rizwan Suliankatchi and Abebe, Zegeye and Abera, Semaw F and Abil, Olifan Zewdie and Abraha, Haftom Niguse and Abu-Raddad, Laith Jamal and Abu-Rmeileh, Niveen M E and Accrombessi, Manfred Mario Kokou and Acharya, Dilaram and Acharya, Pawan and Ackerman, Ilana N and Adamu, Abdu A and Adebayo, Oladimeji M and Adekanmbi, Victor and Adetokunboh, Olatunji O and Adib, Mina G and Adsuar, Jose C and Afanvi, Kossivi Agbelenko and Afarideh, Mohsen and Afshin, Ashkan and Agarwal, Gina and Agesa, Kareha M and Aggarwal, Rakesh and Aghayan, Sargis Aghasi and Agrawal, Sutapa and Ahmadi, Alireza and Ahmadi, Mehdi and Ahmadieh, Hamid and Ahmed, Muktar Beshir and Aichour, Amani Nidhal and Aichour, Ibtihel and Aichour, Miloud Taki Eddine and Akinyemiju, Tomi and Akseer, Nadia and Al-Aly, Ziyad and Al-Eyadhy, Ayman and Al-Mekhlafi, Hesham M and Al-Raddadi, Rajaa M and Alahdab, Fares and Alam, Khurshid and Alam, Tahiya and Alashi, Alaa and Alavian, Seyed Moayed and Alene, Kefyalew Addis and Alijanzadeh, Mehran and Alizadeh-Navaei, Reza and Aljunid, Syed Mohamed and Alkerwi, Ala'a and Alla, François and Allebeck, Peter and Alouani, Mohamed M L and Altirkawi, Khalid and Alvis-Guzman, Nelson and Amare, Azmeraw T and Aminde, Leopold N and Ammar, Walid and Amoako, Yaw Ampem and Anber, Nahla Hamed and Andrei, Catalina Liliana and Androudi, Sofia and Animut, Megbaru Debalkie and Anjomshoa, Mina and Ansha, Mustafa Geleto and Antonio, Carl Abelardo T and Anwari, Palwasha and Arabloo, Jalal and Arauz, Antonio and Aremu, Olatunde and Ariani, Filippo and Armoon, Bahroom and Ärnlöv, Johan and Arora, Amit and Artaman, Al and Aryal, Krishna K and Asayesh, Hamid and Asghar, Rana Jawad and Ataro, Zerihun and Atre, Sachin R and Ausloos, Marcel and Avila-Burgos, Leticia and Avokpaho, Euripide F G A and Awasthi, Ashish and Ayala Quintanilla, Beatriz Paulina and Ayer, Rakesh and Azzopardi, Peter S and Babazadeh, Arefeh and Badali, Hamid and Badawi, Alaa and Bali, Ayele Geleto and Ballesteros, Katherine E and Ballew, Shoshana H and Banach, Maciej and Banoub, Joseph Adel Mattar and Banstola, Amrit and Barac, Aleksandra and Barboza, Miguel A and Barker-Collo, Suzanne Lyn and Bärnighausen, Till Winfried and Barrero, Lope H and Baune, Bernhard T and Bazargan-Hejazi, Shahrzad and Bedi, Neeraj and Beghi, Ettore and Behzadifar, Masoud and Behzadifar, Meysam and Béjot, Yannick and Belachew, Abate Bekele and Belay, Yihalem Abebe and Bell, Michelle L and Bello, Aminu K and Bensenor, Isabela M and Bernabe, Eduardo and Bernstein, Robert S and Beuran, Mircea and Beyranvand, Tina and Bhala, Neeraj and Bhattarai, Suraj and Bhaumik, Soumyadeep and Bhutta, Zulfiqar A and Biadgo, Belete and Bijani, Ali and Bikbov, Boris and Bilano, Ver and Bililign, Nigus and Bin Sayeed, Muhammad Shahdaat and Bisanzio, Donal and Blacker, Brigette F and Blyth, Fiona M and Bou-Orm, Ibrahim R and Boufous, Soufiane and Bourne, Rupert and Brady, Oliver J and Brainin, Michael and Brant, Luisa C and Brazinova, Alexandra and Breitborde, Nicholas J K and Brenner, Hermann and Briant, Paul Svitil and Briggs, Andrew M and Briko, Andrey Nikolaevich and Britton, Gabrielle and Brugha, Traolach and Buchbinder, Rachelle and Busse, Reinhard and Butt, Zahid A and Cahuana-Hurtado, Lucero and Cano, Jorge and Cárdenas, Rosario and Carrero, Juan J and Carter, Austin and Carvalho, Félix and Castañeda-Orjuela, Carlos A and Castillo Rivas, Jacqueline and Castro, Franz and Catalá-López, Ferrán and Cercy, Kelly M and Cerin, Ester and Chaiah, Yazan and Chang, Alex R and Chang, Hsing-Yi and Chang, Jung-Chen and Charlson, Fiona J and Chattopadhyay, Aparajita and Chattu, Vijay Kumar and Chaturvedi, Pankaj and Chiang, Peggy Pei-Chia and Chin, Ken Lee and Chitheer, Abdulaal and Choi, Jee-Young J and Chowdhury, Rajiv and Christensen, Hanne and Christopher, Devasahayam J and Cicuttini, Flavia M and Ciobanu, Liliana G and Cirillo, Massimo and Claro, Rafael M and Collado-Mateo, Daniel and Cooper, Cyrus and Coresh, Josef and Cortesi, Paolo Angelo and Cortinovis, Monica and Costa, Megan and Cousin, Ewerton and Criqui, Michael H and Cromwell, Elizabeth A and Cross, Marita and Crump, John A and Dadi, Abel Fekadu and Dandona, Lalit and Dandona, Rakhi and Dargan, Paul I and Daryani, Ahmad and Das Gupta, Rajat and Das Neves, José and Dasa, Tamirat Tesfaye and Davey, Gail and Davis, Adrian C and Davitoiu, Dragos Virgil and De Courten, Barbora and De La Hoz, Fernando Pio and De Leo, Diego and De Neve, Jan-Walter and Degefa, Meaza Girma and Degenhardt, Louisa and Deiparine, Selina and Dellavalle, Robert P and Demoz, Gebre Teklemariam and Deribe, Kebede and Dervenis, Nikolaos and Des Jarlais, Don C and Dessie, Getenet Ayalew and Dey, Subhojit and Dharmaratne, Samath Dhamminda and Dinberu, Mesfin Tadese and Dirac, M Ashworth and Djalalinia, Shirin and Doan, Linh and Dokova, Klara and Doku, David Teye and Dorsey, E Ray and Doyle, Kerrie E and Driscoll, Tim Robert and Dubey, Manisha and Dubljanin, Eleonora and Duken, Eyasu Ejeta and Duncan, Bruce B and Duraes, Andre R and Ebrahimi, Hedyeh and Ebrahimpour, Soheil and Echko, Michelle Marie and Edvardsson, David and Effiong, Andem and Ehrlich, Joshua R and El Bcheraoui, Charbel and El Sayed Zaki, Maysaa and El-Khatib, Ziad and Elkout, Hajer and Elyazar, Iqbal R F and Enayati, Ahmadali and Endries, Aman Yesuf and Er, Benjamin and Erskine, Holly E and Eshrati, Babak and Eskandarieh, Sharareh and Esteghamati, Alireza and Esteghamati, Sadaf and Fakhim, Hamed and Fallah Omrani, Vahid and Faramarzi, Mahbobeh and Fareed, Mohammad and Farhadi, Farzaneh and Farid, Talha A and Farinha, Carla Sofia E sá and Farioli, Andrea and Faro, Andre and Farvid, Maryam S and Farzadfar, Farshad and Feigin, Valery L and Fentahun, Netsanet and Fereshtehnejad, Seyed-Mohammad and Fernandes, Eduarda and Fernandes, Joao C and Ferrari, Alize J and Feyissa, Garumma Tolu and Filip, Irina and Fischer, Florian and Fitzmaurice, Christina and Foigt, Nataliya A and Foreman, Kyle J and Fox, Jack and Frank, Tahvi D and Fukumoto, Takeshi and Fullman, Nancy and Fürst, Thomas and Furtado, João M and Futran, Neal D and Gall, Seana and Ganji, Morsaleh and Gankpe, Fortune Gbetoho and Garcia-Basteiro, Alberto L and Gardner, William M and Gebre, Abadi Kahsu and Gebremedhin, Amanuel Tesfay and Gebremichael, Teklu Gebrehiwo and Gelano, Tilayie Feto and Geleijnse, Johanna M and Genova-Maleras, Ricard and Geramo, Yilma Chisha Dea and Gething, Peter W and Gezae, Kebede Embaye and Ghadiri, Keyghobad and Ghasemi Falavarjani, Khalil and Ghasemi-Kasman, Maryam and Ghimire, Mamata and Ghosh, Rakesh and Ghoshal, Aloke Gopal and Giampaoli, Simona and Gill, Paramjit Singh and Gill, Tiffany K and Ginawi, Ibrahim Abdelmageed and Giussani, Giorgia and Gnedovskaya, Elena V and Goldberg, Ellen M and Goli, Srinivas and Gómez-Dantés, Hector and Gona, Philimon N and Gopalani, Sameer Vali and Gorman, Taren M and Goulart, Alessandra C and Goulart, Bárbara Niegia Garcia and Grada, Ayman and Grams, Morgan E and Grosso, Giuseppe and Gugnani, Harish Chander and Guo, Yuming and Gupta, Prakash C and Gupta, Rahul and Gupta, Rajeev and Gupta, Tanush and Gyawali, Bishal and Haagsma, Juanita A and Hachinski, Vladimir and Hafezi-Nejad, Nima and Haghparast Bidgoli, Hassan and Hagos, Tekleberhan B and Hailu, Gessessew Bugssa and Haj-Mirzaian, Arvin and Haj-Mirzaian, Arya and Hamadeh, Randah R and Hamidi, Samer and Handal, Alexis J and Hankey, Graeme J and Hao, Yuantao and Harb, Hilda L and Harikrishnan, Sivadasanpillai and Haro, Josep Maria and Hasan, Mehedi and Hassankhani, Hadi and Hassen, Hamid Yimam and Havmoeller, Rasmus and Hawley, Caitlin N and Hay, Roderick J and Hay, Simon I and Hedayatizadeh-Omran, Akbar and Heibati, Behzad and Hendrie, Delia and Henok, Andualem and Herteliu, Claudiu and Heydarpour, Sousan and Hibstu, Desalegn Tsegaw and Hoang, Huong Thanh and Hoek, Hans W and Hoffman, Howard J and Hole, Michael K and Homaie Rad, Enayatollah and Hoogar, Praveen and Hosgood, H Dean and Hosseini, Seyed Mostafa and Hosseinzadeh, Mehdi and Hostiuc, Mihaela and Hostiuc, Sorin and Hotez, Peter J and Hoy, Damian G and Hsairi, Mohamed and Htet, Aung Soe and Hu, Guoqing and Huang, John J and Huynh, Chantal K and Iburg, Kim Moesgaard and Ikeda, Chad Thomas and Ileanu, Bogdan and Ilesanmi, Olayinka Stephen and Iqbal, Usman and Irvani, Seyed Sina Naghibi and Irvine, Caleb Mackay Salpeter and Islam, Sheikh Mohammed Shariful and Islami, Farhad and Jacobsen, Kathryn H and Jahangiry, Leila and Jahanmehr, Nader and Jain, Sudhir Kumar and Jakovljevic, Mihajlo and Javanbakht, Mehdi and Jayatilleke, Achala Upendra and Jeemon, Panniyammakal and Jha, Ravi Prakash and Jha, Vivekanand and Ji, John S and Johnson, Catherine O and Jonas, Jost B and Jozwiak, Jacek Jerzy and Jungari, Suresh Banayya and Jürisson, Mikk and Kabir, Zubair and Kadel, Rajendra and Kahsay, Amaha and Kalani, Rizwan and Kanchan, Tanuj and Karami, Manoochehr and Karami Matin, Behzad and Karch, André and Karema, Corine and Karimi, Narges and Karimi, Seyed M and Kasaeian, Amir and Kassa, Dessalegn H and Kassa, Getachew Mullu and Kassa, Tesfaye Dessale and Kassebaum, Nicholas J and Katikireddi, Srinivasa Vittal and Kawakami, Norito and Karyani, Ali Kazemi and Keighobadi, Masoud Masoud and Keiyoro, Peter Njenga and Kemmer, Laura and Kemp, Grant Rodgers and Kengne, Andre Pascal and Keren, Andre and Khader, Yousef Saleh and Khafaei, Behzad and Khafaie, Morteza Abdullatif and Khajavi, Alireza and Khalil, Ibrahim A and Khan, Ejaz Ahmad and Khan, Muhammad Shahzeb and Khan, Muhammad Ali and Khang, Young-Ho and Khazaei, Mohammad and Khoja, Abdullah T and Khosravi, Ardeshir and Khosravi, Mohammad Hossein and Kiadaliri, Aliasghar A and Kiirithio, Daniel N and Kim, Cho-Il and Kim, Daniel and Kim, Pauline and Kim, Young-Eun and Kim, Yun Jin and Kimokoti, Ruth W and Kinfu, Yohannes and Kisa, Adnan and Kissimova-Skarbek, Katarzyna and Kivimäki, Mika and Knudsen, Ann Kristin Skrindo and Kocarnik, Jonathan M and Kochhar, Sonali and Kokubo, Yoshihiro and Kolola, Tufa and Kopec, Jacek A and Kosen, Soewarta and Kotsakis, Georgios A and Koul, Parvaiz A and Koyanagi, Ai and Kravchenko, Michael A and Krishan, Kewal and Krohn, Kristopher J and Kuate Defo, Barthelemy and Kucuk Bicer, Burcu and Kumar, G Anil and Kumar, Manasi and Kyu, Hmwe Hmwe and Lad, Deepesh P and Lad, Sheetal D and Lafranconi, Alessandra and Lalloo, Ratilal and Lallukka, Tea and Lami, Faris Hasan and Lansingh, Van C and Latifi, Arman and Lau, Kathryn Mei-Ming and Lazarus, Jeffrey V and Leasher, Janet L and Ledesma, Jorge R and Lee, Paul H and Leigh, James and Leung, Janni and Levi, Miriam and Lewycka, Sonia and Li, Shanshan and Li, Yichong and Liao, Yu and Liben, Misgan Legesse and Lim, Lee-Ling and Lim, Stephen S and Liu, Shiwei and Lodha, Rakesh and Looker, Katharine J and Lopez, Alan D and Lorkowski, Stefan and Lotufo, Paulo A and Low, Nicola and Lozano, Rafael and Lucas, Tim C D and Lucchesi, Lydia R and Lunevicius, Raimundas and Lyons, Ronan A and Ma, Stefan and Macarayan, Erlyn Rachelle King and Mackay, Mark T and Madotto, Fabiana and Magdy Abd El Razek, Hassan and Magdy Abd El Razek, Muhammed and Maghavani, Dhaval P and Mahotra, Narayan Bahadur and Mai, Hue Thi and Majdan, Marek and Majdzadeh, Reza and Majeed, Azeem and Malekzadeh, Reza and Malta, Deborah Carvalho and Mamun, Abdullah A and Manda, Ana-Laura and Manguerra, Helena and Manhertz, Treh and Mansournia, Mohammad Ali and Mantovani, Lorenzo Giovanni and Mapoma, Chabila Christopher and Maravilla, Joemer C and Marcenes, Wagner and Marks, Ashley and Martins-Melo, Francisco Rogerlândio and Martopullo, Ira and März, Winfried and Marzan, Melvin B and Mashamba-Thompson, Tivani Phosa and Massenburg, Benjamin Ballard and Mathur, Manu Raj and Matsushita, Kunihiro and Maulik, Pallab K and Mazidi, Mohsen and McAlinden, Colm and McGrath, John J and McKee, Martin and Mehndiratta, Man Mohan and Mehrotra, Ravi and Mehta, Kala M and Mehta, Varshil and Mejia-Rodriguez, Fabiola and Mekonen, Tesfa and Melese, Addisu and Melku, Mulugeta and Meltzer, Michele and Memiah, Peter T N and Memish, Ziad A and Mendoza, Walter and Mengistu, Desalegn Tadese and Mengistu, Getnet and Mensah, George A and Mereta, Seid Tiku and Meretoja, Atte and Meretoja, Tuomo J and Mestrovic, Tomislav and Mezerji, Naser Mohammad Gholi and Miazgowski, Bartosz and Miazgowski, Tomasz and Millear, Anoushka I and Miller, Ted R and Miltz, Benjamin and Mini, G K and Mirarefin, Mojde and Mirrakhimov, Erkin M and Misganaw, Awoke Temesgen and Mitchell, Philip B and Mitiku, Habtamu and Moazen, Babak and Mohajer, Bahram and Mohammad, Karzan Abdulmuhsin and Mohammadifard, Noushin and Mohammadnia-Afrouzi, Mousa and Mohammed, Mohammed A and Mohammed, Shafiu and Mohebi, Farnam and Moitra, Modhurima and Mokdad, Ali H and Molokhia, Mariam and Monasta, Lorenzo and Moodley, Yoshan and Moosazadeh, Mahmood and Moradi, Ghobad and Moradi-Lakeh, Maziar and Moradinazar, Mehdi and Moraga, Paula and Morawska, Lidia and Moreno Velásquez, Ilais and Morgado-Da-Costa, Joana and Morrison, Shane Douglas and Moschos, Marilita M and Mountjoy-Venning, W Cliff and Mousavi, Seyyed Meysam and Mruts, Kalayu Brhane and Muche, Achenef Asmamaw and Muchie, Kindie Fentahun and Mueller, Ulrich Otto and Muhammed, Oumer Sada and Mukhopadhyay, Satinath and Muller, Kate and Mumford, John Everett and Murhekar, Manoj and Musa, Jonah and Musa, Kamarul Imran and Mustafa, Ghulam and Nabhan, Ashraf F and Nagata, Chie and Naghavi, Mohsen and Naheed, Aliya and Nahvijou, Azin and Naik, Gurudatta and Naik, Nitish and Najafi, Farid and Naldi, Luigi and Nam, Hae Sung and Nangia, Vinay and Nansseu, Jobert Richie and Nascimento, Bruno Ramos and Natarajan, Gopalakrishnan and Neamati, Nahid and Negoi, Ionut and Negoi, Ruxandra Irina and Neupane, Subas and Newton, Charles Richard James and Ngunjiri, Josephine W and Nguyen, Anh Quynh and Nguyen, Ha Thu and Nguyen, Huong Lan Thi and Nguyen, Huong Thanh and Nguyen, Long Hoang and Nguyen, Minh and Nguyen, Nam Ba and Nguyen, Son Hoang and Nichols, Emma and Ningrum, Dina Nur Anggraini and Nixon, Molly R and Nolutshungu, Nomonde and Nomura, Shuhei and Norheim, Ole F and Noroozi, Mehdi and Norrving, Bo and Noubiap, Jean Jacques and Nouri, Hamid Reza and Nourollahpour Shiadeh, Malihe and Nowroozi, Mohammad Reza and Nsoesie, Elaine O and Nyasulu, Peter S and Odell, Christopher M and Ofori-Asenso, Richard and Ogbo, Felix Akpojene and Oh, In-Hwan and Oladimeji, Olanrewaju and Olagunju, Andrew T and Olagunju, Tinuke O and Olivares, Pedro R and Olsen, Helen Elizabeth and Olusanya, Bolajoko Olubukunola and Ong, Kanyin L and Ong, Sok King and Oren, Eyal and Ortiz, Alberto and Ota, Erika and Otstavnov, Stanislav S and Øverland, Simon and Owolabi, Mayowa Ojo and P a, Mahesh and Pacella, Rosana and Pakpour, Amir H and Pana, Adrian and Panda-Jonas, Songhomitra and Parisi, Andrea and Park, Eun-Kee and Parry, Charles D H and Patel, Shanti and Pati, Sanghamitra and Patil, Snehal T and Patle, Ajay and Patton, George C and Paturi, Vishnupriya Rao and Paulson, Katherine R and Pearce, Neil and Pereira, David M and Perico, Norberto and Pesudovs, Konrad and Pham, Hai Quang and Phillips, Michael R and Pigott, David M and Pillay, Julian David and Piradov, Michael A and Pirsaheb, Meghdad and Pishgar, Farhad and Plana-Ripoll, Oleguer and Plass, Dietrich and Polinder, Suzanne and Popova, Svetlana and Postma, Maarten J and Pourshams, Akram and Poustchi, Hossein and Prabhakaran, Dorairaj and Prakash, Swayam and Prakash, V and Purcell, Caroline A and Purwar, Manorama B and Qorbani, Mostafa and Quistberg, D Alex and Radfar, Amir and Rafay, Anwar and Rafiei, Alireza and Rahim, Fakher and Rahimi, Kazem and Rahimi-Movaghar, Afarin and Rahimi-Movaghar, Vafa and Rahman, Mahfuzar and Rahman, Mohammad Hifz ur and Rahman, Muhammad Aziz and Rahman, Sajjad Ur and Rai, Rajesh Kumar and Rajati, Fatemeh and Ram, Usha and Ranjan, Prabhat and Ranta, Anna and Rao, Puja C and Rawaf, David Laith and Rawaf, Salman and Reddy, K Srinath and Reiner, Robert C and Reinig, Nickolas and Reitsma, Marissa Bettay and Remuzzi, Giuseppe and Renzaho, Andre M N and Resnikoff, Serge and Rezaei, Satar and Rezai, Mohammad Sadegh and Ribeiro, Antonio Luiz P and Roberts, Nicholas L S and Robinson, Stephen R and Roever, Leonardo and Ronfani, Luca and Roshandel, Gholamreza and Rostami, Ali and Roth, Gregory A and Roy, Ambuj and Rubagotti, Enrico and Sachdev, Perminder S and Sadat, Nafis and Saddik, Basema and Sadeghi, Ehsan and Saeedi Moghaddam, Sahar and Safari, Hosein and Safari, Yahya and Safari-Faramani, Roya and Safdarian, Mahdi and Safi, Sare and Safiri, Saeid and Sagar, Rajesh and Sahebkar, Amirhossein and Sahraian, Mohammad Ali and Sajadi, Haniye Sadat and Salam, Nasir and Salama, Joseph S and Salamati, Payman and Saleem, Komal and Saleem, Zikria and Salimi, Yahya and Salomon, Joshua A and Salvi, Sundeep Santosh and Salz, Inbal and Samy, Abdallah M and Sanabria, Juan and Sang, Yingying and Santomauro, Damian Francesco and Santos, Itamar S and Santos, João Vasco and Santric Milicevic, Milena M and Sao Jose, Bruno Piassi and Sardana, Mayank and Sarker, Abdur Razzaque and Sarrafzadegan, Nizal and Sartorius, Benn and Sarvi, Shahabeddin and Sathian, Brijesh and Satpathy, Maheswar and Sawant, Arundhati R and Sawhney, Monika and Saxena, Sonia and Saylan, Mete and Schaeffner, Elke and Schmidt, Maria Inês and Schneider, Ione J C and Schöttker, Ben and Schwebel, David C and Schwendicke, Falk and Scott, James G and Sekerija, Mario and Sepanlou, Sadaf G and Serván-Mori, Edson and Seyedmousavi, Seyedmojtaba and Shabaninejad, Hosein and Shafieesabet, Azadeh and Shahbazi, Mehdi and Shaheen, Amira A and Shaikh, Masood Ali and Shams-Beyranvand, Mehran and Shamsi, Mohammadbagher and Shamsizadeh, Morteza and Sharafi, Heidar and Sharafi, Kiomars and Sharif, Mehdi and Sharif-Alhoseini, Mahdi and Sharma, Meenakshi and Sharma, Rajesh and She, Jun and Sheikh, Aziz and Shi, Peilin and Shibuya, Kenji and Shigematsu, Mika and Shiri, Rahman and Shirkoohi, Reza and Shishani, Kawkab and Shiue, Ivy and Shokraneh, Farhad and Shoman, Haitham and Shrime, Mark G and Si, Si and Siabani, Soraya and Siddiqi, Tariq J and Sigfusdottir, Inga Dora and Sigurvinsdottir, Rannveig and Silva, João Pedro and Silveira, Dayane Gabriele Alves and Singam, Narayana Sarma Venkata and Singh, Jasvinder A and Singh, Narinder Pal and Singh, Virendra and Sinha, Dhirendra Narain and Skiadaresi, Eirini and Slepak, Erica Leigh N and Sliwa, Karen and Smith, David L and Smith, Mari and Soares Filho, Adauto Martins and Sobaih, Badr Hasan and Sobhani, Soheila and Sobngwi, Eugène and Soneji, Samir S and Soofi, Moslem and Soosaraei, Masoud and Sorensen, Reed J D and Soriano, Joan B and Soyiri, Ireneous N and Sposato, Luciano A and Sreeramareddy, Chandrashekhar T and Srinivasan, Vinay and Stanaway, Jeffrey D and Stein, Dan J and Steiner, Caitlyn and Steiner, Timothy J and Stokes, Mark A and Stovner, Lars Jacob and Subart, Michelle L and Sudaryanto, Agus and Sufiyan, Mu'awiyyah Babale and Sunguya, Bruno F and Sur, Patrick John and Sutradhar, Ipsita and Sykes, Bryan L and Sylte, Dillon O and Tabarés-Seisdedos, Rafael and Tadakamadla, Santosh Kumar and Tadesse, Birkneh Tilahun and Tandon, Nikhil and Tassew, Segen Gebremeskel and Tavakkoli, Mohammad and Taveira, Nuno and Taylor, Hugh R and Tehrani-Banihashemi, Arash and Tekalign, Tigist Gashaw and Tekelemedhin, Shishay Wahdey and Tekle, Merhawi Gebremedhin and Temesgen, Habtamu and Temsah, Mohamad-Hani and Temsah, Omar and Terkawi, Abdullah Sulieman and Teweldemedhin, Mebrahtu and Thankappan, Kavumpurathu Raman and Thomas, Nihal and Tilahun, Binyam and To, Quyen G and Tonelli, Marcello and Topor-Madry, Roman and Topouzis, Fotis and Torre, Anna E and Tortajada-Girbés, Miguel and Touvier, Mathilde and Tovani-Palone, Marcos Roberto and Towbin, Jeffrey A and Tran, Bach Xuan and Tran, Khanh Bao and Troeger, Christopher E and Truelsen, Thomas Clement and Tsilimbaris, Miltiadis K and Tsoi, Derrick and Tudor Car, Lorainne and Tuzcu, E Murat and Ukwaja, Kingsley N and Ullah, Irfan and Undurraga, Eduardo A and Unutzer, Jurgen and Updike, Rachel L and Usman, Muhammad Shariq and Uthman, Olalekan A and Vaduganathan, Muthiah and Vaezi, Afsane and Valdez, Pascual R and Varughese, Santosh and Vasankari, Tommi Juhani and Venketasubramanian, Narayanaswamy and Villafaina, Santos and Violante, Francesco S and Vladimirov, Sergey Konstantinovitch and Vlassov, Vasily and Vollset, Stein Emil and Vosoughi, Kia and Vujcic, Isidora S and Wagnew, Fasil Shiferaw and Waheed, Yasir and Waller, Stephen G and Wang, Yafeng and Wang, Yuan-Pang and Weiderpass, Elisabete and Weintraub, Robert G and Weiss, Daniel J and Weldegebreal, Fitsum and Weldegwergs, Kidu Gidey and Werdecker, Andrea and West, T Eoin and Whiteford, Harvey A and Widecka, Justyna and Wijeratne, Tissa and Wilner, Lauren B and Wilson, Shadrach and Winkler, Andrea Sylvia and Wiyeh, Alison B and Wiysonge, Charles Shey and Wolfe, Charles D A and Woolf, Anthony D and Wu, Shouling and Wu, Yun-Chun and Wyper, Grant M A and Xavier, Denis and Xu, Gelin and Yadgir, Simon and Yadollahpour, Ali and Yahyazadeh Jabbari, Seyed Hossein and Yamada, Tomohide and Yan, Lijing L and Yano, Yuichiro and Yaseri, Mehdi and Yasin, Yasin Jemal and Yeshaneh, Alex and Yimer, Ebrahim M and Yip, Paul and Yisma, Engida and Yonemoto, Naohiro and Yoon, Seok-Jun and Yotebieng, Marcel and Younis, Mustafa Z and Yousefifard, Mahmoud and Yu, Chuanhua and Zadnik, Vesna and Zaidi, Zoubida and Zaman, Sojib Bin and Zamani, Mohammad and Zare, Zohreh and Zeleke, Ayalew Jejaw and Zenebe, Zerihun Menlkalew and Zhang, Kai and Zhao, Zheng and Zhou, Maigeng and Zodpey, Sanjay and Zucker, Inbar and Vos, Theo and Murray, Christopher J L},
	month = nov,
	year = {2018},
	pages = {1789--1858},
}

@article{laranja_chagas_1956,
	title = {Chagas' {Disease}: {A} {Clinical}, {Epidemiologic}, and {Pathologic} {Study}},
	volume = {14},
	issn = {0009-7322, 1524-4539},
	shorttitle = {Chagas' {Disease}},
	url = {https://www.ahajournals.org/doi/10.1161/01.CIR.14.6.1035},
	doi = {10.1161/01.CIR.14.6.1035},
	abstract = {A study of the most important clinical and pathologic aspects of Chagas' disease has been presented, on the basis of the analysis of 180 cases of acute infection (11 with autopsy), 657 cases of chronic asymptomatic infection, and 683 cases of chronic Chagas' heart disease (21 autopsied cases with
              Schizotrypanum cruzi
              in myocardium).},
	language = {en},
	number = {6},
	urldate = {2021-11-25},
	journal = {Circulation},
	author = {Laranja, F. S. and Dias, E. and Nobrega, G. and Miranda, A.},
	month = dec,
	year = {1956},
	pages = {1035--1060},
}

@article{marin-neto_pathogenesis_2007,
	title = {Pathogenesis of {Chronic} {Chagas} {Heart} {Disease}},
	volume = {115},
	url = {https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.106.624296},
	doi = {10.1161/CIRCULATIONAHA.106.624296},
	abstract = {Background— Chagas disease remains a significant public health issue and a major cause of morbidity and mortality in Latin America. Despite nearly 1 century of research, the pathogenesis of chronic Chagas cardiomyopathy is incompletely understood, the most intriguing challenge of which is the complex host-parasite interaction.

Methods and Results— A systematic review of the literature found in MEDLINE, EMBASE, BIREME, LILACS, and SCIELO was performed to search for relevant references on pathogenesis and pathophysiology of Chagas disease. Evidence from studies in animal models and in anima nobile points to 4 main pathogenetic mechanisms to explain the development of chronic Chagas heart disease: autonomic nervous system derangements, microvascular disturbances, parasite-dependent myocardial aggression, and immune-mediated myocardial injury. Despite its prominent peculiarities, the role of autonomic derangements and microcirculatory disturbances is probably ancillary among causes of chronic myocardial damage. The pathogenesis of chronic Chagas heart disease is dependent on a low-grade but incessant systemic infection with documented immune-adverse reaction. Parasite persistence and immunological mechanisms are inextricably related in the myocardial aggression in the chronic phase of Chagas heart disease.

Conclusions— Most clinical studies have been performed in very small number of patients. Future research should explore the clinical potential implications and therapeutic opportunities of these 2 fundamental underlying pathogenetic mechanisms.},
	number = {9},
	urldate = {2021-11-25},
	journal = {Circulation},
	author = {Marin-Neto, Jose Antonio and Cunha-Neto, Edécio and Maciel, Benedito C. and Simões, Marcus V.},
	month = mar,
	year = {2007},
	note = {Publisher: American Heart Association},
	keywords = {Chagas disease, Valsalva maneuver, autonomic nervous system, cardiomyopathy, immune system, inflammation, microcirculation},
	pages = {1109--1123},
}

@article{bern_chagas_2015,
	title = {Chagas' {Disease}},
	volume = {373},
	issn = {1533-4406},
	doi = {10.1056/NEJMra1410150},
	language = {eng},
	number = {5},
	journal = {The New England Journal of Medicine},
	author = {Bern, Caryn},
	month = jul,
	year = {2015},
	pmid = {26222561},
	keywords = {Chagas Disease, Humans, Life Cycle Stages, Prevalence, Trypanocidal Agents, Trypanosoma cruzi},
	pages = {456--466},
}

@article{cardoso_longitudinal_2016,
	title = {Longitudinal study of patients with chronic {Chagas} cardiomyopathy in {Brazil} ({SaMi}-{Trop} project): a cohort profile},
	volume = {6},
	copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://www.bmj.com/company/products-services/rights-and-licensing/. This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/},
	issn = {2044-6055, 2044-6055},
	shorttitle = {Longitudinal study of patients with chronic {Chagas} cardiomyopathy in {Brazil} ({SaMi}-{Trop} project)},
	url = {https://bmjopen.bmj.com/content/6/5/e011181},
	doi = {10.1136/bmjopen-2016-011181},
	abstract = {Purpose We have established a prospective cohort of 1959 patients with chronic Chagas cardiomyopathy to evaluate if a clinical prediction rule based on ECG, brain natriuretic peptide (BNP) levels, and other biomarkers can be useful in clinical practice. This paper outlines the study and baseline characteristics of the participants.
Participants The study is being conducted in 21 municipalities of the northern part of Minas Gerais State in Brazil, and includes a follow-up of 2 years. The baseline evaluation included collection of sociodemographic information, social determinants of health, health-related behaviours, comorbidities, medicines in use, history of previous treatment for Chagas disease, functional class, quality of life, blood sample collection, and ECG. Patients were mostly female, aged 50–74 years, with low family income and educational level, with known Chagas disease for {\textgreater}10 years; 46\% presented with functional class {\textgreater}II. Previous use of benznidazole was reported by 25.2\% and permanent use of pacemaker by 6.2\%. Almost half of the patients presented with high blood cholesterol and hypertension, and one-third of them had diabetes mellitus. N-terminal of the prohormone BNP (NT-ProBNP) level was {\textgreater}300 pg/mL in 30\% of the sample.
Findings to date Clinical and laboratory markers predictive of severe and progressive Chagas disease were identified as high NT-ProBNP levels, as well as symptoms of advanced heart failure. These results confirm the important residual morbidity of Chagas disease in the remote areas, thus supporting political decisions that should prioritise in addition to epidemiological surveillance the medical treatment of chronic Chagas cardiomyopathy in the coming years. The São Paulo-Minas Gerais Tropical Medicine Research Center (SaMi-Trop) represents a major challenge for focused research in neglected diseases, with knowledge that can be applied in primary healthcare.
Future plans We will continue following this patients’ cohort to provide relevant information about the development and progression of Chagas disease in remotes areas, with social and economic inequalities.
Trial registration number NCT02646943; Pre-results.},
	language = {en},
	number = {5},
	urldate = {2021-11-25},
	journal = {BMJ Open},
	author = {Cardoso, Clareci Silva and Sabino, Ester Cerdeira and Oliveira, Claudia Di Lorenzo and Oliveira, Lea Campos de and Ferreira, Ariela Mota and Cunha-Neto, Edécio and Bierrenbach, Ana Luiza and Ferreira, João Eduardo and Haikal, Desirée Sant'Ana and Reingold, Arthur L. and Ribeiro, Antonio Luiz P.},
	month = may,
	year = {2016},
	pmid = {27147390},
	note = {Publisher: British Medical Journal Publishing Group
Section: Cardiovascular medicine},
	keywords = {Biomarkers, CHEMICAL PATHOLOGY, Chagas disease, Cohort Studies},
	pages = {e011181},
}

@article{huang_importance_2021,
	title = {On the {Importance} of {Gradients} for {Detecting} {Distributional} {Shifts} in the {Wild}},
	url = {http://arxiv.org/abs/2110.00218},
	abstract = {Detecting out-of-distribution (OOD) data has become a critical component in ensuring the safe deployment of machine learning models in the real world. Existing OOD detection approaches primarily rely on the output or feature space for deriving OOD scores, while largely overlooking information from the gradient space. In this paper, we present GradNorm, a simple and effective approach for detecting OOD inputs by utilizing information extracted from the gradient space. GradNorm directly employs the vector norm of gradients, backpropagated from the KL divergence between the softmax output and a uniform probability distribution. Our key idea is that the magnitude of gradients is higher for in-distribution (ID) data than that for OOD data, making it informative for OOD detection. GradNorm demonstrates superior performance, reducing the average FPR95 by up to 16.33\% compared to the previous best method.},
	urldate = {2021-11-24},
	journal = {arXiv:2110.00218 [cs]},
	author = {Huang, Rui and Geng, Andrew and Li, Yixuan},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.00218},
	keywords = {Computer Science - Machine Learning},
}

@article{meronen_stationary_2020,
	title = {Stationary {Activations} for {Uncertainty} {Calibration} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2010.09494},
	abstract = {We introduce a new family of non-linear neural network activation functions that mimic the properties induced by the widely-used Mat{\textbackslash}'ern family of kernels in Gaussian process (GP) models. This class spans a range of locally stationary models of various degrees of mean-square differentiability. We show an explicit link to the corresponding GP models in the case that the network consists of one infinitely wide hidden layer. In the limit of infinite smoothness the Mat{\textbackslash}'ern family results in the RBF kernel, and in this case we recover RBF activations. Mat{\textbackslash}'ern activation functions result in similar appealing properties to their counterparts in GP models, and we demonstrate that the local stationarity property together with limited mean-square differentiability shows both good performance and uncertainty calibration in Bayesian deep learning tasks. In particular, local stationarity helps calibrate out-of-distribution (OOD) uncertainty. We demonstrate these properties on classification and regression benchmarks and a radar emitter classification task.},
	urldate = {2021-11-24},
	journal = {arXiv:2010.09494 [cs]},
	author = {Meronen, Lassi and Irwanto, Christabella and Solin, Arno},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.09494},
	keywords = {Computer Science - Machine Learning},
}

@article{genton_classes_2001,
	title = {Classes of {Kernels} for {Machine} {Learning}: {A} {Statistics} {Perspective}},
	volume = {2},
	issn = {ISSN 1533-7928},
	shorttitle = {Classes of {Kernels} for {Machine} {Learning}},
	url = {https://www.jmlr.org/papers/v2/genton01a},
	abstract = {In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.},
	number = {Dec},
	urldate = {2021-11-23},
	journal = {Journal of Machine Learning Research},
	author = {Genton, Marc G.},
	year = {2001},
	pages = {299--312},
}

@article{meronen_periodic_2021,
	title = {Periodic {Activation} {Functions} {Induce} {Stationarity}},
	url = {http://arxiv.org/abs/2110.13572},
	abstract = {Neural network models are known to reinforce hidden data biases, making them unreliable and difficult to interpret. We seek to build models that `know what they do not know' by introducing inductive biases in the function space. We show that periodic activation functions in Bayesian neural networks establish a connection between the prior on the network weights and translation-invariant, stationary Gaussian process priors. Furthermore, we show that this link goes beyond sinusoidal (Fourier) activations by also covering triangular wave and periodic ReLU activation functions. In a series of experiments, we show that periodic activation functions obtain comparable performance for in-domain data and capture sensitivity to perturbed inputs in deep neural networks for out-of-domain detection.},
	urldate = {2021-11-22},
	journal = {arXiv:2110.13572 [cs, stat]},
	author = {Meronen, Lassi and Trapp, Martin and Solin, Arno},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.13572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{novak_neural_2019,
	title = {Neural {Tangents}: {Fast} and {Easy} {Infinite} {Neural} {Networks} in {Python}},
	shorttitle = {Neural {Tangents}},
	url = {http://arxiv.org/abs/1912.02803},
	abstract = {Neural Tangents is a library designed to enable research into infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space. The entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. Neural Tangents is available at www.github.com/google/neural-tangents. We also provide an accompanying interactive Colab notebook.},
	urldate = {2021-11-22},
	journal = {arXiv:1912.02803 [cs, stat]},
	author = {Novak, Roman and Xiao, Lechao and Hron, Jiri and Lee, Jaehoon and Alemi, Alexander A. and Sohl-Dickstein, Jascha and Schoenholz, Samuel S.},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.02803},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{grosse_statistical_2017,
	title = {On the ({Statistical}) {Detection} of {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1702.06280},
	abstract = {Machine Learning (ML) models are applied in a variety of tasks such as network intrusion detection or malware classification. Yet, these models are vulnerable to a class of malicious inputs known as adversarial examples. These are slightly perturbed inputs that are classified incorrectly by the ML model. The mitigation of these adversarial inputs remains an open problem.},
	language = {en},
	urldate = {2021-11-16},
	journal = {arXiv:1702.06280 [cs, stat]},
	author = {Grosse, Kathrin and Manoharan, Praveen and Papernot, Nicolas and Backes, Michael and McDaniel, Patrick},
	month = oct,
	year = {2017},
	note = {arXiv: 1702.06280},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{grosse_statistical_2017-1,
	title = {On the ({Statistical}) {Detection} of {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1702.06280},
	abstract = {Machine Learning (ML) models are applied in a variety of tasks such as network intrusion detection or malware classification. Yet, these models are vulnerable to a class of malicious inputs known as adversarial examples. These are slightly perturbed inputs that are classified incorrectly by the ML model. The mitigation of these adversarial inputs remains an open problem.},
	language = {en},
	urldate = {2021-11-16},
	journal = {arXiv:1702.06280 [cs, stat]},
	author = {Grosse, Kathrin and Manoharan, Praveen and Papernot, Nicolas and Backes, Michael and McDaniel, Patrick},
	month = oct,
	year = {2017},
	note = {arXiv: 1702.06280},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{anand_risk_2021,
	title = {Risk {Assessment} of {Stealthy} {Attacks} on {Uncertain} {Control} {Systems}},
	url = {http://arxiv.org/abs/2106.07071},
	abstract = {In this article, we address the problem of risk assessment of stealthy attacks on uncertain control systems. Considering data injection attacks that aim at maximizing impact while remaining undetected, we use the recently proposed output-to-output gain to characterize the risk associated with the impact of attacks in two setups: A full system knowledge attacker and a limited system knowledge attacker. The risk in each setup is formulated using a well-established risk metric, namely the Value-at-Risk and the maximum expected loss, respectively. Under these setups, the risk assessment problem corresponds to an untractable infinite non-convex optimization problem. To address this limitation, we adopt the framework of scenario-based optimization to approximate the infinite non-convex optimization problem by a sampled non-convex optimization problem. Then, based on the framework of dissipative system theory and S-procedure, the sampled non-convex risk assessment problem is formulated as an equivalent convex semi-definite program. Additionally, we derive the necessary and sufficient conditions for the risk to be bounded. Finally, we illustrate the results through numerical simulation of a hydro-turbine power system.},
	urldate = {2021-11-16},
	journal = {arXiv:2106.07071 [cs, eess, math]},
	author = {Anand, Sribalaji C. and Teixeira, André M. H. and Ahlén, Anders},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.07071},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control},
}

@inproceedings{bhojanapalli_low-rank_2020,
	title = {Low-{Rank} {Bottleneck} in {Multi}-head {Attention} {Models}},
	url = {https://proceedings.mlr.press/v119/bhojanapalli20a.html},
	abstract = {Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. Unfortunately, this leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the current architecture gives rise to a low-rank bottleneck in attention heads, causing this limitation. We further validate this in our experiments. As a solution we propose to set the head size of an attention unit to input sequence length, and independent of the number of heads, resulting in multi-head attention layers with provably more expressive power. We empirically show that this allows us to train models with a relatively smaller embedding dimension and with better performance scaling.},
	language = {en},
	urldate = {2021-11-11},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bhojanapalli, Srinadh and Yun, Chulhee and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {864--873},
}

@article{parhi_role_2020,
	title = {The {Role} of {Neural} {Network} {Activation} {Functions}},
	volume = {27},
	issn = {1070-9908, 1558-2361},
	url = {http://arxiv.org/abs/1910.02333},
	doi = {10.1109/LSP.2020.3027517},
	abstract = {A wide variety of activation functions have been proposed for neural networks. The Rectified Linear Unit (ReLU) is especially popular today. There are many practical reasons that motivate the use of the ReLU. This paper provides new theoretical characterizations that support the use of the ReLU, its variants such as the leaky ReLU, as well as other activation functions in the case of univariate, single-hidden layer feedforward neural networks. Our results also explain the importance of commonly used strategies in the design and training of neural networks such as "weight decay" and "path-norm" regularization, and provide a new justification for the use of "skip connections" in network architectures. These new insights are obtained through the lens of spline theory. In particular, we show how neural network training problems are related to infinite-dimensional optimizations posed over Banach spaces of functions whose solutions are well-known to be fractional and polynomial splines, where the particular Banach space (which controls the order of the spline) depends on the choice of activation function.},
	urldate = {2021-11-05},
	journal = {IEEE Signal Processing Letters},
	author = {Parhi, Rahul and Nowak, Robert D.},
	year = {2020},
	note = {arXiv: 1910.02333},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1779--1783},
}

@article{parhi_what_2021,
	title = {What {Kinds} of {Functions} do {Deep} {Neural} {Networks} {Learn}? {Insights} from {Variational} {Spline} {Theory}},
	shorttitle = {What {Kinds} of {Functions} do {Deep} {Neural} {Networks} {Learn}?},
	url = {http://arxiv.org/abs/2105.03361},
	abstract = {We develop a variational framework to understand the properties of functions learned by fitting deep neural networks with rectified linear unit activations to data. We propose a new function space, which is reminiscent of classical bounded variation-type spaces, that captures the compositional structure associated with deep neural networks. We derive a representer theorem showing that deep ReLU networks are solutions to regularized data fitting problems over functions from this space. The function space consists of compositions of functions from the Banach spaces of second-order bounded variation in the Radon domain. These are Banach spaces with sparsity-promoting norms, giving insight into the role of sparsity in deep neural networks. The neural network solutions have skip connections and rank bounded weight matrices, providing new theoretical support for these common architectural choices. The variational problem we study can be recast as a finite-dimensional neural network training problem with regularization schemes related to the notions of weight decay and path-norm regularization. Finally, our analysis builds on techniques from variational spline theory, providing new connections between deep neural networks and splines.},
	urldate = {2021-11-05},
	journal = {arXiv:2105.03361 [cs, stat]},
	author = {Parhi, Rahul and Nowak, Robert D.},
	month = sep,
	year = {2021},
	note = {arXiv: 2105.03361},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{parhi_near-minimax_2021,
	title = {Near-{Minimax} {Optimal} {Estimation} {With} {Shallow} {ReLU} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2109.08844},
	abstract = {We study the problem of estimating an unknown function from noisy data using shallow (single-hidden layer) ReLU neural networks. The estimators we study minimize the sum of squared data-fitting errors plus a regularization term proportional to the Euclidean norm of the network weights. This minimization corresponds to the common approach of training a neural network with weight decay. We quantify the performance (mean-squared error) of these neural network estimators when the data-generating function belongs to the space of functions of second-order bounded variation in the Radon domain. This space of functions was recently proposed as the natural function space associated with shallow ReLU neural networks. We derive a minimax lower bound for the estimation problem for this function space and show that the neural network estimators are minimax optimal up to logarithmic factors. We also show that this is a "mixed variation" function space that contains classical multivariate function spaces including certain Sobolev spaces and certain spectral Barron spaces. Finally, we use these results to quantify a gap between neural networks and linear methods (which include kernel methods). This paper sheds light on the phenomenon that neural networks seem to break the curse of dimensionality.},
	urldate = {2021-11-05},
	journal = {arXiv:2109.08844 [cs, math, stat]},
	author = {Parhi, Rahul and Nowak, Robert D.},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.08844},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{parhi_banach_2021,
	title = {Banach {Space} {Representer} {Theorems} for {Neural} {Networks} and {Ridge} {Splines}},
	url = {http://arxiv.org/abs/2006.05626},
	abstract = {We develop a variational framework to understand the properties of the functions learned by neural networks fit to data. We propose and study a family of continuous-domain linear inverse problems with total variation-like regularization in the Radon domain subject to data fitting constraints. We derive a representer theorem showing that finite-width, single-hidden layer neural networks are solutions to these inverse problems. We draw on many techniques from variational spline theory and so we propose the notion of polynomial ridge splines, which correspond to single-hidden layer neural networks with truncated power functions as the activation function. The representer theorem is reminiscent of the classical reproducing kernel Hilbert space representer theorem, but we show that the neural network problem is posed over a non-Hilbertian Banach space. While the learning problems are posed in the continuous-domain, similar to kernel methods, the problems can be recast as finite-dimensional neural network training problems. These neural network training problems have regularizers which are related to the well-known weight decay and path-norm regularizers. Thus, our result gives insight into functional characteristics of trained neural networks and also into the design neural network regularizers. We also show that these regularizers promote neural network solutions with desirable generalization properties.},
	urldate = {2021-11-05},
	journal = {arXiv:2006.05626 [cs, stat]},
	author = {Parhi, Rahul and Nowak, Robert D.},
	month = feb,
	year = {2021},
	note = {arXiv: 2006.05626},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{lindholm_machine_2022,
	address = {Cambridge},
	title = {Machine {Learning}: {A} {First} {Course} for {Engineers} and {Scientists}},
	isbn = {978-1-108-84360-7},
	url = {https://smlbook.org},
	abstract = {This book introduces machine learning for readers with some background in basic linear algebra, statistics, probability, and programming. In a coherent statistical framework it covers a selection of supervised machine learning methods, from the most fundamental (k-NN, decision trees, linear and logistic regression) to more advanced methods (deep neural networks, support vector machines, Gaussian processes, random forests and boosting), plus commonly-used unsupervised methods (generative modeling, k-means, PCA, autoencoders and generative adversarial networks). Careful explanations and pseudo-code are presented for all methods. The authors maintain a focus on the fundamentals by drawing connections between methods and discussing general concepts such as loss functions, maximum likelihood, the bias-variance decomposition, ensemble averaging, kernels and the Bayesian approach along with generally useful tools such as regularization, cross validation, evaluation metrics and optimization methods. The final chapters offer practical advice for solving real-world supervised machine learning problems and on ethical aspects of modern machine learning.},
	publisher = {Cambridge University Press},
	author = {Lindholm, Andreas and Wahlström, Niklas and Lindsten, Fredrik and Schön, Thomas B.},
	year = {2022},
}

@article{paixao_electrocardiographic_2021,
	title = {Electrocardiographic {Predictors} of {Mortality}: {Data} from a {Primary} {Care} {Tele}-{Electrocardiography} {Cohort} of {Brazilian} {Patients}},
	volume = {2},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Electrocardiographic {Predictors} of {Mortality}},
	url = {https://www.mdpi.com/2673-3846/2/4/35},
	doi = {10.3390/hearts2040035},
	abstract = {Computerized electrocardiography (ECG) has been widely used and allows linkage to electronic medical records. The present study describes the development and clinical applications of an electronic cohort derived from a digital ECG database obtained by the Telehealth Network of Minas Gerais, Brazil, for the period 2010–2017, linked to the mortality data from the national information system, the Clinical Outcomes in Digital Electrocardiography (CODE) dataset. From 2,470,424 ECGs, 1,773,689 patients were identified. A total of 1,666,778 (94\%) underwent a valid ECG recording for the period 2010 to 2017, with 1,558,421 patients over 16 years old; 40.2\% were men, with a mean age of 51.7 [SD 17.6] years. During a mean follow-up of 3.7 years, the mortality rate was 3.3\%. ECG abnormalities assessed were: atrial fibrillation (AF), right bundle branch block (RBBB), left bundle branch block (LBBB), atrioventricular block (AVB), and ventricular pre-excitation. Most ECG abnormalities (AF: Hazard ratio [HR] 2.10; 95\% CI 2.03–2.17; RBBB: HR 1.32; 95\%CI 1.27–1.36; LBBB: HR 1.69; 95\% CI 1.62–1.76; first degree AVB: Relative survival [RS]: 0.76; 95\% CI0.71–0.81; 2:1 AVB: RS 0.21 95\% CI0.09–0.52; and RS 0.36; third degree AVB: 95\% CI 0.26–0.49) were predictors of overall mortality, except for ventricular pre-excitation (HR 1.41; 95\% CI 0.56–3.57) and Mobitz I AVB (RS 0.65; 95\% CI 0.34–1.24). In conclusion, a large ECG database established by a telehealth network can be a useful tool for facilitating new advances in the fields of digital electrocardiography, clinical cardiology and cardiovascular epidemiology.},
	number = {4},
	urldate = {2021-10-24},
	journal = {Hearts},
	author = {Paixão, Gabriela M. M. and Lima, Emilly M. and Gomes, Paulo R. and Oliveira, Derick M. and Ribeiro, Manoel H. and Nascimento, Jamil S. and Ribeiro, Antonio H. and Macfarlane, Peter W. and Ribeiro, Antonio L. P.},
	month = dec,
	year = {2021},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {big data, electrocardiogram, electronic cohort, mortality, telehealth},
	pages = {449--458},
}

@misc{noauthor_changes_nodate,
	title = {Changes},
	url = {https://docs.google.com/document/u/0/d/1vURop52tULBv271DKzEEOFNZFr9kY1csYlWUTYI_1OU/edit?usp=embed_facebook},
	language = {en-GB},
	urldate = {2021-08-23},
	journal = {Google Docs},
}

@article{combettes_lipschitz_2020,
	title = {Lipschitz {Certificates} for {Layered} {Network} {Structures} {Driven} by {Averaged} {Activation} {Operators}},
	url = {http://arxiv.org/abs/1903.01014},
	abstract = {Obtaining sharp Lipschitz constants for feed-forward neural networks is essential to assess their robustness in the face of perturbations of their inputs. We derive such constants in the context of a general layered network model involving compositions of nonexpansive averaged operators and affine operators. By exploiting this architecture, our analysis finely captures the interactions between the layers, yielding tighter Lipschitz constants than those resulting from the product of individual bounds for groups of layers. The proposed framework is shown to cover in particular many practical instances encountered in feed-forward neural networks. Our Lipschitz constant estimates are further improved in the case of structures employing scalar nonlinear functions, which include standard convolutional networks as special cases.},
	urldate = {2021-08-17},
	journal = {arXiv:1903.01014 [math]},
	author = {Combettes, Patrick L. and Pesquet, Jean-Christophe},
	month = jun,
	year = {2020},
	note = {arXiv: 1903.01014},
	keywords = {Mathematics - Optimization and Control},
}

@article{mahloujifar_curse_2019,
	title = {The {Curse} of {Concentration} in {Robust} {Learning}: {Evasion} and {Poisoning} {Attacks} from {Concentration} of {Measure}},
	shorttitle = {The {Curse} of {Concentration} in {Robust} {Learning}},
	url = {http://arxiv.org/abs/1809.03063},
	abstract = {Many modern machine learning classifiers are shown to be vulnerable to adversarial perturbations of the instances. Despite a massive amount of work focusing on making classifiers robust, the task seems quite challenging. In this work, through a theoretical study, we investigate the adversarial risk and robustness of classifiers and draw a connection to the well-known phenomenon of concentration of measure in metric measure spaces. We show that if the metric probability space of the test instance is concentrated, any classifier with some initial constant error is inherently vulnerable to adversarial perturbations. One class of concentrated metric probability spaces are the so-called Levy families that include many natural distributions. In this special case, our attacks only need to perturb the test instance by at most \$O({\textbackslash}sqrt n)\$ to make it misclassified, where \$n\$ is the data dimension. Using our general result about Levy instance spaces, we first recover as special case some of the previously proved results about the existence of adversarial examples. However, many more Levy families are known (e.g., product distribution under the Hamming distance) for which we immediately obtain new attacks that find adversarial examples of distance \$O({\textbackslash}sqrt n)\$. Finally, we show that concentration of measure for product spaces implies the existence of forms of "poisoning" attacks in which the adversary tampers with the training data with the goal of degrading the classifier. In particular, we show that for any learning algorithm that uses \$m\$ training examples, there is an adversary who can increase the probability of any "bad property" (e.g., failing on a particular test instance) that initially happens with non-negligible probability to \${\textbackslash}approx 1\$ by substituting only \${\textbackslash}tilde\{O\}({\textbackslash}sqrt m)\$ of the examples with other (still correctly labeled) examples.},
	urldate = {2021-08-10},
	journal = {AAAI},
	author = {Mahloujifar, Saeed and Diochnos, Dimitrios I. and Mahmoody, Mohammad},
	year = {2019},
	note = {arXiv: 1809.03063},
	keywords = {Computer Science - Computational Complexity, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{biton_atrial_2021,
	title = {Atrial fibrillation risk prediction from the 12-lead {ECG} using digital biomarkers and deep representation learning},
	copyright = {All rights reserved},
	issn = {2634-3916},
	url = {https://doi.org/10.1093/ehjdh/ztab071},
	doi = {10.1093/ehjdh/ztab071},
	abstract = {This study aims to assess whether information derived from the raw 12-lead electrocardiogram (ECG) combined with clinical information is predictive of atrial fibrillation (AF) development.We use a subset of the Telehealth Network of Minas Gerais (TNMG) database consisting of patients that had repeated 12-lead ECG measurements between 2010-2017 that is 1,130,404 recordings from 415,389 unique patients. Median and interquartile of age for the recordings were 58 (46-69) and 38\% of the patients were males. Recordings were assigned to train-validation and test sets in an 80:20\% split which was stratified by class, age and gender. A random forest classifier was trained to predict, for a given recording, the risk of AF development within 5-years. We use features obtained from different modalities, namely demographics, clinical information, engineered features, and features from deep representation learning.The best model performance on the test set was obtained for the model combining features from all modalities with an AUROC=0.909 against the best single modality model which had an AUROC=0.839.Our study has important clinical implications for AF management. It is the first study integrating feature engineering, deep learning and EMR metadata to create a risk prediction tool for the management of patients at risk of AF. The best model that includes features from all modalities demonstrates that human knowledge in electrophysiology combined with deep learning outperforms any single modality approach. The high performance obtained suggest that structural changes in the 12-lead ECG are associated with existing or impending AF.},
	urldate = {2021-08-10},
	journal = {European Heart Journal - Digital Health},
	author = {Biton, Shany and Gendelman, Sheina and Ribeiro, Antônio H and Miana, Gabriela and Moreira, Carla and Ribeiro, Antonio Luiz P and Behar, Joachim A},
	year = {2021},
}

@inproceedings{mahloujifar_empirically_2019,
	title = {Empirically {Measuring} {Concentration}: {Fundamental} {Limits} on {Intrinsic} {Robustness}},
	abstract = {Many recent works have shown that adversarial examples that fool classiﬁers can be found by minimally perturbing a normal input. Recent theoretical results, starting with Gilmer et al. (2018b), show that if the inputs are drawn from a concentrated metric probability space, then adversarial examples with small perturbation are inevitable. A concentrated space has the property that any subset with Ω(1) (e.g., 1/100) measure, according to the imposed distribution, has small distance to almost all (e.g., 99/100) of the points in the space. It is not clear, however, whether these theoretical results apply to actual distributions such as images. This paper presents a method for empirically measuring and bounding the concentration of a concrete dataset which is proven to converge to the actual concentration. We use it to empirically estimate the intrinsic robustness to ∞ and 2 perturbations of several image classiﬁcation benchmarks. Code for our experiments is available at https://github.com/xiaozhanguva/Measure-Concentration.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	author = {Mahloujifar, Saeed and Zhang, Xiao and Mahmoody, Mohammad and Evans, David},
	year = {2019},
}

@article{prescott_improved_2021,
	title = {Improved {Estimation} of {Concentration} {Under} \${\textbackslash}ell\_p\$-{Norm} {Distance} {Metrics} {Using} {Half} {Spaces}},
	url = {http://arxiv.org/abs/2103.12913},
	abstract = {Concentration of measure has been argued to be the fundamental cause of adversarial vulnerability. Mahloujifar et al. presented an empirical way to measure the concentration of a data distribution using samples, and employed it to find lower bounds on intrinsic robustness for several benchmark datasets. However, it remains unclear whether these lower bounds are tight enough to provide a useful approximation for the intrinsic robustness of a dataset. To gain a deeper understanding of the concentration of measure phenomenon, we first extend the Gaussian Isoperimetric Inequality to non-spherical Gaussian measures and arbitrary \${\textbackslash}ell\_p\$-norms (\$p {\textbackslash}geq 2\$). We leverage these theoretical insights to design a method that uses half-spaces to estimate the concentration of any empirical dataset under \${\textbackslash}ell\_p\$-norm distance metrics. Our proposed algorithm is more efficient than Mahloujifar et al.'s, and our experiments on synthetic datasets and image benchmarks demonstrate that it is able to find much tighter intrinsic robustness bounds. These tighter estimates provide further evidence that rules out intrinsic dataset concentration as a possible explanation for the adversarial vulnerability of state-of-the-art classifiers.},
	urldate = {2021-08-06},
	journal = {arXiv:2103.12913 [cs, stat]},
	author = {Prescott, Jack and Zhang, Xiao and Evans, David},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.12913},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{alemohammad_recurrent_2021,
	title = {The recurrent neural tangent kernel},
	url = {https://openreview.net/forum?id=3T9iFICe0Y9},
	booktitle = {International conference on learning representations},
	author = {Alemohammad, Sina and Wang, Zichao and Balestriero, Randall and Baraniuk, Richard},
	year = {2021},
}

@inproceedings{liu_kernel_2021,
	title = {Kernel regression in high dimensions: {Refined} analysis beyond double descent},
	shorttitle = {Kernel regression in high dimensions},
	url = {http://proceedings.mlr.press/v130/liu21b.html},
	abstract = {In this paper, we provide a precise characterization of generalization properties of high dimensional kernel ridge regression across the under- and over-parameterized regimes, depending on whether...},
	language = {en},
	urldate = {2021-06-29},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Liu, Fanghui and Liao, Zhenyu and Suykens, Johan},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {649--657},
}

@article{far_spectra_2006,
	title = {Spectra of large block matrices},
	url = {http://arxiv.org/abs/cs/0610045},
	abstract = {In a frequency selective slow-fading channel in a MIMO system, the channel matrix is of the form of a block matrix. This paper proposes a method to calculate the limit of the eigenvalue distribution of block matrices if the size of the blocks tends to infinity. While it considers random matrices, it takes an operator-valued free probability approach to achieve this goal. Using this method, one derives a system of equations, which can be solved numerically to compute the desired eigenvalue distribution. The paper initially tackles the problem for square block matrices, then extends the solution to rectangular block matrices. Finally, it deals with Wishart type block matrices. For two special cases, the results of our approach are compared with results from simulations. The first scenario investigates the limit eigenvalue distribution of block Toeplitz matrices. The second scenario deals with the distribution of Wishart type block matrices for a frequency selective slow-fading channel in a MIMO system for two different cases of \$n\_R=n\_T\$ and \$n\_R=2n\_T\$. Using this method, one may calculate the capacity and the Signal-to-Interference-and-Noise Ratio in large MIMO systems.},
	urldate = {2021-06-28},
	journal = {arXiv:cs/0610045},
	author = {Far, Reza Rashidi and Oraby, Tamer and Bryc, Wlodzimierz and Speicher, Roland},
	month = oct,
	year = {2006},
	note = {arXiv: cs/0610045},
	keywords = {Computer Science - Information Theory, H.1.1, Mathematics - Operator Algebras},
}

@article{adlam_random_2019,
	title = {A {Random} {Matrix} {Perspective} on {Mixtures} of {Nonlinearities} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/1912.00827},
	abstract = {One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features \$F=f(WX+B)\$ for a random weight matrix \$W\$ and random bias vector \$B\$, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis directly generalizes to such distributions, even those not expressible with a traditional additive bias. Intriguingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.},
	urldate = {2021-06-28},
	journal = {arXiv:1912.00827 [cs, stat]},
	author = {Adlam, Ben and Levinson, Jake and Pennington, Jeffrey},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.00827},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cadenas_wind_2016a,
	title = {Wind {Speed} {Forecasting} {Using} the {NARX} {Model}, {Case}: {La} {Mata}, {Oaxaca}, {México}},
	volume = {27},
	copyright = {All rights reserved},
	doi = {10/f9k443},
	number = {8},
	journal = {Neural Computing and Applications},
	author = {Cadenas, Erasmo and Rivera, Wilfrido and Campos-Amezcua, Rafael and Cadenas, Roberto},
	year = {2016},
	pages = {2417--2428},
}

@article{lee_wide_2020,
	title = {Wide neural networks of any depth evolve as linear models under gradient descent},
	volume = {2020},
	issn = {1742-5468},
	url = {https://doi.org/10.1088/1742-5468/abc62b},
	doi = {10.1088/1742-5468/abc62b},
	abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks (NNs) have made a theory of learning dynamics elusive. In this work, we show that for wide NNs the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian NNs and Gaussian processes (GPs), gradient-based training of wide NNs with a squared loss produces test set predictions drawn from a GP with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
	language = {en},
	number = {12},
	urldate = {2021-05-04},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S. and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
	month = dec,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {124002},
}

@article{azulay_why_2019,
	title = {Why do deep convolutional networks generalize so poorly to small image transformations?},
	volume = {20},
	abstract = {Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to small image transformations: either because of the convolutional architecture or because they were trained using data augmentation. Recently, several authors have shown that this is not the case: small translations or rescalings of the input image can drastically change the network’s prediction. In this paper, we quantify this phenomena and ask why neither the convolutional architecture nor data augmentation are suﬃcient to achieve the desired invariance. Speciﬁcally, we show that the convolutional architecture does not give invariance since architectures ignore the classical sampling theorem, and data augmentation does not give invariance because the CNNs learn to be invariant to transformations only for images that are very similar to typical images from the training set. We discuss two possible solutions to this problem: (1) antialiasing the intermediate representations and (2) increasing data augmentation and show that they provide only a partial solution at best. Taken together, our results indicate that the problem of insuring invariance to small image transformations in neural networks while preserving high accuracy remains unsolved.},
	language = {en},
	journal = {Journal of Machine Learning Research},
	author = {Azulay, Aharon and Weiss, Yair},
	year = {2019},
	pages = {1--25},
}

@article{erhan_why_,
	title = {Why {Does} {Unsupervised} {Pre}-training {Help} {Deep} {Learning}?},
	abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of autoencoder variants with impressive results being obtained in several areas, mostly on vision and language datasets. The best results obtained on supervised learning tasks often involve an unsupervised learning component, usually in an unsupervised pre-training phase. The main question investigated here is the following: why does unsupervised pre-training work so well? Through extensive experimentation, we explore several possible explanations discussed in the literature including its action as a regularizer (Erhan et al., 2009b) and as an aid to optimization (Bengio et al., 2007). Our results build on the work of Erhan et al. (2009b), showing that unsupervised pre-training appears to play predominantly a regularization role in subsequent supervised training. However our results in an online setting, with a virtually unlimited data stream, point to a somewhat more nuanced interpretation of the roles of optimization and regularization in the unsupervised pre-training effect.},
	language = {en},
	author = {Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal},
	pages = {8},
}

@article{hanin_which_2018,
	title = {Which {Neural} {Net} {Architectures} {Give} {Rise} {To} {Exploding} and {Vanishing} {Gradients}?},
	url = {https://arxiv.org/abs/1801.03744},
	language = {en},
	urldate = {2018-12-10},
	author = {Hanin, Boris},
	month = jan,
	year = {2018},
}

@article{redmon_yolo9000_2016,
	title = {{YOLO9000}: {Better}, {Faster}, {Stronger}},
	shorttitle = {{YOLO9000}},
	url = {http://arxiv.org/abs/1612.08242},
	abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	journal = {arXiv:1612.08242 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = dec,
	year = {2016},
	note = {00000 
arXiv: 1612.08242},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 🔍No DOI found},
}

@inproceedings{redmon_you_2016,
	title = {You only look once: {Unified}, real-time object detection},
	shorttitle = {You only look once},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	year = {2016},
	note = {00000},
	pages = {779--788},
}

@inproceedings{schoukens_wienerhammerstein_2016a,
	title = {Wiener-{Hammerstein} benchmark with process noise},
	url = {https://www.researchgate.net/profile/Maarten_Schoukens/publication/307569307_Wiener-Hammerstein_benchmark_with_process_noise/links/57c92eca08ae28c01d54ab0a.pdf},
	urldate = {2017-08-28},
	booktitle = {Workshop on {Nonlinear} {System} {Identification} {Benchmarks}, {Brussels}},
	author = {Schoukens, M. and Noël, J. P.},
	year = {2016},
	note = {00000},
	pages = {15--19},
}

@article{schoukens_wienerhammerstein_2016,
	title = {Wiener-{Hammerstein} benchmark with process noise},
	language = {en},
	author = {Schoukens, M and Noel, J P},
	year = {2016},
	keywords = {🔍No DOI found},
	pages = {5},
}

@article{schoukens_wienerhammerstein_,
	title = {Wiener-{Hammerstein} {Benchmark}},
	abstract = {This paper describes a benchmark for nonlinear system identification. A Wiener-Hammerstein system is selected as test object. In such a structure there is no direct access to the static nonlinearity starting from the measured input/output, because it is sandwiched between two unknown dynamic systems. The signal-to-noise ratio of the measurements is quite high, which puts the focus of the benchmark on the ability to identify the nonlinear behaviour, and not so much on the noise rejection properties. The benchmark is not intended as a competition, but as a tool to compare the possibilities of different methods to deal with this specific nonlinear structure.},
	language = {en},
	author = {Schoukens, Johan and Suykens, Johan and Ljung, Lennart},
	keywords = {🔍No DOI found},
	pages = {4},
}

@article{miller_when_2018,
	title = {When {Recurrent} {Models} {Don}'t {Need} {To} {Be} {Recurrent}},
	url = {http://arxiv.org/abs/1805.10369},
	abstract = {We prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Our result applies to a broad range of non-linear recurrent neural networks under a natural stability condition, which we observe is also necessary. Complementing our theoretical findings, we verify the conclusions of our theory on both real and synthetic tasks. Furthermore, we demonstrate recurrent models satisfying the stability assumption of our theory can have excellent performance on real sequence learning tasks.},
	urldate = {2018-11-02},
	journal = {arXiv:1805.10369 [cs, stat]},
	author = {Miller, John and Hardt, Moritz},
	month = may,
	year = {2018},
	note = {arXiv: 1805.10369},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, 🔍No DOI found},
}

@article{hinton_potential_2018,
	title = {With the {Potential} to {Transform} {Health} {Care}},
	language = {en},
	author = {Hinton, Geoffrey},
	year = {2018},
	keywords = {🔍No DOI found},
	pages = {2},
}

@article{clark_what_2019,
	title = {What {Does} {BERT} {Look} {At}? {An} {Analysis} of {BERT}'s {Attention}},
	shorttitle = {What {Does} {BERT} {Look} {At}?},
	url = {http://arxiv.org/abs/1906.04341},
	abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
	urldate = {2019-06-25},
	journal = {arXiv:1906.04341 [cs]},
	author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.04341},
	keywords = {Computer Science - Computation and Language},
}

@article{cadenas_wind_2016,
	title = {Wind {Speed} {Forecasting} {Using} the {NARX} {Model}, {Case}: {La} {Mata}, {Oaxaca}, {México}},
	volume = {27},
	doi = {10/f9k443},
	number = {8},
	journal = {Neural Computing and Applications},
	author = {Cadenas, Erasmo and Rivera, Wilfrido and Campos-Amezcua, Rafael and Cadenas, Roberto},
	year = {2016},
	note = {00010},
	pages = {2417--2428},
}

@article{morris_what_2011,
	title = {What is {Hysteresis}?},
	volume = {64},
	issn = {0003-6900},
	url = {https://asmedigitalcollection.asme.org/appliedmechanicsreviews/article/64/5/050801/369998/What-is-Hysteresis},
	doi = {10.1115/1.4007112},
	language = {en},
	number = {5},
	urldate = {2019-11-13},
	journal = {Applied Mechanics Reviews},
	author = {Morris, K. A.},
	month = sep,
	year = {2011},
}

@article{biggio_wild_2018,
	title = {Wild patterns: {Ten} years after the rise of adversarial machine learning},
	volume = {84},
	issn = {0031-3203},
	shorttitle = {Wild patterns},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320318302565},
	doi = {10.1016/j.patcog.2018.07.023},
	abstract = {Learning-based pattern classifiers, including deep networks, have shown impressive performance in several application domains, ranging from computer vision to cybersecurity. However, it has also been shown that adversarial input perturbations carefully crafted either at training or at test time can easily subvert their predictions. The vulnerability of machine learning to such wild patterns (also referred to as adversarial examples), along with the design of suitable countermeasures, have been investigated in the research field of adversarial machine learning. In this work, we provide a thorough overview of the evolution of this research area over the last ten years and beyond, starting from pioneering, earlier work on the security of non-deep learning algorithms up to more recent work aimed to understand the security properties of deep learning algorithms, in the context of computer vision and cybersecurity tasks. We report interesting connections between these apparently-different lines of work, highlighting common misconceptions related to the security evaluation of machine-learning algorithms. We review the main threat models and attacks defined to this end, and discuss the main limitations of current work, along with the corresponding future challenges towards the design of more secure learning algorithms.},
	language = {en},
	urldate = {2021-05-15},
	journal = {Pattern Recognition},
	author = {Biggio, Battista and Roli, Fabio},
	month = dec,
	year = {2018},
	keywords = {Adversarial examples, Adversarial machine learning, Deep learning, Evasion attacks, Poisoning attacks, Secure learning},
	pages = {317--331},
}

@inproceedings{rahimi_weighted_2009,
	title = {Weighted sums of random kitchen sinks: {Replacing} minimization with randomization in learning},
	volume = {21},
	url = {https://proceedings.neurips.cc/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Rahimi, Ali and Recht, Benjamin},
	editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
	year = {2009},
}

@article{zeiler_visualizing_2013b,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	urldate = {2020-05-06},
	journal = {arXiv:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.2901},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{fujieda_wavelet_2018,
	title = {Wavelet {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1805.08620},
	abstract = {Spatial and spectral approaches are two major approaches for image processing tasks such as image classification and object recognition. Among many such algorithms, convolutional neural networks (CNNs) have recently achieved significant performance improvement in many challenging tasks. Since CNNs process images directly in the spatial domain, they are essentially spatial approaches. Given that spatial and spectral approaches are known to have different characteristics, it will be interesting to incorporate a spectral approach into CNNs. We propose a novel CNN architecture, wavelet CNNs, which combines a multiresolution analysis and CNNs into one model. Our insight is that a CNN can be viewed as a limited form of a multiresolution analysis. Based on this insight, we supplement missing parts of the multiresolution analysis via wavelet transform and integrate them as additional components in the entire architecture. Wavelet CNNs allow us to utilize spectral information which is mostly lost in conventional CNNs but useful in most image processing tasks. We evaluate the practical performance of wavelet CNNs on texture classification and image annotation. The experiments show that wavelet CNNs can achieve better accuracy in both tasks than existing models while having significantly fewer parameters than conventional CNNs.},
	urldate = {2020-07-07},
	journal = {arXiv:1805.08620 [cs]},
	author = {Fujieda, Shin and Takayama, Kohei and Hachisuka, Toshiya},
	month = may,
	year = {2018},
	note = {arXiv: 1805.08620},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{williams_wavelet_2018,
	title = {{WAVELET} {POOLING} {FOR} {CONVOLUTIONAL} {NEURAL} {NETWORKS}},
	abstract = {Convolutional Neural Networks continuously advance the progress of 2D and 3D image and object classiﬁcation. The steadfast usage of this algorithm requires constant evaluation and upgrading of foundational concepts to maintain progress. Network regularization techniques typically focus on convolutional layer operations, while leaving pooling layer operations without suitable options. We introduce Wavelet Pooling as another alternative to traditional neighborhood pooling. This method decomposes features into a second level decomposition, and discards the ﬁrst-level subbands to reduce feature dimensions. This method addresses the overﬁtting problem encountered by max pooling, while reducing features in a more structurally compact manner than pooling via neighborhood regions. Experimental results on four benchmark classiﬁcation datasets demonstrate our proposed method outperforms or performs comparatively with methods like max, mean, mixed, and stochastic pooling.},
	language = {en},
	author = {Williams, Travis and Li, Robert},
	year = {2018},
	pages = {12},
}

@article{sontag_vc_,
	title = {{VC} {Dimension} of {Neural} {Networks}},
	abstract = {This paper presents a brief introduction to Vapnik-Chervonenkis (VC) dimension, a quantity which characterizes the diﬃculty of distribution-independent learning. The paper establishes various elementary results, and discusses how to estimate the VC dimension in several examples of interest in neural network theory.},
	language = {en},
	author = {Sontag, Eduardo D},
	pages = {26},
}

@article{tanaka_wavecyclegan2_2019,
	title = {{WaveCycleGAN2}: {Time}-domain {Neural} {Post}-filter for {Speech} {Waveform} {Generation}},
	shorttitle = {{WaveCycleGAN2}},
	url = {http://arxiv.org/abs/1904.02892},
	abstract = {WaveCycleGAN has recently been proposed to bridge the gap between natural and synthesized speech waveforms in statistical parametric speech synthesis and provides fast inference with a moving average model rather than an autoregressive model and high-quality speech synthesis with the adversarial training. However, the human ear can still distinguish the processed speech waveforms from natural ones. One possible cause of this distinguishability is the aliasing observed in the processed speech waveform via down/up-sampling modules. To solve the aliasing and provide higher quality speech synthesis, we propose WaveCycleGAN2, which 1) uses generators without down/up-sampling modules and 2) combines discriminators of the waveform domain and acoustic parameter domain. The results show that the proposed method 1) alleviates the aliasing well, 2) is useful for both speech waveforms generated by analysis-and-synthesis and statistical parametric speech synthesis, and 3) achieves a mean opinion score comparable to those of natural speech and speech synthesized by WaveNet (open WaveNet) and WaveGlow while processing speech samples at a rate of more than 150 kHz on an NVIDIA Tesla P100.},
	urldate = {2020-03-23},
	journal = {arXiv:1904.02892},
	author = {Tanaka, Kou and Kameoka, Hirokazu and Kaneko, Takuhiro and Hojo, Nobukatsu},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.02892},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10/gb2dc6},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	number = {518},
	urldate = {2019-01-10},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	note = {arXiv: 1601.00670},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
	pages = {859--877},
}

@article{oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2019-03-13},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
}

@article{zeiler_visualizing_2013a,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	journal = {arXiv:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = nov,
	year = {2013},
	note = {04774 
arXiv: 1311.2901},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 🔍No DOI found},
}

@article{zeiler_visualizing_2013,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	journal = {arXiv:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.2901},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 🔍No DOI found},
}

@inproceedings{zeiler_visualizing_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	isbn = {978-3-319-10589-5 978-3-319-10590-1},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53},
	doi = {10.1007/978-3-319-10590-1_53},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	language = {en},
	urldate = {2018-01-27},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer, Cham},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = sep,
	year = {2014},
	note = {04774},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {818--833},
}

@article{tecnologia_vocabulario_,
	title = {Vocabulário {Internacional} de {Metrologia}–{Conceitos} fundamentais e gerais e termos associados ({VIM} 2012)},
	author = {Tecnologia, Inmetro and Filipe, Eduarda and Pellegrino, Olivier and Baratto, Antonio Carlos and de Oliveira, Sérgio Pinheiro and Mendoza, Victor Manuel Loayza},
	note = {00000},
	keywords = {🔍No DOI found},
}

@book{strang_wavelets_1996,
	title = {Wavelets and filter banks},
	publisher = {SIAM},
	author = {Strang, Gilbert and Nguyen, Truong},
	year = {1996},
	note = {00000},
}

@article{simonyan_very_2014,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = sep,
	year = {2014},
	note = {00000 
arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 🔍No DOI found},
}

@article{man_vectorcardiographic_2015,
	title = {Vectorcardiographic diagnostic \& prognostic information derived from the 12‐lead electrocardiogram: {Historical} review and clinical perspective},
	volume = {48},
	issn = {00220736},
	shorttitle = {Vectorcardiographic diagnostic \& prognostic information derived from the 12‐lead electrocardiogram},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0022073615001284},
	doi = {10.1016/j.jelectrocard.2015.05.002},
	language = {en},
	number = {4},
	urldate = {2018-07-17},
	journal = {Journal of Electrocardiology},
	author = {Man, Sumche and Maan, Arie C. and Schalij, Martin J. and Swenne, Cees A.},
	month = jul,
	year = {2015},
	pages = {463--475},
}

@article{li_visualizing_2017,
	title = {Visualizing the {Loss} {Landscape} of {Neural} {Nets}},
	url = {http://arxiv.org/abs/1712.09913},
	abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
	urldate = {2019-04-12},
	journal = {arXiv:1712.09913 [cs, stat]},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.09913},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{li_visualizing_2018,
	title = {Visualizing the {Loss} {Landscape} of {Neural} {Nets}},
	url = {http://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf},
	urldate = {2019-04-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {6389--6399},
}

@article{davidon_variable_1991a,
	title = {Variable {Metric} {Method} for {Minimization}},
	volume = {1},
	doi = {10/cjnhbp},
	number = {1},
	journal = {SIAM Journal on Optimization},
	author = {Davidon, William C},
	year = {1991},
	note = {01872},
	pages = {1--17},
}

@article{hendrycks_using_2019,
	title = {Using {Pre}-{Training} {Can} {Improve} {Model} {Robustness} and {Uncertainty}},
	url = {http://arxiv.org/abs/1901.09960},
	abstract = {He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on adversarial examples, label corruption, class imbalance, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We introduce adversarial pre-training and show approximately a 10\% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.},
	urldate = {2020-03-24},
	author = {Hendrycks, Dan and Lee, Kimin and Mazeika, Mantas},
	year = {2019},
	note = {arXiv: 1901.09960},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{howard_universal_2018,
	title = {Universal {Language} {Model} {Fine}-tuning for {Text} {Classification}},
	url = {http://arxiv.org/abs/1801.06146},
	abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
	urldate = {2019-06-10},
	journal = {arXiv:1801.06146 [cs, stat]},
	author = {Howard, Jeremy and Ruder, Sebastian},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.06146},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{park_universal_1991a,
	title = {Universal {Approximation} {Using} {Radial}-{Basis}-{Function} {Networks}},
	volume = {3},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1991.3.2.246},
	doi = {10.1162/neco.1991.3.2.246},
	abstract = {There have been several recent studies concerning feedforward networks and the problem of approximating arbitrary functionals of a finite number of real variables. Some of these studies deal with cases in which the hidden-layer nonlinearity is not a sigmoid. This was motivated by successful applications of feedforward networks with nonsigmoidal hidden-layer units. This paper reports on a related study of radial-basis-function (RBF) networks, and it is proved that RBF networks having one hidden layer are capable of universal approximation. Here the emphasis is on the case of typical RBF networks, and the results show that a certain class of RBF networks with the same smoothing factor in each kernel node is broad enough for universal approximation.},
	number = {2},
	urldate = {2020-11-24},
	journal = {Neural Computation},
	author = {Park, J. and Sandberg, I. W.},
	month = jun,
	year = {1991},
	note = {Publisher: MIT Press},
	pages = {246--257},
}

@article{hughes_using_2018,
	title = {Using {Multitask} {Learning} to {Improve} 12-{Lead} {Electrocardiogram} {Classification}},
	url = {http://arxiv.org/abs/1812.00497},
	abstract = {We develop a multi-task convolutional neural network (CNN) to classify multiple diagnoses from 12-lead electrocardiograms (ECGs) using a dataset comprised of over 40,000 ECGs, with labels derived from cardiologist clinical interpretations. Since many clinically important classes can occur in low frequencies, approaches are needed to improve performance on rare classes. We compare the performance of several single-class classifiers on rare classes to a multi-headed classifier across all available classes. We demonstrate that the addition of common classes can significantly improve CNN performance on rarer classes when compared to a model trained on the rarer class in isolation. Using this method, we develop a model with high performance as measured by F1 score on multiple clinically relevant classes compared against the gold-standard cardiologist interpretation.},
	urldate = {2018-12-04},
	journal = {arXiv:1812.00497 [cs, stat]},
	author = {Hughes, J. Weston and MD, Taylor Sittler and Joseph, Anthony D. and MD, Jeffrey E. Olgin and Gonzalez, Joseph E. and MD, Geoffrey H. Tison},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.00497},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{fletcher_user_1998,
	title = {User manual for {filterSQP}},
	journal = {Numerical Analysis Report NA/181, Department of Mathematics, University of Dundee, Dundee, Scotland},
	author = {Fletcher, Roger and Leyffer, Sven},
	year = {1998},
	keywords = {🔍No DOI found},
}

@article{davidon_variable_1991,
	title = {Variable metric method for minimization},
	volume = {1},
	doi = {10.1137/0801001},
	number = {1},
	journal = {SIAM Journal on Optimization},
	author = {Davidon, William C},
	year = {1991},
	pages = {1--17},
}

@article{zech_variable_2018,
	title = {Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: {A} cross-sectional study},
	volume = {15},
	issn = {1549-1676},
	shorttitle = {Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002683},
	doi = {10/gfj53h},
	abstract = {Background There is interest in using convolutional neural networks (CNNs) to analyze medical imaging to provide computer-aided diagnosis (CAD). Recent work has suggested that image classification CNNs may not generalize to new data as well as previously believed. We assessed how well CNNs generalized across three hospital systems for a simulated pneumonia screening task. Methods and findings A cross-sectional design with multiple model training cohorts was used to evaluate model generalizability to external sites using split-sample validation. A total of 158,323 chest radiographs were drawn from three institutions: National Institutes of Health Clinical Center (NIH; 112,120 from 30,805 patients), Mount Sinai Hospital (MSH; 42,396 from 12,904 patients), and Indiana University Network for Patient Care (IU; 3,807 from 3,683 patients). These patient populations had an age mean (SD) of 46.9 years (16.6), 63.2 years (16.5), and 49.6 years (17) with a female percentage of 43.5\%, 44.8\%, and 57.3\%, respectively. We assessed individual models using the area under the receiver operating characteristic curve (AUC) for radiographic findings consistent with pneumonia and compared performance on different test sets with DeLong’s test. The prevalence of pneumonia was high enough at MSH (34.2\%) relative to NIH and IU (1.2\% and 1.0\%) that merely sorting by hospital system achieved an AUC of 0.861 (95\% CI 0.855–0.866) on the joint MSH–NIH dataset. Models trained on data from either NIH or MSH had equivalent performance on IU (P values 0.580 and 0.273, respectively) and inferior performance on data from each other relative to an internal test set (i.e., new data from within the hospital system used for training data; P values both {\textless}0.001). The highest internal performance was achieved by combining training and test data from MSH and NIH (AUC 0.931, 95\% CI 0.927–0.936), but this model demonstrated significantly lower external performance at IU (AUC 0.815, 95\% CI 0.745–0.885, P = 0.001). To test the effect of pooling data from sites with disparate pneumonia prevalence, we used stratified subsampling to generate MSH–NIH cohorts that only differed in disease prevalence between training data sites. When both training data sites had the same pneumonia prevalence, the model performed consistently on external IU data (P = 0.88). When a 10-fold difference in pneumonia rate was introduced between sites, internal test performance improved compared to the balanced model (10× MSH risk P {\textless} 0.001; 10× NIH P = 0.002), but this outperformance failed to generalize to IU (MSH 10× P {\textless} 0.001; NIH 10× P = 0.027). CNNs were able to directly detect hospital system of a radiograph for 99.95\% NIH (22,050/22,062) and 99.98\% MSH (8,386/8,388) radiographs. The primary limitation of our approach and the available public data is that we cannot fully assess what other factors might be contributing to hospital system–specific biases. Conclusion Pneumonia-screening CNNs achieved better internal than external performance in 3 out of 5 natural comparisons. When models were trained on pooled data from sites with different pneumonia prevalence, they performed better on new pooled data from these sites but not on external data. CNNs robustly identified hospital system and department within a hospital, which can have large differences in disease burden and may confound predictions.},
	language = {en},
	number = {11},
	urldate = {2018-11-19},
	journal = {PLOS Medicine},
	author = {Zech, John R. and Badgeley, Marcus A. and Liu, Manway and Costa, Anthony B. and Titano, Joseph J. and Oermann, Eric Karl},
	month = jun,
	year = {2018},
	keywords = {Critical care and emergency medicine, Deep learning, Indiana, Inpatients, Natural language processing, Nosocomial infections, Pneumonia, Radiology and imaging},
	pages = {e1002683},
}

@article{isaksson_using_2015,
	title = {Using horizon estimation and nonlinear optimization for grey-box identification},
	volume = {30},
	issn = {0959-1524},
	doi = {10.1016/j.jprocont.2014.12.008},
	journal = {Journal of Process Control},
	author = {Isaksson, Alf J and Sjöberg, Johan and Törnqvist, David and Ljung, Lennart and Kok, Manon},
	year = {2015},
	pages = {69--79},
}

@article{park_universal_1991b,
	title = {Universal approximation using radial-basis-function networks},
	volume = {3},
	number = {2},
	journal = {Neural computation},
	author = {Park, Jooyoung and Sandberg, Irwin W},
	year = {1991},
	note = {00000},
	keywords = {❓Multiple DOI},
	pages = {246--257},
}

@article{day_unsupervised_2007,
	title = {Unsupervised segmentation of continuous genomic data},
	volume = {23},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/23/11/1424/200706},
	doi = {10/bbjzck},
	abstract = {Abstract.  Summary: The advent of high-density, high-volume genomic data has created the need for tools to summarize large datasets at multiple scales. HMMSeg i},
	language = {en},
	number = {11},
	urldate = {2019-04-01},
	journal = {Bioinformatics},
	author = {Day, Nathan and Hemmaplardh, Andrew and Thurman, Robert E. and Stamatoyannopoulos, John A. and Noble, William S.},
	month = jun,
	year = {2007},
	pages = {1424--1426},
}

@article{hoffman_unsupervised_2012,
	title = {Unsupervised pattern discovery in human chromatin structure through genomic segmentation},
	volume = {9},
	copyright = {2012 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.1937},
	doi = {10/gfxrbn},
	abstract = {We trained Segway, a dynamic Bayesian network method, simultaneously on chromatin data from multiple experiments, including positions of histone modifications, transcription-factor binding and open chromatin, all derived from a human chronic myeloid leukemia cell line. In an unsupervised fashion, we identified patterns associated with transcription start sites, gene ends, enhancers, transcriptional regulator CTCF-binding regions and repressed regions. Software and genome browser tracks are at http://noble.gs.washington.edu/proj/segway/.},
	language = {en},
	number = {5},
	urldate = {2019-04-01},
	journal = {Nature Methods},
	author = {Hoffman, Michael M. and Buske, Orion J. and Wang, Jie and Weng, Zhiping and Bilmes, Jeff A. and Noble, William Stafford},
	month = may,
	year = {2012},
	pages = {473--476},
}

@article{park_universal_1991,
	title = {Universal {Approximation} {Using} {Radial}-{Basis}-{Function} {Networks}},
	volume = {3},
	doi = {10/d3sv6t},
	number = {2},
	journal = {Neural computation},
	author = {Park, Jooyoung and Sandberg, Irwin W},
	year = {1991},
	note = {03486},
	pages = {246--257},
}

@inproceedings{arjovsky_unitary_2016,
	series = {{ICML}'16},
	title = {Unitary evolution recurrent neural networks},
	url = {http://dl.acm.org/citation.cfm?id=3045390.3045509},
	booktitle = {Proceedings of the 33rd international conference on international conference on machine learning - volume 48},
	publisher = {JMLR.org},
	author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
	year = {2016},
	note = {tex.acmid: 3045509
tex.numpages: 9
event-place: New York, NY, USA},
	keywords = {⛔ No DOI found},
	pages = {1120--1128},
}

@inproceedings{williams_using_2001,
	title = {Using the nyström method to speed up kernel machines},
	abstract = {A major problem for kernel-based predictors (such as Support Vector Machines and Gaussian processes) is that the amount of computation required to find the solution scales as O(n ), where n is the number of training examples. We show that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nyström method (which is used for the numerical solution of eigenproblems). This is achieved by carrying out an eigendecomposition on a smaller system of size m ¡ n, and then expanding the results back up to n dimensions. The computational complexity of a predictor using this approximation is O(m n). We report experiments on the USPS and abalone data sets and show that we can set m n without any significant decrease in the accuracy of the solution.},
	language = {English},
	booktitle = {Advances in neural information processing systems 13 ({NIPS} 2000)},
	publisher = {MIT Press},
	author = {Williams, Christopher K. I. and Seeger, Matthias},
	editor = {Leen, T.K. and Dietterich, T.G. and Tresp, V.},
	year = {2001},
	pages = {682--688},
}

@article{paixao_validation_2020,
	title = {Validation of a {Deep} {Neural} {Network} {Electrocardiographic}-{Age} as a {Mortality} {Predictor}: {The} {CODE} {Study}},
	volume = {142},
	copyright = {All rights reserved},
	shorttitle = {Abstract 16883},
	url = {https://www.ahajournals.org/doi/abs/10.1161/circ.142.suppl_3.16883},
	doi = {10.1161/circ.142.suppl_3.16883},
	abstract = {Introduction: Aging affects the electrocardiogram (ECG) with a higher incidence of abnormalities in older patients. ECG-age can be predicted by artificial intelligence (AI) and can be used as a measure of cardiovascular health.Hypothesis: ECG-age predicted by AI is a risk factor for overall mortality.Methods: The Clinical Outcomes in Digital Electrocardiography (CODE) study is a retrospective cohort with a mean follow-up of 3.67 years.The dataset consists of Brazilian patients, mainly from primary care centers. Two established cohorts, ELSA-Brasil, of Brazilian public servants, and SaMi-Trop, of Chagas disease patients, were used for external validation. 2,322,513 ECGs from 1,558,421 patients over 16 years old that underwent an ECG from 2010 to 2017 were included. A deep convolutional neural network was trained in order to predict the age of the patient based solely on ECG 12-lead tracings. The ECG database was split into 85-15\% training and test datasets, respectively. Death was ascertained using probabilistic linkage with Brazil′s mortality information data. The Cox regression model, adjusted by age and sex, was used for statistical analysis. The model was validated in two cohorts: ELSA-Brasil (n=14,263) and SaMi-Trop (n=1,631).Results: he mean predicted ECG-age was 52.0 years (±18.7) with a mean absolute error of 8.38 (±7.0) years. Patients with ECG-age {\textgreater}8y older than chronological age had higher mortality rate (HR 1.79, 95\%CI 1.69-1.90; p{\textless}0.001), whereas those ECG-age {\textgreater}8y younger than chronological age were associated with a lower mortality rate (HR 0.78, 95\%CI 0.74-0.83; p{\textless}0.001). These results were similar in ELSA-Brasil and SaMi-Trop external validation cohorts (HR 1.75, 95\%CI 1.35-2.27; p{\textless}0.001;HR 2.42, 95\%CI 1.53-3.83; p{\textless}0.001 for {\textgreater}8y difference, retrospectively; HR 0.74, 95\%CI 0.63-0.88; p{\textless}0.001;HR 0.89, 95\%CI 0.52-1.54; p=0.68 for {\textless}8y difference, respectively).Conclusions: ECG-age, predicted by AI, can be useful as a tool for risk stratification of mortality.},
	number = {Suppl\_3},
	urldate = {2020-12-29},
	journal = {Circulation},
	author = {Paixao, Gabriela and Lima, Emilly M and Ribeiro, Antonio H and Gomes, Paulo R and Oliveira, Derick and Junior, Marcelo M. Pinto and Sabino, Ester and Barreto, Sandhi and Giatti, Luana and A, Paulo Andrade Lotufo and Bruce, Duncan and Wagner, Meira Junior and Thomas B. Schon and Ribeiro, Antonio L.},
	month = nov,
	year = {2020},
	note = {Publisher: American Heart Association},
	pages = {A16883--A16883},
}

@article{pathak_using_2017,
	title = {Using machine learning to replicate chaotic attractors and calculate {Lyapunov} exponents from data},
	volume = {27},
	issn = {1054-1500, 1089-7682},
	url = {http://aip.scitation.org/doi/10.1063/1.5010300},
	doi = {10.1063/1.5010300},
	language = {en},
	number = {12},
	urldate = {2021-02-19},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Pathak, Jaideep and Lu, Zhixin and Hunt, Brian R. and Girvan, Michelle and Ott, Edward},
	month = dec,
	year = {2017},
	pages = {121102},
}

@incollection{nagarajan_uniform_2019,
	title = {Uniform convergence may be unable to explain generalization in deep learning},
	url = {http://papers.nips.cc/paper/9336-uniform-convergence-may-be-unable-to-explain-generalization-in-deep-learning.pdf},
	urldate = {2019-12-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Nagarajan, Vaishnavh and Kolter, J. Zico},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {11611--11622},
}

@article{mitra_understanding_2019,
	title = {Understanding overfitting peaks in generalization error: {Analytical} risk curves for \$l\_2\$ and \$l\_1\$ penalized interpolation},
	shorttitle = {Understanding overfitting peaks in generalization error},
	url = {http://arxiv.org/abs/1906.03667},
	abstract = {Traditionally in regression one minimizes the number of fitting parameters or uses smoothing/regularization to trade training (TE) and generalization error (GE). Driving TE to zero by increasing fitting degrees of freedom (dof) is expected to increase GE. However modern big-data approaches, including deep nets, seem to over-parametrize and send TE to zero (data interpolation) without impacting GE. Overparametrization has the benefit that global minima of the empirical loss function proliferate and become easier to find. These phenomena have drawn theoretical attention. Regression and classification algorithms have been shown that interpolate data but also generalize optimally. An interesting related phenomenon has been noted: the existence of non-monotonic risk curves, with a peak in GE with increasing dof. It was suggested that this peak separates a classical regime from a modern regime where over-parametrization improves performance. Similar over-fitting peaks were reported previously (statistical physics approach to learning) and attributed to increased fitting model flexibility. We introduce a generative and fitting model pair ("Misparametrized Sparse Regression" or MiSpaR) and show that the overfitting peak can be dissociated from the point at which the fitting function gains enough dof's to match the data generative model and thus provides good generalization. This complicates the interpretation of overfitting peaks as separating a "classical" from a "modern" regime. Data interpolation itself cannot guarantee good generalization: we need to study the interpolation with different penalty terms. We present analytical formulae for GE curves for MiSpaR with \$l\_2\$ and \$l\_1\$ penalties, in the interpolating limit \${\textbackslash}lambda{\textbackslash}rightarrow 0\$.These risk curves exhibit important differences and help elucidate the underlying phenomena.},
	urldate = {2021-06-24},
	journal = {arXiv:1906.03667 [physics, stat]},
	author = {Mitra, Partha P.},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.03667},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
}

@misc{blog_understanding_,
	title = {Understanding the {Neural} {Tangent} {Kernel}},
	url = {https://rajatvd.github.io/NTK/},
	abstract = {My attempt at distilling the ideas behind the neural tangent kernel that is making waves in recent theoretical deep learning research.},
	urldate = {2020-08-09},
	author = {Blog, Rajat's},
}

@article{bjorck_understanding_2018,
	title = {Understanding {Batch} {Normalization}},
	url = {http://arxiv.org/abs/1806.02375},
	abstract = {Batch normalization (BN) is a technique to normalize activations in intermediate layers of deep neural networks. Its tendency to improve accuracy and speed up training have established BN as a favorite technique in deep learning. Yet, despite its enormous success, there remains little consensus on the exact reason and mechanism behind these improvements. In this paper we take a step towards a better understanding of BN, following an empirical approach. We conduct several experiments, and show that BN primarily enables training with larger learning rates, which is the cause for faster convergence and better generalization. For networks without BN we demonstrate how large gradient updates can result in diverging loss and activations growing uncontrollably with network depth, which limits possible learning rates. BN avoids this problem by constantly correcting activations to be zero-mean and of unit standard deviation, which enables larger gradient steps, yields faster convergence and may help bypass sharp local minima. We further show various ways in which gradients and activations of deep unnormalized networks are ill-behaved. We contrast our results against recent findings in random matrix theory, shedding new light on classical initialization schemes and their consequences.},
	urldate = {2018-12-13},
	journal = {arXiv:1806.02375 [cs, stat]},
	author = {Bjorck, Johan and Gomes, Carla and Selman, Bart and Weinberger, Kilian Q.},
	month = may,
	year = {2018},
	note = {arXiv: 1806.02375},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{karki_understanding_1998,
	title = {Understanding operational amplifier specifications},
	journal = {Mixed Signal and Analog Operational Amplifiers. Digital Signal Processing Solutions, no. White Paper: SLOA011},
	author = {Karki, Jim},
	year = {1998},
	keywords = {🔍No DOI found},
}

@inproceedings{glorot_understanding_2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	author = {Glorot, Xavier and Bengio, Yoshua},
	year = {2010},
	pages = {249--256},
}

@misc{_understandingexplodinggradients_,
	title = {understanding-exploding-gradients},
	url = {https://pt.overleaf.com/project/5cbed10170921e14664561f1},
	abstract = {Um editor de LaTeX online fácil de usar. Sem instalação, colaboração em tempo real, controle de versões, centenas de templates LaTeX e mais.},
	language = {pt},
	urldate = {2019-10-08},
}

@article{damour_underspecification_2020,
	title = {Underspecification {Presents} {Challenges} for {Credibility} in {Modern} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2011.03395},
	abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
	urldate = {2020-11-11},
	journal = {arXiv:2011.03395},
	author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.03395},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{delima_um_2010,
	title = {Um sistema de visao estéreo para navegaçao de um carro autônomo em ambientes com obstáculos},
	author = {de Lima, Danilo Alves and Pereira, Guilherme AS},
	year = {2010},
	keywords = {🔍No DOI found},
}

@article{dennis_two_1979,
	title = {Two new unconstrained optimization algorithms which use function and gradient values},
	volume = {28},
	doi = {10.1007/BF00932218},
	number = {4},
	journal = {Journal of Optimization Theory and Applications},
	author = {Dennis, John E and Mei, H. H. W.},
	year = {1979},
	pages = {453--482},
}

@article{lenders_trlib_2016,
	title = {trlib: {A} vector-free implementation of the {GLTR} method for iterative solution of the trust region problem},
	journal = {arXiv preprint arXiv:1611.04718},
	author = {Lenders, Felix and Kirches, Christian and Potschka, Andreas},
	year = {2016},
	keywords = {35Q90, 65K05, 90C20, 90C30, 97N90, Mathematics - Optimization and Control, 🔍No DOI found},
}

@article{lipton_troubling_2018,
	title = {Troubling {Trends} in {Machine} {Learning} {Scholarship}},
	journal = {arXiv preprint arXiv:1807.03341},
	author = {Lipton, Zachary C. and Steinhardt, Jacob},
	year = {2018},
	keywords = {🔍No DOI found},
}

@book{conn_trustregion_2000,
	address = {Philadelphia, PA},
	series = {{MPS}-{SIAM} series on optimization},
	title = {Trust-region methods},
	isbn = {978-0-89871-460-9},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Conn, A. R. and Gould, Nicholas I. M. and Toint, Ph L.},
	year = {2000},
	keywords = {Mathematical optimization},
}

@article{he_twosided_2016,
	title = {Two-sided coupled generalized {Sylvester} matrix equations solving using a simultaneous decomposition for fifteen matrices},
	volume = {496},
	issn = {0024-3795},
	url = {http://www.sciencedirect.com/science/article/pii/S0024379516001142},
	doi = {10.1016/j.laa.2016.02.013},
	abstract = {In this paper, we investigate and analyze in detail the structure and properties of a simultaneous decomposition for fifteen matrices: Ai∈Cpi×ti, Bi∈Csi×qi, Ci∈Cpi×ti+1, Di∈Csi+1×qi, and Ei∈Cpi×qi (i=1,2,3). We show that from this simultaneous decomposition we can derive some necessary and sufficient conditions for the existence of a solution to the system of two-sided coupled generalized Sylvester matrix equations with four unknowns AiXiBi+CiXi+1Di=Ei (i=1,2,3). Apart from proving an expression for the general solutions to this system, we derive the range of ranks of these solutions using the ranks of the given matrices Ai, Bi, Ci, Di, and Ei. We provide some numerical examples to illustrate our results. Moreover, we present a similar approach to consider the simultaneous decomposition for 5k matrices and the system of k two-sided coupled generalized Sylvester matrix equations with k+1 unknowns AiXiBi+CiXi+1Di=Ei (i=1,…,k, k≥4). The main results are also valid over the real number field and the real quaternion algebra.},
	journal = {Linear Algebra and its Applications},
	author = {He, Zhuo-Heng and Agudelo, Oscar Mauricio and Wang, Qing-Wen and De Moor, Bart},
	month = may,
	year = {2016},
	keywords = {General solution, Generalized Sylvester matrix equations, Matrix decompositions, Rank, Solvability},
	pages = {549--593},
}

@article{vanmulders_two_2009,
	title = {Two {Nonlinear} {Optimization} {Methods} for {Black} {Box} {Identification} {Compared}},
	volume = {42},
	issn = {1474-6670},
	url = {http://www.sciencedirect.com/science/article/pii/S1474667016387948},
	doi = {10/dt72p8},
	abstract = {In this paper, two nonlinear optimization methods for the identification of nonlinear systems are compared. Both methods estimate all the parameters of a polynomial nonlinear state-space model by means of a nonlinear least-squares optimization. While the first method does not estimate the states explicitly, the second estimates both states and parameters adding an extra constraint equation. Both methods are introduced and their similarities and differences are discussed utilizing simulation and experimental data. The unconstrained method appears to be faster and more memory efficient, while the constrained method is robust towards instabilities.},
	number = {10},
	journal = {15th IFAC Symposium on System Identification},
	author = {Van Mulders, Anne and Schoukens, Johan and Volckaert, Marnix and Diehl, Moritz},
	month = jan,
	year = {2009},
	keywords = {Identification algorithms, constraints, nonlinear models, nonlinear systems, parameter estimation, state-space models},
	pages = {1086--1091},
}

@article{vanmulders_two_2010,
	title = {Two nonlinear optimization methods for black box identification compared},
	volume = {46},
	issn = {00051098},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0005109810002712},
	doi = {10/dzv9r8},
	abstract = {In this paper, two nonlinear optimization methods for the identification of nonlinear systems are compared. Both methods estimate the parameters of e.g. a polynomial nonlinear state-space model by means of a nonlinear least-squares optimization of the same cost function. While the first method does not estimate the states explicitly, the second method estimates both states and parameters adding an extra constraint equation. Both methods are introduced and their similarities and differences are discussed utilizing simulation data. The unconstrained method appears to be faster and more memory efficient, but the constrained method has a significant advantage as well: it is robust for unstable systems of which bounded input–output data can be measured (e.g. a system captured in a stabilizing feedback loop). Both methods have successfully been applied on real-life measurement data.},
	language = {en},
	number = {10},
	urldate = {2019-03-19},
	journal = {Automatica},
	author = {Van Mulders, Anne and Schoukens, Johan and Volckaert, Marnix and Diehl, Moritz},
	month = oct,
	year = {2010},
	pages = {1675--1681},
}

@article{ronneberger_unet_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2019-06-10},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{jing_tunable_2017,
	title = {Tunable {Efficient} {Unitary} {Neural} {Networks} ({EUNN}) and their application to {RNNs}},
	abstract = {Using unitary (instead of general) matrices in artiﬁcial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efﬁcient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the entire unitary space. Secondly, the computational complexity for training an EUNN is merely O(1) per parameter. Finally, we test the performance of EUNNs on the standard copying task, the pixelpermuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT). We ﬁnd that our architecture signiﬁcantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the ﬁnal performance and/or the wall-clock training speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.},
	language = {en},
	journal = {Proceedings of the 34 th International Conference on Machine Learning},
	author = {Jing, Li and Shen, Yichen and Dubcek, Tena and Peurifoy, John and Skirlo, Scott and LeCun, Yann and Tegmark, Max and Soljacic, Marin},
	year = {2017},
	keywords = {⛔ No DOI found},
	pages = {9},
}

@article{lezcano-casado_trivializations_2019,
	title = {Trivializations for {Gradient}-{Based} {Optimization} on {Manifolds}},
	journal = {Advances in Neural Information Processing Systems},
	author = {Lezcano-Casado, Mario},
	year = {2019},
}

@article{belkin_two_2020,
	title = {Two {Models} of {Double} {Descent} for {Weak} {Features}},
	volume = {2},
	doi = {10.1137/20M1336072},
	abstract = {The “double descent” risk curve was proposed to qualitatively describe the out-of-sample prediction accuracy of variably parameterized machine learning models. This article provides a precise mathematical analysis for the shape of this curve in two simple data models with the least squares/least norm predictor. Specifically, it is shown that the risk peaks when the number of features \$p\$ is close to the sample size \$n\$ but also that the risk sometimes decreases toward its minimum as \$p\$ increases beyond \$n\$. This behavior parallels some key patterns observed in large models, including modern neural networks, and is contrasted with that of “prescient” models that select features in an a priori optimal order.},
	number = {4},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
	month = jan,
	year = {2020},
	note = {arXiv: 1903.07571},
	pages = {1167--1180},
}

@article{adiwardana_humanlike_2020,
	title = {Towards a {Human}-like {Open}-{Domain} {Chatbot}},
	url = {http://arxiv.org/abs/2001.09977},
	abstract = {We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72\% on multi-turn evaluation) suggests that a human-level SSA of 86\% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79\% SSA, 23\% higher in absolute SSA than the existing chatbots we evaluated.},
	urldate = {2020-02-06},
	journal = {arXiv:2001.09977 [cs, stat]},
	author = {Adiwardana, Daniel and Luong, Minh-Thang and So, David R. and Hall, Jamie and Fiedel, Noah and Thoppilan, Romal and Yang, Zi and Kulshreshtha, Apoorv and Nemade, Gaurav and Lu, Yifeng and Le, Quoc V.},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.09977},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{dai_transformerxl_2019,
	title = {Transformer-{XL}: {Attentive} {Language} {Models} {Beyond} a {Fixed}-{Length} {Context}},
	shorttitle = {Transformer-{XL}},
	url = {http://arxiv.org/abs/1901.02860},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
	urldate = {2020-05-27},
	journal = {arXiv:1901.02860 [cs, stat]},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	note = {arXiv: 1901.02860},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{katharopoulos_transformers_2020,
	title = {Transformers are {RNNs}: {Fast} {Autoregressive} {Transformers} with {Linear} {Attention}},
	shorttitle = {Transformers are {RNNs}},
	url = {http://arxiv.org/abs/2006.16236},
	abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textasciicircum}2{\textbackslash}right)\$ to \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textbackslash}right)\$, where \$N\$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
	urldate = {2020-07-04},
	journal = {arXiv:2006.16236 [cs, stat]},
	author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, François},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.16236},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{belkin_understand_2018,
	title = {To understand deep learning we need to understand kernel learning},
	url = {http://arxiv.org/abs/1802.01396},
	abstract = {Generalization performance of classifiers in deep learning has recently become a subject of intense study. Deep models, typically over-parametrized, tend to fit the training data exactly. Despite this "overfitting", they perform well on test data, a phenomenon not yet fully understood. The first point of our paper is that strong performance of overfitted classifiers is not a unique feature of deep learning. Using six real-world and two synthetic datasets, we establish experimentally that kernel machines trained to have zero classification or near zero regression error perform very well on test data, even when the labels are corrupted with a high level of noise. We proceed to give a lower bound on the norm of zero loss solutions for smooth kernels, showing that they increase nearly exponentially with data size. We point out that this is difficult to reconcile with the existing generalization bounds. Moreover, none of the bounds produce non-trivial results for interpolating solutions. Second, we show experimentally that (non-smooth) Laplacian kernels easily fit random labels, a finding that parallels results for ReLU neural networks. In contrast, fitting noisy data requires many more epochs for smooth Gaussian kernels. Similar performance of overfitted Laplacian and Gaussian classifiers on test, suggests that generalization is tied to the properties of the kernel function rather than the optimization process. Certain key phenomena of deep learning are manifested similarly in kernel methods in the modern "overfitted" regime. The combination of the experimental and theoretical results presented in this paper indicates a need for new theoretical ideas for understanding properties of classical kernel methods. We argue that progress on understanding deep learning will be difficult until more tractable "shallow" kernel methods are better understood.},
	urldate = {2020-08-07},
	journal = {arXiv:1802.01396 [cs, stat]},
	author = {Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.01396},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wang_understanding_2018,
	title = {Towards {Understanding} {Learning} {Representations}: {To} {What} {Extent} {Do} {Different} {Neural} {Networks} {Learn} the {Same} {Representation}},
	shorttitle = {Towards {Understanding} {Learning} {Representations}},
	url = {https://arxiv.org/abs/1810.11750},
	language = {en},
	urldate = {2018-12-06},
	author = {Wang, Liwei and Hu, Lunjia and Gu, Jiayuan and Wu, Yue and Hu, Zhiqiang and He, Kun and Hopcroft, John},
	month = oct,
	year = {2018},
}

@article{finlayson_generative_2018,
	title = {Towards generative adversarial networks as a new paradigm for radiology education},
	url = {https://arxiv.org/abs/1812.01547v1},
	language = {en},
	urldate = {2018-12-13},
	author = {Finlayson, Samuel G. and Lee, Hyunkwang and Kohane, Isaac S. and Oakden-Rayner, Luke},
	month = dec,
	year = {2018},
}

@book{mutto_timeofflight_2012,
	title = {Time-of-flight cameras and microsoft kinect ({TM})},
	publisher = {Springer Publishing Company, Incorporated},
	author = {Mutto, Carlo Dal and Zanuttigh, Pietro and Cortelazzo, Guido M},
	year = {2012},
}

@inproceedings{geisert_trajectory_2016,
	title = {Trajectory generation for quadrotor based systems using numerical optimal control},
	isbn = {1-4673-8026-1},
	publisher = {IEEE},
	author = {Geisert, Mathieu and Mansard, Nicolas},
	year = {2016},
	pages = {2958--2964},
}

@book{eykhoff_trends_2014,
	title = {Trends and {Progress} in {System} {Identification}: {Ifac} {Series} for {Graduates}, {Research} {Workers} \& {Practising} {Engineers}},
	isbn = {978-1-4831-4866-3},
	shorttitle = {Trends and {Progress} in {System} {Identification}},
	abstract = {Trends and Progress in System Identification is a three-part book that focuses on model considerations, identification methods, and experimental conditions involved in system identification. Organized into 10 chapters, this book begins with a discussion of model method in system identification, citing four examples differing on the nature of the models involved, the nature of the fields, and their goals. Subsequent chapters describe the most important aspects of model theory; the ""classical"" methods and time series estimation; application of least squares and related techniques for the estimation of dynamic system parameters; the maximum likelihood and error prediction methods; and the modern development of statistical methods. Non-parametric approaches, identification of nonlinear systems by piecewise approximation, and the minimax identification are then explained. Other chapters explore the Bayesian approach to system identification; choice of input signals; and choice and effect of different feedback configurations in system identification. This book will be useful for control engineers, system scientists, biologists, and members of other disciplines dealing withdynamical relations.},
	language = {en},
	publisher = {Elsevier},
	author = {Eykhoff, Pieter},
	month = may,
	year = {2014},
	note = {Google-Books-ID: hcyjBQAAQBAJ},
	keywords = {Mathematics / Calculus, Mathematics / Mathematical Analysis},
}

@article{kim_languageuniversal_2017,
	title = {Towards {Language}-{Universal} {End}-to-{End} {Speech} {Recognition}},
	url = {http://arxiv.org/abs/1711.02207},
	abstract = {Building speech recognizers in multiple languages typically involves replicating a monolingual training recipe for each language, or utilizing a multi-task learning approach where models for different languages have separate output labels but share some internal parameters. In this work, we exploit recent progress in end-to-end speech recognition to create a single multilingual speech recognition system capable of recognizing any of the languages seen in training. To do so, we propose the use of a universal character set that is shared among all languages. We also create a language-specific gating mechanism within the network that can modulate the network's internal representations in a language-specific way. We evaluate our proposed approach on the Microsoft Cortana task across three languages and show that our system outperforms both the individual monolingual systems and systems built with a multi-task learning approach. We also show that this model can be used to initialize a monolingual speech recognizer, and can be used to create a bilingual model for use in code-switching scenarios.},
	journal = {arXiv:1711.02207 [cs]},
	author = {Kim, Suyoun and Seltzer, Michael L.},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.02207},
	keywords = {Computer Science - Computation and Language, 🔍No DOI found},
}

@book{munkres_topology_2000,
	series = {Featured {Titles} for {Topology} {Series}},
	title = {Topology},
	isbn = {978-0-13-181629-9},
	url = {https://books.google.com.br/books?id=XjoZAQAAIAAJ},
	publisher = {Prentice Hall, Incorporated},
	author = {Munkres, J.R.},
	year = {2000},
}

@article{sigurgeirsson_transport_2003,
	title = {Transport coefficients of hard sphere fluids},
	volume = {101},
	url = {http://www.tandfonline.com/doi/abs/10.1080/0026897021000037717},
	doi = {10.1080/0026897021000037717},
	number = {3},
	urldate = {2017-09-18},
	journal = {Molecular Physics},
	author = {Sigurgeirsson, H. and Heyes, D. M.},
	year = {2003},
	note = {00000},
	pages = {469--482},
}

@article{hagan_training_1994,
	title = {Training feedforward networks with the {Marquardt} algorithm},
	volume = {5},
	doi = {10.1109/72.329697},
	number = {6},
	journal = {Neural Networks, IEEE Transactions on},
	author = {Hagan, Martin T and Menhaj, Mohammad B},
	year = {1994},
	keywords = {Acceleration, Approximation algorithms, Backpropagation algorithms, Convergence, Function approximation, Least squares approximation, Least squares methods, Marquardt algorithm, Neural networks, Testing, backpropagation, feedforward network training, feedforward neural nets, feedforward neural networks, learning, least squares approximations, nonlinear least squares},
	pages = {989--993},
}

@article{bai_trellis_2018,
	title = {Trellis {Networks} for {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1810.06682},
	abstract = {We present trellis networks, a new architecture for sequence modeling. On the one hand, a trellis network is a temporal convolutional network with special structure, characterized by weight tying across depth and direct injection of the input into deep layers. On the other hand, we show that truncated recurrent networks are equivalent to trellis networks with special sparsity structure in their weight matrices. Thus trellis networks with general weight matrices generalize truncated recurrent networks. We leverage these connections to design high-performing trellis networks that absorb structural and algorithmic elements from both recurrent and convolutional models. Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling tasks, and stress tests designed to evaluate long-term memory retention. The code is available at https://github.com/locuslab/trellisnet .},
	urldate = {2019-06-06},
	journal = {arXiv:1810.06682 [cs, stat]},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.06682},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chandar_nonsaturating_2019,
	title = {Towards {Non}-{Saturating} {Recurrent} {Units} for {Modelling} {Long}-{Term} {Dependencies}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/4200},
	doi = {10/gf8gdm},
	abstract = {Modelling long-term dependencies is a challenge for recurrent neural networks. This is primarily due to the fact that gradients vanish during training, as the sequence length increases. Gradients can be attenuated by transition operators and are attenuated or dropped by activation functions. Canonical architectures like LSTM alleviate this issue by skipping information through a memory mechanism. We propose a new recurrent architecture (Non-saturating Recurrent Unit; NRU) that relies on a memory mechanism but forgoes both saturating activation functions and saturating gates, in order to further alleviate vanishing gradients. In a series of synthetic and real world tasks, we demonstrate that the proposed model is the only model that performs among the top 2 models across all tasks with and without long-term dependencies, when compared against a range of other architectures.},
	language = {en},
	urldate = {2019-09-18},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chandar, Sarath and Sankar, Chinnadhurai and Vorontsov, Eugene and Kahou, Samira Ebrahimi and Bengio, Yoshua},
	month = jul,
	year = {2019},
	pages = {3280--3287},
}

@inproceedings{geisert_trajectory_2016a,
	title = {Trajectory {Generation} for {Quadrotor} {Based} {Systems} {Using} {Numerical} {Optimal} {Control}},
	isbn = {1-4673-8026-1},
	booktitle = {2016 {IEEE} international conference on robotics and automation ({ICRA})},
	publisher = {IEEE},
	author = {Geisert, Mathieu and Mansard, Nicolas},
	year = {2016},
	pages = {2958--2964},
}

@book{tao_topics_2012,
	series = {Graduate {Studies} in {Mathematics}},
	title = {Topics in random matrix theory},
	volume = {132},
	language = {en},
	publisher = {American Mathematical Society},
	author = {Tao, Terence},
	year = {2012},
}

@inproceedings{pennington_spectrum_2018,
	title = {The spectrum of the fisher information matrix of a single-hidden-layer neural network},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Pennington, Jeffrey and Worah, Pratik},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
}

@misc{karpathy_unreasonable_2015,
	title = {The {Unreasonable} {Effectiveness} of {Recurrent} {Neural} {Networks}},
	url = {http://karpathy.github.io/2015/05/21/rnn-effectiveness/},
	urldate = {2020-01-27},
	journal = {Andrej Karpathy blog},
	author = {Karpathy, Andrej},
	month = may,
	year = {2015},
}

@article{saito_precisionrecall_2015,
	title = {The {Precision}-{Recall} {Plot} {Is} {More} {Informative} than the {ROC} {Plot} {When} {Evaluating} {Binary} {Classifiers} on {Imbalanced} {Datasets}},
	volume = {10},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432},
	doi = {10/f69237},
	abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
	language = {en},
	number = {3},
	urldate = {2019-02-08},
	journal = {PLOS ONE},
	author = {Saito, Takaya and Rehmsmeier, Marc},
	month = apr,
	year = {2015},
	keywords = {Bioinformatics, Caenorhabditis elegans, Exponential functions, Genome analysis, Genome-wide association studies, Interpolation, MicroRNAs, Support vector machines},
	pages = {e0118432},
}

@inproceedings{macfarlane_university_2005,
	title = {The university of glasgow ({Uni}-{G}) {ECG} analysis program},
	isbn = {0276-6574},
	doi = {10.1109/CIC.2005.1588134},
	booktitle = {Computers in {Cardiology}},
	author = {Macfarlane, P. W. and Devine, B. and Clark, E.},
	year = {2005},
	keywords = {Cardiology, Computer aided manufacturing, Databases, Electrocardiography, Instruments, Pacemakers, Pediatrics, Signal analysis, Signal processing, Terminology, descriptor Uni-G ECG analysis program, diagnostic software, electrocardiography, medical signal processing, neonates, nomenclature, racial variation, signal processing, terminology},
	pages = {451--454},
}

@article{schittkowski_nonlinear_1982,
	title = {The nonlinear programming method of {Wilson}, {Han}, and {Powell} with an augmented {Lagrangian} type line search function. {Part} 1: {Convergence} analysis.},
	volume = {38},
	issn = {0029-599X, 0945-3245},
	url = {https://link.springer.com/article/10.1007/BF01395810},
	doi = {10.1007/BF01395810},
	abstract = {SummaryThe paper represents an outcome of an extensive comparative study of nonlinear optimization algorithms. This study indicates that quadratic approximation methods which are characterized by solving a sequence of quadratic subproblems recursively, belong to the most efficient and reliable nonlinear programming algorithms available at present. The purpose of this paper is to analyse the theoretical convergence properties and to investigate the numerical performance in more detail. In Part 1, the exactL1-penalty function of Han and Powell is replaced by a differentiable augmented Lagrange function for the line search computation to be able to prove the global convergence and to show that the steplength one is chosen in the neighbourhood of a solution. In Part 2, the quadratic subproblem is exchanged by a linear least squares problem to improve the efficiency, and to test the dependence of the performance from different solution methods for the quadratic or least squares subproblem.},
	language = {en},
	number = {1},
	urldate = {2018-03-28},
	journal = {Numerische Mathematik},
	author = {Schittkowski, Klaus},
	month = feb,
	year = {1982},
	note = {00000},
	pages = {83--114},
}

@article{diaconescu_use_2008,
	title = {The use of {NARX} neural networks to predict chaotic time series},
	volume = {3},
	number = {3},
	journal = {WSEAS Transactions on Computer Research},
	author = {Diaconescu, Eugen},
	year = {2008},
	keywords = {🔍No DOI found},
	pages = {182--191},
}

@article{demoor_restricted_1991,
	title = {The {Restricted} {Singular} {Value} {Decomposition}: {Properties} and {Applications}},
	volume = {12},
	issn = {0895-4798},
	shorttitle = {The {Restricted} {Singular} {Value} {Decomposition}},
	url = {http://epubs.siam.org/doi/abs/10.1137/0612029},
	doi = {10.1137/0612029},
	abstract = {The restricted singular value decomposition (RSVD) is the factorization of a given matrix, relative to two other given matrices. It can be interpreted as the ordinary singular value decomposition with different inner products in row and column spaces. Its properties and structure, as well as its connection to generalized eigenvalue problems, canonical correlation analysis, and other generalizations of the singular value decomposition, are investigated in detail.Applications that are discussed include the analysis of the extended shorted operator, unitarily invariant norm minimization with rank constraints, rank minimization in matrix balls, the analysis and solution of linear matrix equations, rank minimization of a partitioned matrix, and the connection with generalized Schur complements, constrained linear and total linear least squares problems with mixed exact and noisy data, including a generalized Gauss–Markov estimation scheme.},
	number = {3},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {De Moor, B. and Golub, G.},
	month = jul,
	year = {1991},
	pages = {401--425},
}

@inproceedings{bousquet_tradeoffs_2008,
	title = {The tradeoffs of large scale learning},
	url = {http://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning},
	urldate = {2017-09-11},
	booktitle = {Advances in neural information processing systems},
	author = {Bousquet, Olivier and Bottou, Léon},
	year = {2008},
	pages = {161--168},
}

@inproceedings{wigren_three_2013,
	title = {Three free data sets for development and benchmarking in nonlinear system identification},
	doi = {10.23919/ECC.2013.6669201},
	abstract = {System identification is a fundamentally experimental field of science in that it deals with modeling of system dynamics using measured data. Despite this fact many algorithms and theoretical results are only tested with simulations at the time of publication. One reason for this may be a lack of easily available live data. This paper therefore presents three sets of data, suitable for development, testing and benchmarking of system identification algorithms for nonlinear systems. The data sets are collected from laboratory processes that can be described by block - oriented dynamic models, and by more general nonlinear difference and differential equation models. All data sets are available for free download.},
	booktitle = {2013 {European} {Control} {Conference} ({ECC})},
	author = {Wigren, T. and Schoukens, J.},
	month = jul,
	year = {2013},
	keywords = {Clocks, Heuristic algorithms, Laboratories, Mathematical model, Nonlinear dynamical systems, Silver, block-oriented dynamic models, differential equation models, differential equations, identification, nonlinear control systems, nonlinear difference models, nonlinear system identification, system dynamics, system identification benchmarking, system identification development},
	pages = {2933--2938},
}

@book{box_time_2015,
	title = {Time series analysis: forecasting and control},
	shorttitle = {Time series analysis},
	publisher = {John Wiley \& Sons},
	author = {Box, George EP and Jenkins, Gwilym M. and Reinsel, Gregory C. and Ljung, Greta M.},
	year = {2015},
}

@article{hochreiter_vanishing_1998,
	title = {The {Vanishing} {Gradient} {Problem} {During} {Learning} {Recurrent} {Neural} {Nets} and {Problem} {Solutions}},
	volume = {06},
	issn = {0218-4885, 1793-6411},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218488598000094},
	doi = {10.1142/S0218488598000094},
	language = {en},
	number = {02},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	author = {Hochreiter, Sepp},
	month = apr,
	year = {1998},
	pages = {107--116},
}

@article{guzman_use_2017,
	title = {The {Use} of {NARX} {Neural} {Networks} to {Forecast} {Daily} {Groundwater} {Levels}},
	volume = {31},
	doi = {10.1007/s11269-017-1598-5},
	number = {5},
	journal = {Water Resources Management},
	author = {Guzman, Sandra M and Paz, Joel O and Tagert, Mary Love M},
	year = {2017},
	pages = {1591--1603},
}

@inproceedings{bengio_problem_1993,
	title = {The problem of learning long-term dependencies in recurrent networks},
	doi = {10/d7zs24},
	abstract = {The authors seek to train recurrent neural networks in order to map input sequences to output sequences, for applications in sequence recognition or production. Results are presented showing that learning long-term dependencies in such recurrent networks using gradient descent is a very difficult task. It is shown how this difficulty arises when robustly latching bits of information with certain attractors. The derivatives of the output at time t with respect to the unit activations at time zero tend rapidly to zero as t increases for most input values. In such a situation, simple gradient descent techniques appear inappropriate. The consideration of alternative optimization methods and architectures is suggested.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	booktitle = {{IEEE} {International} {Conference} on {Neural} {Networks}},
	author = {Bengio, Y. and Frasconi, P. and Simard, P.},
	month = mar,
	year = {1993},
	keywords = {Background noise, Discrete transforms, Intelligent networks, Neural networks, Optimization methods, Production, Recurrent neural networks, Robustness, Speech, Text recognition, gradient descent, input sequences, learning (artificial intelligence), long-term dependencies, neural networks, optimization methods, output sequences, recurrent networks, recurrent neural nets, sequence recognition, unit activations},
	pages = {1183--1188 vol.3},
}

@article{guzman_use_2017a,
	title = {The {Use} of {NARX} {Neural} {Networks} to {Forecast} {Daily} {Groundwater} {Levels}},
	volume = {31},
	doi = {10/f92hfc},
	number = {5},
	journal = {Water Resources Management},
	author = {Guzman, Sandra M and Paz, Joel O and Tagert, Mary Love M},
	year = {2017},
	pages = {1591--1603},
}

@article{schittkowski_nonlinear_1982a,
	title = {The nonlinear programming method of {Wilson}, {Han}, and {Powell} with an augmented {Lagrangian} type line search function. {Part} 2: {An} efficient implementation with linear least squares subproblems},
	volume = {38},
	issn = {0029-599X, 0945-3245},
	url = {https://link.springer.com/article/10.1007/BF01395811},
	doi = {10.1007/BF01395811},
	abstract = {SummaryThe paper represents an outcome of an extensive comparative study of nonlinear optimization algorithms. This study indicates that quadratic approximation methods which are characterized by solving a sequence of quadratic subproblems recursively, belong to the most efficient and reliable nonlinear programming algorithms available at present. The purpose of this paper is to analyse the theoretical convergence properties and to investigate the numerical performance in more detail. In Part 1, the exactL1-penalty function of Han and Powell is replaced by a differentiable augmented Lagrange function for the line search computation to the able to prove the global convergence and to show that the steplength one is chosen in the neighbourhood of a solution. In Part 2, the quadratic subproblem is exchanged by a linear least squares problem to improve the efficiency, and to test the dependence of the performance from different solution methods for the quadratic or least squares subproblems.},
	language = {en},
	number = {1},
	urldate = {2018-03-28},
	journal = {Numerische Mathematik},
	author = {Schittkowski, Klaus},
	month = feb,
	year = {1982},
	note = {00000},
	pages = {115--127},
}

@inproceedings{verdie_tilde_2015,
	title = {{TILDE}: {A} {Temporally} {Invariant} {Learned} {DEtector}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Verdie, Yannick and Yi, Kwang and Fua, Pascal and Lepetit, Vincent},
	year = {2015},
	note = {00000},
	pages = {5279--5288},
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: a probabilistic model for information storage and organization in the brain.},
	volume = {65},
	issn = {1939-1471},
	number = {6},
	journal = {Psychological review},
	author = {Rosenblatt, Frank},
	year = {1958},
	pages = {386},
}

@article{adlam_neural_2020,
	title = {The {Neural} {Tangent} {Kernel} in {High} {Dimensions}: {Triple} {Descent} and a {Multi}-{Scale} {Theory} of {Generalization}},
	abstract = {Modern deep learning models employ considerably more parameters than required to ﬁt the training data. Whereas conventional statistical wisdom suggests such models should drastically overﬁt, in practice these models generalize remarkably well. An emerging paradigm for describing this unexpected behavior is in terms of a double descent curve, in which increasing a model’s capacity causes its test error to ﬁrst decrease, then increase to a maximum near the interpolation threshold, and then decrease again in the overparameterized regime. Recent efforts to explain this phenomenon theoretically have focused on simple settings, such as linear regression or kernel regression with unstructured random features, which we argue are too coarse to reveal important nuances of actual neural networks. We provide a precise high-dimensional asymptotic analysis of generalization under kernel regression with the Neural Tangent Kernel, which characterizes the behavior of wide neural networks optimized with gradient descent. Our results reveal that the test error has non-monotonic behavior deep in the overparameterized regime and can even exhibit additional peaks and descents when the number of parameters scales quadratically with the dataset size.},
	language = {en},
	journal = {Proceedings of the 37 th International Conference on Machine Learning, PMLR 119},
	author = {Adlam, Ben and Pennington, Jeffrey},
	year = {2020},
}

@misc{_lottery_2020,
	title = {The {Lottery} {Ticket} {Hypothesis}: {A} {Survey}},
	shorttitle = {The {Lottery} {Ticket} {Hypothesis}},
	url = {https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/},
	abstract = {Metaphors are powerful tools to transfer ideas from one mind to another. Alan Kay introduced the alternative meaning of the term ‘desktop’ at Xerox PARC in 1970. Nowadays everyone - for a glimpse of a second - has to wonder what is actually meant when referring to a desktop. Recently, Deep Learning had the pleasure to welcome a new powerful metaphor: The Lottery Ticket Hypothesis (LTH).},
	language = {en},
	urldate = {2020-06-29},
	journal = {Rob's Homepage},
	month = jun,
	year = {2020},
	note = {Library Catalog: roberttlange.github.io},
}

@article{lange_lottery_2020,
	title = {The lottery ticket hypothesis: {A} survey},
	url = {https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/},
	journal = {https://roberttlange.github.io/year-archive/posts/2020/06/lottery-ticket-hypothesis/},
	author = {Lange, Robert Tjarko},
	year = {2020},
}

@book{prineas_minnesota_2010,
	address = {London},
	title = {The {Minnesota} {Code} {Manual} of {Electrocardiographic} {Findings}},
	isbn = {978-1-84882-777-6 978-1-84882-778-3},
	url = {http://link.springer.com/10.1007/978-1-84882-778-3},
	language = {en},
	urldate = {2020-07-08},
	publisher = {Springer London},
	author = {Prineas, Ronald J. and Crow, Richard S. and Zhang, Zhu-Ming},
	year = {2010},
	doi = {10.1007/978-1-84882-778-3},
}

@article{daskalakis_limit_2018,
	title = {The {Limit} {Points} of ({Optimistic}) {Gradient} {Descent} in {Min}-{Max} {Optimization}},
	url = {http://arxiv.org/abs/1807.03907},
	abstract = {Motivated by applications in Optimization, Game Theory, and the training of Generative Adversarial Networks, the convergence properties of first order methods in min-max problems have received extensive study. It has been recognized that they may cycle, and there is no good understanding of their limit points when they do not. When they converge, do they converge to local min-max solutions? We characterize the limit points of two basic first order methods, namely Gradient Descent/Ascent (GDA) and Optimistic Gradient Descent Ascent (OGDA). We show that both dynamics avoid unstable critical points for almost all initializations. Moreover, for small step sizes and under mild assumptions, the set of {\textbackslash}\{OGDA{\textbackslash}\}-stable critical points is a superset of {\textbackslash}\{GDA{\textbackslash}\}-stable critical points, which is a superset of local min-max solutions (strict in some cases). The connecting thread is that the behavior of these dynamics can be studied from a dynamical systems perspective.},
	urldate = {2018-12-13},
	journal = {arXiv:1807.03907 [math, stat]},
	author = {Daskalakis, Constantinos and Panageas, Ioannis},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.03907},
	keywords = {Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@book{prineas_minnesota_2009,
	title = {The {Minnesota} code manual of electrocardiographic findings},
	isbn = {1-84882-778-4},
	publisher = {Springer Science \& Business Media},
	author = {Prineas, Ronald J and Crow, Richard S and Zhang, Zhu-Ming},
	year = {2009},
}

@article{frankle_lottery_2018,
	title = {The {Lottery} {Ticket} {Hypothesis}: {Finding} {Small}, {Trainable} {Neural} {Networks}},
	shorttitle = {The {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1803.03635},
	abstract = {Neural network compression techniques are able to reduce the parameter counts of trained networks by over 90\%--decreasing storage requirements and improving inference performance--without compromising accuracy. However, contemporary experience is that it is difficult to train small architectures from scratch, which would similarly improve training performance. We articulate a new conjecture to explain why it is easier to train large networks: the "lottery ticket hypothesis." It states that large networks that train successfully contain subnetworks that--when trained in isolation--converge in a comparable number of iterations to comparable accuracy. These subnetworks, which we term "winning tickets," have won the initialization lottery: their connections have initial weights that make training particularly effective. We find that a standard technique for pruning unnecessary network weights naturally uncovers a subnetwork which, at the start of training, comprised a winning ticket. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis. We consistently find winning tickets that are less than 20\% of the size of several fully-connected, convolutional, and residual architectures for MNIST and CIFAR10. Furthermore, winning tickets at moderate levels of pruning (20-50\% of the original network size) converge up to 6.7x faster than the original network and exhibit higher test accuracy.},
	urldate = {2018-11-06},
	journal = {arXiv:1803.03635 [cs]},
	author = {Frankle, Jonathan and Carbin, Michael},
	month = mar,
	year = {2018},
	note = {00000 
arXiv: 1803.03635},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{hoffman_nouturn_2011,
	title = {The {No}-{U}-{Turn} {Sampler}: {Adaptively} {Setting} {Path} {Lengths} in {Hamiltonian} {Monte} {Carlo}},
	shorttitle = {The {No}-{U}-{Turn} {Sampler}},
	url = {http://arxiv.org/abs/1111.4246},
	abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{{\textbackslash}epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{{\textbackslash}epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
	journal = {arXiv:1111.4246 [cs, stat]},
	author = {Hoffman, Matthew D. and Gelman, Andrew},
	month = nov,
	year = {2011},
	note = {arXiv: 1111.4246},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, 🔍No DOI found},
}

@book{vapnik_nature_2013,
	series = {Information {Science} and {Statistics}},
	title = {The {Nature} of {Statistical} {Learning} {Theory}},
	isbn = {978-1-4757-3264-1},
	url = {https://books.google.com.br/books?id=EqgACAAAQBAJ},
	publisher = {Springer New York},
	author = {Vapnik, V.},
	year = {2013},
	note = {00000},
}

@incollection{powell_newuoa_2006,
	title = {The {NEWUOA} software for unconstrained optimization without derivatives},
	booktitle = {Large-{Scale} {Nonlinear} {Optimization}},
	publisher = {Springer},
	author = {Powell, Michael J. D.},
	year = {2006},
	note = {00000},
	pages = {255--297},
}

@inproceedings{lejeune_implicit_2020,
	series = {{PMLR}},
	title = {The implicit regularization of ordinary least squares ensembles},
	volume = {108},
	url = {http://proceedings.mlr.press/v108/lejeune20b.html},
	abstract = {Ensemble methods that average over a collection of independent predictors that are each limited to a subsampling of both the examples and features of the training data command a significant presence in machine learning, such as the ever-popular random forest, yet the nature of the subsampling effect, particularly of the features, is not well understood. We study the case of an ensemble of linear predictors, where each individual predictor is fit using ordinary least squares on a random submatrix of the data matrix. We show that, under standard Gaussianity assumptions, when the number of features selected for each predictor is optimally tuned, the asymptotic risk of a large ensemble is equal to the asymptotic ridge regression risk, which is known to be optimal among linear predictors in this setting. In addition to eliciting this implicit regularization that results from subsampling, we also connect this ensemble to the dropout technique used in training deep (neural) networks, another strategy that has been shown to have a ridge-like regularizing effect.},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {LeJeune, Daniel and Javadi, Hamid and Baraniuk, Richard},
	editor = {Chiappa, Silvia and Calandra, Roberto},
	year = {2020},
	pages = {3525--3535},
}

@article{soudry_implicit_2017,
	title = {The {Implicit} {Bias} of {Gradient} {Descent} on {Separable} {Data}},
	url = {https://arxiv.org/abs/1710.10345},
	language = {en},
	urldate = {2018-12-13},
	author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
	month = oct,
	year = {2017},
}

@incollection{more_levenbergmarquardt_1978,
	title = {The {Levenberg}-{Marquardt} algorithm: implementation and theory},
	booktitle = {Numerical {Analysis}},
	publisher = {Springer},
	author = {Moré, Jorge J},
	year = {1978},
	pages = {105--116},
}

@article{batselier_geometry_2013,
	title = {The {Geometry} of {Multivariate} {Polynomial} {Division} and {Elimination}},
	volume = {34},
	issn = {0895-4798},
	url = {http://epubs.siam.org/doi/abs/10.1137/120863782},
	doi = {10.1137/120863782},
	abstract = {Multivariate polynomials are usually discussed in the framework of algebraic geometry. Solving problems in algebraic geometry usually involves the use of a Gröbner basis. This article shows that linear algebra without any Gröbner basis computation suffices to solve basic problems from algebraic geometry by describing three operations: multiplication, division, and elimination. This linear algebra framework will also allow us to give a geometric interpretation. Multivariate division will involve oblique projections, and a link between elimination and principal angles between subspaces (CS decomposition) is revealed. The main computational tool in this approach is the QR decomposition.},
	number = {1},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Batselier, K. and Dreesen, P. and Moor, B.},
	month = jan,
	year = {2013},
	pages = {102--125},
}

@article{meier_group_2008,
	title = {The {Group} {Lasso} for {Logistic} {Regression}},
	volume = {70},
	issn = {1369-7412},
	url = {http://www.jstor.org/stable/20203811},
	doi = {10.1111/j.1467-9868.2007.00627.x},
	abstract = {The group lasso is an extension of the lasso to do variable selection on (predefined) groups of variables in linear regression models. The estimates have the attractive property of being invariant under groupwise orthogonal reparameterizations. We extend the group lasso to logistic regression models and present an efficient algorithm, that is especially suitable for high dimensional problems, which can also be applied to generalized linear models to solve the corresponding convex optimization problem. The group lasso estimator for logistic regression is shown to be statistically consistent even if the number of predictors is much larger than sample size but with sparse true underlying structure. We further use a two-stage procedure which aims for sparser models than the group lasso, leading to improved prediction performance for some cases. Moreover, owing to the two-stage nature, the estimates can be constructed to be hierarchical. The methods are used on simulated and real data sets about splice site detection in DNA sequences.},
	number = {1},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Meier, Lukas and van de Geer, Sara and Bühlmann, Peter},
	year = {2008},
	pages = {53--71},
}

@article{hurvich_impact_1990,
	title = {The {Impact} of {Model} {Selection} on {Inference} in {Linear} {Regression}},
	volume = {44},
	issn = {0003-1305},
	url = {http://amstat.tandfonline.com/doi/abs/10.1080/00031305.1990.10475722},
	doi = {10.1080/00031305.1990.10475722},
	number = {3},
	journal = {The American Statistician},
	author = {Hurvich, Clifford M. and Tsai, Chih—Ling},
	month = aug,
	year = {1990},
	pages = {214--217},
}

@book{zurawski_industrial_2005,
	address = {Boca Raton, Fla},
	series = {The industrial information technology series},
	title = {The industrial communication technology handbook},
	isbn = {978-0-8493-3077-3},
	number = {1},
	publisher = {Taylor \& Francis},
	editor = {Zurawski, Richard},
	year = {2005},
	keywords = {Computer networks, Data transmission systems, Wireless communication systems},
}

@article{berger_formal_2009,
	title = {The formal definition of reference priors},
	volume = {37},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/0904.0156},
	doi = {10.1214/07-AOS587},
	abstract = {Reference analysis produces objective Bayesian inference, in the sense that inferential statements depend only on the assumed model and the available data, and the prior distribution used to make an inference is least informative in a certain information-theoretic sense. Reference priors have been rigorously defined in specific contexts and heuristically defined in general, but a rigorous general definition has been lacking. We produce a rigorous general definition here and then show how an explicit expression for the reference prior can be obtained under very weak regularity conditions. The explicit expression can be used to derive new reference priors both analytically and numerically.},
	number = {2},
	urldate = {2018-10-07},
	journal = {The Annals of Statistics},
	author = {Berger, James O. and Bernardo, José M. and Sun, Dongchu},
	month = apr,
	year = {2009},
	note = {arXiv: 0904.0156},
	keywords = {62F15 (Primary) 62A01, 62B10 (Secondary), Mathematics - Statistics Theory},
	pages = {905--938},
}

@misc{alammar_illustrated_b,
	title = {The {Illustrated} {Transformer}},
	url = {http://jalammar.github.io/illustrated-transformer/},
	abstract = {Discussions:
Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)


Translations: Chinese (Simplified), Korean

Watch: MIT’s Deep Learning State of the Art lecture referencing this post

In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.

The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.

A High-Level Look
Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
	urldate = {2019-06-26},
	author = {Alammar, Jay},
}

@misc{alammar_illustrated_a,
	title = {The {Illustrated} {Transformer}},
	url = {http://jalammar.github.io/illustrated-transformer/},
	abstract = {Discussions:
Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)


Translations: Chinese (Simplified), Korean

Watch: MIT’s Deep Learning State of the Art lecture referencing this post

In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.

The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.

A High-Level Look
Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
	urldate = {2019-06-10},
	author = {Alammar, Jay},
}

@misc{alammar_illustrated_,
	title = {The {Illustrated} {BERT}, {ELMo}, and co. ({How} {NLP} {Cracked} {Transfer} {Learning})},
	url = {http://jalammar.github.io/illustrated-bert/},
	abstract = {Discussions:
Hacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)


Translations: Chinese (Simplified), Persian

The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).},
	urldate = {2019-06-08},
	author = {Alammar, Jay},
}

@inproceedings{paperno_lambada_2016,
	title = {The {LAMBADA} dataset: {Word} prediction requiring a broad discourse context},
	shorttitle = {The {LAMBADA} dataset},
	url = {https://www.aclweb.org/anthology/P16-1144/},
	doi = {10/gf7pwc},
	language = {en-us},
	urldate = {2019-09-08},
	author = {Paperno, Denis and Kruszewski, Germán and Lazaridou, Angeliki and Pham, Ngoc-Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fernández, Raquel},
	month = aug,
	year = {2016},
	pages = {1525--1534},
}

@article{ashton_that_2009,
	title = {That ‘internet of things’ thing},
	volume = {22},
	number = {7},
	journal = {RFID journal},
	author = {Ashton, Kevin},
	year = {2009},
	pages = {97--114},
}

@article{ribeiro_teleelectrocardiography_2019,
	title = {Tele-electrocardiography and bigdata: {The} {CODE} ({Clinical} {Outcomes} in {Digital} {Electrocardiography}) study},
	copyright = {All rights reserved},
	issn = {0022-0736},
	doi = {10/gf7pwg},
	abstract = {Digital electrocardiographs are now widely available and a large number of digital electrocardiograms (ECGs) have been recorded and stored. The present study describes the development and clinical applications of a large database of such digital ECGs, namely the CODE (Clinical Outcomes in Digital Electrocardiology) study. ECGs obtained by the Telehealth Network of Minas Gerais, Brazil, from 2010 to 17, were organized in a structured database. A hierarchical free-text machine learning algorithm recognized specific ECG diagnoses from cardiologist reports. The Glasgow ECG Analysis Program provided Minnesota Codes and automatic diagnostic statements. The presence of a specific ECG abnormality was considered when both automatic and medical diagnosis were concordant; cases of discordance were decided using heuristisc rules and manual review. The ECG database was linked to the national mortality information system using probabilistic linkage methods. From 2,470,424 ECGs, 1,773,689 patients were identified. After excluding the ECGs with technical problems and patients {\textless}16 years-old, 1,558,415 patients were studied. High performance measures were obtained using an end-to-end deep neural network trained to detect 6 types of ECG abnormalities, with F1 scores {\textgreater}80\% and specificity {\textgreater}99\% in an independent test dataset. We also evaluated the risk of mortality associated with the presence of atrial fibrillation (AF), which showed that AF was a strong predictor of cardiovascular mortality and mortality for all causes, with increased risk in women. In conclusion, a large database that comprises all ECGs performed by a large telehealth network can be useful for further developments in the field of digital electrocardiography, clinical cardiology and cardiovascular epidemiology.},
	journal = {Journal of Electrocardiology},
	author = {Ribeiro, Antonio Luiz P. and Paixão, Gabriela M. M. and Gomes, Paulo R. and Ribeiro, Manoel Horta and Ribeiro, Antônio H. and Canazart, Jéssica A. and Oliveira, Derick M. and Ferreira, Milton P. and Lima, Emilly M. and de Moraes, Jermana Lopes and Castro, Nathalia and Ribeiro, Leonardo B. and MacFarlane, Peter W.},
	month = sep,
	year = {2019},
	keywords = {Artificial intelligence, Big-data, Electrocardiography, Telehealth},
}

@techreport{abadi_tensorflow_2015,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Systems}},
	url = {http://tensorflow.org/},
	author = {Abadi, Mart́ın and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2015},
	keywords = {⛔ No DOI found},
}

@inproceedings{frankle_early_2020,
	title = {The early phase of neural network training},
	url = {https://openreview.net/forum?id=Hkl1iRNFwS},
	booktitle = {International conference on learning representations},
	author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
	year = {2020},
}

@article{batselier_canonical_2014,
	title = {The {Canonical} {Decomposition} of \${\textbackslash}mathcal\{{C}\}{\textasciicircum}n\_d\$ and {Numerical} {Gröbner} and {Border} {Bases}},
	volume = {35},
	issn = {0895-4798},
	url = {http://epubs.siam.org/doi/abs/10.1137/130927176},
	doi = {10.1137/130927176},
	abstract = {This article introduces the canonical decomposition of the vector space of multivariate polynomials for a given monomial ordering. Its importance lies in solving multivariate polynomial systems, computing Gröbner bases, and solving the ideal membership problem. An SVD-based algorithm is presented that numerically computes the canonical decomposition. It is then shown how, by introducing the notion of divisibility into this algorithm, a numerical Gröbner basis can also be computed. In addition, we demonstrate how the canonical decomposition can be used to decide whether the affine solution set of a multivariate polynomial system is zero-dimensional and to solve the ideal membership problem numerically. The SVD-based canonical decomposition algorithm is also extended to numerically compute border bases. A tolerance for each of the algorithms is derived using perturbation theory of principal angles. This derivation shows that the condition number of computing the canonical decomposition and numerical Gröbner basis is essentially the condition number of the Macaulay matrix. Numerical experiments with both exact and noisy coefficients are presented and discussed.},
	number = {4},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Batselier, K. and Dreesen, P. and De Moor, B.},
	month = jan,
	year = {2014},
	pages = {1242--1264},
}

@inproceedings{faugeras_calibration_1986,
	title = {The calibration problem for stereo},
	volume = {86},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Faugeras, Olivier D and Toscani, Giorgio},
	year = {1986},
	pages = {15--20},
}

@article{broyden_convergence_1970,
	title = {The convergence of a class of double-rank minimization algorithms 1. general considerations},
	volume = {6},
	doi = {10.1093/imamat/6.1.76},
	number = {1},
	journal = {IMA Journal of Applied Mathematics},
	author = {Broyden, Charles George},
	year = {1970},
	pages = {76--90},
}

@article{boynton_analysis_2013,
	title = {The analysis of electron fluxes at geosynchronous orbit employing a {NARMAX} approach},
	volume = {118},
	doi = {10.1002/jgra.50192},
	number = {4},
	journal = {Journal of Geophysical Research: Space Physics},
	author = {Boynton, R. J. and Balikhin, M. A. and Billings, S. A. and Reeves, G. D. and Ganushkina, N and Gedalin, M and Amariutei, OA and Borovsky, J. E. and Walker, S. N.},
	year = {2013},
	pages = {1500--1513},
}

@article{willems_testing_1987,
	title = {Testing the performance of {ECG} computer programs: the {CSE} diagnostic pilot study},
	volume = {20 Suppl},
	issn = {0022-0736},
	shorttitle = {Testing the performance of {ECG} computer programs},
	abstract = {In an international project investigators from 21 institutes are trying to establish a common reference library and evaluation methods for testing the diagnostic performance of various ECG computer programs using ECG independent clinical information. Preliminary results indicate that the classification accuracy of different programs varies widely.},
	language = {eng},
	journal = {Journal of Electrocardiology},
	author = {Willems, J. L. and Abreu-Lima, C. and Arnaud, P. and van Bemmel, J. H. and Brohet, C. and Degani, R. and Denis, B. and Graham, I. and van Herpen, G. and Macfarlane, P. W.},
	month = oct,
	year = {1987},
	keywords = {Electrocardiography, Humans, Information Systems, Pilot Projects, Signal Processing, Computer-Assisted, Software, Software Validation},
	pages = {73--77},
}

@article{willems_diagnostic_1991,
	title = {The diagnostic performance of computer programs for the interpretation of electrocardiograms},
	volume = {325},
	issn = {0028-4793},
	doi = {10.1056/NEJM199112193252503},
	abstract = {BACKGROUND: Computer programs for the interpretation of electrocardiograms (ECGs) are now widely used. However, a systematic assessment of various computer programs for the interpretation of ECGs has not been performed.
METHODS: We undertook a large international study to compare the performance of nine electrocardiographic computer programs with that of eight cardiologists in interpreting ECGs in 1220 clinically validated cases of various cardiac disorders. ECGs from the following groups were included in the sample: control patients (n = 382); patients with left ventricular hypertrophy (n = 183), right ventricular hypertrophy (n = 55), or biventricular hypertrophy (n = 53); patients with anterior myocardial infarction (n = 170), inferior myocardial infarction (n = 273), or combined myocardial infarction (n = 73); and patients with combined infarction and hypertrophy (n = 31). The interpretations of the computer programs and the cardiologists were compared with the clinical diagnoses made independently of the ECGs, and the computer interpretations were compared with those of the cardiologists.
RESULTS: The percentage of ECGs correctly classified by the computer programs (median, 91.3 percent) was lower than that of the cardiologists (median, 96.0 percent; P less than 0.01). The median sensitivity of the computer programs was also significantly lower than that of the cardiologists in diagnosing left ventricular hypertrophy (56.6 percent vs. 63.9 percent, P less than 0.02), right ventricular hypertrophy (31.8 percent vs. 46.6 percent, P less than 0.01), anterior myocardial infarction (77.1 percent vs. 84.9 percent, P less than 0.001), and inferior myocardial infarction (58.8 percent vs. 71.7 percent, P less than 0.0001). The median total accuracy level (the percentage of correct classifications) was 6.6 percent lower for the computer programs (69.7 percent) than for the cardiologists (76.3 percent; P less than 0.001). However, the performance of the best programs nearly matched that of the most accurate cardiologists.
CONCLUSIONS: Our study shows that some but not all computer programs for the interpretation of ECGs perform almost as well as cardiologists in identifying seven major cardiac disorders.},
	language = {eng},
	number = {25},
	journal = {The New England Journal of Medicine},
	author = {Willems, J. L. and Abreu-Lima, C. and Arnaud, P. and van Bemmel, J. H. and Brohet, C. and Degani, R. and Denis, B. and Gehring, J. and Graham, I. and van Herpen, G.},
	month = dec,
	year = {1991},
	pmid = {1834940},
	keywords = {Cardiology, Cardiomegaly, Diagnosis, Computer-Assisted, Electrocardiography, Evaluation Studies as Topic, Humans, Myocardial Infarction, Sensitivity and Specificity, Software},
	pages = {1767--1773},
}

@article{candes_dantzig_2007,
	title = {The {Dantzig} {Selector}: {Statistical} {Estimation} {When} p {Is} {Much} {Larger} than n},
	volume = {35},
	issn = {0090-5364},
	shorttitle = {The {Dantzig} {Selector}},
	url = {http://www.jstor.org/stable/25464587},
	abstract = {In many important statistical applications, the number of variables or parameters p is much larger than the number of observations n. Suppose then that we have observations y = Xβ + z, where \${\textbackslash}beta {\textbackslash}in \{{\textbackslash}bf R\}{\textasciicircum}\{p\}\$ is a parameter vector of interest, X is a data matrix with possibly far fewer rows than columns, n « p, and the \$z\_\{i\}{\textbackslash}text\{'\}\{{\textbackslash}rm s\}\$ are i.i.d. N(0, σ²). Is it possible to estimate β reliably based on the noisy data y? To estimate β, we introduce a new estimator-we call it the Dantzig selector-which is a solution to the l₁-regularization problem \${\textbackslash}underset {\textbackslash}tilde\{{\textbackslash}beta\}{\textbackslash}in \{{\textbackslash}bf R\}{\textasciicircum}\{p\}{\textbackslash}to\{\{{\textbackslash}rm min\}\}{\textbackslash}{\textbar}{\textbackslash}tilde\{{\textbackslash}beta\}{\textbackslash}{\textbar}\_\{{\textbackslash}ell \_\{1\}\}\$ subject to \${\textbackslash}{\textbar}X{\textasciicircum}\{{\textbackslash}ast \}r{\textbackslash}{\textbar}\_\{{\textbackslash}ell \_\{{\textbackslash}infty\}\}{\textbackslash}leq (1+t{\textasciicircum}\{-1\}){\textbackslash}sqrt\{2{\textbackslash},\{{\textbackslash}rm log\}{\textbackslash},p\}{\textbackslash}cdot {\textbackslash}sigma \$, where r is the residual vector \$y-X{\textbackslash}tilde\{{\textbackslash}beta\}\$ and t is a positive scalar. We show that if X obeys a uniform uncertainty principle (with unit-normed columns) and if the true parameter vector β is sufficiently sparse (which here roughly guarantees that the model is identifiable), then with very large probability, \${\textbackslash}{\textbar}{\textbackslash}hat\{{\textbackslash}beta\}-{\textbackslash}beta {\textbackslash}{\textbar}\_\{{\textbackslash}ell \_\{2\}\}{\textasciicircum}\{2\}{\textbackslash}leq C{\textasciicircum}\{2\}{\textbackslash}cdot 2{\textbackslash},\{{\textbackslash}rm log\}{\textbackslash},p{\textbackslash}cdot {\textbackslash}left({\textbackslash}sigma {\textasciicircum}\{2\}+{\textbackslash}sum\_\{i\}\{{\textbackslash}rm min\}({\textbackslash}beta \_\{i\}{\textasciicircum}\{2\},{\textbackslash}sigma {\textasciicircum}\{2\}){\textbackslash}right)\$. Our results are nonasymptotic and we give values for the constant C. Even though n may be much smaller than p, our estimator achieves a loss within a logarithmic factor of the ideal mean squared error one would achieve with an oracle which would supply perfect information about which coordinates are nonzero, and which were above the noise level. In multivariate regression and from a model selection viewpoint, our result says that it is possible nearly to select the best subset of variables by solving a very simple convex program, which, in fact, can easily be recast as a convenient linear program (LP).},
	number = {6},
	journal = {The Annals of Statistics},
	author = {Candes, Emmanuel and Tao, Terence},
	year = {2007},
	keywords = {❓Multiple DOI},
	pages = {2313--2351},
}

@book{daubechies_ten_1992,
	address = {Philadelphia, Pa},
	series = {{CBMS}-{NSF} regional conference series in applied mathematics},
	title = {Ten lectures on wavelets},
	isbn = {978-0-89871-274-2},
	number = {61},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Daubechies, Ingrid},
	year = {1992},
	keywords = {Congresses, Wavelets (Mathematics)},
}

@article{signoretto_tensor_2011,
	title = {Tensor {Versus} {Matrix} {Completion}: {A} {Comparison} {With} {Application} to {Spectral} {Data}},
	volume = {18},
	issn = {1070-9908},
	shorttitle = {Tensor {Versus} {Matrix} {Completion}},
	doi = {10.1109/LSP.2011.2151856},
	abstract = {Tensor completion recently emerged as a generalization of matrix completion for higher order arrays. This problem formulation allows one to exploit the structure of data that intrinsically have multiple dimensions. In this work, we recall a convex formulation for minimum (multilinear) ranks completion of arrays of arbitrary order. Successively we focus on completion of partially observed spectral images; the latter can be naturally represented as third order tensors and typically exhibit intraband correlations. We compare different convex formulations and assess them through case studies.},
	number = {7},
	journal = {IEEE Signal Processing Letters},
	author = {Signoretto, M. and Plas, R. Van de and Moor, B. De and Suykens, J. A. K.},
	month = jul,
	year = {2011},
	note = {00000},
	keywords = {Arrays, Hyperspectral imaging, Indexes, Least squares approximation, Matrix decomposition, Tensile stress, arbitrary order, convex formulation, convex programming, higher order arrays, image processing, image reconstruction, intraband correlations, matrix completion, matrix completion generalization, minimum ranks completion, multilinear ranks completion, partially observed spectral images, spectral data, tensor completion, tensors},
	pages = {403--406},
}

@article{steihaug_conjugate_1983,
	title = {The conjugate gradient method and trust regions in large scale optimization},
	volume = {20},
	doi = {10.1137/0720042},
	number = {3},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Steihaug, Trond},
	year = {1983},
	note = {00000},
	pages = {626--637},
}

@article{boynton_analysis_2013a,
	title = {The {Analysis} of {Electron} {Fluxes} at {Geosynchronous} {Orbit} {Employing} a {NARMAX} {Approach}},
	volume = {118},
	doi = {10/f42b3m},
	number = {4},
	journal = {Journal of Geophysical Research: Space Physics},
	author = {Boynton, R. J. and Balikhin, M. A. and Billings, S. A. and Reeves, G. D. and Ganushkina, N and Gedalin, M and Amariutei, OA and Borovsky, J. E. and Walker, S. N.},
	year = {2013},
	pages = {1500--1513},
}

@book{friedman_elements_2001,
	title = {The elements of statistical learning},
	volume = {1},
	publisher = {Springer series in statistics New York},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	year = {2001},
}

@article{jaeger_echo_,
	title = {The “echo state” approach to analysing and training recurrent neural networks – with an {Erratum} note},
	language = {en},
	author = {Jaeger, Herbert},
	pages = {48},
}

@article{pennington_emergence_2018,
	title = {The {Emergence} of {Spectral} {Universality} in {Deep} {Networks}},
	abstract = {Recent work has shown that tight concentration of the entire spectrum of singular values of a deep network's input-output Jacobian around one at initialization can speed up learning by orders of magnitude. Therefore, to guide important design choices, it is important to build a full theoretical understanding of the spectra of Jacobians at initialization. To this end, we leverage powerful tools from free probability theory to provide a detailed analytic understanding of how a deep network's Jacobian spectrum depends on various hyperparameters including the nonlinearity, the weight and bias distributions, and the depth. For a variety of nonlinearities, our work reveals the emergence of new universal limiting spectral distributions that remain concentrated around one even as the depth goes to infinity.},
	journal = {21st International Conference on Artificial Intelligence and Statistics (AISTATS)},
	author = {Pennington, Jeffrey and Schoenholz, Samuel S. and Ganguli, Surya},
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hastie_surprises_2019,
	title = {Surprises in {High}-{Dimensional} {Ridgeless} {Least} {Squares} {Interpolation}},
	url = {http://arxiv.org/abs/1903.08560},
	abstract = {Interpolators---estimators that achieve zero training error---have attracted growing attention in machine learning, mainly because state-of-the art neural networks appear to be models of this type. In this paper, we study minimum \${\textbackslash}ell\_2\$ norm ("ridgeless") interpolation in high-dimensional least squares regression. We consider two different models for the feature distribution: a linear model, where the feature vectors \$x\_i {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}p\$ are obtained by applying a linear transform to a vector of i.i.d. entries, \$x\_i = {\textbackslash}Sigma{\textasciicircum}\{1/2\} z\_i\$ (with \$z\_i {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}p\$); and a nonlinear model, where the feature vectors are obtained by passing the input through a random one-layer neural network, \$x\_i = {\textbackslash}varphi(W z\_i)\$ (with \$z\_i {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}d\$, \$W {\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{p {\textbackslash}times d\}\$ a matrix of i.i.d. entries, and \${\textbackslash}varphi\$ an activation function acting componentwise on \$W z\_i\$). We recover---in a precise quantitative way---several phenomena that have been observed in large-scale neural networks and kernel machines, including the "double descent" behavior of the prediction risk, and the potential benefits of overparametrization.},
	urldate = {2020-07-23},
	journal = {arXiv:1903.08560},
	author = {Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J.},
	month = nov,
	year = {2019},
	note = {arXiv: 1903.08560},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@book{parziale_tcp_2006,
	series = {{IBM} redbooks},
	title = {{TCP}/{IP} {Tutorial} and {Technical} {Overview}},
	isbn = {978-0-7384-9468-5},
	url = {https://books.google.com.br/books?id=TgwWAgAAQBAJ},
	publisher = {IBM Redbooks},
	author = {Parziale, L. and Liu, W. and Matthews, C. and Rosselot, N. and Davis, C. and Forrester, J. and Britt, D.T. and Redbooks, IBM},
	year = {2006},
	note = {00000},
}

@inproceedings{tao_system_2017,
	title = {System identification of fractional-order for non-minimum-phase and non-self-balancing system},
	doi = {10.1109/ICMA.2017.8015911},
	abstract = {The steam generator is one of the most essential equipment of the nuclear power device which takes part in heat exchanging. The water level system of a steam generator has the peculiarities of apparent nonlinearity, large inertia and time-delaying. Furthermore, the water level of a steam generator has a direct impact on the quality of the outlet steam and the security of the system. Thus, it is significant and necessary to maintain the water level within a safety limit. the mathematical model of the steam generator, which is difficult to be described with integer order, can be established relatively clearly and precisely by using the fractional order calculus. With the progress of the modern technology, the merits of using a fractional order model become increasingly obvious due to its good performance in describing the actual plants and the dynamic process. This paper proposes a fractional order system identification method which can identify the model parameters concisely. This method is simple to be realized and also has a good adaptability to the initial value. The proposed fractional order identification scheme provides a new method of the important equipment modeling in nuclear power plant. The model that established by this method can provide research basis and technical support of the high precision and performance index control system.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Mechatronics} and {Automation} ({ICMA})},
	author = {Tao, M. and Ke, Z. and Yu, Y.},
	month = aug,
	year = {2017},
	note = {00000},
	keywords = {Non-Self-Balancing, Steam generator, fractional order, non-minimum-phase, system identification},
	pages = {758--763},
}

@article{astrom_system_1971a,
	title = {System identification—{A} survey},
	volume = {7},
	issn = {0005-1098},
	url = {http://www.sciencedirect.com/science/article/pii/0005109871900598},
	doi = {10.1016/0005-1098(71)90059-8},
	number = {2},
	journal = {Automatica},
	author = {Åström, K.J. and Eykhoff, P.},
	month = mar,
	year = {1971},
	pages = {123--162},
}

@article{goethals_subspace_2005,
	title = {Subspace identification of {Hammerstein} systems using least squares support vector machines},
	volume = {50},
	issn = {0018-9286},
	doi = {10.1109/TAC.2005.856647},
	abstract = {This paper presents a method for the identification of multiple-input-multiple-output (MIMO) Hammerstein systems for the goal of prediction. The method extends the numerical algorithms for subspace state space system identification (N4SID), mainly by rewriting the oblique projection in the N4SID algorithm as a set of componentwise least squares support vector machines (LS-SVMs) regression problems. The linear model and static nonlinearities follow from a low-rank approximation of a matrix obtained from this regression problem.},
	number = {10},
	journal = {IEEE Transactions on Automatic Control},
	author = {Goethals, I. and Pelckmans, K. and Suykens, J. A. K. and Moor, Bart De},
	month = oct,
	year = {2005},
	keywords = {Biological processes, Biological system modeling, Hammerstein models, Hammerstein system, Least squares approximation, Least squares methods, MIMO, MIMO systems, N4SID, Signal processing algorithms, control nonlinearities, identification, least squares approximations, least squares support vector machines, linear model, linear systems, multiple input multiple output system, nonlinear systems, numerical algorithms, regression analysis, regression problem, state-space methods, static nonlinearities, subspace identification, subspace state space identification, support vector machines, system identification},
	pages = {1509--1519},
}

@book{eykhoff_system_1974,
	title = {System identification: parameter and state estimation},
	isbn = {978-0-471-24980-1},
	shorttitle = {System identification},
	language = {en},
	publisher = {Wiley-Interscience},
	author = {Eykhoff, Pieter},
	month = may,
	year = {1974},
	note = {Google-Books-ID: 8f0pAQAAMAAJ},
	keywords = {Control theory, Estimation theory, Mathematics / Probability \& Statistics / General, Mathematics / Probability {\textbackslash}\& Statistics / General, Science / System Theory, System analysis, Technology \& Engineering / Electrical, Technology {\textbackslash}\& Engineering / Electrical, parameter estimation, system identification},
}

@book{ljung_system_1998,
	title = {System identification},
	publisher = {Springer},
	author = {Ljung, Lennart},
	year = {1998},
}

@article{chen_system_2014,
	title = {System identification via sparse multiple kernel-based regularization using sequential convex optimization techniques},
	volume = {59},
	doi = {10.1109/TAC.2014.2351851},
	number = {11},
	journal = {IEEE Transactions on Automatic Control},
	author = {Chen, Tianshi and Andersen, Martin S. and Ljung, Lennart and Chiuso, Alessandro and Pillonetto, Gianluigi},
	year = {2014},
	pages = {2933--2945},
}

@article{wills_stochastic_2018,
	title = {Stochastic quasi-{Newton} with adaptive step lengths for large-scale problems},
	url = {http://arxiv.org/abs/1802.04310},
	abstract = {We provide a numerically robust and fast method capable of exploiting the local geometry when solving large-scale stochastic optimisation problems. Our key innovation is an auxiliary variable construction coupled with an inverse Hessian approximation computed using a receding history of iterates and gradients. It is the Markov chain nature of the classic stochastic gradient algorithm that enables this development. The construction offers a mechanism for stochastic line search adapting the step length. We numerically evaluate and compare against current state-of-the-art with encouraging performance on real-world benchmark problems where the number of observations and unknowns is in the order of millions.},
	journal = {arXiv:1802.04310 [cs, stat]},
	author = {Wills, Adrian and Schön, Thomas},
	month = feb,
	year = {2018},
	note = {00001 
arXiv: 1802.04310},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, 🔍No DOI found},
}

@book{vanoverschee_subspace_2012,
	title = {Subspace identification for linear systems: {Theory}—{Implementation}—{Applications}},
	shorttitle = {Subspace identification for linear systems},
	publisher = {Springer Science \& Business Media},
	author = {Van Overschee, Peter and De Moor, B. L.},
	year = {2012},
	note = {00000},
}

@book{miller_subset_2002,
	title = {Subset selection in regression},
	isbn = {1-4200-3593-2},
	publisher = {CRC Press},
	author = {Miller, Alan},
	year = {2002},
}

@article{hsu_subset_2008,
	title = {Subset selection for vector autoregressive processes using {Lasso}},
	volume = {52},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947307004549},
	doi = {10.1016/j.csda.2007.12.004},
	abstract = {A subset selection method is proposed for vector autoregressive (VAR) processes using the Lasso [Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B 58, 267–288] technique. Simply speaking, Lasso is a shrinkage method in a regression setup which selects the model and estimates the parameters simultaneously. Compared to the conventional information-based methods such as AIC and BIC, the Lasso approach avoids computationally intensive and exhaustive search. On the other hand, compared to the existing subset selection methods with parameter constraints such as the top-down and bottom-up strategies, the Lasso method is computationally efficient and its result is robust to the order of series included in the autoregressive model. We derive the asymptotic theorem for the Lasso estimator under VAR processes. Simulation results demonstrate that the Lasso method performs better than several conventional subset selection methods for small samples in terms of prediction mean squared errors and estimation errors under various settings. The methodology is applied to modeling U.S. macroeconomic data for illustration.},
	number = {7},
	journal = {Computational Statistics \& Data Analysis},
	author = {Hsu, Nan-Jung and Hung, Hung-Lin and Chang, Ya-Mei},
	month = mar,
	year = {2008},
	pages = {3645--3657},
}

@book{tanenbaum_structured_2006,
	address = {Upper Saddle River, N.J},
	edition = {5th ed},
	title = {Structured computer organization},
	isbn = {978-0-13-148521-1},
	publisher = {Pearson Prentice Hall},
	author = {Tanenbaum, Andrew S.},
	year = {2006},
	note = {OCLC: ocm57506907},
	keywords = {Computer organization, Computer programming},
}

@book{mccool_structured_2012,
	address = {Amsterdam},
	title = {Structured parallel programing: patterns for efficient computation},
	isbn = {978-0-12-415993-8},
	shorttitle = {Structured parallel programing},
	publisher = {Elsevier, Morgan Kaufmann},
	author = {McCool, Michael D. and Robison, Arch D. and Reinders, James},
	year = {2012},
	keywords = {Software patterns, Structured programming},
}

@article{schon_system_2011,
	title = {System identification of nonlinear state-space models},
	volume = {47},
	issn = {0005-1098},
	url = {http://www.sciencedirect.com/science/article/pii/S0005109810004279},
	doi = {10.1016/j.automatica.2010.10.013},
	abstract = {This paper is concerned with the parameter estimation of a general class of nonlinear dynamic systems in state-space form. More specifically, a Maximum Likelihood (ML) framework is employed and an Expectation Maximisation (EM) algorithm is derived to compute these ML estimates. The Expectation (E) step involves solving a nonlinear state estimation problem, where the smoothed estimates of the states are required. This problem lends itself perfectly to the particle smoother, which provides arbitrarily good estimates. The maximisation (M) step is solved using standard techniques from numerical optimisation theory. Simulation examples demonstrate the efficacy of our proposed solution.},
	number = {1},
	journal = {Automatica},
	author = {Schön, Thomas B. and Wills, Adrian and Ninness, Brett},
	month = jan,
	year = {2011},
	note = {00000},
	keywords = {Dynamic systems, Expectation maximisation algorithm, Monte Carlo method, Nonlinear models, Particle methods, Smoothing filters, system identification},
	pages = {39--49},
}

@inproceedings{saggar_system_2007,
	title = {System identification for the {Hodgkin}-{Huxley} model using artificial neural networks},
	booktitle = {Neural {Networks}, 2007. {IJCNN} 2007. {International} {Joint} {Conference} on},
	publisher = {IEEE},
	author = {Saggar, Manish and Meriçli, Tekin and Andoni, Sari and Miikkulainen, Risto},
	year = {2007},
	note = {00000},
	pages = {2239--2244},
}

@inproceedings{overschee_subspace_1991,
	title = {Subspace algorithms for the stochastic identification problem},
	doi = {10.1109/CDC.1991.261604},
	abstract = {The authors derive a novel algorithm to consistently identify stochastic state space models from given output data without forming the covariance matrix and using only semi-infinite block Hankel matrices. The algorithm is based on the concept of principle angles and directions. The authors describe how these can be calculated with only QR and QSVD decompositions. They also provide an interpretation of the principle directions as states of a non-steady-state Kalman filter. With a couple of examples, it is shown that the proposed algorithm is superior to the classical canonical correlation algorithms},
	booktitle = {[1991] {Proceedings} of the 30th {IEEE} {Conference} on {Decision} and {Control}},
	author = {Overschee, P. Van and Moor, B. De},
	month = dec,
	year = {1991},
	keywords = {Gaussian noise, H infinity control, Kalman filters, Least squares approximation, Matrix decomposition, QR decompositions, QSVD decompositions, Robustness, State estimation, Stochastic resonance, covariance matrix, identification, matrix algebra, nonsteady-state Kalman filter, semi-infinite block Hankel matrices, state-space methods, stochastic identification, stochastic processes, stochastic state space models},
	pages = {1321--1326 vol.2},
}

@article{astrom_system_1971,
	title = {System {Identification}—{A} {Survey}},
	volume = {7},
	issn = {0005-1098},
	doi = {10/dw3ktq},
	number = {2},
	journal = {Automatica},
	author = {Åström, K.J. and Eykhoff, P.},
	month = mar,
	year = {1971},
	pages = {123--162},
}

@inproceedings{saggar_system_2007a,
	title = {System {Identification} for the {Hodgkin}-{Huxley} {Model} {Using} {Artificial} {Neural} {Networks}},
	booktitle = {Neural {Networks}, 2007. {IJCNN} 2007. {International} {Joint} {Conference} {On}},
	publisher = {IEEE},
	author = {Saggar, Manish and Meriçli, Tekin and Andoni, Sari and Miikkulainen, Risto},
	year = {2007},
	note = {00016},
	pages = {2239--2244},
}

@misc{tihonenko_st_2007,
	title = {St.-{Petersburg} {Institute} of {Cardiological} {Technics} 12-lead {Arrhythmia} {Database}},
	url = {https://physionet.org/content/incartdb/},
	doi = {10.13026/C2V88N},
	abstract = {This database consists of 75 annotated recordings extracted from 32 Holter records. Each record is 30 minutes long and contains 12 standard leads, each sampled at 257 Hz, with gains varying from 250 to 1100 analog-to-digital converter units per millivolt. Gains for each record are specified in its .hea file. The reference annotation files contain over 175,000 beat annotations in all.},
	urldate = {2020-11-03},
	publisher = {physionet.org},
	author = {Tihonenko, Viktor and Khaustov, Alexander and Ivanov, Sergey and Rivin, Alexei},
	year = {2007},
}

@article{frankle_stabilizing_2019,
	title = {Stabilizing the {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1903.01611},
	abstract = {Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar benefits during training. In particular, the "lottery ticket hypothesis" conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably finds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations. In this paper, we argue that these efforts have struggled on deeper networks because they have focused on pruning precisely at initialization. We modify IMP to search for subnetworks that could have been obtained by pruning early in training (0.1\% to 7\% through) rather than at iteration 0. With this change, it finds small subnetworks of deeper networks (e.g., 80\% sparsity on Resnet-50) that can complete the training process to match the accuracy of the original network on more challenging tasks (e.g., ImageNet). In situations where IMP fails at iteration 0, the accuracy benefits of delaying pruning accrue rapidly over the earliest iterations of training. To explain these behaviors, we study subnetwork "stability," finding that - as accuracy improves in this fashion - IMP subnetworks train to parameters closer to those of the full network and do so with improved consistency in the face of gradient noise. These results offer new insights into the opportunity to prune large-scale networks early in training and the behaviors underlying the lottery ticket hypothesis.},
	urldate = {2020-06-29},
	journal = {arXiv:1903.01611 [cs, stat]},
	author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
	month = jun,
	year = {2019},
	note = {arXiv: 1903.01611},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zou_stochastic_2018,
	title = {Stochastic {Gradient} {Descent} {Optimizes} {Over}-parameterized {Deep} {ReLU} {Networks}},
	url = {http://arxiv.org/abs/1811.08888},
	abstract = {We study the problem of training deep neural networks with Rectified Linear Unit (ReLU) activation function using gradient descent and stochastic gradient descent. In particular, we study the binary classification problem and show that for a broad family of loss functions, with proper random weight initialization, both gradient descent and stochastic gradient descent can find the global minima of the training loss for an over-parameterized deep ReLU network, under mild assumption on the training data. The key idea of our proof is that Gaussian random initialization followed by (stochastic) gradient descent produces a sequence of iterates that stay inside a small perturbation region centering around the initial weights, in which the empirical loss function of deep ReLU networks enjoys nice local curvature properties that ensure the global convergence of (stochastic) gradient descent. Our theoretical results shed light on understanding the optimization for deep learning, and pave the way for studying the optimization dynamics of training modern deep neural networks.},
	urldate = {2020-08-10},
	journal = {arXiv:1811.08888 [cs, math, stat]},
	author = {Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
	month = dec,
	year = {2018},
	note = {arXiv: 1811.08888},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{rakhlin_statistical_,
	title = {Statistical {Learning} and {Sequential} {Prediction}},
	language = {en},
	author = {Rakhlin, Alexander and Sridharan, Karthik},
	pages = {259},
}

@incollection{nar_step_2018a,
	title = {Step {Size} {Matters} in {Deep} {Learning}},
	url = {http://papers.nips.cc/paper/7603-step-size-matters-in-deep-learning.pdf},
	urldate = {2018-12-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Nar, Kamil and Sastry, Shankar},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {3439--3447},
}

@article{nar_step_2018,
	title = {Step {Size} {Matters} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1805.08890},
	abstract = {Training a neural network with the gradient descent algorithm gives rise to a discrete-time nonlinear dynamical system. Consequently, behaviors that are typically observed in these systems emerge during training, such as convergence to an orbit but not to a fixed point or dependence of convergence on the initialization. Step size of the algorithm plays a critical role in these behaviors: it determines the subset of the local optima that the algorithm can converge to, and it specifies the magnitude of the oscillations if the algorithm converges to an orbit. To elucidate the effects of the step size on training of neural networks, we study the gradient descent algorithm as a discrete-time dynamical system, and by analyzing the Lyapunov stability of different solutions, we show the relationship between the step size of the algorithm and the solutions that can be obtained with this algorithm. The results provide an explanation for several phenomena observed in practice, including the deterioration in the training error with increased depth, the hardness of estimating linear mappings with large singular values, and the distinct performance of deep residual networks.},
	urldate = {2018-12-10},
	journal = {arXiv:1805.08890 [cs, math, stat]},
	author = {Nar, Kamil and Sastry, S. Shankar},
	month = may,
	year = {2018},
	note = {arXiv: 1805.08890},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{szeliski_stereo_1999,
	title = {Stereo matching with transparency and matting},
	volume = {32},
	number = {1},
	journal = {International Journal of Computer Vision},
	author = {Szeliski, Richard and Golland, Polina},
	year = {1999},
	note = {00000},
	keywords = {❓Multiple DOI},
	pages = {45--61},
}

@article{zheng_state_2017,
	title = {State {Space} {LSTM} {Models} with {Particle} {MCMC} {Inference}},
	url = {http://arxiv.org/abs/1711.11179},
	abstract = {Long Short-Term Memory (LSTM) is one of the most powerful sequence models. Despite the strong performance, however, it lacks the nice interpretability as in state space models. In this paper, we present a way to combine the best of both worlds by introducing State Space LSTM (SSL) models that generalizes the earlier work {\textbackslash}cite\{zaheer2017latent\} of combining topic models with LSTM. However, unlike {\textbackslash}cite\{zaheer2017latent\}, we do not make any factorization assumptions in our inference algorithm. We present an efficient sampler based on sequential Monte Carlo (SMC) method that draws from the joint posterior directly. Experimental results confirms the superiority and stability of this SMC inference algorithm on a variety of domains.},
	urldate = {2018-11-26},
	journal = {arXiv:1711.11179 [cs, stat]},
	author = {Zheng, Xun and Zaheer, Manzil and Ahmed, Amr and Wang, Yuan and Xing, Eric P. and Smola, Alexander J.},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.11179},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chiu_stateoftheart_2017,
	title = {State-of-the-art {Speech} {Recognition} {With} {Sequence}-to-{Sequence} {Models}},
	url = {http://arxiv.org/abs/1712.01769},
	abstract = {Attention-based encoder-decoder architectures such as Listen, Attend, and Spell (LAS), subsume the acoustic, pronunciation and language model components of a traditional automatic speech recognition (ASR) system into a single neural network. In previous work, we have shown that such architectures are comparable to state-of-theart ASR systems on dictation tasks, but it was not clear if such architectures would be practical for more challenging tasks such as voice search. In this work, we explore a variety of structural and optimization improvements to our LAS model which significantly improve performance. On the structural side, we show that word piece models can be used instead of graphemes. We also introduce a multi-head attention architecture, which offers improvements over the commonly-used single-head attention. On the optimization side, we explore synchronous training, scheduled sampling, label smoothing, and minimum word error rate optimization, which are all shown to improve accuracy. We present results with a unidirectional LSTM encoder for streaming recognition. On a 12, 500 hour voice search task, we find that the proposed changes improve the WER from 9.2\% to 5.6\%, while the best conventional system achieves 6.7\%; on a dictation task our model achieves a WER of 4.1\% compared to 5\% for the conventional system.},
	journal = {arXiv:1712.01769 [cs, eess, stat]},
	author = {Chiu, Chung-Cheng and Sainath, Tara N. and Wu, Yonghui and Prabhavalkar, Rohit and Nguyen, Patrick and Chen, Zhifeng and Kannan, Anjuli and Weiss, Ron J. and Rao, Kanishka and Gonina, Ekaterina and Jaitly, Navdeep and Li, Bo and Chorowski, Jan and Bacchiani, Michiel},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.01769},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, 🔍No DOI found},
}

@article{breiman_statistical_2001,
	title = {Statistical modeling: {The} two cultures (with comments and a rejoinder by the author)},
	volume = {16},
	shorttitle = {Statistical modeling},
	doi = {10.1214/ss/1009213726},
	number = {3},
	journal = {Statistical science},
	author = {Breiman, Leo},
	year = {2001},
	pages = {199--231},
}

@article{zhou_state_2017,
	title = {State estimation of a compound non-smooth sandwich system with backlash and dead zone},
	volume = {83},
	issn = {0888-3270},
	doi = {10.1016/j.ymssp.2016.06.023},
	journal = {Mechanical Systems and Signal Processing},
	author = {Zhou, Zupeng and Tan, Yonghong and Xie, Yangqiu and Dong, Ruili},
	year = {2017},
	note = {00007},
	pages = {439--449},
}

@inproceedings{bensrhair_stereo_2002,
	title = {Stereo vision-based feature extraction for vehicle detection},
	volume = {2},
	booktitle = {Intelligent {Vehicle} {Symposium}, 2002. {IEEE}},
	publisher = {IEEE},
	author = {Bensrhair, Abdelaziz and Bertozzi, Massimo and Broggi, Alberto and Fascioli, Alessandra and Mousset, Stephane and Toulminet, Gwenadelle},
	year = {2002},
	pages = {465--470},
}

@article{stelzer_stereovisionbased_2012,
	title = {Stereo-vision-based navigation of a six-legged walking robot in unknown rough terrain},
	volume = {31},
	doi = {10.1177/0278364911435161},
	number = {4},
	journal = {The International Journal of Robotics Research},
	author = {Stelzer, Annett and Hirschmüller, Heiko and Görner, Martin},
	year = {2012},
	note = {00000},
	pages = {381--402},
}

@article{hirschmuller_stereo_2008,
	title = {Stereo processing by semiglobal matching and mutual information},
	volume = {30},
	doi = {10.1109/TPAMI.2007.1166},
	number = {2},
	journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
	author = {Hirschmüller, Heiko},
	year = {2008},
	pages = {328--341},
}

@article{hirschmuller_stereo_2005,
	title = {Stereo vision based reconstruction of huge urban areas from an airborne pushbroom camera ({HRSC})},
	volume = {3663},
	url = {http://link.springer.com/content/pdf/10.1007/11550518.pdf#page=72},
	doi = {10.1007/11550518_8},
	urldate = {2017-08-20},
	journal = {Lecture notes in computer science},
	author = {Hirschmüller, Heiko and Scholten, Frank and Hirzinger, Gerd},
	year = {2005},
	pages = {58},
}

@book{hastie_statistical_2015,
	title = {Statistical learning with sparsity: the lasso and generalizations},
	shorttitle = {Statistical learning with sparsity},
	url = {http://books.google.com/books?hl=en&lr=&id=f-A_CQAAQBAJ&oi=fnd&pg=PP1&dq=%22Example:+Distribution%22+%22Computation+for+the+Group%22+%22The+Fused%22+%22Nondi%EF%AC%80erentiable+Functions+and%22+%22A+Simulation%22+%22Fixed-%CE%BB+Inference+for+the%22+%22Cox+Proportional+Hazards%22+%22The+Overlap+Group%22+%22A+Dual+Path%22+&ots=G4ROH6i_SZ&sig=E1_E5N4-_lnR4S_DAyoMex6Xz2g},
	urldate = {2017-09-20},
	publisher = {CRC press},
	author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
	year = {2015},
}

@inproceedings{hassan_statistical_2015,
	title = {Statistical design centering of {RF} cavity linear accelerator via non-derivative trust region optimization},
	booktitle = {Numerical {Electromagnetic} and {Multiphysics} {Modeling} and {Optimization} ({NEMO}), 2015 {IEEE} {MTT}-{S} {International} {Conference} on},
	publisher = {IEEE},
	author = {Hassan, Abdel-Karim SO and Abdel-Malek, Hany L and Mohamed, Ahmed SA and Abuelfadl, Tamer M and Elqenawy, Ahmed E},
	year = {2015},
	pages = {1--3},
}

@article{dejesusrubio_stable_2017,
	title = {Stable {Kalman} filter and neural network for the chaotic systems identification},
	volume = {354},
	issn = {0016-0032},
	url = {http://www.sciencedirect.com/science/article/pii/S0016003217304325},
	doi = {10/gcjcxf},
	abstract = {In this research, a modified Kalman filter is introduced for the adaptation of a neural network. The modified Kalman filter is an improved version of the extended Kalman filter based in the following two changes: (1) a term of the weights adaptation is modified in the modified algorithm to assure the uniform stability, convergence of the weights error, and local minimums avoidance, (2) the activation functions are used instead of the Jacobian terms in the modified algorithm to assure the boundedness of the weights error. The suggested algorithm is applied for the chaotic systems identification.},
	number = {16},
	journal = {Journal of the Franklin Institute},
	author = {de Jesús Rubio, José},
	month = nov,
	year = {2017},
	pages = {7444--7462},
}

@inproceedings{chaudhari_stochastic_2018,
	address = {San Diego, CA},
	title = {Stochastic {Gradient} {Descent} {Performs} {Variational} {Inference}, {Converges} to {Limit} {Cycles} for {Deep} {Networks}},
	isbn = {978-1-72810-124-8},
	url = {https://ieeexplore.ieee.org/document/8503224/},
	doi = {10/gf3d8t},
	abstract = {Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such “out-of-equilibrium” behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1\% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.},
	language = {en},
	urldate = {2019-06-01},
	booktitle = {2018 {Information} {Theory} and {Applications} {Workshop} ({ITA})},
	publisher = {IEEE},
	author = {Chaudhari, Pratik and Soatto, Stefano},
	month = feb,
	year = {2018},
	pages = {1--10},
}

@article{miller_stable_2018,
	title = {Stable {Recurrent} {Models}},
	url = {http://arxiv.org/abs/1805.10369},
	abstract = {Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.},
	urldate = {2019-07-27},
	journal = {arXiv:1805.10369 [cs, stat]},
	author = {Miller, John and Hardt, Moritz},
	month = may,
	year = {2018},
	note = {arXiv: 1805.10369},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhang_stabilizing_2018,
	title = {Stabilizing {Gradients} for {Deep} {Neural} {Networks} via {Efﬁcient} {SVD} {Parameterization}},
	abstract = {Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks (RNNs). In this paper, we present an efﬁcient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Speciﬁcally, we parameterize the transition matrix by its singular value decomposition (SVD), which allows us to explicitly track and control its singular values. We attain efﬁciency by using tools that are common in numerical linear algebra, namely Householder reﬂectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed Spectral-RNN method allows us to provably solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it controls generalization of RNN for the classiﬁcation task. Our extensive experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially in capturing long range dependencies, as shown on the synthetic addition and copy tasks, as well as on the MNIST and Penn Tree Bank data sets.},
	language = {en},
	journal = {Proceedings of the 35 th International Conference on Machine Learning},
	author = {Zhang, Jiong and Lei, Qi and Dhillon, Inderjit S},
	year = {2018},
	pages = {9},
}

@article{xie_smooth_2020,
	title = {Smooth {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2006.14536},
	abstract = {It is commonly believed that networks cannot be both accurate and robust, that gaining robustness means losing accuracy. It is also generally believed that, unless making networks larger, network architectural elements would otherwise matter little in improving adversarial robustness. Here we present evidence to challenge these common beliefs by a careful study about adversarial training. Our key observation is that the widely-used ReLU activation function significantly weakens adversarial training due to its non-smooth nature. Hence we propose smooth adversarial training (SAT), in which we replace ReLU with its smooth approximations to strengthen adversarial training. The purpose of smooth activation functions in SAT is to allow it to find harder adversarial examples and compute better gradient updates during adversarial training. Compared to standard adversarial training, SAT improves adversarial robustness for "free", i.e., no drop in accuracy and no increase in computational cost. For example, without introducing additional computations, SAT significantly enhances ResNet-50's robustness from 33.0\% to 42.3\%, while also improving accuracy by 0.9\% on ImageNet. SAT also works well with larger networks: it helps EfficientNet-L1 to achieve 82.2\% accuracy and 58.6\% robustness on ImageNet, outperforming the previous state-of-the-art defense by 9.5\% for accuracy and 11.6\% for robustness.},
	urldate = {2020-07-07},
	journal = {arXiv:2006.14536 [cs]},
	author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Yuille, Alan and Le, Quoc V.},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.14536},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{mezic_spectral_2005,
	title = {Spectral {Properties} of {Dynamical} {Systems}, {Model} {Reduction} and {Decompositions}},
	volume = {41},
	issn = {1573-269X},
	url = {https://doi.org/10.1007/s11071-005-2824-x},
	doi = {10.1007/s11071-005-2824-x},
	abstract = {In this paper we discuss two issues related to model reduction of deterministic or stochastic processes. The first is the relationship of the spectral properties of the dynamics on the attractor of the original, high-dimensional dynamical system with the properties and possibilities for model reduction. We review some elements of the spectral theory of dynamical systems. We apply this theory to obtain a decomposition of the process that utilizes spectral properties of the linear Koopman operator associated with the asymptotic dynamics on the attractor. This allows us to extract the almost periodic part of the evolving process. The remainder of the process has continuous spectrum. The second topic we discuss is that of model validation, where the original, possibly high-dimensional dynamics and the dynamics of the reduced model – that can be deterministic or stochastic – are compared in some norm. Using the “statistical Takens theorem” proven in (Mezić, I. and Banaszuk, A. Physica D, 2004) we argue that comparison of average energy contained in the finite-dimensional projection is one in the hierarchy of functionals of the field that need to be checked in order to assess the accuracy of the projection.},
	language = {en},
	number = {1},
	urldate = {2020-12-15},
	journal = {Nonlinear Dynamics},
	author = {Mezić, Igor},
	month = aug,
	year = {2005},
	pages = {309--325},
}

@article{bryc_spectral_2006,
	title = {Spectral measure of large random {Hankel}, {Markov} and {Toeplitz} matrices},
	url = {http://arxiv.org/abs/math/0307330},
	doi = {10.1214/009117905000000495},
	abstract = {We study the limiting spectral measure of large symmetric random matrices of linear algebraic structure. For Hankel and Toeplitz matrices generated by i.i.d. random variables \${\textbackslash}\{X\_k{\textbackslash}\}\$ of unit variance, and for symmetric Markov matrices generated by i.i.d. random variables \${\textbackslash}\{X\_\{ij\}{\textbackslash}\}\_\{j{\textgreater}i\}\$ of zero mean and unit variance, scaling the eigenvalues by \${\textbackslash}sqrt\{n\}\$ we prove the almost sure, weak convergence of the spectral measures to universal, nonrandom, symmetric distributions \${\textbackslash}gamma\_H\$, \${\textbackslash}gamma\_M\$ and \${\textbackslash}gamma\_T\$ of unbounded support. The moments of \${\textbackslash}gamma\_H\$ and \${\textbackslash}gamma\_T\$ are the sum of volumes of solids related to Eulerian numbers, whereas \${\textbackslash}gamma\_M\$ has a bounded smooth density given by the free convolution of the semicircle and normal densities. For symmetric Markov matrices generated by i.i.d. random variables \${\textbackslash}\{X\_\{ij\}{\textbackslash}\}\_\{j{\textgreater}i\}\$ of mean \$m\$ and finite variance, scaling the eigenvalues by \$\{n\}\$ we prove the almost sure, weak convergence of the spectral measures to the atomic measure at \$-m\$. If \$m=0\$, and the fourth moment is finite, we prove that the spectral norm of \${\textbackslash}mathbf \{M\}\_n\$ scaled by \${\textbackslash}sqrt\{2n{\textbackslash}log n\}\$ converges almost surely to 1.},
	urldate = {2020-12-14},
	journal = {arXiv:math/0307330},
	author = {Bryc, Włodzimierz and Dembo, Amir and Jiang, Tiefeng},
	month = feb,
	year = {2006},
	note = {arXiv: math/0307330},
	keywords = {15A52 (Primary) 60F99, 62H10, 60F10 (Secondary), Mathematics - Combinatorics, Mathematics - Probability, Mathematics - Statistics Theory},
}

@article{rubio_spectral_2011,
	title = {Spectral convergence for a general class of random matrices},
	volume = {81},
	url = {https://hal.archives-ouvertes.fr/hal-00725102},
	doi = {10.1016/j.spl.2011.01.004},
	abstract = {Let be an complex random matrix with i.i.d. entries having mean zero and variance and consider the class of matrices of the type , where , and are Hermitian nonnegative definite matrices, such that and have bounded spectral norm with being diagonal, and is the nonnegative definite square-root of . Under some assumptions on the moments of the entries of , it is proved in this paper that, for any matrix with bounded trace norm and for each complex outside the positive real line, almost surely as at the same rate, where is deterministic and solely depends on and . The previous result can be particularized to the study of the limiting behavior of the Stieltjes transform as well as the eigenvectors of the random matrix model . The study is motivated by applications in the field of statistical signal processing.},
	number = {5},
	urldate = {2020-12-22},
	journal = {Statistics and Probability Letters},
	author = {Rubio, Francisco and Mestre, Xavier},
	month = feb,
	year = {2011},
	note = {Publisher: Elsevier},
	keywords = {Multivariate statistics, Random matrix theory, Sample covariance matrix, Separable covariance model, Stieltjes transform},
	pages = {592},
}

@incollection{bailey_sizenoise_2018,
	title = {Size-{Noise} {Tradeoffs} in {Generative} {Networks}},
	url = {http://papers.nips.cc/paper/7884-size-noise-tradeoffs-in-generative-networks.pdf},
	urldate = {2018-12-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Bailey, Bolton and Telgarsky, Matus J},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {6489--6499},
}

@article{barakat_simultaneous_1999,
	title = {Simultaneous determination of the modulus and phase of a coherently illuminated object from its measured diffraction image},
	volume = {1},
	doi = {10.1088/1464-4258/1/5/309},
	number = {5},
	journal = {Journal of Optics A: Pure and Applied Optics},
	author = {Barakat, Richard and Sandler, Barbara H.},
	year = {1999},
	pages = {629},
}

@inproceedings{farina_convergence_2008a,
	title = {Some convergence properties of multi-step prediction error identification criteria},
	booktitle = {Decision and {Control}, 2008. {CDC} 2008. 47th {IEEE} {Conference} on},
	publisher = {IEEE},
	author = {Farina, Marcello and Piroddi, Luigi},
	year = {2008},
	pages = {756--761},
}

@article{farina_simulation_2011,
	title = {Simulation error minimization identification based on multi-stage prediction},
	volume = {25},
	doi = {10.1002/acs.1203},
	number = {5},
	journal = {International Journal of Adaptive Control and Signal Processing},
	author = {Farina, M and Piroddi, L},
	year = {2011},
	pages = {389--406},
}

@article{calafiore_sparse_2015,
	title = {Sparse identification of posynomial models},
	volume = {59},
	issn = {0005-1098},
	url = {http://www.sciencedirect.com/science/article/pii/S0005109815002319},
	doi = {10.1016/j.automatica.2015.06.003},
	abstract = {Posynomials are nonnegative combinations of monomials with possibly fractional and both positive and negative exponents. Posynomial models are widely used in various engineering design endeavors, such as circuits, aerospace and structural design, mainly due to the fact that design problems cast in terms of posynomial objectives and constraints can be solved efficiently by means of a convex optimization technique known as geometric programming (GP). However, while quite a vast literature exists on GP-based design, very few contributions can yet be found on the problem of identifying posynomial models from experimental data. Posynomial identification amounts to determining not only the coefficients of the combination, but also the exponents in the monomials, which renders the identification problem hard. In this paper, we propose an approach to the identification of multivariate posynomial models based on the expansion on a given large-scale basis of monomials. The model is then identified by seeking coefficients of the combination that minimize a mixed objective, composed by a term representing the fitting error and a term inducing sparsity in the representation, which results in a problem formulation of the “square-root LASSO” type, with nonnegativity constraints on the variables. We propose to solve the problem via a sequential coordinate-minimization scheme, which is suitable for large-scale implementations. A numerical example is finally presented, dealing with the identification of a posynomial model for a NACA 4412 airfoil.},
	number = {Supplement C},
	journal = {Automatica},
	author = {Calafiore, Giuseppe C. and El Ghaoui, Laurent M. and Novara, Carlo},
	month = sep,
	year = {2015},
	keywords = {Geometric programming, Posynomial models, Sparse optimization, Square-root LASSO, regularizazion},
	pages = {27--34},
}

@article{xie_squareroot_2018,
	title = {Square-{Root} {LASSO} for {High}-{Dimensional} {Sparse} {Linear} {Systems} with {Weakly} {Dependent} {Errors}: {Square}-root {LASSO} with time series errors},
	volume = {39},
	issn = {01439782},
	shorttitle = {Square-{Root} {LASSO} for {High}-{Dimensional} {Sparse} {Linear} {Systems} with {Weakly} {Dependent} {Errors}},
	url = {http://doi.wiley.com/10.1111/jtsa.12278},
	doi = {10.1111/jtsa.12278},
	language = {en},
	number = {2},
	urldate = {2018-02-22},
	journal = {Journal of Time Series Analysis},
	author = {Xie, Fang and Xiao, Zhijie},
	month = mar,
	year = {2018},
	note = {00000},
	pages = {212--238},
}

@inproceedings{umenberger_specialized_2016,
	title = {Specialized algorithm for identification of stable linear systems using {Lagrangian} relaxation},
	doi = {10.1109/ACC.2016.7525034},
	booktitle = {2016 {American} {Control} {Conference} ({ACC})},
	author = {Umenberger, Jack and Manchester, Ian R},
	month = jul,
	year = {2016},
	note = {00000},
	keywords = {Approximation algorithms, Lagrangian relaxation, Linear systems, Minimization, Stability analysis, Symmetric matrices, convex approximations, convex bound optimization, convex programming, general-purpose semidefinite programming solvers, linear state-space models, minimisation, model stability, optimization, simulation error minimization problem, stability, stable linear system identification, state-space methods, system identification},
	pages = {930--935},
}

@article{friedman_sparse_2008,
	title = {Sparse inverse covariance estimation with the graphical lasso},
	volume = {9},
	issn = {1465-4644, 1468-4357},
	url = {https://academic.oup.com/biostatistics/article-lookup/doi/10.1093/biostatistics/kxm045},
	doi = {10.1093/biostatistics/kxm045},
	language = {en},
	number = {3},
	urldate = {2017-09-18},
	journal = {Biostatistics},
	author = {Friedman, J. and Hastie, T. and Tibshirani, R.},
	month = jul,
	year = {2008},
	pages = {432--441},
}

@article{calafiore_sparse_2014,
	title = {Sparse {Identification} of {Polynomial} and {Posynomial} {Models}},
	volume = {47},
	issn = {1474-6670},
	url = {http://www.sciencedirect.com/science/article/pii/S1474667016421063},
	doi = {10.3182/20140824-6-ZA-1003.01549},
	number = {3},
	journal = {19th IFAC World Congress},
	author = {Calafiore, Giuseppe Carlo and Ghaoui, Laurent El and Novara, Carlo},
	month = jan,
	year = {2014},
	keywords = {Coordinate-descent methods, Identification, Posynomial models, Sparse optimization, Square-root LASSO},
	pages = {3238--3243},
}

@article{jacobs_sparse_2018,
	title = {Sparse {Bayesian} {Nonlinear} {System} {Identification} using {Variational} {Inference}},
	issn = {0018-9286},
	doi = {10.1109/TAC.2018.2813004},
	abstract = {Bayesian nonlinear system identification for one of the major classes of dynamic model, the nonlinear autoregressive with exogenous input (NARX) model, has not been widely studied to date. Markov chain Monte Carlo (MCMC) methods have been developed, which tend to be accurate but can also be slow to converge. In this contribution, we present a novel, computationally efficient solution to sparse Bayesian identification of the NARX model using variational inference, which is orders of magnitude faster than MCMC methods. A sparsity-inducing hyper-prior is used to solve the structure detection problem. Key results include: 1. successful demonstration of the method on low signal-to-noise ratio signals (down to 2dB); 2. successful benchmarking in terms of speed and accuracy against a number of other algorithms: Bayesian LASSO, reversible jump MCMC, forward regression orthogonalisation, LASSO and simulation error minimisation with pruning; 3. accurate identification of a real world system, an electroactive polymer; and 4. demonstration for the first time of numerically propagating the estimated nonlinear time-domain model parameter uncertainty into the frequency-domain.},
	journal = {IEEE Transactions on Automatic Control},
	author = {Jacobs, W. R. and Baldacchino, T. and Dodd, T. J. and Anderson, S. R.},
	year = {2018},
	keywords = {Analytical models, Bayes methods, Bayesian estimation, Computational modeling, Data models, NARX model, Numerical models, nonlinear systems, system identification, variational inference},
	pages = {1--1},
}

@article{farina_simulation_2011a,
	title = {Simulation error minimization identification based on multi-stage prediction},
	volume = {25},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1099-1115},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/acs.1203},
	doi = {10/bn2vnd},
	abstract = {Classical prediction error minimization (PEM) methods are widely used for model identification, but they are also known to provide satisfactory results only in specific identification conditions, e.g. disturbance model matching. If these conditions are not met, the obtained model may have quite different dynamical behavior compared with the original system, resulting in poor long range prediction or simulation performance, which is a critical factor for model analysis, simulation, model-based control design. In the mentioned non-ideal conditions a robust and reliable alternative is based on the minimization of the simulation error. Unfortunately, direct optimization of a simulation error minimization (SEM) criterion is an intrinsically complex and computationally intensive task. In this paper a low-complexity approximate SEM approach is discussed, based on the iteration of multi-step PEM methods. The soundness of the proposed approach is demonstrated by showing that, for sufficiently high prediction horizons, the k-steps ahead (single- or multi-step) PEM criteria converge to the SEM one. Identifiability issues and convergence properties of the algorithm are also discussed. Some examples are provided to illustrate the mentioned properties of the algorithm. Copyright © 2010 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5},
	urldate = {2019-04-04},
	journal = {International Journal of Adaptive Control and Signal Processing},
	author = {Farina, M. and Piroddi, L.},
	year = {2011},
	keywords = {multi-step prediction, output error identification, prediction error minimization, simulation, simulation error minimization},
	pages = {389--406},
}

@inproceedings{farina_convergence_2008,
	title = {Some convergence properties of multi-step prediction error identification criteria},
	doi = {10/d92tnw},
	abstract = {Multi-step prediction error identification methods are preferred over plain one-step ahead prediction error ones in application contexts (e.g., predictive control) where model accuracy is required over a wide horizon. For sufficiently high prediction horizons, their properties can be shown to be conveniently related to those of output error methods, for which several important issues (e.g., uniqueness of estimation, robustness with respect to the noise model) have been characterized in the literature. The convergence properties of such criteria with respect to the prediction horizon are analyzed.},
	booktitle = {2008 47th {IEEE} {Conference} on {Decision} and {Control}},
	author = {Farina, M. and Piroddi, L.},
	month = dec,
	year = {2008},
	keywords = {Computational modeling, Context modeling, Convergence, Error correction, Frequency estimation, Minimization methods, Noise robustness, Predictive control, Predictive models, Time series analysis, black-box identification, convergence of numerical methods, convergence property, error analysis, identification, iterative methods, multistep prediction error identification criteria, prediction error minimization, prediction horizon analysis, simulation, simulation error minimization identification},
	pages = {756--761},
}

@article{gould_solving_1999,
	title = {Solving the trust-region subproblem using the {Lanczos} method},
	volume = {9},
	doi = {10.1137/S1052623497322735},
	number = {2},
	journal = {SIAM Journal on Optimization},
	author = {Gould, Nicholas I. M. and Lucidi, Stefano and Roma, Massimo and Toint, Philippe L},
	year = {1999},
	pages = {504--525},
}

@article{piroddi_simulation_2008,
	title = {Simulation {Error} {Minimisation} {Methods} for {NARX} {Model} {Identification}},
	volume = {3},
	doi = {10/bmvskt},
	number = {4},
	journal = {International Journal of Modelling, Identification and Control},
	author = {Piroddi, Luigi},
	year = {2008},
	pages = {392--403},
}

@misc{_sponsors_,
	title = {Sponsors},
	url = {https://www.ifac2020.org/sponsors.html},
	language = {en},
	urldate = {2019-10-28},
}

@article{tan_simulating_2019,
	title = {Simulating extrapolated dynamics with parameterization networks},
	url = {http://arxiv.org/abs/1902.03440},
	abstract = {An artificial neural network architecture, parameterization networks, is proposed for simulating extrapolated dynamics beyond observed data in dynamical systems. Parameterization networks are used to ensure the long term integrity of extrapolated dynamics, while careful tuning of model hyperparameters against validation errors controls overfitting. A parameterization network is demonstrated on the logistic map, where chaos and other nonlinear phenomena consistent with the underlying model can be extrapolated from non-chaotic training time series with good fidelity. The stated results are a lot less fantastical than they appear to be because the neural network is only extrapolating between quadratic return maps. Nonetheless, the results do suggest that successful extrapolation of qualitatively different behaviors requires learning to occur on a level of abstraction where the corresponding behaviors are more similar in nature.},
	urldate = {2021-03-30},
	journal = {arXiv:1902.03440 [nlin]},
	author = {Tan, James P. L.},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.03440},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Nonlinear Sciences - Chaotic Dynamics},
}

@article{ling_spectrum_2019,
	title = {Spectrum {Concentration} in {Deep} {Residual} {Learning}: {A} {Free} {Probability} {Approach}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Spectrum {Concentration} in {Deep} {Residual} {Learning}},
	doi = {10.1109/ACCESS.2019.2931991},
	abstract = {We revisit the weight initialization of deep residual networks (ResNets) by introducing a novel analytical tool in free probability to the community of deep learning. This tool deals with the limiting spectral distribution of non-Hermitian random matrices, rather than their conventional Hermitian counterparts in the literature. This new tool enables us to evaluate the singular value spectrum of the input-output Jacobian of a fully connected deep ResNet in both linear and nonlinear cases. With the powerful tool of free probability, we conduct an asymptotic analysis of the (limiting) spectrum on the single-layer case, and then extend this analysis to the multi-layer case of an arbitrary number of layers. The asymptotic analysis illustrates the necessity and university of rescaling the classical random initialization by the number of residual units L, so that the squared singular value of the associated Jacobian remains of order O(1), when compared with the large width and depth of the network. We empirically demonstrate that the proposed initialization scheme learns at a speed of orders of magnitudes faster than the classical ones, and thus attests a strong practical relevance of this investigation.},
	journal = {IEEE Access},
	author = {Ling, Zenan and Qiu, Robert C.},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Eigenvalues and eigenfunctions, Jacobian matrices, Jacobian matrix, Limiting, Neural networks, Random variables, Residual network, Tools, Training, non-Hermitian free probability theory, random matrix theory, spectral density, weight initialization},
	pages = {105212--105223},
}

@book{bai_spectral_2010,
	series = {Springer {Series} in {Statistics}},
	title = {Spectral analysis of large dimensional random matrices},
	volume = {20},
	publisher = {Springer},
	author = {Bai, Zhidong and Silverstein, Jack W},
	year = {2010},
}

@article{dai_semisupervised_2015,
	title = {Semi-supervised {Sequence} {Learning}},
	url = {http://arxiv.org/abs/1511.01432},
	abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
	urldate = {2019-06-08},
	journal = {arXiv:1511.01432 [cs]},
	author = {Dai, Andrew M. and Le, Quoc V.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.01432},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{xie_selftraining_2020,
	address = {Seattle, WA, USA},
	title = {Self-{Training} {With} {Noisy} {Student} {Improves} {ImageNet} {Classification}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156610/},
	doi = {10.1109/CVPR42600.2020.01070},
	abstract = {We present a simple self-training method that achieves 88.4\% top-1 accuracy on ImageNet, which is 2.0\% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0\% to 83.7\%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean ﬂip rate from 27.8 to 12.2.},
	language = {en},
	urldate = {2020-10-16},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
	month = jun,
	year = {2020},
	pages = {10684--10695},
}

@incollection{thekumparampil_robustness_2018,
	title = {Robustness of conditional {GANs} to noisy labels},
	url = {http://papers.nips.cc/paper/8229-robustness-of-conditional-gans-to-noisy-labels.pdf},
	urldate = {2018-12-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Thekumparampil, Kiran K and Khetan, Ashish and Lin, Zinan and Oh, Sewoong},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {10291--10302},
}

@article{delmoral_sequential_2006,
	title = {Sequential {Monte} {Carlo} samplers},
	volume = {68},
	issn = {1369-7412, 1467-9868},
	url = {http://doi.wiley.com/10.1111/j.1467-9868.2006.00553.x},
	doi = {10/cfbsfg},
	abstract = {We propose a methodology to sample sequentially from a sequence of probability distributions that are deﬁned on a common space, each distribution being known up to a normalizing constant. These probability distributions are approximated by a cloud of weighted random samples which are propagated over time by using sequential Monte Carlo methods. This methodology allows us to derive simple algorithms to make parallel Markov chain Monte Carlo algorithms interact to perform global optimization and sequential Bayesian estimation and to compute ratios of normalizing constants. We illustrate these algorithms for various integration tasks arising in the context of Bayesian inference.},
	language = {en},
	number = {3},
	urldate = {2018-12-12},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
	month = jun,
	year = {2006},
	pages = {411--436},
}

@article{attia_screening_2019,
	title = {Screening for cardiac contractile dysfunction using an artificial intelligence–enabled electrocardiogram},
	volume = {25},
	issn = {1546-170X},
	url = {https://doi.org/10.1038/s41591-018-0240-2},
	doi = {10.1038/s41591-018-0240-2},
	abstract = {Asymptomatic left ventricular dysfunction (ALVD) is present in 3–6\% of the general population, is associated with reduced quality of life and longevity, and is treatable when found1–4. An inexpensive, noninvasive screening tool for ALVD in the doctor’s office is not available. We tested the hypothesis that application of artificial intelligence (AI) to the electrocardiogram (ECG), a routine method of measuring the heart’s electrical activity, could identify ALVD. Using paired 12-lead ECG and echocardiogram data, including the left ventricular ejection fraction (a measure of contractile function), from 44,959 patients at the Mayo Clinic, we trained a convolutional neural network to identify patients with ventricular dysfunction, defined as ejection fraction ≤35\%, using the ECG data alone. When tested on an independent set of 52,870 patients, the network model yielded values for the area under the curve, sensitivity, specificity, and accuracy of 0.93, 86.3\%, 85.7\%, and 85.7\%, respectively. In patients without ventricular dysfunction, those with a positive AI screen were at 4 times the risk (hazard ratio, 4.1; 95\% confidence interval, 3.3 to 5.0) of developing future ventricular dysfunction compared with those with a negative screen. Application of AI to the ECG—a ubiquitous, low-cost test—permits the ECG to serve as a powerful screening tool in asymptomatic individuals to identify ALVD.},
	number = {1},
	journal = {Nature Medicine},
	author = {Attia, Zachi I. and Kapa, Suraj and Lopez-Jimenez, Francisco and McKie, Paul M. and Ladewig, Dorothy J. and Satam, Gaurav and Pellikka, Patricia A. and Enriquez-Sarano, Maurice and Noseworthy, Peter A. and Munger, Thomas M. and Asirvatham, Samuel J. and Scott, Christopher G. and Carter, Rickey E. and Friedman, Paul A.},
	month = jan,
	year = {2019},
	pages = {70--74},
}

@article{li_semisupervised_2018,
	title = {Semi-supervised {Rare} {Disease} {Detection} {Using} {Generative} {Adversarial} {Network}},
	url = {http://arxiv.org/abs/1812.00547},
	abstract = {Rare diseases affect a relatively small number of people, which limits investment in research for treatments and cures. Developing an efficient method for rare disease detection is a crucial first step towards subsequent clinical research. In this paper, we present a semi-supervised learning framework for rare disease detection using generative adversarial networks. Our method takes advantage of the large amount of unlabeled data for disease detection and achieves the best results in terms of precision-recall score compared to baseline techniques.},
	urldate = {2018-12-13},
	journal = {arXiv:1812.00547 [cs, stat]},
	author = {Li, Wenyuan and Wang, Yunlong and Cai, Yong and Arnold, Corey and Zhao, Emily and Yuan, Yilian},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.00547},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ugray_scatter_2007,
	title = {Scatter search and local {NLP} solvers: {A} multistart framework for global optimization},
	volume = {19},
	doi = {10.1287/ijoc.1060.0175},
	number = {3},
	journal = {INFORMS Journal on Computing},
	author = {Ugray, Zsolt and Lasdon, Leon and Plummer, John and Glover, Fred and Kelly, James and Martí, Rafael},
	year = {2007},
	note = {00000},
	pages = {328--340},
}

@article{medioni_segmentbased_1985,
	title = {Segment-based stereo matching},
	volume = {31},
	doi = {10.1016/S0734-189X(85)80073-6},
	number = {1},
	journal = {Computer Vision, Graphics, and Image Processing},
	author = {Medioni, Gerard and Nevatia, Ramakant},
	year = {1985},
	pages = {2--18},
}

@article{may_simple_1976,
	title = {Simple mathematical models with very complicated dynamics},
	volume = {261},
	doi = {10.1038/261459a0},
	number = {5560},
	journal = {Nature},
	author = {May, Robert M},
	year = {1976},
	pages = {459--467},
}

@inproceedings{umenberger_scalable_2016,
	title = {Scalable identification of stable positive systems},
	doi = {10.1109/CDC.2016.7798974},
	booktitle = {2016 {IEEE} 55th {Conference} on {Decision} and {Control} ({CDC})},
	author = {Umenberger, Jack and Manchester, Ian R},
	month = dec,
	year = {2016},
	note = {00000},
	keywords = {Cost function, Linear systems, Lyapunov methods, Mathematical model, Minimization, Stability analysis, analogous methods, control system analysis, convex programming, decomposability, general LTI, large-scale networked systems, large-scale systems, linear program, linear programming, optimization, polytopic parameterization, positive LTI, scalable identification, semidefinite programs, simulation error, stability, stable positive systems},
	pages = {4630--4635},
}

@inproceedings{sutskever_sequence_2014,
	title = {Sequence to sequence learning with neural networks},
	booktitle = {Advances in neural information processing systems},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	year = {2014},
	note = {00000},
	pages = {3104--3112},
}

@phdthesis{keskar_secondorder_2017,
	type = {{PhD} {Thesis}},
	title = {Second-{Order} {Methods} for {Stochastic} and {Nonsmooth} {Optimization}},
	school = {Northwestern University},
	author = {Keskar, Nitish Shirish},
	year = {2017},
}

@book{jones_scipy_2001,
	title = {{SciPy}: {Open} source scientific tools for {Python}},
	url = {http://www.scipy.org/},
	author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu and {others}},
	year = {2001},
}

@article{hwang_simulated_1988,
	title = {Simulated annealing: theory and applications},
	volume = {12},
	number = {1},
	journal = {Acta Applicandae Mathematicae},
	author = {Hwang, Chii-Ruey},
	year = {1988},
	keywords = {🔍No DOI found},
	pages = {108--111},
}

@article{kiefer_sequential_1953,
	title = {Sequential minimax search for a maximum},
	volume = {4},
	doi = {10.1090/S0002-9939-1953-0055639-3},
	number = {3},
	journal = {Proceedings of the American Mathematical Society},
	author = {Kiefer, Jack},
	year = {1953},
	pages = {502--506},
}

@article{khandelwal_sharp_2018,
	title = {Sharp {Nearby}, {Fuzzy} {Far} {Away}: {How} {Neural} {Language} {Models} {Use} {Context}},
	shorttitle = {Sharp {Nearby}, {Fuzzy} {Far} {Away}},
	url = {http://arxiv.org/abs/1805.04623},
	abstract = {We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.},
	urldate = {2019-06-14},
	journal = {arXiv:1805.04623 [cs]},
	author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
	month = may,
	year = {2018},
	note = {arXiv: 1805.04623},
	keywords = {Computer Science - Computation and Language},
}

@article{medioni_segmentbased_1985a,
	title = {Segment-{Based} {Stereo} {Matching}},
	volume = {31},
	doi = {10/dc7k3t},
	number = {1},
	journal = {Computer Vision, Graphics, and Image Processing},
	author = {Medioni, Gerard and Nevatia, Ramakant},
	year = {1985},
	note = {00598},
	pages = {2--18},
}

@article{geiger_scaling_2020,
	title = {Scaling description of generalization with number of parameters in deep learning},
	volume = {2020},
	issn = {1742-5468},
	url = {http://arxiv.org/abs/1901.01608},
	doi = {10.1088/1742-5468/ab633c},
	abstract = {Supervised deep learning involves the training of neural networks with a large number \$N\$ of parameters. For large enough \$N\$, in the so-called over-parametrized regime, one can essentially fit the training data points. Sparsity-based arguments would suggest that the generalization error increases as \$N\$ grows past a certain threshold \$N{\textasciicircum}\{*\}\$. Instead, empirical studies have shown that in the over-parametrized regime, generalization error keeps decreasing with \$N\$. We resolve this paradox through a new framework. We rely on the so-called Neural Tangent Kernel, which connects large neural nets to kernel methods, to show that the initialization causes finite-size random fluctuations \${\textbackslash}{\textbar}f\_\{N\}-{\textbackslash}bar\{f\}\_\{N\}{\textbackslash}{\textbar}{\textbackslash}sim N{\textasciicircum}\{-1/4\}\$ of the neural net output function \$f\_\{N\}\$ around its expectation \${\textbackslash}bar\{f\}\_\{N\}\$. These affect the generalization error \${\textbackslash}epsilon\_\{N\}\$ for classification: under natural assumptions, it decays to a plateau value \${\textbackslash}epsilon\_\{{\textbackslash}infty\}\$ in a power-law fashion \${\textbackslash}sim N{\textasciicircum}\{-1/2\}\$. This description breaks down at a so-called jamming transition \$N=N{\textasciicircum}\{*\}\$. At this threshold, we argue that \${\textbackslash}{\textbar}f\_\{N\}{\textbackslash}{\textbar}\$ diverges. This result leads to a plausible explanation for the cusp in test error known to occur at \$N{\textasciicircum}\{*\}\$. Our results are confirmed by extensive empirical observations on the MNIST and CIFAR image datasets. Our analysis finally suggests that, given a computational envelope, the smallest generalization error is obtained using several networks of intermediate sizes, just beyond \$N{\textasciicircum}\{*\}\$, and averaging their outputs.},
	number = {2},
	urldate = {2021-05-23},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and d'Ascoli, Stéphane and Biroli, Giulio and Hongler, Clément and Wyart, Matthieu},
	month = feb,
	year = {2020},
	note = {arXiv: 1901.01608},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks},
	pages = {023401},
}

@incollection{maheswaranathan_reverse_2019a,
	title = {Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics},
	url = {http://papers.nips.cc/paper/9700-reverse-engineering-recurrent-networks-for-sentiment-classification-reveals-line-attractor-dynamics.pdf},
	booktitle = {Advances in neural information processing systems 32},
	publisher = {Curran Associates, Inc.},
	author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew and Ganguli, Surya and Sussillo, David},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and dAlché-Buc, F. and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {15696--15705},
}

@incollection{maheswaranathan_reverse_2019,
	title = {Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics},
	url = {http://papers.nips.cc/paper/9700-reverse-engineering-recurrent-networks-for-sentiment-classification-reveals-line-attractor-dynamics.pdf},
	booktitle = {Advances in neural information processing systems 32},
	publisher = {Curran Associates, Inc.},
	author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew and Ganguli, Surya and Sussillo, David},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and dAlché-Buc, F. and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {15696--15705},
}

@article{xu_robustness_2012,
	title = {Robustness and generalization},
	volume = {86},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-011-5268-1},
	doi = {10.1007/s10994-011-5268-1},
	abstract = {We derive generalization bounds for learning algorithms based on their robustness: the property that if a testing sample is “similar” to a training sample, then the testing error is close to the training error. This provides a novel approach, different from complexity or stability arguments, to study generalization of learning algorithms. One advantage of the robustness approach, compared to previous methods, is the geometric intuition it conveys. Consequently, robustness-based analysis is easy to extend to learning in non-standard setups such as Markovian samples or quantile loss. We further show that a weak notion of robustness is both sufﬁcient and necessary for generalizability, which implies that robustness is a fundamental property that is required for learning algorithms to work.},
	language = {en},
	number = {3},
	urldate = {2020-07-14},
	journal = {Machine Learning},
	author = {Xu, Huan and Mannor, Shie},
	month = mar,
	year = {2012},
	pages = {391--423},
}

@article{lin_resnet_2018,
	title = {{ResNet} with one-neuron hidden layers is a {Universal} {Approximator}},
	url = {https://arxiv.org/abs/1806.10909},
	language = {en},
	urldate = {2018-12-06},
	author = {Lin, Hongzhou and Jegelka, Stefanie},
	month = jun,
	year = {2018},
}

@article{elvira_rethinking_2018,
	title = {Rethinking the {Effective} {Sample} {Size}},
	url = {https://arxiv.org/abs/1809.04129},
	language = {en},
	urldate = {2018-11-12},
	author = {Elvira, Víctor and Martino, Luca and Robert, Christian P.},
	month = sep,
	year = {2018},
	note = {00000},
}

@article{chen_representations_1989,
	title = {Representations of non-linear systems: the {NARMAX} model},
	volume = {49},
	doi = {10.1080/00207178908559683},
	number = {3},
	journal = {International Journal of Control},
	author = {Chen, S and Billings, SA},
	year = {1989},
	pages = {1013--1032},
}

@phdthesis{rojas_robust_2008,
	address = {Australia},
	title = {Robust {Experiment} {Design}},
	school = {The University of Newcastle},
	author = {Rojas, Cristian R.},
	year = {2008},
}

@article{cantwell_rethinking_2018,
	title = {Rethinking multiscale cardiac electrophysiology with machine learning and predictive modelling},
	url = {http://arxiv.org/abs/1810.04227},
	abstract = {We review some of the latest approaches to analysing cardiac electrophysiology data using machine learning and predictive modelling. Cardiac arrhythmias, particularly atrial fibrillation, are a major global healthcare challenge. Treatment is often through catheter ablation, which involves the targeted localized destruction of regions of the myocardium responsible for initiating or perpetuating the arrhythmia. Ablation targets are either anatomically defined, or identified based on their functional properties as determined through the analysis of contact intracardiac electrograms acquired with increasing spatial density by modern electroanatomic mapping systems. While numerous quantitative approaches have been investigated over the past decades for identifying these critical curative sites, few have provided a reliable and reproducible advance in success rates. Machine learning techniques, including recent deep-learning approaches, offer a potential route to gaining new insight from this wealth of highly complex spatio-temporal information that existing methods struggle to analyse. Coupled with predictive modelling, these techniques offer exciting opportunities to advance the field and produce more accurate diagnoses and robust personalised treatment. We outline some of these methods and illustrate their use in making predictions from the contact electrogram and augmenting predictive modelling tools, both by more rapidly predicting future states of the system and by inferring the parameters of these models from experimental observations.},
	urldate = {2018-10-20},
	journal = {arXiv:1810.04227},
	author = {Cantwell, Chris D. and Mohamied, Yumnah and Tzortzis, Konstantinos N. and Garasto, Stef and Houston, Charles and Chowdhury, Rasheda A. and Ng, Fu Siong and Bharath, Anil A. and Peters, Nicholas S.},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.04227},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Quantitative Biology - Tissues and Organs, Statistics - Machine Learning, 🔍No DOI found},
}

@article{gurushewal_robust_2012,
	title = {Robust {Stability} of {LTI} ({SISO}) {System} with {System} {Gain} ({A}) under uncertainty set using {PID} controller},
	journal = {International Journal of Advances in Computing and Information Technology},
	author = {Gurushewal, Singh},
	year = {2012},
	keywords = {🔍No DOI found},
}

@inproceedings{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	year = {2014},
	pages = {580--587},
}

@article{byrd_representations_1994,
	title = {Representations of quasi-{Newton} matrices and their use in limited memory methods},
	volume = {63},
	issn = {0025-5610, 1436-4646},
	url = {https://link.springer.com/article/10.1007/BF01582063},
	doi = {10.1007/BF01582063},
	abstract = {We derive compact representations of BFGS and symmetric rank-one matrices for optimization. These representations allow us to efficiently implement limited memory methods for large constrained optimization problems. In particular, we discuss how to compute projections of limited memory matrices onto subspaces. We also present a compact representation of the matrices generated by Broyden's update for solving systems of nonlinear equations.},
	language = {en},
	number = {1-3},
	urldate = {2017-10-30},
	journal = {Mathematical Programming},
	author = {Byrd, Richard H. and Nocedal, Jorge and Schnabel, Robert B.},
	month = jan,
	year = {1994},
	pages = {129--156},
}

@book{lewis_robot_2004,
	address = {New York},
	edition = {2nd ed., rev. and expanded},
	series = {Control engineering series},
	title = {Robot manipulator control: theory and practice},
	isbn = {978-0-8247-4072-6},
	shorttitle = {Robot manipulator control},
	publisher = {Marcel Dekker},
	author = {Lewis, Frank L. and Abdallah, C. T. and Dawson, D. M. and Lewis, Frank L.},
	year = {2004},
	keywords = {Automatic control, Control systems, Manipulators (Mechanism), Robots},
}

@article{koziel_robust_2010,
	title = {Robust trust-region space-mapping algorithms for microwave design optimization},
	volume = {58},
	number = {8},
	journal = {IEEE Transactions on Microwave Theory and Techniques},
	author = {Koziel, Slawomir and Bandler, John W and Cheng, Qingsha S},
	year = {2010},
	keywords = {❓Multiple DOI},
	pages = {2166--2174},
}

@article{liu_reliable_2018,
	title = {Reliable {Semi}-{Supervised} {Learning} when {Labels} are {Missing} at {Random}},
	url = {http://arxiv.org/abs/1811.10947},
	abstract = {Semi-supervised learning methods are motivated by the availability of large datasets with unlabeled features in addition to labeled data. Unlabeled data is, however, not guaranteed to improve classification performance and has in fact been reported to impair the performance in certain cases. A fundamental source of error arises from restrictive assumptions about the unlabeled features, which result in unreliable classifiers. In this paper, we develop a semi-supervised learning approach that relaxes such assumptions and is capable of providing classifiers that reliably measure the label uncertainty. The approach is applicable using any generative model with a supervised learning algorithm. We illustrate the approach using both handwritten digit and cloth classification data where the labels are missing at random.},
	urldate = {2019-03-21},
	journal = {arXiv:1811.10947 [cs, stat]},
	author = {Liu, Xiuming and Zachariah, Dave and Wågberg, Johan and Schön, Thomas},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.10947},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{maheswaranathan_reverse_2019b,
	title = {Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics},
	url = {http://arxiv.org/abs/1906.10720},
	abstract = {Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.},
	urldate = {2019-06-28},
	journal = {arXiv:1906.10720 [cs, stat]},
	author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew D. and Ganguli, Surya and Sussillo, David},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.10720},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{koziel_robust_2010a,
	title = {Robust {Trust}-{Region} {Space}-{Mapping} {Algorithms} for {Microwave} {Design} {Optimization}},
	volume = {58},
	doi = {10/dk8kvf},
	number = {8},
	journal = {IEEE Transactions on Microwave Theory and Techniques},
	author = {Koziel, Slawomir and Bandler, John W and Cheng, Qingsha S},
	year = {2010},
	note = {00063},
	pages = {2166--2174},
}

@article{turner_residential_2017,
	title = {Residential {HVAC} {Fault} {Detection} {Using} a {System} {Identification} {Approach}},
	doi = {10/gbxqqf},
	journal = {Energy and Buildings},
	author = {Turner, W. J. N. and Staino, A and Basu, B},
	year = {2017},
	note = {00013},
}

@book{jaeger_reservoir_2009,
	title = {Reservoir computing approaches to recurrent neural network training},
	abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both: current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current “brand-names” of reservoir methods, and thus aims to help unifying the field and providing the reader with a detailed “map” of it.},
	author = {Jaeger, Herbert},
	year = {2009},
}

@inproceedings{ribeiro_relacoes_2014,
	title = {Relaçoes {Estáticas} de {Modelos} {NARX} {MISO} e sua {Representaçao} de {Hammerstein}},
	copyright = {All rights reserved},
	url = {http://www.swge.inf.br/CBA2014/anais/PDF/1569890815.pdf},
	booktitle = {{XX} {Congresso} {Brasileiro} de {Automática}},
	author = {Ribeiro, Antônio H. and Aguirre, Luis A.},
	year = {2014},
}

@article{lu_reservoir_2017,
	title = {Reservoir observers: {Model}-free inference of unmeasured variables in chaotic systems},
	volume = {27},
	issn = {1054-1500, 1089-7682},
	shorttitle = {Reservoir observers},
	url = {http://aip.scitation.org/doi/10.1063/1.4979665},
	doi = {10.1063/1.4979665},
	language = {en},
	number = {4},
	urldate = {2021-04-01},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Lu, Zhixin and Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Brockett, Roger and Ott, Edward},
	month = apr,
	year = {2017},
	pages = {041102},
}

@article{gouk_regularisation_2018,
	title = {Regularisation of {Neural} {Networks} by {Enforcing} {Lipschitz} {Continuity}},
	url = {http://arxiv.org/abs/1804.04368},
	abstract = {We investigate the effect of explicitly enforcing the Lipschitz continuity of neural networks with respect to their inputs. To this end, we provide a simple technique for computing an upper bound to the Lipschitz constant of a feed forward neural network composed of commonly used layer types and demonstrate inaccuracies in previous work on this topic. Our technique is then used to formulate training a neural network with a bounded Lipschitz constant as a constrained optimisation problem that can be solved using projected stochastic gradient methods. Our evaluation study shows that, in isolation, our method performs comparatively to state-of-the-art regularisation techniques. Moreover, when combined with existing approaches to regularising neural networks the performance gains are cumulative. We also provide evidence that the hyperparameters are intuitive to tune and demonstrate how the choice of norm for computing the Lipschitz constant impacts the resulting model.},
	urldate = {2020-06-05},
	journal = {arXiv:1804.04368 [cs, stat]},
	author = {Gouk, Henry and Frank, Eibe and Pfahringer, Bernhard and Cree, Michael},
	month = sep,
	year = {2018},
	note = {arXiv: 1804.04368},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{dreesen_recovering_2015,
	series = {17th {IFAC} {Symposium} on {System} {Identification} {SYSID} 2015},
	title = {Recovering {Wiener}-{Hammerstein} nonlinear state-space models using linear algebra},
	volume = {48},
	issn = {2405-8963},
	url = {http://www.sciencedirect.com/science/article/pii/S2405896315028773},
	doi = {10/gfkd8f},
	abstract = {This paper considers Wiener-Hammerstein systems consisting of a cascade of a linear dynamical system, a static nonlinearity and another linear dynamical system. We start from a black-box nonlinear state-space description of the system and develop a method to reconstruct the parameters of the underlying Wiener-Hammerstein block structure by means of linear algebra operations. First, the static nonlinearity is retrieved by decoupling the nonlinear part of the state-space equations into a single-branch nonlinear function. From there on, a canonical Wiener-Hammerstein nonlinear state-space model is recovered by using linear algebraic and geometric tools. The method is validated on a simulation example.},
	number = {28},
	urldate = {2018-11-26},
	journal = {IFAC-PapersOnLine},
	author = {Dreesen, Philippe and Ishteva, Mariya and Schoukens, Johan},
	month = jan,
	year = {2015},
	keywords = {Wiener-Hammerstein systems, block-oriented system identification, nonlinear system identification, state-space models},
	pages = {951--956},
}

@inproceedings{liwan_regularization_2013,
	title = {Regularization of {Neural} {Networks} using {DropConnect}},
	url = {http://proceedings.mlr.press/v28/wan13.html},
	abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {{Li Wan} and {Matthew Zeiler} and {Sixin Zhang} and {Yann Le Cun} and {Rob Fergus}},
	editor = {{Sanjoy Dasgupta} and {David McAllester}},
	month = feb,
	year = {2013},
	pages = {1058--1066},
}

@inproceedings{maas_rectifier_2013,
	title = {Rectifier nonlinearities improve neural network acoustic models},
	booktitle = {in {ICML} {Workshop} on {Deep} {Learning} for {Audio}, {Speech} and {Language} {Processing}},
	author = {Maas, Andrew L. and Hannun, Awni Y. and Ng, Andrew Y.},
	year = {2013},
	keywords = {⛔ No DOI found},
}

@article{tibshirani_regression_2011,
	title = {Regression shrinkage and selection via the lasso: a retrospective},
	volume = {73},
	shorttitle = {Regression shrinkage and selection via the lasso},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2011.00771.x/full},
	doi = {10.1111/j.1467-9868.2011.00771.x},
	number = {3},
	urldate = {2017-09-14},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Tibshirani, Robert},
	year = {2011},
	note = {00000},
	pages = {273--282},
}

@article{wang_recovery_2011,
	title = {Recovery of seismic wavefields based on compressive sensing by an l1-norm constrained trust region method and the piecewise random subsampling},
	volume = {187},
	doi = {10.1111/j.1365-246X.2011.05130.x},
	number = {1},
	journal = {Geophysical Journal International},
	author = {Wang, Yanfei and Cao, Jingjie and Yang, Changchun},
	year = {2011},
	note = {00000},
	keywords = {❓Multiple DOI},
	pages = {199--213},
}

@article{mehrkanoon_regularized_2017,
	title = {Regularized {Semipaired} {Kernel} {CCA} for {Domain} {Adaptation}},
	volume = {PP},
	issn = {2162-237X},
	doi = {10.1109/TNNLS.2017.2728719},
	abstract = {Domain adaptation learning is one of the fundamental research topics in pattern recognition and machine learning. This paper introduces a regularized semipaired kernel canonical correlation analysis formulation for learning a latent space for the domain adaptation problem. The optimization problem is formulated in the primal-dual least squares support vector machine setting where side information can be readily incorporated through regularization terms. The proposed model learns a joint representation of the data set across different domains by solving a generalized eigenvalue problem or linear system of equations in the dual. The approach is naturally equipped with out-of-sample extension property, which plays an important role for model selection. Furthermore, the Nyström approximation technique is used to make the computational issues due to the large size of the matrices involved in the eigendecomposition feasible. The learned latent space of the source domain is fed to a multiclass semisupervised kernel spectral clustering model that can learn from both labeled and unlabeled data points of the source domain in order to classify the data instances of the target domain. Experimental results are given to illustrate the effectiveness of the proposed approaches on synthetic and real-life data sets.},
	number = {99},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Mehrkanoon, S. and Suykens, J. A. K.},
	year = {2017},
	keywords = {Adaptation models, Correlation, Data models, Domain adaption, Eigenvalues and eigenfunctions, Kernel, Mathematical model, Nyström approximation, Optimization, kernel canonical correlation analysis (KCCA), semisupervised learning},
	pages = {1--15},
}

@article{friedman_regularization_2010,
	title = {Regularization paths for generalized linear models via coordinate descent},
	volume = {33},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/pmc2929880/},
	doi = {10.18637/jss.v033.i01},
	number = {1},
	urldate = {2017-09-14},
	journal = {Journal of statistical software},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
	year = {2010},
	pages = {1},
}

@article{abdulhai_reinforcement_2003,
	title = {Reinforcement learning: {Introduction} to theory and potential for transport applications},
	volume = {30},
	shorttitle = {Reinforcement learning},
	doi = {10.1139/l03-014},
	number = {6},
	journal = {Canadian Journal of Civil Engineering},
	author = {Abdulhai, Baher and Kattan, Lina},
	year = {2003},
	pages = {981--991},
}

@article{zou_regularization_2005,
	title = {Regularization and {Variable} {Selection} via the {Elastic} {Net}},
	volume = {67},
	issn = {1369-7412},
	url = {http://www.jstor.org/stable/3647580},
	doi = {10.1111/j.1467-9868.2005.00503.x},
	abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p ≫ n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
	number = {2},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Zou, Hui and Hastie, Trevor},
	year = {2005},
	note = {08250},
	pages = {301--320},
}

@article{wagberg_regularized_2017,
	title = {Regularized parametric system identification: a decision-theoretic formulation},
	shorttitle = {Regularized parametric system identification},
	url = {http://arxiv.org/abs/1710.04009},
	abstract = {Parametric prediction error methods constitute a classical approach to the identification of linear dynamic systems with excellent large-sample properties. A more recent regularized approach, inspired by machine learning and Bayesian methods, has also gained attention. Methods based on this approach estimate the system impulse response with excellent small-sample properties. In several applications, however, it is desirable to obtain a compact representation of the system in the form of a parametric model. By viewing the identification of such models as a decision, we develop a decision-theoretic formulation of the parametric system identification problem that bridges the gap between the classical and regularized approaches above. Using the output-error model class as an illustration, we show that this decision-theoretic approach leads to a regularized method that is robust to small sample-sizes as well as overparameterization.},
	journal = {arXiv:1710.04009 [cs]},
	author = {Wågberg, Johan and Zachariah, Dave and Schön, Thomas B.},
	month = oct,
	year = {2017},
	note = {00000 
arXiv: 1710.04009},
	keywords = {Computer Science - Systems and Control, 🔍No DOI found},
}

@article{wang_regression_2007,
	title = {Regression {Coefficient} and {Autoregressive} {Order} {Shrinkage} and {Selection} {Via} the {Lasso}},
	volume = {69},
	issn = {1369-7412},
	url = {http://www.jstor.org/stable/4623254},
	doi = {10.1111/j.1467-9868.2007.00577.x},
	abstract = {The least absolute shrinkage and selection operator ('lasso') has been widely used in regression shrinkage and selection. We extend its application to the regression model with autoregressive errors. Two types of lasso estimators are carefully studied. The first is similar to the traditional lasso estimator with only two tuning parameters (one for regression coefficients and the other for autoregression coeffi). These tuning parameters can be easily calculated via a data-driven method, but the resulting lasso estimator may not be fully efficient. To overcome this limitation, we propose a second lasso estimator which uses different tuning parameters for each coefficient. We show that this modified lasso can produce the estimator as efficiently as the oracle. Moreover, we propose an algorithm for tuning parameter estimates to obtain the modified lasso estimator. Simulation studies demonstrate that the modified estimator is superior to the traditional estimator. One empirical example is also presented to illustrate the usefulness of lasso estimators. The extension of the lasso to the autoregression with exogenous variables model is briefly discussed.},
	number = {1},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Wang, Hansheng and Li, Guodong and Tsai, Chih-Ling},
	year = {2007},
	note = {00225},
	pages = {63--78},
}

@article{furnival_regressions_1974,
	title = {Regressions by {Leaps} and {Bounds}},
	volume = {16},
	issn = {0040-1706},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1974.10489231},
	doi = {10.1080/00401706.1974.10489231},
	number = {4},
	journal = {Technometrics},
	author = {Furnival, George M. and Wilson, Robert W.},
	month = nov,
	year = {1974},
	pages = {499--511},
}

@inproceedings{mnih_recurrent_2014,
	title = {Recurrent models of visual attention},
	booktitle = {Advances in neural information processing systems},
	author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex},
	year = {2014},
	pages = {2204--2212},
}

@book{mandic_recurrent_2001,
	address = {Chichester ; New York},
	series = {Wiley series in adaptive and learning systems for signal processing, communications, and control},
	title = {Recurrent neural networks for prediction: learning algorithms, architectures, and stability},
	isbn = {978-0-471-49517-8},
	shorttitle = {Recurrent neural networks for prediction},
	publisher = {John Wiley},
	author = {Mandic, Danilo P. and Chambers, Jonathon A.},
	year = {2001},
	keywords = {Neural networks (Computer science), machine learning},
}

@article{basu_regularized_2015,
	title = {Regularized estimation in sparse high-dimensional time series models},
	volume = {43},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1311.4175},
	doi = {10.1214/15-AOS1315},
	abstract = {Many scientific and economic problems involve the analysis of high-dimensional time series datasets. However, theoretical studies in high-dimensional statistics to date rely primarily on the assumption of independent and identically distributed (i.i.d.) samples. In this work, we focus on stable Gaussian processes and investigate the theoretical properties of \${\textbackslash}ell \_1\$-regularized estimates in two important statistical problems in the context of high-dimensional time series: (a) stochastic regression with serially correlated errors and (b) transition matrix estimation in vector autoregressive (VAR) models. We derive nonasymptotic upper bounds on the estimation errors of the regularized estimates and establish that consistent estimation under high-dimensional scaling is possible via \${\textbackslash}ell\_1\$-regularization for a large class of stable processes under sparsity constraints. A key technical contribution of the work is to introduce a measure of stability for stationary processes using their spectral properties that provides insight into the effect of dependence on the accuracy of the regularized estimates. With this proposed stability measure, we establish some useful deviation bounds for dependent data, which can be used to study several important regularized estimates in a time series setting.},
	number = {4},
	journal = {The Annals of Statistics},
	author = {Basu, Sumanta and Michailidis, George},
	month = aug,
	year = {2015},
	note = {arXiv: 1311.4175},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology},
	pages = {1535--1567},
}

@article{wang_recovery_2011a,
	title = {Recovery of {Seismic} {Wavefields} {Based} on {Compressive} {Sensing} by an {L1}-{Norm} {Constrained} {Trust} {Region} {Method} and the {Piecewise} {Random} {Subsampling}},
	volume = {187},
	number = {1},
	journal = {Geophysical Journal International},
	author = {Wang, Yanfei and Cao, Jingjie and Yang, Changchun},
	year = {2011},
	note = {00000},
	keywords = {❓Multiple DOI},
	pages = {199--213},
}

@article{sutton_reinforcement_2011,
	title = {Reinforcement learning: {An} introduction},
	author = {Sutton, Richard S and Barto, Andrew G},
	year = {2011},
}

@article{tibshirani_regression_1996,
	title = {Regression shrinkage and selection via the {LASSO}},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	note = {00000},
	keywords = {🔍No DOI found},
	pages = {267--288},
}

@phdthesis{ribeiro_recurrent_2017,
	address = {Belo Horizonte, Brazil},
	type = {{MSc} {Dissertation}},
	title = {Recurrent {Structures} in {System} {Identification}},
	copyright = {All rights reserved},
	school = {Universidade Federal de Minas Gerais},
	author = {Ribeiro, Antônio H.},
	year = {2017},
}

@article{oliver_realistic_2018,
	title = {Realistic {Evaluation} of {Deep} {Semi}-{Supervised} {Learning} {Algorithms}},
	url = {http://arxiv.org/abs/1804.09170},
	abstract = {Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that these algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, that SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.},
	urldate = {2018-12-06},
	journal = {arXiv:1804.09170 [cs, stat]},
	author = {Oliver, Avital and Odena, Augustus and Raffel, Colin and Cubuk, Ekin D. and Goodfellow, Ian J.},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.09170},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{minar_recent_2018,
	title = {Recent {Advances} in {Deep} {Learning}: {An} {Overview}},
	shorttitle = {Recent {Advances} in {Deep} {Learning}},
	url = {https://arxiv.org/abs/1807.08169v1},
	doi = {10/gdvmmz},
	abstract = {Deep Learning is one of the newest trends in Machine Learning and Artificial
Intelligence research. It is also one of the most popular scientific research
trends now-a-days. Deep learning methods have brought revolutionary advances in
computer vision and machine learning. Every now and then, new and new deep
learning techniques are being born, outperforming state-of-the-art machine
learning and even existing deep learning techniques. In recent years, the world
has seen many major breakthroughs in this field. Since deep learning is
evolving at a huge speed, its kind of hard to keep track of the regular
advances especially for new researchers. In this paper, we are going to briefly
discuss about recent advances in Deep Learning for past few years.},
	language = {en},
	urldate = {2019-01-07},
	author = {Minar, Matiur Rahman and Naher, Jibon},
	month = jul,
	year = {2018},
}

@article{kligfield_recommendations_2007,
	title = {Recommendations for the {Standardization} and {Interpretation} of the {Electrocardiogram}},
	volume = {49},
	url = {http://www.onlinejacc.org/content/49/10/1109.abstract},
	doi = {10/bwp48j},
	abstract = {This statement examines the relation of the resting ECG to its technology. Its purpose is to foster understanding of how the modern ECG is derived and displayed and to establish standards that will improve the accuracy and usefulness of the ECG in practice. Derivation of representative waveforms and measurements based on global intervals are described. Special emphasis is placed on digital signal acquisition and computer-based signal processing, which provide automated measurements that lead to computer-generated diagnostic statements. Lead placement, recording methods, and waveform presentation are reviewed. Throughout the statement, recommendations for ECG standards are placed in context of the clinical implications of evolving ECG technology.},
	number = {10},
	journal = {Journal of the American College of Cardiology},
	author = {Kligfield, Paul and Gettes, Leonard S. and Bailey, James J. and Childers, Rory and Deal, Barbara J. and Hancock, E. William and van Herpen, Gerard and Kors, Jan A. and Macfarlane, Peter and Mirvis, David M. and Pahlm, Olle and Rautaharju, Pentti and Wagner, Galen S.},
	month = mar,
	year = {2007},
	pages = {1109},
}

@inproceedings{chang_realtime_2007,
	title = {Real-time {DSP} implementation on local stereo matching},
	booktitle = {Multimedia and {Expo}, 2007 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Chang, Nelson and Lin, Ting-Min and Tsai, Tsung-Hsien and Tseng, Yu-Cheng and Chang, Tian-Sheuan},
	year = {2007},
	pages = {2090--2093},
}

@article{bock_recent_1983,
	title = {Recent advances in parameter identification problems for {ODE}},
	doi = {10.1007/978-1-4684-7324-7_7},
	journal = {Numerical Treatment of Inverse Problems in Differential and Integral Equations},
	author = {Bock, HG},
	year = {1983},
	pages = {95--121},
}

@book{stein_real_2005,
	address = {Princeton, N.J},
	series = {Princeton lectures in analysis},
	title = {Real analysis: measure theory, integration, and {Hilbert} spaces},
	isbn = {978-0-691-11386-9},
	shorttitle = {Real analysis},
	number = {v. 3},
	publisher = {Princeton University Press},
	author = {Stein, Elias M. and Shakarchi, Rami},
	year = {2005},
	keywords = {Functional analysis, Hilbertruimten, Integrals, Generalized, Maattheorie, Measure theory},
}

@book{osullivan_real_2010,
	address = {Beijing},
	edition = {1. ed., [Nachdr.]},
	title = {Real world {Haskell}: code you can believe in},
	isbn = {978-0-596-51498-3},
	shorttitle = {Real world {Haskell}},
	language = {eng},
	publisher = {O'Reilly},
	author = {O'Sullivan, Bryan and Goerzen, John and Stewart, Donald Bruce},
	year = {2010},
	note = {OCLC: 837707964},
}

@article{daniel_realtime_2007,
	title = {Real-time {3D} vectorcardiography: an application for didactic use},
	volume = {90},
	issn = {1742-6596},
	shorttitle = {Real-time {3D} vectorcardiography},
	url = {http://stacks.iop.org/1742-6596/90/i=1/a=012013?key=crossref.695ba585841996824e286ada8e1662f0},
	doi = {10.1088/1742-6596/90/1/012013},
	urldate = {2018-07-17},
	journal = {Journal of Physics: Conference Series},
	author = {Daniel, G and Lissa, G and Redondo, D Medina and Vásquez, L and Zapata, D},
	month = nov,
	year = {2007},
	pages = {012013},
}

@inproceedings{park_realtime_2007,
	title = {Real-time stereo vision {FPGA} chip with low error rate},
	booktitle = {Multimedia and {Ubiquitous} {Engineering}, 2007. {MUE}'07. {International} {Conference} on},
	publisher = {IEEE},
	author = {Park, Sungchan and Jeong, Hong},
	year = {2007},
	note = {00000},
	pages = {751--756},
}

@article{salehinejad_recent_2017,
	title = {Recent {Advances} in {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1801.01078},
	abstract = {Recurrent neural networks (RNNs) are capable of learning features and long term dependencies from sequential and time-series data. The RNNs have a stack of non-linear units where at least one connection between units forms a directed cycle. A well-trained RNN can model any dynamical system; however, training RNNs is mostly plagued by issues in learning long-term dependencies. In this paper, we present a survey on RNNs and several new advances for newcomers and professionals in the field. The fundamentals and recent advances are explained and the research challenges are introduced.},
	urldate = {2019-05-23},
	journal = {arXiv:1801.01078 [cs]},
	author = {Salehinejad, Hojjat and Sankar, Sharan and Barfett, Joseph and Colak, Errol and Valaee, Shahrokh},
	month = dec,
	year = {2017},
	note = {arXiv: 1801.01078},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@book{rudin_real_1987,
	address = {New York},
	edition = {3rd ed},
	title = {Real and complex analysis},
	isbn = {978-0-07-054234-1},
	language = {en},
	publisher = {McGraw-Hill},
	author = {Rudin, Walter},
	year = {1987},
	keywords = {Mathematical analysis},
}

@article{bock_recent_1983a,
	title = {Recent {Advances} in {Parameter} {Identification} {Problems} for {ODE}},
	doi = {10/gfjwmg},
	journal = {Numerical Treatment of Inverse Problems in Differential and Integral Equations},
	author = {Bock, HG},
	year = {1983},
	pages = {95--121},
}

@article{yildiz_revisiting_2012,
	title = {Re-visiting the echo state property},
	volume = {35},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608012001852},
	doi = {10.1016/j.neunet.2012.07.005},
	abstract = {An echo state network (ESN) consists of a large, randomly connected neural network, the reservoir, which is driven by an input signal and projects to output units. During training, only the connections from the reservoir to these output units are learned. A key requisite for output-only training is the echo state property (ESP), which means that the effect of initial conditions should vanish as time passes. In this paper, we use analytical examples to show that a widely used criterion for the ESP, the spectral radius of the weight matrix being smaller than unity, is not sufficient to satisfy the echo state property. We obtain these examples by investigating local bifurcation properties of the standard ESNs. Moreover, we provide new sufficient conditions for the echo state property of standard sigmoid and leaky integrator ESNs. We furthermore suggest an improved technical definition of the echo state property, and discuss what practicians should (and should not) observe when they optimize their reservoirs for specific tasks.},
	urldate = {2019-10-28},
	journal = {Neural Networks},
	author = {Yildiz, Izzet B. and Jaeger, Herbert and Kiebel, Stefan J.},
	month = nov,
	year = {2012},
	keywords = {Bifurcation, Diagonally Schur stable, Echo state network, Lyapunov, Spectral radius},
	pages = {1--9},
}

@article{maass_realtime_2002,
	title = {Real-{Time} {Computing} {Without} {Stable} {States}: {A} {New} {Framework} for {Neural} {Computation} {Based} on {Perturbations}},
	volume = {14},
	issn = {0899-7667, 1530-888X},
	shorttitle = {Real-{Time} {Computing} {Without} {Stable} {States}},
	url = {http://www.mitpressjournals.org/doi/10.1162/089976602760407955},
	doi = {10.1162/089976602760407955},
	language = {en},
	number = {11},
	urldate = {2019-10-28},
	journal = {Neural Computation},
	author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
	month = nov,
	year = {2002},
	pages = {2531--2560},
}

@book{rudin_principles_1964,
	series = {International series in pure and applied mathematics},
	title = {Principles of mathematical analysis},
	url = {https://books.google.com.br/books?id=iifvAAAAMAAJ},
	publisher = {McGraw-Hill},
	author = {Rudin, W.},
	year = {1964},
}

@article{tanaka_pruning_2020,
	title = {Pruning neural networks without any data by iteratively conserving synaptic flow},
	url = {http://arxiv.org/abs/2006.05467},
	abstract = {Pruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.9 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that data must be used to quantify which synapses are important.},
	urldate = {2020-06-15},
	journal = {arXiv:2006.05467 [cond-mat, q-bio, stat]},
	author = {Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L. K. and Ganguli, Surya},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.05467},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{raghunath_prediction_2020,
	title = {Prediction of mortality from 12-lead electrocardiogram voltage data using a deep neural network},
	issn = {1078-8956, 1546-170X},
	url = {http://www.nature.com/articles/s41591-020-0870-z},
	doi = {10.1038/s41591-020-0870-z},
	language = {en},
	urldate = {2020-06-16},
	journal = {Nature Medicine},
	author = {Raghunath, Sushravya and Ulloa Cerna, Alvaro E. and Jing, Linyuan and vanMaanen, David P. and Stough, Joshua and Hartzel, Dustin N. and Leader, Joseph B. and Kirchner, H. Lester and Stumpe, Martin C. and Hafez, Ashraf and Nemani, Arun and Carbonati, Tanner and Johnson, Kipp W. and Young, Katelyn and Good, Christopher W. and Pfeifer, John M. and Patel, Aalpen A. and Delisle, Brian P. and Alsaid, Amro and Beer, Dominik and Haggerty, Christopher M. and Fornwalt, Brandon K.},
	month = may,
	year = {2020},
}

@article{wagner_ptbxl_2020,
	title = {{PTB}-{XL}, a large publicly available electrocardiography dataset},
	volume = {7},
	copyright = {2020 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-020-0495-6},
	doi = {10.1038/s41597-020-0495-6},
	abstract = {Electrocardiography (ECG) is a key non-invasive diagnostic tool for cardiovascular diseases which is increasingly supported by algorithms based on machine learning. Major obstacles for the development of automatic ECG interpretation algorithms are both the lack of public datasets and well-defined benchmarking procedures to allow comparison s of different algorithms. To address these issues, we put forward PTB-XL, the to-date largest freely accessible clinical 12-lead ECG-waveform dataset comprising 21837 records from 18885 patients of 10 seconds length. The ECG-waveform data was annotated by up to two cardiologists as a multi-label dataset, where diagnostic labels were further aggregated into super and subclasses. The dataset covers a broad range of diagnostic classes including, in particular, a large fraction of healthy records. The combination with additional metadata on demographics, additional diagnostic statements, diagnosis likelihoods, manually annotated signal properties as well as suggested folds for splitting training and test sets turns the dataset into a rich resource for the development and the evaluation of automatic ECG interpretation algorithms.},
	language = {en},
	number = {1},
	urldate = {2020-07-14},
	journal = {Scientific Data},
	author = {Wagner, Patrick and Strodthoff, Nils and Bousseljot, Ralf-Dieter and Kreiseler, Dieter and Lunze, Fatima I. and Samek, Wojciech and Schaeffter, Tobias},
	month = may,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {154},
}

@incollection{paszke_pytorch_2019,
	title = {{PyTorch}: {An} imperative style, high-performance deep learning library},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	booktitle = {Advances in neural information processing systems 32},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	pages = {8024--8035},
}

@article{breiman_random_2001,
	title = {Random forests},
	volume = {45},
	number = {1},
	journal = {Machine learning},
	author = {Breiman, Leo},
	year = {2001},
	note = {Publisher: Springer},
	pages = {5--32},
}

@book{ash_probability_2000,
	edition = {2nd},
	title = {Probability and {Measure} {Theory}},
	publisher = {Harcourt/Academic Press},
	author = {Ash, Robert B. and Doléans-Dale, Catherine},
	year = {2000},
}

@article{mallon_projective_2005,
	title = {Projective rectification from the fundamental matrix},
	volume = {23},
	doi = {10.1016/j.imavis.2005.03.002},
	number = {7},
	journal = {Image and Vision Computing},
	author = {Mallon, John and Whelan, Paul F},
	year = {2005},
	pages = {643--650},
}

@article{ljung_prediction_2002,
	title = {Prediction error estimation methods},
	volume = {21},
	doi = {10.1007/BF01211648},
	number = {1},
	journal = {Circuits, Systems and Signal Processing},
	author = {Ljung, Lennart},
	year = {2002},
	pages = {11--21},
}

@article{connally_predictionand_2007,
	title = {Prediction-and simulation-error based perceptron training: solution space analysis and a novel combined training scheme},
	volume = {70},
	number = {4},
	journal = {Neurocomputing},
	author = {Connally, Patrick and Li, Kang and Irwin, George W},
	year = {2007},
	keywords = {🔍No DOI found},
	pages = {819--827},
}

@article{dennis_quasinewton_1977,
	title = {Quasi-{Newton} methods, motivation and theory},
	volume = {19},
	doi = {10.1137/1019005},
	number = {1},
	journal = {SIAM review},
	author = {Dennis, Jr, John E and Moré, Jorge J},
	year = {1977},
	pages = {46--89},
}

@article{dayan_qlearning_1992,
	title = {Q-learning},
	volume = {8},
	doi = {10.1023/A:1022657612745},
	number = {3},
	journal = {Machine learning},
	author = {Dayan, Peter and Watkins, CJCH},
	year = {1992},
	pages = {279--292},
}

@book{sen_principles_2007,
	title = {Principles of electric machines and power electronics},
	isbn = {978-81-265-1101-3},
	url = {https://books.google.com.br/books?id=KlN84tZ1p9cC},
	publisher = {Wiley India Pvt. Limited},
	author = {Sen, P.C.},
	year = {2007},
	note = {00000},
}

@book{koller_probabilistic_2009,
	address = {Cambridge, MA},
	series = {Adaptive computation and machine learning},
	title = {Probabilistic graphical models: principles and techniques},
	isbn = {978-0-262-01319-2},
	shorttitle = {Probabilistic graphical models},
	publisher = {MIT Press},
	author = {Koller, Daphne and Friedman, Nir},
	year = {2009},
	keywords = {Bayesian statistical decision theory, Graphic methods, Graphical modeling (Statistics)},
}

@book{degroot_probability_2012,
	address = {Boston},
	edition = {4th ed},
	title = {Probability and statistics},
	isbn = {978-0-321-50046-5},
	publisher = {Addison-Wesley},
	author = {DeGroot, Morris H. and Schervish, Mark J.},
	year = {2012},
	note = {OCLC: ocn502674206},
	keywords = {Mathematical statistics, Probabilities, Textbooks},
}

@book{papoulis_probability_2002,
	address = {Boston},
	edition = {4th ed},
	title = {Probability, random variables, and stochastic processes},
	isbn = {978-0-07-366011-0 978-0-07-112256-6},
	publisher = {McGraw-Hill},
	author = {Papoulis, Athanasios and Pillai, S. Unnikrishna},
	year = {2002},
	keywords = {Probabilities, Random variables, stochastic processes},
}

@article{beck_protecting_2016,
	title = {Protecting the confidentiality and security of personal health information in low- and middle-income countries in the era of {SDGs} and {Big} {Data}},
	volume = {9},
	issn = {1654-9880},
	abstract = {BACKGROUND: As increasing amounts of personal information are being collected through a plethora of electronic modalities by statutory and non-statutory organizations, ensuring the confidentiality and security of such information has become a major issue globally. While the use of many of these media can be beneficial to individuals or populations, they can also be open to abuse by individuals or statutory and non-statutory organizations. Recent examples include collection of personal information by national security systems and the development of national programs like the Chinese Social Credit System. In many low- and middle-income countries, an increasing amount of personal health information is being collected. The collection of personal health information is necessary, in order to develop longitudinal medical records and to monitor and evaluate the use, cost, outcome, and impact of health services at facility, sub-national, and national levels. However, if personal health information is not held confidentially and securely, individuals with communicable or non-communicable diseases (NCDs) may be reluctant to use preventive or therapeutic health services, due to fear of being stigmatized or discriminated against. While policymakers and other stakeholders in these countries recognize the need to develop and implement policies for protecting the privacy, confidentiality and security of personal health information, to date few of these countries have developed, let alone implemented, coherent policies. The global HIV response continues to emphasize the importance of collecting HIV-health information, recently re-iterated by the Fast Track to End AIDS by 2030 program and the recent changes in the Guidelines on When to Start Antiretroviral Therapy and on Pre-exposure Prophylaxis for HIV. The success of developing HIV treatment cascades in low- and middle-income countries will require the development of National Health Identification Systems. The success of programs like Universal Health Coverage, under the recently ratified Sustainable Development Goals is also contingent on the availability of personal health information for communicable and non-communicable diseases.
DESIGN: Guidance for countries to develop and implement their own guidelines for protecting HIV-information formed the basis of identifying a number of fundamental principles, governing the areas of privacy, confidentiality and security. The use of individual-level data must balance maximizing the benefits from their most effective and fullest use, and minimizing harm resulting from their malicious or inadvertent release.
DISCUSSION: These general principles are described in this paper, as along with a bibliography referring to more detailed technical information. A country assessment tool and user's manual, based on these principles, have been developed to support countries to assess the privacy, confidentiality, and security of personal health information at facility, data warehouse/repository, and national levels. The successful development and implementation of national guidance will require strong collaboration at local, regional, and national levels, and this is a pre-condition for the successful implementation of a range of national and global programs.
CONCLUSION: This paper is a call for action for stakeholders in low- and middle-income countries to develop and implement such coherent policies and provides fundamental principles governing the areas of privacy, confidentiality, and security of personal health information being collected in low- and middle-income countries.},
	language = {eng},
	journal = {Global Health Action},
	author = {Beck, Eduard J. and Gill, Wayne and De Lay, Paul R.},
	year = {2016},
	pmid = {27885972},
	pmcid = {PMC5123209},
	keywords = {Big Data, SDGs, confidentiality, discrimination, ethics, personal health information, privacy laws, security, stigma},
	pages = {32089},
}

@article{dennisjr_quasinewton_1977,
	title = {Quasi-{Newton} {Methods}, {Motivation} and {Theory}},
	volume = {19},
	doi = {10/cxhdhx},
	number = {1},
	journal = {SIAM review},
	author = {Dennis, Jr, John E and Moré, Jorge J},
	year = {1977},
	note = {01684},
	pages = {46--89},
}

@inproceedings{ding_r1pca_2006,
	address = {New York, NY, USA},
	series = {{ICML} '06},
	title = {R1-pca: {Rotational} invariant l1-norm principal component analysis for robust subspace factorization},
	isbn = {1-59593-383-2},
	url = {https://doi.org/10.1145/1143844.1143880},
	doi = {10.1145/1143844.1143880},
	abstract = {Principal component analysis (PCA) minimizes the sum of squared errors (L2-norm) and is sensitive to the presence of outliers. We propose a rotational invariant L1-norm PCA (R1-PCA). R1-PCA is similar to PCA in that (1) it has a unique global solution, (2) the solution are principal eigenvectors of a robust covariance matrix (re-weighted to soften the effects of outliers), (3) the solution is rotational invariant. These properties are not shared by the L1-norm PCA. A new subspace iteration algorithm is given to compute R1-PCA efficiently. Experiments on several real-life datasets show R1-PCA can effectively handle outliers. We extend R1-norm to K-means clustering and show that L1-norm K-means leads to poor results while R1-K-means outperforms standard K-means.},
	booktitle = {Proceedings of the 23rd international conference on machine learning},
	publisher = {Association for Computing Machinery},
	author = {Ding, Chris and Zhou, Ding and He, Xiaofeng and Zha, Hongyuan},
	year = {2006},
	note = {Number of pages: 8
Place: Pittsburgh, Pennsylvania, USA},
	pages = {281--288},
}

@article{ljung_perspectives_2010a,
	title = {Perspectives on system identification},
	volume = {34},
	issn = {13675788},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1367578810000027},
	doi = {10.1016/j.arcontrol.2009.12.001},
	abstract = {System identiﬁcation is the art and science of building mathematical models of dynamic systems from observed input–output data. It can be seen as the interface between the real world of applications and the mathematical world of control theory and model abstractions. As such, it is an ubiquitous necessity for successful applications. System identiﬁcation is a very large topic, with different techniques that depend on the character of the models to be estimated: linear, nonlinear, hybrid, nonparametric, etc. At the same time, the area can be characterized by a small number of leading principles, e.g. to look for sustainable descriptions by proper decisions in the triangle of model complexity, information contents in the data, and effective validation. The area has many facets and there are many approaches and methods. A tutorial or a survey in a few pages is not quite possible. Instead, this presentation aims at giving an overview of the ‘‘science’’ side, i.e. basic principles and results and at pointing to open problem areas in the practical, ‘‘art’’, side of how to approach and solve a real problem.},
	language = {en},
	number = {1},
	urldate = {2020-07-07},
	journal = {Annual Reviews in Control},
	author = {Ljung, Lennart},
	month = apr,
	year = {2010},
	pages = {1--12},
}

@article{sassi_pdfecg_2017,
	title = {{PDF}-{ECG} in clinical practice: {A} model for long-term preservation of digital 12-lead {ECG} data},
	volume = {50},
	issn = {1532-8430},
	shorttitle = {{PDF}-{ECG} in clinical practice},
	doi = {10.1016/j.jelectrocard.2017.08.001},
	abstract = {BACKGROUND: In clinical practice, data archiving of resting 12-lead electrocardiograms (ECGs) is mainly achieved by storing a PDF report in the hospital electronic health record (EHR). When available, digital ECG source data (raw samples) are only retained within the ECG management system.
OBJECTIVE: The widespread availability of the ECG source data would undoubtedly permit successive analysis and facilitate longitudinal studies, with both scientific and diagnostic benefits.
METHODS \& RESULTS: PDF-ECG is a hybrid archival format which allows to store in the same file both the standard graphical report of an ECG together with its source ECG data (waveforms). Using PDF-ECG as a model to address the challenge of ECG data portability, long-term archiving and documentation, a real-world proof-of-concept test was conducted in a northern Italy hospital. A set of volunteers undertook a basic ECG using routine hospital equipment and the source data captured. Using dedicated web services, PDF-ECG documents were then generated and seamlessly uploaded in the hospital EHR, replacing the standard PDF reports automatically generated at the time of acquisition. Finally, the PDF-ECG files could be successfully retrieved and re-analyzed.
CONCLUSION: Adding PDF-ECG to an existing EHR had a minimal impact on the hospital's workflow, while preserving the ECG digital data.},
	language = {eng},
	number = {6},
	journal = {Journal of Electrocardiology},
	author = {Sassi, Roberto and Bond, Raymond R. and Cairns, Andrew and Finlay, Dewar D. and Guldenring, Daniel and Libretti, Guido and Isola, Lamberto and Vaglio, Martino and Poeta, Roberto and Campana, Marco and Cuccia, Claudio and Badilini, Fabio},
	month = dec,
	year = {2017},
	pmid = {28843654},
	keywords = {Clinical ECG, ECG digital data, ECG long-time preservation, Electrocardiography, Electronic Health Records, Humans, Information Storage and Retrieval, PDF report, Software, Systems Integration, Workflow},
	pages = {776--780},
}

@article{roll_piecewise_2008,
	title = {Piecewise linear solution paths with application to direct weight optimization},
	volume = {44},
	issn = {0005-1098},
	url = {http://www.sciencedirect.com/science/article/pii/S000510980800263X},
	doi = {10.1016/j.automatica.2008.03.020},
	abstract = {Recently, pathfollowing algorithms for parametric optimization problems with piecewise linear solution paths have been developed within the field of regularized regression. This paper presents a generalization of these algorithms to a wider class of problems. It is shown that the approach can be applied to the nonparametric system identification method, Direct Weight Optimization (DWO), and be used to enhance the computational efficiency of this method. The most important design parameter in the DWO method is a parameter (λ) controlling the bias-variance trade-off, and the use of parametric optimization with piecewise linear solution paths means that the DWO estimates can be efficiently computed for all values of λ simultaneously. This allows for designing computationally attractive adaptive bandwidth selection algorithms. One such algorithm for DWO is proposed and demonstrated in two examples.},
	number = {11},
	journal = {Automatica},
	author = {Roll, Jacob},
	month = nov,
	year = {2008},
	note = {00000},
	keywords = {Function approximation, Nonlinear system identification, Nonparametric identification, Parametric programming, Piecewise quadratic programming},
	pages = {2745--2753},
}

@article{ljung_perspectives_2010,
	title = {Perspectives on system identification},
	volume = {34},
	doi = {10.1016/j.arcontrol.2009.12.001},
	number = {1},
	journal = {Annual Reviews in Control},
	author = {Ljung, Lennart},
	year = {2010},
	pages = {1--12},
}

@article{aguirre_prediction_2010,
	title = {Prediction and simulation errors in parameter estimation for nonlinear systems},
	volume = {24},
	doi = {10.1016/j.ymssp.2010.05.003},
	number = {8},
	journal = {Mechanical Systems and Signal Processing},
	author = {Aguirre, Luis A and Barbosa, Bruno HG and Braga, Antônio P},
	year = {2010},
	pages = {2855--2867},
}

@article{friedman_pathwise_2007,
	title = {Pathwise coordinate optimization},
	volume = {1},
	issn = {1932-6157},
	url = {http://projecteuclid.org/euclid.aoas/1196438020},
	doi = {10.1214/07-AOAS131},
	language = {en},
	number = {2},
	urldate = {2017-09-13},
	journal = {The Annals of Applied Statistics},
	author = {Friedman, Jerome and Hastie, Trevor and Höfling, Holger and Tibshirani, Robert},
	month = dec,
	year = {2007},
	pages = {302--332},
}

@article{chen_practical_1990,
	title = {Practical identification of {NARMAX} models using radial basis functions},
	volume = {52},
	doi = {10.1080/00207179008953599},
	number = {6},
	journal = {International Journal of Control},
	author = {Chen, S. and Billings, S. A. and Cowan, C. F. N. and Grant, P. M.},
	year = {1990},
	pages = {1327--1350},
}

@article{andrieu_particle_2010,
	title = {Particle {Markov} chain {Monte} {Carlo} methods: {Particle} {Markov} {Chain} {Monte} {Carlo} {Methods}},
	volume = {72},
	issn = {13697412, 14679868},
	shorttitle = {Particle {Markov} chain {Monte} {Carlo} methods},
	url = {http://doi.wiley.com/10.1111/j.1467-9868.2009.00736.x},
	doi = {10/dzzss3},
	abstract = {Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efﬁcient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a Lévy-driven stochastic volatility model.},
	language = {en},
	number = {3},
	urldate = {2018-11-27},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
	month = jun,
	year = {2010},
	pages = {269--342},
}

@article{esfahani_polynomial_2017,
	title = {Polynomial state-space model decoupling for the identification of hysteretic systems},
	volume = {50},
	doi = {10.1016/j.ifacol.2017.08.082},
	number = {1},
	journal = {IFAC-PapersOnLine},
	author = {Esfahani, Alireza Fakhrizadeh and Dreesen, Philippe and Tiels, Koen and Noël, Jean-Philippe and Schoukens, Johan},
	year = {2017},
	pages = {458--463},
}

@article{yoon_penalized_2013,
	title = {Penalized regression models with autoregressive error terms},
	volume = {83},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949655.2012.669383},
	doi = {10.1080/00949655.2012.669383},
	abstract = {Penalized regression methods have recently gained enormous attention in statistics and the field of machine learning due to their ability of reducing the prediction error and identifying important variables at the same time. Numerous studies have been conducted for penalized regression, but most of them are limited to the case when the data are independently observed. In this paper, we study a variable selection problem in penalized regression models with autoregressive (AR) error terms. We consider three estimators, adaptive least absolute shrinkage and selection operator, bridge, and smoothly clipped absolute deviation, and propose a computational algorithm that enables us to select a relevant set of variables and also the order of AR error terms simultaneously. In addition, we provide their asymptotic properties such as consistency, selection consistency, and asymptotic normality. The performances of the three estimators are compared with one another using simulated and real examples.},
	number = {9},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Yoon, Young Joo and Park, Cheolwoo and Lee, Taewook},
	month = sep,
	year = {2013},
	note = {00022},
	keywords = {asymptotic normality, autoregressive error models, consistency, oracle property, penalized regression, variable selection},
	pages = {1756--1772},
}

@article{fu_penalized_1998,
	title = {Penalized {Regressions}: {The} {Bridge} versus the {Lasso}},
	volume = {7},
	issn = {1061-8600},
	shorttitle = {Penalized {Regressions}},
	url = {http://www.jstor.org/stable/1390712},
	doi = {10.2307/1390712},
	abstract = {Bridge regression, a special family of penalized regressions of a penalty function ∑{\textbar}β $_{\textrm{j}}${\textbar}$^{\textrm{γ}}$ with γ ≥ 1, is considered. A general approach to solve for the bridge estimator is developed. A new algorithm for the lasso (γ = 1) is obtained by studying the structure of the bridge estimators. The shrinkage parameter γ and the tuning parameter λ are selected via generalized cross-validation (GCV). Comparison between the bridge model (γ ≥ 1) and several other shrinkage models, namely the ordinary least squares regression (λ = 0), the lasso (γ = 1) and ridge regression (γ = 2), is made through a simulation study. It is shown that the bridge regression performs well compared to the lasso and ridge regression. These methods are demonstrated through an analysis of a prostate cancer data. Some computational advantages and limitations are discussed.},
	number = {3},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Fu, Wenjiang J.},
	year = {1998},
	pages = {397--416},
}

@book{bishop_pattern_2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Pattern perception, machine learning},
}

@book{mohan_power_1995,
	title = {Power electronics},
	volume = {3},
	publisher = {John wiley \& sons New York},
	author = {Mohan, Ned and Mohan, Tore M},
	year = {1995},
}

@book{rashid_power_2011,
	address = {Burlington, MA},
	edition = {3rd ed},
	title = {Power electronics handbook: devices, circuits, and applications handbook},
	isbn = {978-0-12-382036-5},
	shorttitle = {Power electronics handbook},
	abstract = {"Designed to appeal to a new generation of engineering professionals, Power Electronics Handbook, 3rd Edition features four new chapters covering renewable energy, energy transmission, energy storage, as well as an introduction to Distributed and Cogeneration (DCG) technology, including gas turbines, gensets, microturbines, wind turbines, variable speed generators, photovoltaics and fuel cells, has been gaining momentum for quite some time now.smart grid technology. With this book readers should be able to provide technical design leadership on assigned power electronics design projects and lead the design from the concept to production involving significant scope and complexity"--},
	publisher = {Elsevier},
	editor = {Rashid, Muhammad H.},
	year = {2011},
	keywords = {Encyclopedias, Power electronics},
}

@book{fletcher_practical_2013,
	title = {Practical methods of optimization},
	isbn = {1-118-72318-X},
	publisher = {John Wiley \& Sons},
	author = {Fletcher, Roger},
	year = {2013},
}

@article{goldberger_physiobank_2000,
	title = {{PhysioBank}, {PhysioToolkit}, and {PhysioNet}: {Components} of a {New} {Research} {Resource} for {Complex} {Physiologic} {Signals}},
	volume = {101},
	issn = {0009-7322, 1524-4539},
	shorttitle = {{PhysioBank}, {PhysioToolkit}, and {PhysioNet}},
	url = {https://www.ahajournals.org/doi/10.1161/01.CIR.101.23.e215},
	doi = {10.1161/01.CIR.101.23.e215},
	language = {en},
	number = {23},
	urldate = {2018-10-21},
	journal = {Circulation},
	author = {Goldberger, Ary L. and Amaral, Luis A. N. and Glass, Leon and Hausdorff, Jeffrey M. and Ivanov, Plamen Ch. and Mark, Roger G. and Mietus, Joseph E. and Moody, George B. and Peng, Chung-Kang and Stanley, H. Eugene},
	month = jun,
	year = {2000},
}

@incollection{kennedy_particle_2011,
	title = {Particle swarm optimization},
	booktitle = {Encyclopedia of machine learning},
	publisher = {Springer},
	author = {Kennedy, James},
	year = {2011},
	pages = {760--766},
}

@misc{ng_practical_,
	title = {Practical aspects of {Deep} {Learning} ({Coursera})},
	url = {https://www.coursera.org/lecture/deep-neural-network/train-dev-test-sets-cxG1s},
	abstract = {Learn online and earn valuable credentials from top universities like Yale, Michigan, Stanford, and leading companies like Google and IBM. Join Coursera for free and transform your career with degrees, certificates, Specializations, \& MOOCs in data science, computer science, business, and dozens of other topics.},
	language = {pt},
	urldate = {2019-07-18},
	journal = {Coursera},
	author = {Ng, Andrew Y.},
}

@article{merity_pointer_2016,
	title = {Pointer {Sentinel} {Mixture} {Models}},
	url = {http://arxiv.org/abs/1609.07843},
	abstract = {Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.},
	urldate = {2019-05-31},
	journal = {arXiv:1609.07843},
	author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.07843},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{peifer_parameter_2007,
	title = {Parameter estimation in ordinary differential equations for biochemical processes using the method of multiple shooting},
	volume = {1},
	doi = {10.1049/iet-syb:20060067},
	number = {2},
	journal = {IET Systems Biology},
	author = {Peifer, M and Timmer, J},
	year = {2007},
	pages = {78--88},
}

@article{timmer_parametric_2000,
	title = {Parametric, nonparametric and parametric modelling of a chaotic circuit time series},
	volume = {274},
	number = {3},
	journal = {Physics Letters A},
	author = {Timmer, J and Rust, H and Horbelt, W and Voss, HU},
	year = {2000},
	keywords = {Nonlinear Sciences - Chaotic Dynamics, 🔍No DOI found},
	pages = {123--134},
}

@article{lindsten_particle_2014,
	title = {Particle {Gibbs} with {Ancestor} {Sampling}},
	abstract = {Particle Markov chain Monte Carlo (PMCMC) is a systematic way of combining the two main tools used for Monte Carlo statistical inference: sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). We present a new PMCMC algorithm that we refer to as particle Gibbs with ancestor sampling (PGAS). PGAS provides the data analyst with an oﬀ-the-shelf class of Markov kernels that can be used to simulate, for instance, the typically high-dimensional and highly autocorrelated state trajectory in a state-space model. The ancestor sampling procedure enables fast mixing of the PGAS kernel even when using seemingly few particles in the underlying SMC sampler. This is important as it can signiﬁcantly reduce the computational burden that is typically associated with using SMC. PGAS is conceptually similar to the existing PG with backward simulation (PGBS) procedure. Instead of using separate forward and backward sweeps as in PGBS, however, we achieve the same eﬀect in a single forward sweep. This makes PGAS well suited for addressing inference problems not only in state-space models, but also in models with more complex dependencies, such as non-Markovian, Bayesian nonparametric, and general probabilistic graphical models.},
	language = {en},
	author = {Lindsten, Fredrik and Jordan, Michael I and Schon, Thomas B},
	year = {2014},
	keywords = {🔍No DOI found},
	pages = {40},
}

@book{omnivision_ov9121_2003,
	title = {{OV9121} datasheet},
	author = {{OmniVision}},
	year = {2003},
}

@article{chen_orthogonal_1989,
	title = {Orthogonal least squares methods and their application to non-linear system identification},
	volume = {50},
	doi = {10.1080/00207178908953472},
	number = {5},
	journal = {International Journal of Control},
	author = {Chen, Sheng and Billings, Stephen A and Luo, Wan},
	year = {1989},
	pages = {1873--1896},
}

@article{chen_orthogonalleastsquares_2009,
	title = {Orthogonal-least-squares regression: {A} unified approach for data modelling},
	volume = {72},
	number = {10},
	journal = {Neurocomputing},
	author = {Chen, Sheng and Hong, Xia and Luk, Bing Lam and Harris, Chris J},
	year = {2009},
	keywords = {🔍No DOI found},
	pages = {2670--2681},
}

@article{schoukens_parametric_2015,
	title = {Parametric identification of parallel {Wiener}-{Hammerstein} systems},
	volume = {51},
	issn = {00051098},
	url = {http://arxiv.org/abs/1708.06543},
	doi = {10/f6w4rj},
	abstract = {Block-oriented nonlinear models are popular in nonlinear modeling because of their advantages to be quite simple to understand and easy to use. To increase the flexibility of single branch block-oriented models, such as Hammerstein, Wiener, and Wiener-Hammerstein models, parallel block-oriented models can be considered. This paper presents a method to identify parallel Wiener-Hammerstein systems starting from input-output data only. In the first step, the best linear approximation is estimated for different input excitation levels. In the second step, the dynamics are decomposed over a number of parallel orthogonal branches. Next, the dynamics of each branch are partitioned into a linear time invariant subsystem at the input and a linear time invariant subsystem at the output. This is repeated for each branch of the model. The static nonlinear part of the model is also estimated during this step. The consistency of the proposed initialization procedure is proven. The method is validated on real-world measurements using a custom built parallel Wiener-Hammerstein test system.},
	urldate = {2018-11-20},
	journal = {Automatica},
	author = {Schoukens, Maarten and Marconato, Anna and Pintelon, Rik and Vandersteen, Gerd and Rolain, Yves},
	month = jan,
	year = {2015},
	note = {arXiv: 1708.06543},
	keywords = {Computer Science - Systems and Control},
	pages = {111--122},
}

@article{kristensen_parameter_2004,
	title = {Parameter estimation in stochastic grey-box models},
	volume = {40},
	issn = {0005-1098},
	doi = {10.1016/j.automatica.2003.10.001},
	number = {2},
	journal = {Automatica},
	author = {Kristensen, Niels Rode and Madsen, Henrik and Jørgensen, Sten Bay},
	year = {2004},
	pages = {225--237},
}

@article{wang_parsimonious_2014,
	title = {Parsimonious {Extreme} {Learning} {Machine} {Using} {Recursive} {Orthogonal} {Least} {Squares}},
	volume = {25},
	issn = {2162-237X},
	doi = {10.1109/TNNLS.2013.2296048},
	abstract = {Novel constructive and destructive parsimonious extreme learning machines (CP- and DP-ELM) are proposed in this paper. By virtue of the proposed ELMs, parsimonious structure and excellent generalization of multiinput-multioutput single hidden-layer feedforward networks (SLFNs) are obtained. The proposed ELMs are developed by innovative decomposition of the recursive orthogonal least squares procedure into sequential partial orthogonalization (SPO). The salient features of the proposed approaches are as follows: 1) Initial hidden nodes are randomly generated by the ELM methodology and recursively orthogonalized into an upper triangular matrix with dramatic reduction in matrix size; 2) the constructive SPO in the CP-ELM focuses on the partial matrix with the subcolumn of the selected regressor including nonzeros as the first column while the destructive SPO in the DP-ELM operates on the partial matrix including elements determined by the removed regressor; 3) termination criteria for CP- and DP-ELM are simplified by the additional residual error reduction method; and 4) the output weights of the SLFN need not be solved in the model selection procedure and is derived from the final upper triangular equation by backward substitution. Both single- and multi-output real-world regression data sets are used to verify the effectiveness and superiority of the CP- and DP-ELM in terms of parsimonious architecture and generalization accuracy. Innovative applications to nonlinear time-series modeling demonstrate superior identification results.},
	number = {10},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Wang, N. and Er, M. J. and Han, M.},
	month = oct,
	year = {2014},
	note = {00083},
	keywords = {CP-ELM, Context, DP-ELM, ELM methodology, Educational institutions, Mathematical model, Matrix decomposition, Recursive orthogonal least squares (ROLS), SLFN, Training, Training data, Vectors, backward substitution, constructive parsimonious extreme learning machine, data analysis, destructive SPO, destructive parsimonious extreme learning machine, extreme learning machine (ELM), feedforward neural nets, generalisation (artificial intelligence), generalization accuracy, hidden node random generation, learning (artificial intelligence), least squares approximations, matrix algebra, matrix size reduction, multiinput-multioutput single hidden-layer feedforward networks, nonlinear time-series modeling, parsimonious architecture, parsimonious model selection, parsimonious structure, partial matrix, recursive functions, recursive orthogonal least squares decomposition, recursive orthogonalization, regression analysis, regression data set, regressor, residual error reduction method, sequential partial orthogonalization, sequential partial orthogonalization (SPO), single hidden-layer feedforward network (SLFN), single hidden-layer feedforward network (SLFN)., termination criteria, time series, upper triangular equation, upper triangular matrix},
	pages = {1828--1841},
}

@article{sermanet_overfeat_2013,
	title = {{OverFeat}: {Integrated} {Recognition}, {Localization} and {Detection} using {Convolutional} {Networks}},
	shorttitle = {{OverFeat}},
	url = {http://arxiv.org/abs/1312.6229},
	abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
	journal = {arXiv:1312.6229 [cs]},
	author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
	month = dec,
	year = {2013},
	note = {00000 
arXiv: 1312.6229},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 🔍No DOI found},
}

@incollection{rosipal_overview_2006,
	address = {Berlin, Heidelberg},
	title = {Overview and {Recent} {Advances} in {Partial} {Least} {Squares}},
	volume = {3940},
	isbn = {978-3-540-34137-6 978-3-540-34138-3},
	url = {http://link.springer.com/10.1007/11752790_2},
	urldate = {2017-09-08},
	booktitle = {Subspace, {Latent} {Structure} and {Feature} {Selection}},
	publisher = {Springer Berlin Heidelberg},
	author = {Rosipal, Roman and Krämer, Nicole},
	editor = {Saunders, Craig and Grobelnik, Marko and Gunn, Steve and Shawe-Taylor, John},
	year = {2006},
	doi = {10.1007/11752790_2},
	note = {00000 },
	pages = {34--51},
}

@book{pfanzagl_parametric_1994,
	title = {Parametric statistical theory},
	publisher = {Walter de Gruyter},
	author = {Pfanzagl, Johann},
	year = {1994},
	note = {00000},
}

@article{chen_orthogonal_1989a,
	title = {Orthogonal {Least} {Squares} {Methods} and {Their} {Application} to {Non}-{Linear} {System} {Identification}},
	volume = {50},
	doi = {10/dd4g9h},
	number = {5},
	journal = {International Journal of Control},
	author = {Chen, Sheng and Billings, Stephen A and Luo, Wan},
	year = {1989},
	note = {01591},
	pages = {1873--1896},
}

@inproceedings{helfrich_orthogonal_2018,
	address = {Stockholmsmässan, Stockholm Sweden},
	series = {Proceedings of machine learning research},
	title = {Orthogonal recurrent neural networks with scaled {Cayley} transform},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/helfrich18a.html},
	abstract = {Recurrent Neural Networks (RNNs) are designed to handle sequential data but suffer from vanishing or exploding gradients. Recent work on Unitary Recurrent Neural Networks (uRNNs) have been used to address this issue and in some cases, exceed the capabilities of Long Short-Term Memory networks (LSTMs). We propose a simpler and novel update scheme to maintain orthogonal recurrent weight matrices without using complex valued matrices. This is done by parametrizing with a skew-symmetric matrix using the Cayley transform; such a parametrization is unable to represent matrices with negative one eigenvalues, but this limitation is overcome by scaling the recurrent weight matrix by a diagonal matrix consisting of ones and negative ones. The proposed training scheme involves a straightforward gradient calculation and update step. In several experiments, the proposed scaled Cayley orthogonal recurrent neural network (scoRNN) achieves superior results with fewer trainable parameters than other unitary RNNs.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Helfrich, Kyle and Willmott, Devin and Ye, Qiang},
	editor = {Dy, Jennifer and Krause, Andreas},
	year = {2018},
	note = {tex.pdf: http://proceedings.mlr.press/v80/helfrich18a/helfrich18a.pdf},
	pages = {1969--1978},
}

@article{ribeiro_parallel_2017,
	title = {"{Parallel} {Training} {Considered} {Harmful}?": {Comparing} {Series}-{Parallel} and {Parallel} {Feedforward} {Network} {Training}},
	shorttitle = {"{Parallel} {Training} {Considered} {Harmful}?},
	abstract = {Neural network models for dynamic systems can be trained either in parallel or in series-parallel configurations. Influenced by early arguments, several papers justify the choice of series-parallel rather than parallel configuration claiming it has a lower computational cost, better stability properties during training and provides more accurate results. The purpose of this work is to review some of those arguments and to present both methods in an unifying framework, showing that parallel and series-parallel training actually results from optimal predictors that use different noise models. A numerical example illustrate that each method provides better results when the noise model they implicit consider are consistent with the error in the data. Furthermore, it is argued that for feedforward networks with bounded activation functions the possible lack of stability does not jeopardize the training; and, a novel complexity analysis indicates the computational cost in the two configurations is not significantly different. This is confirmed through numerical examples.},
	journal = {arXiv:1706.07119 [cs]},
	author = {Ribeiro, Antônio H. and Aguirre, Luis A.},
	month = jun,
	year = {2017},
	note = {tex.eprintclass= cs
00000 
 
arXiv: 1706.07119},
	keywords = {Computer Science - Learning, Computer Science - Systems and Control, 🔍No DOI found},
}

@inproceedings{ribeiro_overparametrized_2021,
	title = {Overparametrized {Regression} {Under} {L2} {Adversarial} {Attacks}},
	copyright = {All rights reserved},
	booktitle = {Workshop on the {Theory} of {Overparameterized} {Machine} {Learning} ({TOPML})},
	author = {Ribeiro, Antonio H and Schön, Thomas B},
	month = apr,
	year = {2021},
}

@inproceedings{cisse_parseval_2017,
	series = {{ICML}’17},
	title = {Parseval networks: {Improving} robustness to adversarial examples},
	booktitle = {Proceedings of the 34th international conference on machine learning - volume 70},
	publisher = {JMLR},
	author = {Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard and Dauphin, Yann and Usunier, Nicolas},
	year = {2017},
	pages = {854--863},
}

@article{teschl_ordinary_,
	title = {Ordinary {Differential} {Equations} and {Dynamical} {Systems}},
	language = {en},
	author = {Teschl, Gerald},
	pages = {364},
}

@article{morcos_one_2019,
	title = {One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
	shorttitle = {One ticket to win them all},
	url = {http://arxiv.org/abs/1906.02773},
	abstract = {The success of lottery ticket initializations (Frankle and Carbin, 2019) suggests that small, sparsified networks can be trained so long as the network is initialized appropriately. Unfortunately, finding these "winning ticket" initializations is computationally expensive. One potential solution is to reuse the same winning tickets across a variety of datasets and optimizers. However, the generality of winning ticket initializations remains unclear. Here, we attempt to answer this question by generating winning tickets for one training configuration (optimizer and dataset) and evaluating their performance on another configuration. Perhaps surprisingly, we found that, within the natural images domain, winning ticket initializations generalized across a variety of datasets, including Fashion MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365, often achieving performance close to that of winning tickets generated on the same dataset. Moreover, winning tickets generated using larger datasets consistently transferred better than those generated using smaller datasets. We also found that winning ticket initializations generalize across optimizers with high performance. These results suggest that winning ticket initializations generated by sufficiently large datasets contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods.},
	urldate = {2020-07-05},
	journal = {arXiv:1906.02773 [cs, stat]},
	author = {Morcos, Ari S. and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
	month = oct,
	year = {2019},
	note = {arXiv: 1906.02773},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{ribeiro_smoothness_2020,
	title = {On the smoothness of nonlinear system identification},
	volume = {121},
	copyright = {All rights reserved},
	doi = {10.1016/j.automatica.2020.109158},
	abstract = {We shed new light on the smoothness of optimization problems arising in prediction error parameter estimation of linear and nonlinear systems. We show that for regions of the parameter space where the model is not contractive, the Lipschitz constant and beta-smoothness of the objective function might blow up exponentially with the simulation length, making it hard to numerically find minima within those regions or, even, to escape from them. In addition to providing theoretical understanding of this problem, this paper also proposes the use of multiple shooting as a viable solution. The proposed method minimizes the error between a prediction model and the observed values. Rather than running the prediction model over the entire dataset, multiple shooting splits the data into smaller subsets and runs the prediction model over each subset, making the simulation length a design parameter and making it possible to solve problems that would be infeasible using a standard approach. The equivalence to the original problem is obtained by including constraints in the optimization. The new method is illustrated by estimating the parameters of nonlinear systems with chaotic or unstable behavior, as well as neural networks. We also present a comparative analysis of the proposed method with multi-step-ahead prediction error minimization.},
	journal = {Automatica},
	author = {Ribeiro, Antônio H. and Tiels, Koen and Umenberger, Jack and Schön, Thomas B. and Aguirre, Luis A.},
	month = nov,
	year = {2020},
	note = {arXiv: 1905.00820},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control},
	pages = {109158},
}

@article{kobak_optimal_2020,
	title = {Optimal ridge penalty for real-world high-dimensional data can be zero or negative due to the implicit ridge regularization},
	url = {http://arxiv.org/abs/1805.10939},
	abstract = {A conventional wisdom in statistical learning is that large models require strong regularization to prevent overfitting. Here we show that this rule can be violated by linear regression in the underdetermined \$n{\textbackslash}ll p\$ situation under realistic conditions. Using simulations and real-life high-dimensional data sets, we demonstrate that an explicit positive ridge penalty can fail to provide any improvement over the minimum-norm least squares estimator. Moreover, the optimal value of ridge penalty in this situation can be negative. This happens when the high-variance directions in the predictor space can predict the response variable, which is often the case in the real-world high-dimensional data. In this regime, low-variance directions provide an implicit ridge regularization and can make any further positive ridge penalty detrimental. We prove that augmenting any linear model with random covariates and using minimum-norm estimator is asymptotically equivalent to adding the ridge penalty. We use a spiked covariance model as an analytically tractable example and prove that the optimal ridge penalty in this case is negative when \$n{\textbackslash}ll p\$.},
	urldate = {2020-11-12},
	journal = {arXiv:1805.10939 [math, stat]},
	author = {Kobak, Dmitry and Lomond, Jonathan and Sanchez, Benoit},
	month = apr,
	year = {2020},
	note = {arXiv: 1805.10939
version: 4},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@incollection{levy_online_2018,
	title = {Online {Adaptive} {Methods}, {Universality} and {Acceleration}},
	url = {http://papers.nips.cc/paper/7885-online-adaptive-methods-universality-and-acceleration.pdf},
	urldate = {2018-12-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Levy, Yehuda Kfir and Yurtsever, Alp and Cevher, Volkan},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {6501--6510},
}

@article{cho_properties_2014,
	title = {On the {Properties} of {Neural} {Machine} {Translation}: {Encoder}-{Decoder} {Approaches}},
	shorttitle = {On the {Properties} of {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1409.1259},
	abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
	journal = {arXiv:1409.1259 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.1259},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning, 🔍No DOI found},
}

@book{silberschatz_operating_2014,
	title = {Operating system concepts essentials},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Silberschatz, Abraham and Galvin, Peter Baer and Gagne, Greg},
	year = {2014},
	note = {00000},
}

@book{mancini_op_2003,
	title = {Op amps for everyone: design reference},
	shorttitle = {Op amps for everyone},
	publisher = {Newnes},
	author = {Mancini, Ron},
	year = {2003},
}

@book{jung_op_2006,
	address = {Burlington, MA},
	series = {Analog {Devices} series},
	title = {Op {Amp} applications handbook},
	isbn = {978-0-7506-7844-5},
	publisher = {Newnes},
	editor = {Jung, Walter G.},
	year = {2006},
	note = {OCLC: ocm55633774},
	keywords = {Handbooks, manuals, etc, Operational amplifiers},
}

@article{bottou_optimization_2018,
	title = {Optimization {Methods} for {Large}-{Scale} {Machine} {Learning}},
	volume = {60},
	issn = {0036-1445, 1095-7200},
	url = {https://epubs.siam.org/doi/10.1137/16M1080173},
	doi = {10.1137/16M1080173},
	language = {en},
	number = {2},
	urldate = {2018-05-08},
	journal = {SIAM Review},
	author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
	month = jan,
	year = {2018},
	pages = {223--311},
}

@book{kirk_optimal_2012,
	title = {Optimal control theory: an introduction},
	shorttitle = {Optimal control theory},
	publisher = {Courier Corporation},
	author = {Kirk, Donald E.},
	year = {2012},
}

@book{graeme_operational_1971,
	series = {{McGraw}-{Hill} {Electrical} and {Electronic} {Engineering} {Series}},
	title = {Operational amplifiers: design and applications},
	url = {https://books.google.com.br/books?id=rbvPswEACAAJ},
	publisher = {McGraw-Hill},
	author = {Graeme, J.G. and Tobey, G.E. and Huelsman, L.P. and {Burr-Brown Research Corporation}},
	year = {1971},
}

@article{naylorc_prospects_2018,
	title = {On the prospects for a (deep) learning health care system},
	volume = {320},
	issn = {0098-7484},
	url = {http://dx.doi.org/10.1001/jama.2018.11103},
	doi = {10.1001/jama.2018.11103},
	abstract = {In 1976, Maxmen1 predicted that artificial intelligence (AI) in the 21st century would usher in “the post-physician era,” with health care provided by paramedics and computers. Today, the mass extinction of physicians remains unlikely. However, as outlined by Hinton2 in a related Viewpoint, the emergence of a radically different approach to AI, called deep learning, has the potential to effect major changes in clinical medicine and health care delivery. This Viewpoint reviews some of the factors driving wide adoption of deep learning and other forms of machine learning in the health ecosystem.},
	number = {11},
	journal = {JAMA},
	author = {{Naylor C}},
	month = sep,
	year = {2018},
	pages = {1099--1100},
}

@article{wu_optical_2007,
	title = {Optical imaging for medical diagnosis based on active stereo vision and motion tracking},
	volume = {15},
	doi = {10.1364/OE.15.010421},
	number = {16},
	journal = {Optics express},
	author = {Wu, Tao T and Qu, Jianan Y},
	year = {2007},
	note = {00025},
	pages = {10421--10426},
}

@article{kirkpatrick_optimization_1983,
	title = {Optimization by simulated annealing},
	volume = {220},
	doi = {10.1126/science.220.4598.671},
	number = {4598},
	journal = {science},
	author = {Kirkpatrick, Scott and Gelatt, C Daniel and Vecchi, Mario P and {others}},
	year = {1983},
	pages = {671--680},
}

@inproceedings{hassibi_optimal_1993,
	title = {Optimal brain surgeon and general network pruning},
	booktitle = {Neural {Networks}, 1993., {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Hassibi, Babak and Stork, David G and Wolff, Gregory J},
	year = {1993},
	pages = {293--299},
}

@article{gould_solution_2001,
	title = {On the solution of equality constrained quadratic programming problems arising in optimization},
	volume = {23},
	issn = {1064-8275},
	doi = {10.1137/S1064827598345667},
	number = {4},
	journal = {SIAM Journal on Scientific Computing},
	author = {Gould, Nicholas IM and Hribar, Mary E and Nocedal, Jorge},
	year = {2001},
	pages = {1376--1395},
}

@article{sussillo_opening_2013,
	title = {Opening the {Black} {Box}: {Low}-{Dimensional} {Dynamics} in {High}-{Dimensional} {Recurrent} {Neural} {Networks}},
	volume = {25},
	issn = {0899-7667, 1530-888X},
	shorttitle = {Opening the {Black} {Box}},
	url = {http://www.mitpressjournals.org/doi/10.1162/NECO_a_00409},
	doi = {10/f4kmg4},
	language = {en},
	number = {3},
	urldate = {2019-03-08},
	journal = {Neural Computation},
	author = {Sussillo, David and Barak, Omri},
	month = mar,
	year = {2013},
	pages = {626--649},
}

@article{jaeger_optimization_2007,
	title = {Optimization and applications of echo state networks with leaky- integrator neurons},
	volume = {20},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S089360800700041X},
	doi = {10.1016/j.neunet.2007.04.016},
	language = {en},
	number = {3},
	urldate = {2019-10-28},
	journal = {Neural Networks},
	author = {Jaeger, Herbert and Lukoševičius, Mantas and Popovici, Dan and Siewert, Udo},
	month = apr,
	year = {2007},
	pages = {335--352},
}

@inproceedings{sharif_suitability_2018,
	title = {On the {Suitability} of {Lp}-{Norms} for {Creating} and {Preventing} {Adversarial} {Examples}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018_workshops/w32/html/Sharif_On_the_Suitability_CVPR_2018_paper.html},
	urldate = {2021-05-15},
	author = {Sharif, Mahmood and Bauer, Lujo and Reiter, Michael K.},
	year = {2018},
	pages = {1605--1613},
}

@inproceedings{liang_multiple_2020,
	series = {Proceedings of machine learning research},
	title = {On the multiple descent of minimum-norm interpolants and restricted lower isometry of kernels},
	volume = {125},
	url = {http://proceedings.mlr.press/v125/liang20a.html},
	abstract = {We study the risk of minimum-norm interpolants of data in Reproducing Kernel Hilbert Spaces. Our upper bounds on the risk are of a multiple-descent shape for the various scalings of d = n$^{\textrm{α}}$, α∈(0,1), for the input dimension d and sample size n. Empirical evidence supports our finding that minimum-norm interpolants in RKHS can exhibit this unusual non-monotonicity in sample size; furthermore, locations of the peaks in our experiments match our theoretical predictions. Since gradient flow on appropriately initialized wide neural networks converges to a minimum-norm interpolant with respect to a certain kernel, our analysis also yields novel estimation and generalization guarantees for these over-parametrized models. At the heart of our analysis is a study of spectral properties of the random kernel matrix restricted to a filtration of eigen-spaces of the population covariance operator, and may be of independent interest.},
	booktitle = {Proceedings of thirty third conference on learning theory},
	publisher = {PMLR},
	author = {Liang, Tengyuan and Rakhlin, Alexander and Zhai, Xiyu},
	editor = {Abernethy, Jacob and Agarwal, Shivani},
	month = jul,
	year = {2020},
	note = {tex.pdf: http://proceedings.mlr.press/v125/liang20a/liang20a.pdf},
	pages = {2683--2711},
}

@article{bach_equivalence_2017,
	title = {On the equivalence between kernel quadrature rules and random feature expansions},
	volume = {18},
	url = {http://jmlr.org/papers/v18/15-178.html},
	number = {21},
	journal = {Journal of Machine Learning Research},
	author = {Bach, Francis},
	year = {2017},
	pages = {1--38},
}

@inproceedings{cortes_impact_2010,
	address = {Chia Laguna Resort, Sardinia, Italy},
	series = {Proceedings of machine learning research},
	title = {On the impact of kernel approximation on learning accuracy},
	volume = {9},
	url = {http://proceedings.mlr.press/v9/cortes10a.html},
	abstract = {Kernel approximation is commonly used to scale kernel-based algorithms to applications containing as many as several million instances. This paper analyzes the effect of such approximations in the kernel matrix on the hypothesis generated by several widely used learning algorithms. We give stability bounds based on the norm of the kernel approximation for these algorithms, including SVMs, KRR, and graph Laplacian-based regularization algorithms. These bounds help determine the degree of approximation that can be tolerated in the estimation of the kernel matrix. Our analysis is general and applies to arbitrary approximations of the kernel matrix. However, we also give a specific analysis of the Nystrom low-rank approximation in this context and report the results of experiments evaluating the quality of the Nystrom low-rank kernel approximation when used with ridge regression.},
	booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	publisher = {PMLR},
	author = {Cortes, Corinna and Mohri, Mehryar and Talwalkar, Ameet},
	editor = {Teh, Yee Whye and Titterington, Mike},
	month = may,
	year = {2010},
	note = {tex.pdf: http://proceedings.mlr.press/v9/cortes10a/cortes10a.pdf},
	pages = {113--120},
}

@article{cucker_mathematical_2001,
	title = {On the mathematical foundations of learning},
	volume = {39},
	issn = {0273-0979},
	url = {http://www.ams.org/journal-getitem?pii=S0273-0979-01-00923-5},
	doi = {10.1090/S0273-0979-01-00923-5},
	language = {en},
	number = {01},
	urldate = {2019-12-31},
	journal = {Bulletin of the American Mathematical Society},
	author = {Cucker, Felipe and Smale, Steve},
	month = oct,
	year = {2001},
	pages = {1--50},
}

@article{liu_limited_1989,
	title = {On the limited memory {BFGS} method for large scale optimization},
	volume = {45},
	url = {https://people.sc.fsu.edu/~inavon/5420a/liu89limited.pdf},
	number = {1-3},
	urldate = {2020-02-07},
	journal = {Mathematical programming},
	author = {Liu, Dong C and Nocedal, Jorge},
	year = {1989},
	pages = {503--528},
}

@article{loubaton_almost_2016,
	title = {On the almost sure location of the singular values of certain {Gaussian} block-{Hankel} large random matrices},
	volume = {29},
	issn = {0894-9840, 1572-9230},
	url = {http://arxiv.org/abs/1405.2006},
	doi = {10.1007/s10959-015-0614-z},
	abstract = {This paper studies the almost sure location of the eigenvalues of matrices \$\{{\textbackslash}bf W\}\_N \{{\textbackslash}bf W\}\_N{\textasciicircum}\{*\}\$ where \$\{{\textbackslash}bf W\}\_N = (\{{\textbackslash}bf W\}\_N{\textasciicircum}\{(1)T\}, ..., \{{\textbackslash}bf W\}\_N{\textasciicircum}\{(M)T\}){\textasciicircum}\{T\}\$ is a \$ML {\textbackslash}times N\$ block-line matrix whose block-lines \$(\{{\textbackslash}bf W\}\_N{\textasciicircum}\{(m)\})\_\{m=1, ..., M\}\$ are independent identically distributed \$L {\textbackslash}times N\$ Hankel matrices built from i.i.d. standard complex Gaussian sequences. It is shown that if \$M {\textbackslash}rightarrow +{\textbackslash}infty\$ and \${\textbackslash}frac\{ML\}\{N\} {\textbackslash}rightarrow c\_*\$ (\$c\_* {\textbackslash}in (0, {\textbackslash}infty)\$), then the empirical eigenvalue distribution of \$\{{\textbackslash}bf W\}\_N \{{\textbackslash}bf W\}\_N{\textasciicircum}\{*\}\$ converges almost surely towards the Marcenko-Pastur distribution. More importantly, it is established that if \$L = {\textbackslash}mathcal\{O\}(N{\textasciicircum}\{{\textbackslash}alpha\})\$ with \${\textbackslash}alpha {\textless} 2/3\$, then, almost surely, for \$N\$ large enough, the eigenvalues of \$\{{\textbackslash}bf W\}\_N \{{\textbackslash}bf W\}\_N{\textasciicircum}\{*\}\$ are located in the neighbourhood of the Marcenko-Pastur distribution.},
	number = {4},
	urldate = {2020-11-23},
	journal = {Journal of Theoretical Probability},
	author = {Loubaton, Philippe},
	month = dec,
	year = {2016},
	note = {arXiv: 1405.2006},
	keywords = {60B20, Mathematics - Probability},
	pages = {1339--1443},
}

@article{eldar_asymptotic_2003,
	title = {On the asymptotic performance of the decorrelator},
	volume = {49},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1226621/},
	doi = {10.1109/TIT.2003.815781},
	abstract = {We derive the asymptotic signal-to-interference ratio (SIR) of the decorrelator in the large system limit, both for the case in which the number of users exceeds the spreading gain and for the case in which the number of users is less than the spreading gain. We show that, contrary to what is claimed in [1], [2], when the number of users exceeds the spreading gain and the decorrelator is defined in terms of the Moore–Penrose pseudoinverse, the SIR does not converge to zero.},
	language = {en},
	number = {9},
	urldate = {2020-12-23},
	journal = {IEEE Transactions on Information Theory},
	author = {Eldar, Y.C. and Chan, A.M.},
	month = sep,
	year = {2003},
	pages = {2309--2313},
}

@article{silverstein_empirical_1995,
	title = {On the {Empirical} {Distribution} of {Eigenvalues} of a {Class} of {Large} {Dimensional} {Random} {Matrices}},
	volume = {54},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X85710512},
	doi = {10.1006/jmva.1995.1051},
	abstract = {A stronger result on the limiting distribution of the eigenvalues of random Hermitian matrices of the form A+XT X∗, originally studied in Marˇcenko and Pastur [4], is presented. Here, X (N ×n), T (n×n), and A (N ×N ) are independent, with X containing i.i.d. entries having ﬁnite second moments, T is diagonal with real (diagonal) entries, A is Hermitian, and n/N → c {\textgreater} 0 as N → ∞. Under addtional assumptions on the eigenvalues of A and T , almost sure convergence of the empirical distribution function of the eigenvalues of A + XT X∗ is proven with the aid of Stieltjes transforms, taking a more direct approach than previous methods.},
	language = {en},
	number = {2},
	urldate = {2020-12-22},
	journal = {Journal of Multivariate Analysis},
	author = {Silverstein, J.W. and Bai, Z.D.},
	month = aug,
	year = {1995},
	pages = {175--192},
}

@inproceedings{reddi_convergence_2018,
	title = {On the convergence of adam and beyond},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
	year = {2018},
	note = {00000},
}

@article{wachter_implementation_2006,
	title = {On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming},
	volume = {106},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/10.1007/s10107-004-0559-y},
	doi = {10.1007/s10107-004-0559-y},
	language = {en},
	number = {1},
	urldate = {2017-08-20},
	journal = {Mathematical Programming},
	author = {Wächter, Andreas and Biegler, Lorenz T.},
	month = mar,
	year = {2006},
	note = {04288},
	pages = {25--57},
}

@article{byrd_local_1997,
	title = {On the local behavior of an interior point method for nonlinear programming},
	volume = {1997},
	journal = {Numerical analysis},
	author = {Byrd, Richard H and Liu, Guanghui and Nocedal, Jorge},
	year = {1997},
	keywords = {🔍No DOI found},
	pages = {37--56},
}

@article{cucker_mathematical_2002,
	title = {On the mathematical foundations of learning},
	volume = {39},
	doi = {10.1090/S0273-0979-01-00923-5},
	number = {1},
	journal = {Bulletin of the American mathematical society},
	author = {Cucker, Felipe and Smale, Steve},
	year = {2002},
	pages = {1--49},
}

@article{schittkowski_convergence_1983,
	title = {On the convergence of a sequential quadratic programming method with an augmented lagrangian line search function},
	volume = {14},
	issn = {0323-3898},
	url = {https://doi.org/10.1080/02331938308842847},
	doi = {10.1080/02331938308842847},
	number = {2},
	journal = {Mathematische Operationsforschung und Statistik. Series Optimization},
	author = {Schittkowski, Klaus},
	month = jan,
	year = {1983},
	note = {00000},
	pages = {197--216},
}

@incollection{ljung_consistency_1976,
	series = {System {Identification} {Advances} and {Case} {Studies}},
	title = {On {The} {Consistency} of {Prediction} {Error} {Identification} {Methods}},
	volume = {126},
	url = {http://www.sciencedirect.com/science/article/pii/S0076539208608711},
	abstract = {The problem of identification is to determine a model that describes input–output data obtained from a certain system. In this chapter, strong consistency for general prediction error methods, including the maximum-likelihood (ML) method is considered. The results are valid for general process models: linear and nonlinear. An error identification method is discussed in the chapter along with a general model for stochastic dynamic systems. Different identifiability concepts are also introduced, where a procedure to prove consistency is outlined. Consistency is shown for a general system structure, as well as for linear systems. The application of the results to linear time-invariant systems is also discussed in the chapter.},
	booktitle = {Mathematics in {Science} and {Engineering}},
	publisher = {Elsevier},
	author = {Ljung, Lennart},
	editor = {Mehra, Raman K. and Lainiotis, Dimitri G.},
	month = jan,
	year = {1976},
	doi = {10.1016/S0076-5392(08)60871-1},
	pages = {121--164},
}

@book{augustyniak_equivalence_,
	title = {On {The} {Equivalence} {Of} {The} 12-{Lead} {Ecg} {And} {The} {Vcg} {Representations} {Of} {The} {Cardiac} {Electrical} {Activity}},
	abstract = {The electrocardiography (ECG) and vectocardiography (VCG) both describe the same phenomena: the temporal changes of the surface potentials resulting from cardiac electrical field. The ECG uses 12 leads positioned as it was found optimal from the medical point of view during a hundred years of practice. The VCG uses 3 channels connected to the psuedoorthogonal leads that placement is determined by the orthogonality of main Cartesian axes - in consequence a three-dimensional recording is performed. This paper is devoted to the experimental verification of the likeness of the data provided by both recording techniques. Two different transforms re-mapping the ECG to the VCG domain and viceversa were studied with use of the set of 125 simultaneous ECG and VCG signals from the CSE Multilead Database. One of the possible technical interests of transforming the ECG signals to the VCG domain is reducing the data volume thanks to eliminating the information redundancy typical for ECG. Our results demonstrate that the forward and inverse transform has no perfect reconstruction property and some extent of distortion should be considered when applying this technique to the signal compression.},
	author = {Augustyniak, Piotr},
}

@article{lalee_implementation_1998,
	title = {On the implementation of an algorithm for large-scale equality constrained optimization},
	volume = {8},
	url = {http://epubs.siam.org/doi/abs/10.1137/S1052623493262993},
	doi = {10.1137/S1052623493262993},
	number = {3},
	urldate = {2017-08-20},
	journal = {SIAM Journal on Optimization},
	author = {Lalee, Marucha and Nocedal, Jorge and Plantenga, Todd},
	year = {1998},
	pages = {682--706},
}

@inproceedings{sinai_notion_1959,
	title = {On the notion of entropy of a dynamical system},
	volume = {124},
	author = {Sinai, Yaha G},
	year = {1959},
	keywords = {⛔ No DOI found},
	pages = {768},
}

@article{byrd_local_1997a,
	title = {On the {Local} {Behavior} of an {Interior} {Point} {Method} for {Nonlinear} {Programming}},
	volume = {1997},
	journal = {Numerical analysis},
	author = {Byrd, Richard H and Liu, Guanghui and Nocedal, Jorge},
	year = {1997},
	keywords = {🔍No DOI found},
	pages = {37--56},
}

@article{lalee_implementation_1998a,
	title = {On the {Implementation} of an {Algorithm} for {Large}-{Scale} {Equality} {Constrained} {Optimization}},
	volume = {8},
	doi = {10/dhjrhk},
	number = {3},
	urldate = {2017-08-20},
	journal = {SIAM Journal on Optimization},
	author = {Lalee, Marucha and Nocedal, Jorge and Plantenga, Todd},
	year = {1998},
	pages = {682--706},
}

@misc{pascanu_difficulty_2013,
	title = {On the {Difficulty} of {Training} {Recurrent} {Neural} {Networks}},
	volume = {28},
	abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
	journal = {Proceedings of the 30th International Conference on International Conference on Machine Learning},
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	year = {2013},
	keywords = {⛔ No DOI found},
	pages = {1310--1318},
}

@inproceedings{chizat_global_2018,
	title = {On the global convergence of gradient descent for over-parameterized models using optimal transport},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	author = {Chizat, Lénaïc and Bach, Francis},
	year = {2018},
}

@inproceedings{wiklicky_nonexistence_1994,
	title = {On the {Non}-{Existence} of a {Universal} {Learning} {Algorithm} for {Recurrent} {Neural} {Networks}},
	volume = {6},
	url = {https://proceedings.neurips.cc/paper/1993/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf},
	urldate = {2021-05-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {Wiklicky, Herbert},
	editor = {Cowan, J. and Tesauro, G. and Alspector, J.},
	year = {1994},
}

@article{mokus_bayesian_1974,
	title = {On {Bayesian} {Methods} for {Seeking} the {Extremum}},
	language = {en},
	journal = {Optimization Techniques},
	author = {Mokus, J},
	year = {1974},
	pages = {400--404},
}

@inproceedings{zeiler_rectified_2013,
	title = {On rectified linear units for speech processing},
	isbn = {1520-6149},
	doi = {10/gfv3hc},
	booktitle = {2013 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	author = {Zeiler, M. D. and Ranzato, M. and Monga, R. and Mao, M. and Yang, K. and {Q. V. Le} and {P. Nguyen} and {A. Senior} and {V. Vanhoucke} and {J. Dean} and {G. E. Hinton}},
	month = may,
	year = {2013},
	keywords = {Accuracy, Deep Learning, Encoding, Error analysis, Hybrid System, Logistics, Neural Networks, Neural networks, Rectified Linear units, Training, Unsupervised Learning, Unsupervised learning, acoustic modeling, deep neural networks, discriminative tasks, distributed environment, gold standard, key computational unit, large vocabulary speech recognition task, linear projection, logistic function, logistic units, neural nets, point-wise nonlinearity, rectified linear units, sparse features, speech processing, speech recognition, supervised setting, vocabulary, word error rates},
	pages = {3517--3521},
}

@article{guo_calibration_2017,
	title = {On {Calibration} of {Modern} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.04599},
	abstract = {Conﬁdence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classiﬁcation models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors inﬂuencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classiﬁcation datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a singleparameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.},
	language = {en},
	urldate = {2020-11-10},
	journal = {arXiv:1706.04599 [cs]},
	author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
	month = aug,
	year = {2017},
	note = {arXiv: 1706.04599},
	keywords = {Computer Science - Machine Learning},
}

@article{chizat_lazy_2019,
	title = {On {Lazy} {Training} in {Differentiable} {Programming}},
	url = {http://arxiv.org/abs/1812.07956},
	abstract = {In a series of recent theoretical works, it was shown that strongly over-parameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this "lazy training" phenomenon is not specific to over-parameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely that "lazy training" is behind the many successes of neural networks in difficult high dimensional tasks.},
	journal = {Advances in Neural Information Processing Systems 32},
	author = {Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
	year = {2019},
	note = {arXiv: 1812.07956},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@article{bai_asymptotics_2007,
	title = {On asymptotics of eigenvectors of large sample covariance matrix},
	volume = {35},
	issn = {0091-1798},
	url = {http://arxiv.org/abs/0708.1720},
	doi = {10.1214/009117906000001079},
	abstract = {Let {\textbackslash}\{\$X\_\{ij\}\${\textbackslash}\}, \$i,j=...,\$ be a double array of i.i.d. complex random variables with \$EX\_\{11\}=0,E{\textbar}X\_\{11\}{\textbar}{\textasciicircum}2=1\$ and \$E{\textbar}X\_\{11\}{\textbar}{\textasciicircum}4{\textless}{\textbackslash}infty\$, and let \$A\_n={\textbackslash}frac\{1\}\{N\}T\_n{\textasciicircum}\{\{1\}/\{2\}\}X\_nX\_n{\textasciicircum}*T\_n{\textasciicircum}\{\{1\}/\{2\}\}\$, where \$T\_n{\textasciicircum}\{\{1\}/\{2\}\}\$ is the square root of a nonnegative definite matrix \$T\_n\$ and \$X\_n\$ is the \$n{\textbackslash}times N\$ matrix of the upper-left corner of the double array. The matrix \$A\_n\$ can be considered as a sample covariance matrix of an i.i.d. sample from a population with mean zero and covariance matrix \$T\_n\$, or as a multivariate \$F\$ matrix if \$T\_n\$ is the inverse of another sample covariance matrix. To investigate the limiting behavior of the eigenvectors of \$A\_n\$, a new form of empirical spectral distribution is defined with weights defined by eigenvectors and it is then shown that this has the same limiting spectral distribution as the empirical spectral distribution defined by equal weights. Moreover, if {\textbackslash}\{\$X\_\{ij\}\${\textbackslash}\} and \$T\_n\$ are either real or complex and some additional moment assumptions are made then linear spectral statistics defined by the eigenvectors of \$A\_n\$ are proved to have Gaussian limits, which suggests that the eigenvector matrix of \$A\_n\$ is nearly Haar distributed when \$T\_n\$ is a multiple of the identity matrix, an easy consequence for a Wishart matrix.},
	number = {4},
	urldate = {2020-12-22},
	journal = {The Annals of Probability},
	author = {Bai, Z. D. and Miao, B. Q. and Pan, G. M.},
	month = jul,
	year = {2007},
	note = {arXiv: 0708.1720},
	keywords = {15A52, 60F15, 62E20 (Primary), 60F17, 62H99 (Secondary), Mathematics - Probability},
	pages = {1532--1572},
}

@incollection{baldi_neuronal_2018,
	title = {On {Neuronal} {Capacity}},
	url = {http://papers.nips.cc/paper/7999-on-neuronal-capacity.pdf},
	urldate = {2018-12-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Baldi, Pierre and Vershynin, Roman},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {7739--7748},
}

@article{stentoumis_accurate_2014,
	title = {On accurate dense stereo-matching using a local adaptive multi-cost approach},
	volume = {91},
	doi = {10.1016/j.isprsjprs.2014.02.006},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Stentoumis, C and Grammatikopoulos, L and Kalisperakis, I and Karras, G},
	year = {2014},
	note = {00000},
	pages = {29--49},
}

@inproceedings{mei_building_2011,
	title = {On building an accurate stereo matching system on graphics hardware},
	booktitle = {Computer {Vision} {Workshops} ({ICCV} {Workshops}), 2011 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Mei, Xing and Sun, Xun and Zhou, Mingcai and Jiao, Shaohui and Wang, Haitao and Zhang, Xiaopeng},
	year = {2011},
	pages = {467--474},
}

@article{umenberger_identification_2015,
	series = {17th {IFAC} {Symposium} on {System} {Identification} {SYSID} 2015},
	title = {On {Identification} via {EM} with {Latent} {Disturbances} and {Lagrangian} {Relaxation}},
	volume = {48},
	issn = {2405-8963},
	shorttitle = {On {Identification} via {EM} with {Latent} {Disturbances} and {Lagrangian} {Relaxation}},
	url = {http://www.sciencedirect.com/science/article/pii/S2405896315027263},
	doi = {10.1016/j.ifacol.2015.12.102},
	abstract = {In the application of the Expectation Maximization (EM) algorithm to identification of dynamical systems, latent variables are typically taken as system states, for simplicity. In this work, we propose a different choice of latent variables, namely, system disturbances. Such a formulation is shown, under certain circumstances, to improve the fidelity of bounds on the likelihood, and circumvent difficulties related to intractable model transition densities. To access these benefits, we propose a Lagrangian relaxation of the challenging optimization problem that arises when formulating over latent disturbances, and fully develop the method for linear models.},
	number = {28},
	journal = {IFAC-PapersOnLine},
	author = {Umenberger, Jack and Wågberg, Johan and Manchester, Ian R. and Schön, Thomas B.},
	month = jan,
	year = {2015},
	note = {00000},
	keywords = {convex relaxation, expectation maximization, system identification},
	pages = {69--74},
}

@inproceedings{ng_discriminative_2002,
	title = {On discriminative vs. generative classifiers: {A} comparison of logistic regression and naive bayes},
	shorttitle = {On discriminative vs. generative classifiers},
	booktitle = {Advances in neural information processing systems},
	author = {Ng, Andrew Y. and Jordan, Michael I.},
	year = {2002},
	pages = {841--848},
}

@article{carin_deep_2018,
	title = {On {Deep} {Learning} for {Medical} {Image} {Analysis}},
	volume = {320},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/2702856},
	doi = {10.1001/jama.2018.13316},
	abstract = {This JAMA Guide to Statistics and Methods explains the basic concepts underlying convolutional neural networks (CNNs), a type of machine learning being used to automate the reading of medical images.},
	language = {en},
	number = {11},
	urldate = {2018-10-20},
	journal = {JAMA},
	author = {Carin, Lawrence and Pencina, Michael J.},
	month = sep,
	year = {2018},
	pages = {1192--1193},
}

@article{powell_search_1973,
	title = {On search directions for minimization algorithms},
	volume = {4},
	doi = {10.1007/BF01584660},
	number = {1},
	journal = {Mathematical Programming},
	author = {Powell, Michael JD},
	year = {1973},
	note = {00000},
	pages = {193--201},
}

@article{keskar_largebatch_2016,
	title = {On {Large}-{Batch} {Training} for {Deep} {Learning}: {Generalization} {Gap} and {Sharp} {Minima}},
	shorttitle = {On {Large}-{Batch} {Training} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/1609.04836},
	abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
	journal = {arXiv:1609.04836 [cs, math]},
	author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.04836},
	keywords = {Computer Science - Learning, Mathematics - Optimization and Control, 🔍No DOI found},
}

@article{decuyper_selecting_,
	title = {On selecting appropriate training data to model an au- tonomous oscillator},
	abstract = {In this work we study the effect of using different types of excitation signals as training data when constructing black box nonlinear models. We focus in particular on the class of nonlinear systems which exhibit autonomous oscillations. Three type of excitation signals are considered: random-phase multisines, ﬁltered white noise and swept sines. It is shown that depending on the excitation signal, the resulting model can fail to reproduce the autonomous output. Results are presented for the forced Van der Pol oscillator and the oscillatory wake of an submerged circular cylinder in a ﬂow. It is moreover shown that broadband excitations such as multisines or noise, are appropriate signals when intending to capture the synchronisation principle observed for autonomous oscillators. The latter is shown on computational ﬂuid dynamic data of a submerged cylinder in a ﬂow.},
	language = {en},
	author = {Decuyper, J and Troyer, T De and Runacres, M C and Tiels, K and Schoukens, J},
	keywords = {⛔ No DOI found},
	pages = {15},
}

@article{vorontsov_orthogonality_2017,
	title = {On orthogonality and learning recurrent networks with long term dependencies},
	url = {http://arxiv.org/abs/1702.00071},
	abstract = {It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and may therefore be a desirable property. This paper explores issues with optimization convergence, speed and gradient stability when encouraging or enforcing orthogonality. To perform this analysis, we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation. We find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance.},
	urldate = {2019-07-27},
	journal = {arXiv:1702.00071 [cs]},
	author = {Vorontsov, Eugene and Trabelsi, Chiheb and Kadoury, Samuel and Pal, Chris},
	month = jan,
	year = {2017},
	note = {arXiv: 1702.00071},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{osborne_shooting_1969,
	title = {On {Shooting} {Methods} for {Boundary} {Value} {Problems}},
	volume = {27},
	doi = {10/cgt6pc},
	number = {2},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Osborne, Mike R},
	year = {1969},
	pages = {417--433},
}

@article{sacker_invariant_1967,
	title = {{ON} {INVARIANT} {SURFACES} {AND} {BIFURCATION} {OF} {PERIODIC} {SOLUTIONS} {OF} {ORDINARY} {DIFFERENTIAL} {EQUATIONS}.},
	author = {Sacker, Robert John},
	year = {1967},
}

@inproceedings{neimark_cases_1959,
	title = {On some cases of periodic motions depending on parameters},
	volume = {129},
	author = {Neimark, Ju},
	year = {1959},
	pages = {736--739},
}

@article{osborne_shooting_1969a,
	title = {On shooting methods for boundary value problems},
	volume = {27},
	doi = {10.1016/0022-247X(69)90059-6},
	number = {2},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Osborne, Mike R},
	year = {1969},
	pages = {417--433},
}

@article{pastur_random_2020,
	title = {On {Random} {Matrices} {Arising} in {Deep} {Neural} {Networks}. {Gaussian} {Case}},
	url = {http://arxiv.org/abs/2001.06188},
	abstract = {The paper deals with distribution of singular values of product of random matrices arising in the analysis of deep neural networks. The matrices resemble the product analogs of the sample covariance matrices, however, an important difference is that the population covariance matrices, which are assumed to be non-random in the standard setting of statistics and random matrix theory, are now random, moreover, are certain functions of random data matrices. The problem has been considered in recent work [21] by using the techniques of free probability theory. Since, however, free probability theory deals with population matrices which are independent of the data matrices, its applicability in this case requires an additional justification. We present this justification by using a version of the standard techniques of random matrix theory under the assumption that the entries of data matrices are independent Gaussian random variables. In the subsequent paper [18] we extend our results to the case where the entries of data matrices are just independent identically distributed random variables with several finite moments. This, in particular, extends the property of the so-called macroscopic universality on the considered random matrices.},
	urldate = {2021-03-12},
	journal = {arXiv:2001.06188},
	author = {Pastur, Leonid},
	month = apr,
	year = {2020},
	note = {arXiv: 2001.06188},
	keywords = {Mathematical Physics, Primary 15B52, Secondary 92B20, Statistics - Machine Learning},
}

@article{bousseljot_nutzung_1995,
	title = {Nutzung der {EKG}-{Signaldatenbank} {CARDIODAT} der {PTB} über das internet},
	volume = {40},
	number = {s1},
	journal = {Biomedizinische Technik/Biomedical Engineering},
	author = {Bousseljot, R and Kreiseler, D and Schnabel, A},
	year = {1995},
	note = {Publisher: Walter de Gruyter, Berlin/New York},
	pages = {317--318},
}

@article{mcnemar_note_1947,
	title = {Note on the sampling error of the difference between correlated proportions or percentages},
	volume = {12},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02295996},
	doi = {10/d9pvhs},
	abstract = {Two formulas are presented for judging the significance of the difference between correlated proportions. The chi square equivalent of one of the developed formulas is pointed out.},
	language = {en},
	number = {2},
	urldate = {2019-01-28},
	journal = {Psychometrika},
	author = {McNemar, Quinn},
	month = jun,
	year = {1947},
	keywords = {Correlate Proportion, Develop Formula, Public Policy, Sampling Error, Statistical Theory},
	pages = {153--157},
}

@article{polak_note_1969,
	title = {Note sur la convergence de méthodes de directions conjuguées},
	volume = {3},
	issn = {0373-8000},
	doi = {10.1051/m2an/196903R100351},
	number = {16},
	journal = {Revue française d'informatique et de recherche opérationnelle. Série rouge},
	author = {Polak, Elijah and Ribiere, Gerard},
	year = {1969},
	note = {00000},
	pages = {35--43},
}

@book{vidyasagar_nonlinear_1993,
	address = {Englewood Cliffs, N.J},
	edition = {2nd ed},
	title = {Nonlinear systems analysis},
	isbn = {978-0-13-623463-0},
	language = {en},
	publisher = {Prentice Hall},
	author = {Vidyasagar, M.},
	year = {1993},
	note = {01559},
	keywords = {Differential equations, Nonlinear, System analysis},
}

@incollection{aastrom_numerical_1966,
	title = {Numerical identification of linear dynamic systems from normal operating records},
	booktitle = {{PH} {Hammond}: {Theory} of {Self}-{Adaptive} {Control} {Systems}},
	publisher = {Plenum Press},
	author = {{\textbackslash}AAström, Karl Johan and Bohlin, Torsten},
	year = {1966},
	pages = {96--111},
}

@inproceedings{falck_nuclear_2010,
	title = {Nuclear norm regularization for overparametrized {Hammerstein} systems},
	doi = {10.1109/CDC.2010.5717892},
	abstract = {In this paper we study the overparametrization scheme for Hammerstein systems in the presence of regularization. The quality of the convex approximation is analysed, that is obtained by relaxing the implicit rank one constraint. To obtain an improved convex relaxation we propose the use of nuclear norms, instead of using ridge regression. On several simple examples we illustrate that this yields a solution close to the best possible convex approximation. Furthermore the experiments suggest that ridge regression in combination with a projection step yield a generalization performance close to the one obtained by nuclear norms.},
	booktitle = {49th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Falck, T. and Suykens, J. A. K. and Schoukens, J. and Moor, B. De},
	month = dec,
	year = {2010},
	keywords = {Bandwidth, Convex functions, Correlation, Estimation, Fasteners, Hammerstein systems, Least squares approximation, approximation theory, convex approximation, convex programming, identification, nuclear norm regularization, overparametrization scheme, regression analysis, ridge regression, system identification},
	pages = {7202--7207},
}

@incollection{bock_numerical_1981,
	title = {Numerical treatment of inverse problems in chemical reaction kinetics},
	booktitle = {Modelling of chemical reaction systems},
	publisher = {Springer},
	author = {Bock, Hans Georg},
	year = {1981},
	pages = {102--125},
}

@inproceedings{chiuso_nonparametric_2010,
	title = {Nonparametric sparse estimators for identification of large scale linear systems},
	doi = {10.1109/CDC.2010.5717169},
	abstract = {Identification of sparse high dimensional linear systems pose sever challenges to off-the-shelf techniques for system identification. This is particularly so when relatively small data sets, as compared to the number of inputs and outputs, have to be used. In this paper we introduce a new nonparametric technique which borrows ideas from a recently introduced Kernel estimator called “stable-spline” as well as from sparsity inducing priors which use ℓ1 penalty. We compare the new method with a group LAR-type of algorithm applied to estimation of sparse Vector Autoregressive models and to standard PEM methods.},
	booktitle = {49th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Chiuso, A. and Pillonetto, G.},
	month = dec,
	year = {2010},
	keywords = {Data models, Estimation, Hidden Markov models, Kernel, Kernel estimator, Predictive models, Vectors, autoregressive processes, group LAR-type, identification, large scale linear systems, linear systems, nonparametric sparse estimator, nonparametric statistics, nonparametric technique, off-the-shelf techniques, sparse high dimensional linear systems, sparse vector autoregressive model, stable spline, standard PEM method, system identification},
	pages = {2942--2947},
}

@book{dennisjr_numerical_1996,
	title = {Numerical methods for unconstrained optimization and nonlinear equations},
	volume = {16},
	isbn = {1-61197-120-9},
	publisher = {Siam},
	author = {Dennis Jr, John E and Schnabel, Robert B},
	year = {1996},
}

@book{press_numerical_1992,
	address = {Cambridge ; New York},
	edition = {2nd ed},
	title = {Numerical recipes in {C}: the art of scientific computing},
	isbn = {978-0-521-43108-8 978-0-521-43720-2},
	shorttitle = {Numerical recipes in {C}},
	publisher = {Cambridge University Press},
	editor = {Press, William H.},
	year = {1992},
	keywords = {C (Computer program language)},
}

@book{trefethen_numerical_1997,
	address = {Philadelphia},
	title = {Numerical linear algebra},
	isbn = {978-0-89871-361-9},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Trefethen, Lloyd N. and Bau, David},
	year = {1997},
	keywords = {Algebras, Linear, Numerical calculations},
}

@book{burden_numerical_2011,
	address = {Boston, MA},
	edition = {9th ed},
	title = {Numerical analysis},
	isbn = {978-0-538-73351-9},
	publisher = {Brooks/Cole, Cengage Learning},
	author = {Burden, Richard L. and Faires, J. Douglas},
	year = {2011},
	note = {OCLC: ocn496962633},
	keywords = {numerical analysis},
}

@book{lambert_numerical_1991,
	title = {Numerical methods for ordinary differential systems: the initial value problem},
	isbn = {0-471-92990-5},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Lambert, John Denholm},
	year = {1991},
}

@book{billings_nonlinear_2013,
	address = {Chichester, West Sussex, United Kingdom},
	title = {Nonlinear system identification: {NARMAX} methods in the time, frequency, and spatio-temporal domains},
	isbn = {978-1-119-94359-4},
	shorttitle = {Nonlinear system identification},
	publisher = {Wiley},
	author = {Billings, S. A.},
	year = {2013},
	keywords = {Mathematical models, Nonlinear theories, Systems engineering, nonlinear systems},
}

@article{ghosh_objective_2011,
	title = {Objective {Priors}: {An} {Introduction} for {Frequentists}},
	volume = {26},
	issn = {0883-4237},
	shorttitle = {Objective {Priors}},
	url = {http://arxiv.org/abs/1108.2120},
	doi = {10.1214/10-STS338},
	abstract = {Bayesian methods are increasingly applied in these days in the theory and practice of statistics. Any Bayesian inference depends on a likelihood and a prior. Ideally one would like to elicit a prior from related sources of information or past data. However, in its absence, Bayesian methods need to rely on some "objective" or "default" priors, and the resulting posterior inference can still be quite valuable. Not surprisingly, over the years, the catalog of objective priors also has become prohibitively large, and one has to set some specific criteria for the selection of such priors. Our aim is to review some of these criteria, compare their performance, and illustrate them with some simple examples. While for very large sample sizes, it does not possibly matter what objective prior one uses, the selection of such a prior does influence inference for small or moderate samples. For regular models where asymptotic normality holds, Jeffreys' general rule prior, the positive square root of the determinant of the Fisher information matrix, enjoys many optimality properties in the absence of nuisance parameters. In the presence of nuisance parameters, however, there are many other priors which emerge as optimal depending on the criterion selected. One new feature in this article is that a prior different from Jeffreys' is shown to be optimal under the chi-square divergence criterion even in the absence of nuisance parameters. The latter is also invariant under one-to-one reparameterization.},
	number = {2},
	urldate = {2018-10-07},
	journal = {Statistical Science},
	author = {Ghosh, Malay},
	month = may,
	year = {2011},
	note = {arXiv: 1108.2120},
	keywords = {Statistics - Methodology},
	pages = {187--202},
}

@incollection{aastrom_numerical_1966a,
	title = {Numerical {Identification} of {Linear} {Dynamic} {Systems} from {Normal} {Operating} {Records}},
	booktitle = {{PH} {Hammond}: {Theory} of {Self}-{Adaptive} {Control} {Systems}},
	publisher = {Plenum Press},
	author = {{\textbackslash}AAström, Karl Johan and Bohlin, Torsten},
	year = {1966},
	pages = {96--111},
}

@incollection{bock_numerical_1981a,
	title = {Numerical {Treatment} of {Inverse} {Problems} in {Chemical} {Reaction} {Kinetics}},
	booktitle = {Modelling of {Chemical} {Reaction} {Systems}},
	publisher = {Springer},
	author = {Bock, Hans Georg},
	year = {1981},
	pages = {102--125},
}

@book{khalil_nonlinear_2002,
	edition = {Third},
	title = {Nonlinear systems},
	publisher = {Upper Saddle River},
	author = {Khalil, Hassan K},
	year = {2002},
}

@article{yang_nystrom_2012,
	title = {Nyström {Method} vs {Random} {Fourier} {Features}: {A} {Theoretical} and {Empirical} {Comparison}},
	volume = {25},
	shorttitle = {Nyström {Method} vs {Random} {Fourier} {Features}},
	url = {https://proceedings.neurips.cc/paper/2012/hash/621bf66ddb7c962aa0d22ac97d69b793-Abstract.html},
	language = {en},
	urldate = {2021-01-29},
	journal = {Advances in Neural Information Processing Systems},
	author = {Yang, Tianbao and Li, Yu-feng and Mahdavi, Mehrdad and Jin, Rong and Zhou, Zhi-Hua},
	year = {2012},
	pages = {476--484},
}

@article{tijani_nonlinear_2014,
	title = {Nonlinear identification of a small scale unmanned helicopter using optimized {NARX} network with multiobjective differential evolution},
	volume = {33},
	doi = {10.1016/j.engappai.2014.04.003},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Tijani, Ismaila B and Akmeliawati, Rini and Legowo, Ari and Budiyono, Agus},
	year = {2014},
	note = {00000},
	pages = {99--115},
}

@article{sjoberg_nonlinear_1995,
	title = {Nonlinear black-box modeling in system identification: a unified overview},
	volume = {31},
	doi = {10.1016/0005-1098(95)00120-8},
	number = {12},
	journal = {Automatica},
	author = {Sjöberg, Jonas and Zhang, Qinghua and Ljung, Lennart and Benveniste, Albert and Delyon, Bernard and Glorennec, Pierre-Yves and Hjalmarsson, H{\textbackslash}a akan and Juditsky, Anatoli},
	year = {1995},
	note = {00000},
	keywords = {🔍No DOI found},
	pages = {1691--1724},
}

@book{nelles_nonlinear_2013,
	title = {Nonlinear system identification: from classical approaches to neural networks and fuzzy models},
	isbn = {3-662-04323-8},
	publisher = {Springer Science \& Business Media},
	author = {Nelles, Oliver},
	year = {2013},
}

@article{noel_nonlinear_2017a,
	title = {Nonlinear system identification in structural dynamics: 10 more years of progress},
	volume = {83},
	issn = {0888-3270},
	shorttitle = {Nonlinear system identification in structural dynamics},
	url = {http://www.sciencedirect.com/science/article/pii/S088832701630245X},
	doi = {10.1016/j.ymssp.2016.07.020},
	abstract = {Nonlinear system identification is a vast research field, today attracting a great deal of attention in the structural dynamics community. Ten years ago, an MSSP paper reviewing the progress achieved until then [1] concluded that the identification of simple continuous structures with localised nonlinearities was within reach. The past decade witnessed a shift in emphasis, accommodating the growing industrial need for a first generation of tools capable of addressing complex nonlinearities in larger-scale structures. The objective of the present paper is to survey the key developments which arose in the field since 2006, and to illustrate state-of-the-art techniques using a real-world satellite structure. Finally, a broader perspective to nonlinear system identification is provided by discussing the central role played by experimental models in the design cycle of engineering structures.},
	journal = {Mechanical Systems and Signal Processing},
	author = {Noël, J. P. and Kerschen, G.},
	month = jan,
	year = {2017},
	keywords = {Nonlinear system identification, Review, Structural dynamics},
	pages = {2--35},
}

@article{novak_nonlinear_2010,
	title = {Nonlinear {System} {Identification} {Using} {Exponential} {Swept}-{Sine} {Signal}},
	volume = {59},
	issn = {0018-9456},
	doi = {10.1109/TIM.2009.2031836},
	abstract = {In this paper, we propose a method for nonlinear system (NLS) identification using a swept-sine input signal and based on nonlinear convolution. The method uses a nonlinear model, namely, the nonparametric generalized polynomial Hammerstein model made of power series associated with linear filters. Simulation results show that the method identifies the nonlinear model of the system under test and estimates the linear filters of the unknown NLS. The method has also been tested on a real-world system: an audio limiter. Once the nonlinear model of the limiter is identified, a test signal can be regenerated to compare the outputs of both the real-world system and its nonlinear model. The results show good agreement between both model-based and real-world system outputs.},
	number = {8},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Novak, A. and Simon, L. and Kadlec, F. and Lotton, P.},
	month = aug,
	year = {2010},
	keywords = {Analysis, Nonlinear system identification, exponential swept sine signal, filtering theory, generalized polynomial Hammerstein model, identification, linear filter, nonlinear convolution, nonlinear filters, nonlinear system (NLS), nonparametric generalized polynomial Hammerstein model, polynomials, power series, series (mathematics), swept-sine},
	pages = {2220--2229},
}

@article{daehlen_nonlinear_2014,
	title = {Nonlinear model predictive control using trust-region derivative-free optimization},
	volume = {24},
	number = {7},
	journal = {Journal of Process Control},
	author = {D{\textbackslash}a ehlen, Jon S and Eikrem, Gisle Otto and Johansen, Tor Arne},
	year = {2014},
	keywords = {🔍No DOI found},
	pages = {1106--1120},
}

@article{billings_nonlinear_1994,
	title = {Nonlinear model validation using correlation tests},
	volume = {60},
	doi = {10.1080/00207179408921513},
	number = {6},
	journal = {International Journal of Control},
	author = {Billings, S. A. and Zhu, Q. M.},
	year = {1994},
	pages = {1107--1120},
}

@book{strogatz_nonlinear_2018,
	title = {Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering},
	isbn = {0-429-96111-1},
	publisher = {CRC Press},
	author = {Strogatz, Steven H},
	year = {2018},
	note = {00000},
}

@book{bertsekas_nonlinear_1999,
	title = {Nonlinear programming},
	isbn = {1-886529-00-0},
	publisher = {Athena scientific Belmont},
	author = {Bertsekas, Dimitri P},
	year = {1999},
}

@book{grune_nonlinear_2017,
	address = {Cham},
	series = {Communications and {Control} {Engineering}},
	title = {Nonlinear {Model} {Predictive} {Control}},
	isbn = {978-3-319-46023-9 978-3-319-46024-6},
	url = {http://link.springer.com/10.1007/978-3-319-46024-6},
	urldate = {2018-04-27},
	publisher = {Springer International Publishing},
	author = {Grüne, Lars and Pannek, Jürgen},
	year = {2017},
	doi = {10.1007/978-3-319-46024-6},
}

@article{voss_nonlinear_2004,
	title = {Nonlinear dynamical system identification from uncertain and indirect measurements},
	volume = {14},
	doi = {10.1142/S0218127404010345},
	number = {06},
	journal = {International Journal of Bifurcation and Chaos},
	author = {Voss, Henning U and Timmer, Jens and Kurths, Jürgen},
	year = {2004},
	note = {00000},
	pages = {1905--1933},
}

@article{patan_nonlinear_2012,
	title = {Nonlinear model predictive control of a boiler unit: {A} fault tolerant control study},
	volume = {22},
	doi = {10.2478/v10006-012-0017-6},
	number = {1},
	journal = {International Journal of Applied Mathematics and Computer Science},
	author = {Patan, Krzysztof and Korbicz, Józef},
	year = {2012},
	note = {00000},
	pages = {225--237},
}

@article{schoukens_nonlinear_2019a,
	title = {Nonlinear {System} {Identification}: {A} {User}-{Oriented} {Roadmap}},
	shorttitle = {Nonlinear {System} {Identification}},
	url = {http://arxiv.org/abs/1902.00683},
	abstract = {The goal of this article is twofold. Firstly, nonlinear system identification is introduced to a wide audience, guiding practicing engineers and newcomers in the field to a sound solution of their data driven modeling problems for nonlinear dynamic systems. In addition, the article also provides a broad perspective on the topic to researchers that are already familiar with the linear system identification theory, showing the similarities and differences between the linear and nonlinear problem. The reader will be referred to the existing literature for detailed mathematical explanations and formal proofs. Here the focus is on the basic philosophy, giving an intuitive understanding of the problems and the solutions, by making a guided tour along the wide range of user choices in nonlinear system identification. Guidelines will be given in addition to many examples, to reach that goal.},
	urldate = {2019-04-08},
	journal = {arXiv:1902.00683 [cs]},
	author = {Schoukens, Johan and Ljung, Lennart},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.00683},
	keywords = {Computer Science - Systems and Control},
}

@book{nelles_nonlinear_2013a,
	title = {Nonlinear {System} {Identification}: {From} {Classical} {Approaches} to {Neural} {Networks} and {Fuzzy} {Models}},
	isbn = {3-662-04323-8},
	publisher = {Springer Science \& Business Media},
	author = {Nelles, Oliver},
	year = {2013},
}

@article{patan_nonlinear_2012a,
	title = {Nonlinear {Model} {Predictive} {Control} of a {Boiler} {Unit}: {A} {Fault} {Tolerant} {Control} {Study}},
	volume = {22},
	doi = {10/gfjwmj},
	number = {1},
	journal = {International Journal of Applied Mathematics and Computer Science},
	author = {Patan, Krzysztof and Korbicz, Józef},
	year = {2012},
	pages = {225--237},
}

@article{tijani_nonlinear_2014a,
	title = {Nonlinear {Identification} of a {Small} {Scale} {Unmanned} {Helicopter} {Using} {Optimized} {NARX} {Network} with {Multiobjective} {Differential} {Evolution}},
	volume = {33},
	doi = {10/f58zk9},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Tijani, Ismaila B and Akmeliawati, Rini and Legowo, Ari and Budiyono, Agus},
	year = {2014},
	pages = {99--115},
}

@techreport{vandomselaar_nonlinear_1975,
	title = {Nonlinear {Parameter} {Estimation} in {Initial} {Value} {Problems}},
	institution = {SIS-76-1121},
	author = {Van Domselaar, B and Hemker, Piet W},
	year = {1975},
	note = {00002},
}

@techreport{vandomselaar_nonlinear_1975a,
	title = {Nonlinear {Parameter} {Estimation} in {Initial} {Value} {Problems}},
	institution = {SIS-76-1121},
	author = {Van Domselaar, B and Hemker, Piet W},
	year = {1975},
}

@article{schoukens_nonlinear_2019,
	title = {Nonlinear {System} {Identification}: {A} {User}-{Oriented} {Road} {Map}},
	volume = {39},
	issn = {1941-000X},
	shorttitle = {Nonlinear {System} {Identification}},
	doi = {10.1109/MCS.2019.2938121},
	abstract = {Nonlinear system identification is an extremely broad topic, since every system that is not linear is nonlinear. That makes it impossible to give a full overview of all aspects of the fi eld. For this reason, the selection of topics and the organization of the discussion are strongly colored by the personal journey of the authors in this nonlinear universe.},
	number = {6},
	journal = {IEEE Control Systems Magazine},
	author = {Schoukens, Johan and Ljung, Lennart},
	month = dec,
	year = {2019},
	keywords = {Computational modeling, Data models, Nonlinear distortion, Nonlinear systems, Oscillators, Predictive models},
	pages = {28--99},
}

@inproceedings{pennington_nonlinear_2017,
	title = {Nonlinear random matrix theory for deep learning},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Pennington, Jeffrey and Worah, Pratik},
	year = {2017},
	pages = {2637--2646},
}

@article{jacot_neural_2018,
	title = {Neural {Tangent} {Kernel}: {Convergence} and {Generalization} in {Neural} {Networks}},
	shorttitle = {Neural {Tangent} {Kernel}},
	url = {http://arxiv.org/abs/1806.07572},
	abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function \$f\_{\textbackslash}theta\$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function \$f\_{\textbackslash}theta\$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
	urldate = {2020-07-27},
	journal = {Advances in Neural Information Processing Systems 31},
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clément},
	year = {2018},
	note = {arXiv: 1806.07572},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Probability, Statistics - Machine Learning},
}

@incollection{chen_neural_2018,
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf},
	urldate = {2018-12-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {6571--6582},
}

@article{garnelo_neural_2018,
	title = {Neural {Processes}},
	url = {http://arxiv.org/abs/1807.01622},
	abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
	urldate = {2019-01-07},
	journal = {arXiv:1807.01622 [cs, stat]},
	author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.01622},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kalchbrenner_neural_2016,
	title = {Neural {Machine} {Translation} in {Linear} {Time}},
	url = {http://arxiv.org/abs/1610.10099},
	abstract = {We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efﬁcient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive ﬁeld. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We ﬁnd that the latent alignment structure contained in the representations reﬂects the expected alignment between the tokens.},
	language = {en},
	urldate = {2019-03-13},
	journal = {arXiv:1610.10099 [cs]},
	author = {Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron van den and Graves, Alex and Kavukcuoglu, Koray},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.10099},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, ⛔ No DOI found},
}

@article{tan_neuralnetworksbased_2000,
	title = {Neural-networks-based nonlinear dynamic modeling for automotive engines},
	volume = {30},
	number = {1},
	journal = {Neurocomputing},
	author = {Tan, Yonghong and Saif, Mehrdad},
	year = {2000},
	note = {00000},
	keywords = {🔍No DOI found},
	pages = {129--142},
}

@inproceedings{su_neural_1993,
	title = {Neural model predictive control of nonlinear chemical processes},
	booktitle = {Intelligent {Control}, 1993., {Proceedings} of the 1993 {IEEE} {International} {Symposium} on},
	publisher = {IEEE},
	author = {Su, H-T and McAvoy, Thomas J},
	year = {1993},
	note = {00000},
	pages = {358--363},
}

@article{li_neurofuzzy_2017,
	title = {Neuro-fuzzy based identification method for {Hammerstein} output error model with colored noise},
	volume = {244},
	issn = {09252312},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231217305210},
	doi = {10.1016/j.neucom.2017.03.026},
	language = {en},
	urldate = {2017-08-23},
	journal = {Neurocomputing},
	author = {Li, Feng and Jia, Li and Peng, Daogang and Han, Chao},
	month = jun,
	year = {2017},
	pages = {90--101},
}

@book{norgaard_neural_2000,
	title = {Neural {Networks} for {Modelling} and {Control} of {Dynamic} {Systems}-{A} {Practitioner}'s {Handbook}},
	publisher = {Springer-London},
	author = {Nørgaard, Peter Magnus and Ravn, Ole and Poulsen, Niels Kjølstad and Hansen, Lars Kai},
	year = {2000},
}

@techreport{beale_neural_2017,
	title = {Neural network toolbox for use with {MATLAB}},
	institution = {Mathworks},
	author = {Beale, Mark Hudson and Hagan, Martin T. and Demuth, Howard B.},
	year = {2017},
}

@article{chen_nonlinear_1990,
	title = {Non-linear system identification using neural networks},
	volume = {51},
	number = {6},
	journal = {International Journal of Control},
	author = {Chen, Sheng and Billings, S. A. and Grant, P. M.},
	year = {1990},
	keywords = {❓Multiple DOI},
	pages = {1191--1214},
}

@article{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, 🔍No DOI found},
}

@article{nash_newtontype_1984,
	title = {Newton-type minimization via the {Lanczos} method},
	volume = {21},
	doi = {10.1137/0721052},
	number = {4},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Nash, Stephen G.},
	year = {1984},
	pages = {770--788},
}

@article{rahman_neural_2000,
	title = {Neural network approach for linearizing control of nonlinear process plants},
	volume = {47},
	number = {2},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {Rahman, MHR Fazlur and Devanathan, Rajagopalan and Kuanyi, Zhu},
	year = {2000},
	note = {00000},
	keywords = {🔍No DOI found},
	pages = {470--477},
}

@techreport{instruments_ni_2015,
	title = {{NI} {USB}-6008/6009 {User} {Guide}},
	institution = {National Instruments},
	author = {Instruments, National},
	year = {2015},
}

@article{shi_neural_2018,
	title = {Neural {Lander}: {Stable} {Drone} {Landing} {Control} using {Learned} {Dynamics}},
	shorttitle = {Neural {Lander}},
	url = {http://arxiv.org/abs/1811.08027},
	abstract = {Precise near-ground trajectory control is difficult for multi-rotor drones, due to the complex aerodynamic effects caused by interactions between multi-rotor airflow and the environment. Conventional control methods often fail to properly account for these complex effects and fall short in accomplishing smooth landing. In this paper, we present a novel deep-learning-based robust nonlinear controller (Neural Lander) that improves control performance of a quadrotor during landing. Our approach combines a nominal dynamics model with a Deep Neural Network (DNN) that learns high-order interactions. We apply spectral normalization (SN) to constrain the Lipschitz constant of the DNN. Leveraging this Lipschitz property, we design a nonlinear feedback linearization controller using the learned model and prove system stability with disturbance rejection. To the best of our knowledge, this is the first DNN-based nonlinear feedback controller with stability guarantees that can utilize arbitrarily large neural nets. Experimental results demonstrate that the proposed controller significantly outperforms a Baseline Nonlinear Tracking Controller in both landing and cross-table trajectory tracking cases. We also empirically show that the DNN generalizes well to unseen data outside the training domain.},
	urldate = {2019-06-01},
	journal = {arXiv:1811.08027 [cs]},
	author = {Shi, Guanya and Shi, Xichen and O'Connell, Michael and Yu, Rose and Azizzadenesheli, Kamyar and Anandkumar, Animashree and Yue, Yisong and Chung, Soon-Jo},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.08027},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{kerg_nonnormal_2019,
	title = {Non-normal {Recurrent} {Neural} {Network} ({nnRNN}): learning long time dependencies while improving expressivity with transient dynamics},
	shorttitle = {Non-normal {Recurrent} {Neural} {Network} ({nnRNN})},
	url = {http://arxiv.org/abs/1905.12080},
	abstract = {A recent strategy to circumvent the exploding and vanishing gradient problem in RNNs, and to allow the stable propagation of signals over long time scales, is to constrain recurrent connectivity matrices to be orthogonal or unitary. This ensures eigenvalues with unit norm and thus stable dynamics and training. However this comes at the cost of reduced expressivity due to the limited variety of orthogonal transformations. We propose a novel connectivity structure based on the Schur decomposition and a splitting of the Schur form into normal and non-normal parts. This allows to parametrize matrices with unit-norm eigenspectra without orthogonality constraints on eigenbases. The resulting architecture ensures access to a larger space of spectrally constrained matrices, of which orthogonal matrices are a subset. This crucial difference retains the stability advantages and training speed of orthogonal RNNs while enhancing expressivity, especially on tasks that require computations over ongoing input sequences.},
	urldate = {2019-06-06},
	journal = {arXiv:1905.12080 [cs, stat]},
	author = {Kerg, Giancarlo and Goyette, Kyle and Touzel, Maximilian Puelma and Gidel, Gauthier and Vorontsov, Eugene and Bengio, Yoshua and Lajoie, Guillaume},
	month = may,
	year = {2019},
	note = {arXiv: 1905.12080},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chen_nonlinear_1990a,
	title = {Non-{Linear} {System} {Identification} {Using} {Neural} {Networks}},
	volume = {51},
	doi = {10/cg8bhx},
	number = {6},
	journal = {International Journal of Control},
	author = {Chen, Sheng and Billings, S. A. and Grant, P. M.},
	year = {1990},
	note = {01127},
	pages = {1191--1214},
}

@article{rahman_neural_2000a,
	title = {Neural {Network} {Approach} for {Linearizing} {Control} of {Nonlinear} {Process} {Plants}},
	volume = {47},
	doi = {10/b7qwf3},
	number = {2},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {Rahman, MHR Fazlur and Devanathan, Rajagopalan and Kuanyi, Zhu},
	year = {2000},
	note = {00030},
	pages = {470--477},
}

@inproceedings{su_neural_1993a,
	title = {Neural {Model} {Predictive} {Control} of {Nonlinear} {Chemical} {Processes}},
	booktitle = {Intelligent {Control}, 1993., {Proceedings} of the 1993 {IEEE} {International} {Symposium} {On}},
	publisher = {IEEE},
	author = {Su, H-T and McAvoy, Thomas J},
	year = {1993},
	pages = {358--363},
}

@incollection{vuorio_multimodal_2019,
	title = {Multimodal model-agnostic meta-learning via task-aware modulation},
	url = {http://papers.nips.cc/paper/8296-multimodal-model-agnostic-meta-learning-via-task-aware-modulation.pdf},
	booktitle = {Advances in neural information processing systems 32},
	publisher = {Curran Associates, Inc.},
	author = {Vuorio, Risto and Sun, Shao-Hua and Hu, Hexiang and Lim, Joseph J},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and dAlché-Buc, F. and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {1--12},
}

@article{liu_multilevel_2019,
	title = {Multi-{Level} {Wavelet} {Convolutional} {Neural} {Networks}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2921451},
	abstract = {In computer vision, convolutional networks (CNNs) often adopt pooling to enlarge receptive field which has the advantage of low computational complexity. However, pooling can cause information loss and thus is detrimental to further operations such as features extraction and analysis. Recently, dilated filter has been proposed to tradeoff between receptive field size and efficiency. But the accompanying gridding effect can cause a sparse sampling of input images with checkerboard patterns. To address this problem, in this paper, we propose a novel multi-level wavelet CNN (MWCNN) model to achieve a better tradeoff between receptive field size and computational efficiency. The core idea is to embed wavelet transform into CNN architecture to reduce the resolution of feature maps while at the same time, increasing receptive field. Specifically, MWCNN for image restoration is based on U-Net architecture, and inverse wavelet transform (IWT) is deployed to reconstruct the high resolution (HR) feature maps. The proposed MWCNN can also be viewed as an improvement of dilated filter and a generalization of average pooling and can be applied to not only image restoration tasks, but also any CNNs requiring a pooling operation. The experimental results demonstrate the effectiveness of the proposed MWCNN for tasks, such as image denoising, single image super-resolution, JPEG image artifacts removal and object classification.},
	journal = {IEEE Access},
	author = {Liu, Pengju and Zhang, Hongzhi and Lian, Wei and Zuo, Wangmeng},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {CNN architecture, Computer architecture, Convolutional networks, Discrete wavelet transforms, Feature extraction, Image restoration, MWCNN, PEG image artifacts removal, Task analysis, U-Net architecture, average pooling generalization, checkerboard patterns, computational efficiency, computer vision, convolutional neural nets, dilated filter, efficiency, feature extraction, feature maps, features extraction, filtering theory, gridding effect, image denoising, image resolution, image restoration, image restoration tasks, image sampling, information loss, input images, inverse transforms, inverse wavelet transform, low computational complexity, multi-level wavelet, multilevel wavelet convolutional neural networks, novel multilevel wavelet CNN model, object classification, pooling operation, receptive field size, single image super-resolution, sparse sampling, wavelet transforms},
	pages = {74973--74985},
}

@article{shanmugam_multiple_2018a,
	title = {Multiple {Instance} {Learning} for {ECG} {Risk} {Stratification}},
	url = {http://arxiv.org/abs/1812.00475},
	abstract = {In this paper, we apply a multiple instance learning paradigm to signal-based risk stratiﬁcation for cardiovascular outcomes. In contrast to methods that require hand-crafted features or domain knowledge, our method learns a representation with state-of-the-art predictive power from the raw ECG signal. We accomplish this by leveraging the multiple instance learning framework. This framework is particularly valuable to learning from biometric signals, where patient-level labels are available but signal segments are rarely annotated. We make two contributions in this paper: 1) reframing risk stratiﬁcation for cardiovascular death (CVD) as a multiple instance learning problem, and 2) using this framework to design a new risk score, for which patients in the highest quartile are 15.9 times more likely to die of CVD within 90 days of hospital admission for an acute coronary syndrome.},
	language = {en},
	urldate = {2018-12-10},
	journal = {arXiv:1812.00475 [cs, stat]},
	author = {Shanmugam, Divya and Blalock, Davis and Gong, Jen G. and Guttag, John},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.00475},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
}

@article{shanmugam_multiple_2018,
	title = {Multiple {Instance} {Learning} for {ECG} {Risk} {Stratification}},
	url = {https://arxiv.org/abs/1812.00475},
	language = {en},
	urldate = {2018-12-10},
	author = {Shanmugam, Divya and Blalock, Davis and Gong, Jen G. and Guttag, John},
	month = dec,
	year = {2018},
}

@article{shanmugam_multiple_2018b,
	title = {Multiple {Instance} {Learning} for {ECG} {Risk} {Stratification}},
	url = {http://arxiv.org/abs/1812.00475},
	abstract = {In this paper, we apply a multiple instance learning paradigm to signal-based risk stratification for cardiovascular outcomes. In contrast to methods that require hand-crafted features or domain knowledge, our method learns a representation with state-of-the-art predictive power from the raw ECG signal. We accomplish this by leveraging the multiple instance learning framework. This framework is particularly valuable to learning from biometric signals, where patient-level labels are available but signal segments are rarely annotated. We make two contributions in this paper: 1) reframing risk stratification for cardiovascular death (CVD) as a multiple instance learning problem, and 2) using this framework to design a new risk score, for which patients in the highest quartile are 15.9 times more likely to die of CVD within 90 days of hospital admission for an acute coronary syndrome.},
	urldate = {2018-12-13},
	journal = {arXiv:1812.00475 [cs, stat]},
	author = {Shanmugam, Divya and Blalock, Davis and Gong, Jen G. and Guttag, John},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.00475},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{vanoverschee_n4sid_1994,
	series = {Special issue on statistical signal processing and control},
	title = {{N4SID}: {Subspace} algorithms for the identification of combined deterministic-stochastic systems},
	volume = {30},
	issn = {0005-1098},
	shorttitle = {{N4SID}},
	url = {http://www.sciencedirect.com/science/article/pii/0005109894902305},
	doi = {10.1016/0005-1098(94)90230-5},
	abstract = {Recently a great deal of attention has been given to numerical algorithms for subspace state space system identification (N4SID). In this paper, we derive two new N4SID algorithms to identify mixed deterministic-stochastic systems. Both algorithms determine state sequences through the projection of input and output data. These state sequences are shown to be outputs of non-steady state Kalman filter banks. From these it is easy to determine the state space system matrices. The N4SID algorithms are always convergent (non-iterative) and numerically stable since they only make use of QR and Singular Value Decompositions. Both N4SID algorithms are similar, but the second one trades off accuracy for simplicity. These new algorithms are compared with existing subspace algorithms in theory and in practice.},
	number = {1},
	journal = {Automatica},
	author = {Van Overschee, Peter and De Moor, Bart},
	month = jan,
	year = {1994},
	note = {00000},
	keywords = {Kalman filters, Multivariable systems, QR and singular value decomposition, difference equations, state space methods, system identification},
	pages = {75--93},
}

@article{lu_multiscale_2017,
	title = {Multiscale {Support} {Vector} {Learning} {With} {Projection} {Operator} {Wavelet} {Kernel} for {Nonlinear} {Dynamical} {System} {Identification}},
	volume = {28},
	issn = {2162-237X},
	doi = {10.1109/TNNLS.2015.2513902},
	abstract = {A giant leap has been made in the past couple of decades with the introduction of kernel-based learning as a mainstay for designing effective nonlinear computational learning algorithms. In view of the geometric interpretation of conditional expectation and the ubiquity of multiscale characteristics in highly complex nonlinear dynamic systems [1]-[3], this paper presents a new orthogonal projection operator wavelet kernel, aiming at developing an efficient computational learning approach for nonlinear dynamical system identification. In the framework of multiresolution analysis, the proposed projection operator wavelet kernel can fulfill the multiscale, multidimensional learning to estimate complex dependencies. The special advantage of the projection operator wavelet kernel developed in this paper lies in the fact that it has a closed-form expression, which greatly facilitates its application in kernel learning. To the best of our knowledge, it is the first closed-form orthogonal projection wavelet kernel reported in the literature. It provides a link between grid-based wavelets and mesh-free kernel-based methods. Simulation studies for identifying the parallel models of two benchmark nonlinear dynamical systems confirm its superiority in model accuracy and sparsity.},
	number = {1},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Lu, Z. and Sun, J. and Butts, K.},
	month = jan,
	year = {2017},
	keywords = {Composite kernel, Computational modeling, Kernel, Learning systems, closed-form expression, closed-form orthogonal projection wavelet kernel, complex dependency estimation, conditional expectation geometric interpretation, grid-based wavelets, identification, kernel-based learning, learning (artificial intelligence), linear programming support vector regression (LP-SVR), mesh-free kernel-based methods, multidimensional learning, multiresolution analysis, multiscale characteristics ubiquity, multiscale modeling, multiscale support vector learning, nonlinear computational learning algorithms, nonlinear dynamical system identification, nonlinear dynamical systems, nonlinear systems identification, orthogonal projection operator, orthogonal projection operator wavelet kernel, raised-cosine wavelet, support vector machines, wavelet transforms},
	pages = {231--243},
}

@article{liu_multiview_2013,
	title = {Multiview {Partitioning} via {Tensor} {Methods}},
	volume = {25},
	issn = {1041-4347},
	doi = {10.1109/TKDE.2012.95},
	abstract = {Clustering by integrating multiview representations has become a crucial issue for knowledge discovery in heterogeneous environments. However, most prior approaches assume that the multiple representations share the same dimension, limiting their applicability to homogeneous environments. In this paper, we present a novel tensor-based framework for integrating heterogeneous multiview data in the context of spectral clustering. Our framework includes two novel formulations; that is multiview clustering based on the integration of the Frobenius-norm objective function (MC-FR-OI) and that based on matrix integration in the Frobenius-norm objective function (MC-FR-MI). We show that the solutions for both formulations can be computed by tensor decompositions. We evaluated our methods on synthetic data and two real-world data sets in comparison with baseline methods. Experimental results demonstrate that the proposed formulations are effective in integrating multiview data in heterogeneous environments.},
	number = {5},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Liu, X. and Ji, S. and Glänzel, W. and Moor, B. De},
	month = may,
	year = {2013},
	keywords = {Clustering algorithms, Data mining, Frobenius-norm objective function, Kernel, MC-FR-MI, MC-FR-OI, Matrix decomposition, Optimization, Tensile stress, Tin, Vectors, heterogeneous multiview data, higher order orthogonal iteration, knowledge discovery, matrix algebra, matrix integration, multilinear singular value decomposition, multiview clustering, multiview partitioning, multiview representations, novel tensor-based framework, pattern clustering, spectral clustering, tensor decomposition, tensor decompositions, tensor methods, tensors},
	pages = {1056--1069},
}

@article{aizenberg_multilayer_2016,
	title = {Multilayer {Neural} {Network} with {Multi}-{Valued} {Neurons} in time series forecasting of oil production},
	volume = {175},
	doi = {10.1016/j.neucom.2015.06.092},
	journal = {Neurocomputing},
	author = {Aizenberg, Igor and Sheremetov, Leonid and Villa-Vargas, Luis and Martinez-Muñoz, Jorge},
	year = {2016},
	pages = {980--989},
}

@incollection{ernst_mutual_2008,
	title = {Mutual information based semi-global stereo matching on the {GPU}},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer},
	author = {Ernst, Ines and Hirschmüller, Heiko},
	year = {2008},
	pages = {228--239},
}

@article{bonin_narx_2010,
	title = {{NARX} model selection based on simulation error minimisation and {LASSO}},
	volume = {4},
	doi = {10.1049/iet-cta.2009.0217},
	number = {7},
	journal = {IET Control Theory \& Applications},
	author = {Bonin, M and Seghezza, V and Piroddi, L},
	year = {2010},
	pages = {1157--1168},
}

@article{friedman_multivariate_1991,
	title = {Multivariate adaptive regression splines},
	issn = {0090-5364},
	doi = {10.1214/aos/1176347963},
	journal = {The annals of statistics},
	author = {Friedman, Jerome H},
	year = {1991},
	pages = {1--67},
}

@article{wilson_multidecadal_2017,
	title = {Multi-decadal time series of remotely sensed vegetation improves prediction of soil carbon in a subtropical grassland},
	volume = {27},
	issn = {1939-5582},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/eap.1557/abstract},
	doi = {10.1002/eap.1557},
	abstract = {Soil carbon sequestration in agroecosystems could play a key role in climate change mitigation but will require accurate predictions of soil organic carbon (SOC) stocks over spatial scales relevant to land management. Spatial variation in underlying drivers of SOC, such as plant productivity and soil mineralogy, complicates these predictions. Recent advances in the availability of remotely sensed data make it practical to generate multidecadal time series of vegetation indices with high spatial resolution and coverage. However, the utility of such data largely is unknown, only having been tested with shorter (e.g., 1–2 yr) data summaries. Across a 2,000 ha subtropical grassland, we found that a long time series (28 yr) of a vegetation index (Enhanced Vegetation Index; EVI) derived from the Landsat 5 satellite significantly enhanced prediction of spatially varying SOC pools, while a short summary (2 yr) was an ineffective predictor. EVI was the best predictor for surface SOC (0–5 cm depth) and total measured SOC stocks (0–15 cm). The optimum models for SOC in the upper soil layer combined EVI records with elevation and calcium concentration, while deeper SOC was more strongly associated with calcium availability. We demonstrate how data from the open access Landsat archive can predict SOC stocks, a key ecosystem metric, and illustrate the rich variety of analytical approaches that can be applied to long time series of remotely sensed greenness. Overall, our results showed that SOC pools were closely coupled to EVI in this ecosystem, demonstrating that maintenance of higher average green leaf area is correlated with higher SOC. The strong associations of vegetation greenness and calcium concentration with SOC suggest that the ability to sequester additional SOC likely will rely on strategic management of pasture vegetation and soil fertility.},
	language = {en},
	number = {5},
	journal = {Ecological Applications},
	author = {Wilson, Chris H. and Caughlin, T. Trevor and Rifai, Sami W. and Boughton, Elizabeth H. and Mack, Michelle C. and Flory, S. Luke},
	month = jul,
	year = {2017},
	note = {00007},
	keywords = {Google Earth Engine, Landsat time series, enhanced vegetation index, remote sensing, soil carbon sequestration, soil organic carbon, subtropical grasslands},
	pages = {1646--1656},
}

@book{tanenbaum_modern_2009,
	title = {Modern operating system},
	publisher = {Pearson Education, Inc},
	author = {Tanenbaum, Andrew S.},
	year = {2009},
	note = {00000},
}

@article{yan_narmax_2015,
	title = {{NARMAX} model identification using a set-theoretic evolutionary approach},
	journal = {Signal Processing},
	author = {Yan, Jinyao and Deller, JR},
	year = {2015},
	note = {00009},
	keywords = {🔍No DOI found},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	number = {5},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	year = {1989},
	keywords = {❓Multiple DOI},
	pages = {359--366},
}

@article{baake_modelling_1992,
	title = {Modelling the fast fluorescence rise of photosynthesis},
	volume = {54},
	number = {6},
	journal = {Bulletin of mathematical biology},
	author = {Baake, Ellen and Schlöder, Johannes P},
	year = {1992},
	keywords = {❓Multiple DOI},
	pages = {999--1021},
}

@article{ayalasolares_modeling_2016,
	title = {Modeling and prediction of global magnetic disturbance in near-{Earth} space: {A} case study for {Kp} index using {NARX} models},
	volume = {14},
	number = {10},
	journal = {Space Weather},
	author = {Ayala Solares, Jose Roberto and Wei, Hua-Liang and Boynton, R. J. and Walker, Simon N and Billings, Stephen A},
	year = {2016},
	keywords = {🔍No DOI found},
	pages = {899--916},
}

@article{yuan_model_2006,
	title = {Model {Selection} and {Estimation} in {Regression} with {Grouped} {Variables}},
	volume = {68},
	issn = {1369-7412},
	url = {http://www.jstor.org/stable/3647556},
	doi = {10.1111/j.1467-9868.2005.00532.x},
	abstract = {We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multi-factor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
	number = {1},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Yuan, Ming and Lin, Yi},
	year = {2006},
	note = {04899},
	pages = {49--67},
}

@book{hoffer_modern_2011,
	address = {Upper Saddle River, N.J},
	edition = {10th ed},
	title = {Modern database management},
	isbn = {978-0-13-608839-4},
	publisher = {Prentice Hall},
	author = {Hoffer, Jeffrey A. and Ramesh, V. and Topi, Heikki},
	year = {2011},
	note = {OCLC: ocn613293263},
	keywords = {Database management},
}

@book{ogata_modern_2010,
	address = {Boston},
	edition = {5th ed},
	series = {Prentice-{Hall} electrical engineering series. {Instrumentation} and controls series},
	title = {Modern control engineering},
	isbn = {978-0-13-615673-4},
	publisher = {Prentice-Hall},
	author = {Ogata, Katsuhiko},
	year = {2010},
	keywords = {Automatic control, Control theory},
}

@article{zhang_modeling_2006,
	title = {Modeling of temperature-humidity for wood drying based on time-delay neural network},
	volume = {17},
	doi = {10.1007/s11676-006-0033-1},
	number = {2},
	journal = {Journal of Forestry Research},
	author = {Zhang, Dong-yan and Sun, Li-ping and Cao, Jun},
	year = {2006},
	note = {00028},
	pages = {141--144},
}

@article{poon_modelbased_2017,
	title = {Model-based fault detection and identification for switching power converters},
	volume = {32},
	doi = {10.1109/TPEL.2016.2541342},
	number = {2},
	journal = {IEEE Transactions on Power Electronics},
	author = {Poon, Jason and Jain, Palak and Konstantakopoulos, Ioannis C and Spanos, Costas and Panda, Sanjib Kumar and Sanders, Seth R},
	year = {2017},
	note = {00000},
	pages = {1419--1430},
}

@article{rossiter_modelling_2001,
	title = {Modelling and implicit modelling for predictive control},
	volume = {74},
	issn = {0020-7179, 1366-5820},
	url = {http://www.tandfonline.com/doi/full/10.1080/00207170110054129},
	doi = {10/dv4phx},
	language = {en},
	number = {11},
	urldate = {2019-04-04},
	journal = {International Journal of Control},
	author = {Rossiter, J. A. and Kouvaritakis, B.},
	month = jan,
	year = {2001},
	pages = {1085--1095},
}

@article{ayalasolares_modeling_2016a,
	title = {Modeling and {Prediction} of {Global} {Magnetic} {Disturbance} in {Near}-{Earth} {Space}: {A} {Case} {Study} for {Kp} {Index} {Using} {NARX} {Models}},
	volume = {14},
	doi = {10/gfjwmm},
	number = {10},
	journal = {Space Weather},
	author = {Ayala Solares, Jose Roberto and Wei, Hua-Liang and Boynton, R. J. and Walker, Simon N and Billings, Stephen A},
	year = {2016},
	pages = {899--916},
}

@article{pathak_modelfree_2018,
	title = {Model-{Free} {Prediction} of {Large} {Spatiotemporally} {Chaotic} {Systems} from {Data}: {A} {Reservoir} {Computing} {Approach}},
	volume = {120},
	shorttitle = {Model-{Free} {Prediction} of {Large} {Spatiotemporally} {Chaotic} {Systems} from {Data}},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.024102},
	doi = {10.1103/PhysRevLett.120.024102},
	abstract = {We demonstrate the effectiveness of using machine learning for model-free prediction of spatiotemporally chaotic systems of arbitrarily large spatial extent and attractor dimension purely from observations of the system’s past evolution. We present a parallel scheme with an example implementation based on the reservoir computing paradigm and demonstrate the scalability of our scheme using the Kuramoto-Sivashinsky equation as an example of a spatiotemporally chaotic system.},
	number = {2},
	urldate = {2021-04-02},
	journal = {Physical Review Letters},
	author = {Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
	month = jan,
	year = {2018},
	note = {Publisher: American Physical Society},
	pages = {024102},
}

@article{singya_mitigating_2017,
	title = {Mitigating {NLD} for {Wireless} {Networks}: {Effect} of {Nonlinear} {Power} {Amplifiers} on {Future} {Wireless} {Communication} {Networks}},
	volume = {18},
	issn = {1557-9581},
	shorttitle = {Mitigating {NLD} for {Wireless} {Networks}},
	doi = {10.1109/MMM.2017.2691423},
	abstract = {Efficient utilization of limited bandwidth with high-data-rate transmission while serving a large number of users is a prime requirement for present and future wireless communication systems. To meet this rising demand, orthogonal frequency-division multiplexing (OFDM) and cooperative communication have emerged as promising solutions due to their robustness in severely degraded channel conditions, link reliability, and spectral efficiency. Both OFDM and cooperative systems have challenged RF front-end specifications such as bandwidth and power-efficiency requirements for end users as well as for the base station due to the high peak-to-average power ratio (PAPR). In present and future communication systems, the power amplifier (PA) is a key component at the transmitter. To obtain maximum power efficiency, the PA is operated near its saturation point, which leads to nonlinear distortion (NLD), which is further exaggerated due to the high PAPR of the input signal. This NLD is expected to increase in future fifth-generation (5G) communication networks, due to the use of large bandwidth at millimeter-wave (mmW) frequencies.},
	number = {5},
	journal = {IEEE Microwave Magazine},
	author = {Singya, Praveen Kumar and Kumar, Nagendra and Bhatia, Vimal},
	month = jul,
	year = {2017},
	keywords = {5G communication networks, 5G mobile communication, NLD, OFDM modulation, OFDM system, PA, Peak to average power ratio, Power amplifiers, Power generation, RF front-end specifications, Wireless communication, Wireless networks, channel conditions, cooperative communication, fifth-generation communication networks, high peak-to-average power ratio, high-data-rate transmission, input signal PAPR, link reliability, millimeter-wave frequencies, nonlinear distortion, orthogonal frequency-division multiplexing, power amplifier, radio links, radio transmitter, radio transmitters, radiofrequency power amplifiers, spectral analysis, spectral efficiency, telecommunication network reliability, wireless channels, wireless communication system},
	pages = {73--90},
}

@article{tierney_markov_1994,
	title = {Markov {Chains} for {Exploring} {Posterior} {Distributions}},
	volume = {22},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/2242477},
	abstract = {[Several Markov chain methods are available for sampling from a posterior distribution. Two important examples are the Gibbs sampler and the Metropolis algorithm. In addition, several strategies are available for constructing hybrid algorithms. This paper outlines some of the basic methods and strategies and discusses some related theoretical and practical issues. On the theoretical side, results from the theory of general state space Markov chains can be used to obtain convergence rates, laws of large numbers and central limit theorems for estimates obtained from Markov chain methods. These theoretical results can be used to guide the construction of more efficient algorithms. For the practical use of Markov chain methods, standard simulation methodology provides several variance reduction techniques and also give guidance on the choice of sample size and allocation.]},
	number = {4},
	urldate = {2018-12-13},
	journal = {The Annals of Statistics},
	author = {Tierney, Luke},
	year = {1994},
	pages = {1701--1728},
}

@article{macfarlane_methodology_1990,
	title = {Methodology of {ECG} interpretation in the {Glasgow} program},
	volume = {29},
	issn = {0026-1270},
	doi = {10/gftz83},
	number = {04},
	journal = {Methods of information in medicine},
	author = {Macfarlane, PW and Devine, B and Latif, S and McLaughlin, S and Shoat, DB and Watts, MP},
	year = {1990},
	pages = {354--361},
}

@inproceedings{iangoodfellow_maxout_2013,
	title = {Maxout {Networks}},
	url = {http://proceedings.mlr.press/v28/goodfellow13.html},
	abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {{Ian Goodfellow} and {David Warde-Farley} and {Mehdi Mirza} and {Aaron Courville} and {Yoshua Bengio}},
	editor = {{Sanjoy Dasgupta} and {David McAllester}},
	month = feb,
	year = {2013},
	pages = {1319--1327},
}

@misc{schon_manipulating_2011,
	title = {Manipulating the multivariate gaussian density},
	author = {Schön, Thomas B. and Lindsten, Fredrik},
	year = {2011},
	note = {00000},
}

@article{thomas_maximum_2014,
	title = {Maximum {Likelihood} {Estimation} of {GEVD}: {Applications} in {Bioinformatics}},
	volume = {11},
	issn = {1545-5963},
	shorttitle = {Maximum {Likelihood} {Estimation} of {GEVD}},
	doi = {10.1109/TCBB.2014.2304292},
	abstract = {We propose a method, maximum likelihood estimation of generalized eigenvalue decomposition (MLGEVD) that employs a well known technique relying on the generalization of singular value decomposition (SVD). The main aim of the work is to show the tight equivalence between MLGEVD and generalized ridge regression. This relationship reveals an important mathematical property of GEVD in which the second argument act as prior information in the model. Thus we show that MLGEVD allows the incorporation of external knowledge about the quantities of interest into the estimation problem. We illustrate the importance of prior knowledge in clinical decision making/identifying differentially expressed genes with case studies for which microarray data sets with corresponding clinical/literature information are available. On all of these three case studies, MLGEVD outperformed GEVD on prediction in terms of test area under the ROC curve (test AUC). MLGEVD results in significantly improved diagnosis, prognosis and prediction of therapy response.},
	number = {4},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	author = {Thomas, M. and Daemen, A. and Moor, B. De},
	month = jul,
	year = {2014},
	note = {00000},
	keywords = {Breast cancer, Eigenvalue decomposition, Eigenvalues and eigenfunctions, MLGEVD, Matrix decomposition, Principal component analysis, ROC curve, SVD, bioinformatics, clinical decision making-identification, clinical-literature information, expressed genes, external knowledge incorporation, generalized eigenvalue decomposition, generalized ridge regression, generalized singular value decomposition, genetics, mathematical property, maximum likelihood estimation, maximum likelihood generalized eigenvalue decomposition, microarray data sets, quantities-of-interest, regression analysis, sensitivity analysis, singular value decomposition, therapy response diagnosis, therapy response prediction, therapy response prognosis},
	pages = {673--680},
}

@article{milanese_model_2005,
	title = {Model quality in identification of nonlinear systems},
	volume = {50},
	doi = {10.1109/TAC.2005.856657},
	number = {10},
	journal = {IEEE Transactions on Automatic Control},
	author = {Milanese, Mario and Novara, Carlo},
	year = {2005},
	pages = {1606--1611},
}

@article{cauchy_methode_1847,
	title = {Méthode générale pour la résolution des systemes déquations simultanées},
	volume = {25},
	number = {1847},
	journal = {Comp. Rend. Sci. Paris},
	author = {Cauchy, Augustin},
	year = {1847},
	keywords = {🔍No DOI found},
	pages = {536--538},
}

@article{neal_mcmc_2011,
	title = {{MCMC} using {Hamiltonian} dynamics},
	volume = {2},
	doi = {10.1201/b10905-6},
	number = {11},
	journal = {Handbook of Markov Chain Monte Carlo},
	author = {Neal, Radford M.},
	year = {2011},
	pages = {2},
}

@article{morari_model_2002,
	title = {Model predictive control},
	journal = {Preprint},
	author = {Morari, Manfred and Lee, Jay H. and Garcia, C. and Prett, D. M.},
	year = {2002},
	keywords = {🔍No DOI found},
}

@article{astrom_maximum_1979,
	title = {Maximum likelihood and prediction error methods},
	volume = {12},
	doi = {10.1016/S1474-6670(17)53976-2},
	number = {8},
	journal = {IFAC Proceedings Volumes},
	author = {Astrom, K. J.},
	year = {1979},
	pages = {551--574},
}

@book{umenberger_maximum_2017,
	title = {Maximum likelihood identification of stable linear dynamical systems},
	author = {Umenberger, Jack and Wågberg, Johan and Manchester, Ian and Schön, Thomas},
	month = jun,
	year = {2017},
	note = {00000},
}

@book{sontag_mathematical_2013,
	title = {Mathematical control theory: deterministic finite dimensional systems},
	volume = {6},
	shorttitle = {Mathematical control theory},
	publisher = {Springer Science \& Business Media},
	author = {Sontag, Eduardo D.},
	year = {2013},
	note = {00000},
}

@inproceedings{ha_model_2015,
	title = {Model order selection for continuous time instrumental variable methods using regularization},
	doi = {10.1109/CDC.2015.7402323},
	abstract = {The aim of this paper is to propose a new method to select the model order in continuous time system identification, instrumental variable methods. The idea is to over-parameterize the model and utilize regularization based on the l1 norm to obtain a sparse estimate. The model order of the identified system is then determined by the rank of the Hankel matrix of the estimated parameter. Simulation results show that the proposed method works very effectively. For low signal to noise ratio (SNR), it offers a significant improvement to existing model order selection methods with the performance at high SNR comparable to the existing methods.},
	booktitle = {2015 54th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Ha, H. and Welsh, J. S.},
	month = dec,
	year = {2015},
	keywords = {Computational modeling, Continuous time identification, Data models, Estimation, Hankel matrices, Hankel matrix, Instruments, Mathematical model, Regularization, SNR, Transfer functions, Yttrium, continuous time system identification, continuous time systems, identification, instrumental variable methods, l1 norm, model order selection methods, order selection, signal to noise ratio, sparse estimation},
	pages = {771--776},
}

@book{bronson_matrix_1991,
	address = {Boston},
	edition = {2nd ed},
	title = {Matrix methods: an introduction},
	isbn = {978-0-12-135251-6},
	shorttitle = {Matrix methods},
	publisher = {Academic Press},
	author = {Bronson, Richard},
	year = {1991},
	keywords = {Matrices},
}

@techreport{labvolt_mobile_2015,
	title = {Mobile {Instrumentation} and {Process} {Control} {Training} {Systems}},
	institution = {Festo},
	author = {{LabVolt}},
	year = {2015},
}

@article{he_model_2017,
	title = {Model identification and control design for a humanoid robot},
	volume = {47},
	doi = {10.1109/TSMC.2016.2557227},
	number = {1},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {He, Wei and Ge, Weiliang and Li, Yunchuan and Liu, Yan-Jun and Yang, Chenguang and Sun, Changyin},
	year = {2017},
	pages = {45--57},
}

@book{hestenes_methods_1952,
	title = {Methods of conjugate gradients for solving linear systems},
	volume = {49},
	publisher = {NBS},
	author = {Hestenes, Magnus Rudolph and Stiefel, Eduard},
	year = {1952},
}

@book{golub_matrix_2012,
	title = {Matrix {Computations}},
	volume = {3},
	publisher = {JHU Press},
	author = {Golub, Gene H and Van Loan, Charles F},
	year = {2012},
}

@article{he_model_2017a,
	title = {Model {Identification} and {Control} {Design} for a {Humanoid} {Robot}},
	volume = {47},
	doi = {10/f9kc4j},
	number = {1},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {He, Wei and Ge, Weiliang and Li, Yunchuan and Liu, Yan-Jun and Yang, Chenguang and Sun, Changyin},
	year = {2017},
	note = {00054},
	pages = {45--57},
}

@techreport{labvolt_mobile_2015a,
	title = {Mobile {Instrumentation} and {Process} {Control} {Training} {Systems}},
	institution = {Festo},
	author = {{LabVolt}},
	year = {2015},
	note = {00000},
}

@article{wen_method_1976,
	title = {Method for random vibration of hysteretic systems},
	volume = {102},
	issn = {0044-7951},
	number = {2},
	journal = {Journal of the engineering mechanics division},
	author = {Wen, Yi-Kwei},
	year = {1976},
	note = {Publisher: American Society of Civil Engineers},
	pages = {249--263},
}

@misc{_mais_,
	title = {Mais 1 milhão de brasileiros passaram a trabalhar como motorista de aplicativo ou ambulante em 2018 - {Economia}},
	url = {https://economia.estadao.com.br/noticias/geral,mais-1-milhao-de-brasileiros-passaram-a-trabalhar-como-motorista-de-aplicativo-ou-ambulante-em-2018,70003129796},
	abstract = {Com aumento no trabalho informal, número de pessoas trabalhando com apps de transporte cresceu 30\% e nas ruas, 12\%},
	language = {pt-BR},
	urldate = {2020-01-23},
	journal = {Estadão},
}

@article{amorim_mais_2019,
	title = {Mais 1 milhão de brasileiros passaram a trabalhar como motorista de aplicativo ou ambulante em 2018},
	url = {https://economia.estadao.com.br/noticias/geral,mais-1-milhao-de-brasileiros-passaram-a-trabalhar-como-motorista-de-aplicativo-ou-ambulante-em-2018,70003129796},
	abstract = {Com aumento no trabalho informal, número de pessoas trabalhando com apps de transporte cresceu 30\% e nas ruas, 12\%},
	language = {pt-BR},
	urldate = {2020-01-28},
	journal = {O Estado de S. Paulo},
	author = {Amorim, Daniela},
	month = dec,
	year = {2019},
}

@inproceedings{zhang_making_2019,
	title = {Making {Convolutional} {Networks} {Shift}-{Invariant} {Again}},
	url = {http://arxiv.org/abs/1904.11486},
	abstract = {Modern convolutional networks are not shift-invariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and average-pooling, ignore the sampling theorem. The well-known signal processing fix is anti-aliasing by low-pass filtering before downsampling. However, simply inserting this module into deep networks degrades performance; as a result, it is seldomly used today. We show that when integrated correctly, it is compatible with existing architectural components, such as max-pooling and strided-convolution. We observe {\textbackslash}textit\{increased accuracy\} in ImageNet classification, across several commonly-used architectures, such as ResNet, DenseNet, and MobileNet, indicating effective regularization. Furthermore, we observe {\textbackslash}textit\{better generalization\}, in terms of stability and robustness to input corruptions. Our results demonstrate that this classical signal processing technique has been undeservingly overlooked in modern deep networks. Code and anti-aliased versions of popular networks are available at https://richzhang.github.io/antialiased-cnns/ .},
	urldate = {2020-03-23},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning} ({ICML})},
	author = {Zhang, Richard},
	month = jun,
	year = {2019},
	note = {arXiv: 1904.11486},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{su_longterm_1992,
	title = {Long-term predictions of chemical processes using recurrent neural networks: {A} parallel training approach},
	volume = {31},
	doi = {10.1021/ie00005a014},
	number = {5},
	journal = {Industrial \& Engineering Chemistry Research},
	author = {Su, Hong Te and McAvoy, Thomas J and Werbos, Paul},
	year = {1992},
	pages = {1338--1352},
}

@incollection{virmaux_lipschitz_2018,
	title = {Lipschitz regularity of deep neural networks: analysis and efficient estimation},
	shorttitle = {Lipschitz regularity of deep neural networks},
	url = {http://papers.nips.cc/paper/7640-lipschitz-regularity-of-deep-neural-networks-analysis-and-efficient-estimation.pdf},
	urldate = {2020-06-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Virmaux, Aladin and Scaman, Kevin},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {3835--3844},
}

@book{chen_linear_1998,
	edition = {3rd},
	title = {Linear system theory and design, holt, winehart and winston},
	author = {Chen, C.T.},
	year = {1998},
}

@article{verstraete_lorentz_2002,
	title = {Lorentz singular-value decomposition and its applications to pure states of three qubits},
	volume = {65},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.65.032308},
	doi = {10.1103/PhysRevA.65.032308},
	abstract = {All mixed states of two qubits can be brought into normal form by the action of local operations and classical communication operations of the kind ρ′=(A⊗B)ρ(A⊗B)†. These normal forms can be obtained by considering a Lorentz singular-value decomposition on a real parametrization of the density matrix. We show that the Lorentz singular values are variationally defined and give rise to entanglement monotones, with as a special case the concurrence. Next a necessary and sufficient criterion is conjectured for a mixed state to be convertible into another specific one with a nonzero probability. Finally the formalism of the Lorentz singular-value decomposition is applied to tripartite pure states of qubits. New proofs are given for the existence of the Greenberger-Horne-Zeilinger (GHZ) class and W class of states, and a rigorous proof for the optimal distillation of a GHZ state is derived.},
	number = {3},
	journal = {Physical Review A},
	author = {Verstraete, Frank and Dehaene, Jeroen and De Moor, Bart},
	month = feb,
	year = {2002},
	note = {00000},
	pages = {032308},
}

@inproceedings{murphy_lowcost_2007,
	title = {Low-cost stereo vision on an {FPGA}},
	booktitle = {Field-{Programmable} {Custom} {Computing} {Machines}, 2007. {FCCM} 2007. 15th {Annual} {IEEE} {Symposium} on},
	publisher = {IEEE},
	author = {Murphy, Chris and Lindquist, Daniel and Rynning, Ann Marie and Cecil, Thomas and Leavitt, Sarah and Chang, Mark L},
	year = {2007},
	pages = {333--334},
}

@article{azad_longterm_2014,
	title = {Long-term wind speed forecasting and general pattern recognition using neural networks},
	volume = {5},
	doi = {10.1109/TSTE.2014.2300150},
	number = {2},
	journal = {IEEE Transactions on Sustainable Energy},
	author = {Azad, Hanieh Borhan and Mekhilef, Saad and Ganapathy, Vellapa Gounder},
	year = {2014},
	pages = {546--553},
}

@article{hochreiter_long_1997,
	title = {Long short-term memory},
	volume = {9},
	issn = {0899-7667},
	doi = {10.1162/neco.1997.9.8.1735},
	number = {8},
	journal = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997},
	pages = {1735--1780},
}

@article{menezes_longterm_2008,
	series = {Advances in {Neural} {Information} {Processing} ({ICONIP} 2006) / {Brazilian} {Symposium} on {Neural} {Networks} ({SBRN} 2006)},
	title = {Long-term time series prediction with the {NARX} network: {An} empirical evaluation},
	volume = {71},
	issn = {0925-2312},
	shorttitle = {Long-term time series prediction with the {NARX} network},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231208003081},
	doi = {10.1016/j.neucom.2008.01.030},
	abstract = {The NARX network is a dynamical neural architecture commonly used for input–output modeling of nonlinear dynamical systems. When applied to time series prediction, the NARX network is designed as a feedforward time delay neural network (TDNN), i.e., without the feedback loop of delayed outputs, reducing substantially its predictive performance. In this paper, we show that the original architecture of the NARX network can be easily and efficiently applied to long-term (multi-step-ahead) prediction of univariate time series. We evaluate the proposed approach using two real-world data sets, namely the well-known chaotic laser time series and a variable bit rate (VBR) video traffic time series. All the results show that the proposed approach consistently outperforms standard neural network based predictors, such as the TDNN and Elman architectures.},
	number = {16},
	journal = {Neurocomputing},
	author = {Menezes, José Maria P. and Barreto, Guilherme A.},
	month = oct,
	year = {2008},
	keywords = {Chaotic time series, Long-term prediction, NARX neural network, Nonlinear traffic modeling, Recurrence plot},
	pages = {3335--3343},
}

@inproceedings{bolukbasi_man_2016,
	title = {Man is to computer programmer as woman is to homemaker? debiasing word embeddings},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
	year = {2016},
	pages = {4349--4357},
}

@inproceedings{mikolov_linguistic_2013,
	address = {Atlanta, Georgia},
	title = {Linguistic {Regularities} in {Continuous} {Space} {Word} {Representations}},
	url = {http://www.aclweb.org/anthology/N13-1090},
	booktitle = {Proceedings of the 2013 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
	month = jun,
	year = {2013},
	pages = {746--751},
}

@book{corbet_linux_2005,
	address = {Beijing ; Sebastopol, CA},
	edition = {3rd ed},
	title = {Linux device drivers},
	isbn = {978-0-596-00590-0},
	publisher = {O'Reilly},
	author = {Corbet, Jonathan and Rubini, Alessandro and Kroah-Hartman, Greg and Rubini, Alessandro},
	year = {2005},
	keywords = {Linux device drivers (Computer programs)},
}

@book{beekmans_linux_2012,
	title = {Linux {From} {Scratch}},
	isbn = {978-1-300-01983-1},
	url = {https://books.google.com.br/books?id=N7vJAwAAQBAJ},
	publisher = {Lulu Press, Incorporated},
	author = {Beekmans, G.},
	year = {2012},
}

@book{love_linux_2010,
	series = {Developer's {Library}},
	title = {Linux {Kernel} {Development}},
	isbn = {978-0-7686-9679-0},
	url = {https://books.google.com.br/books?id=3MWRMYRwulIC},
	publisher = {Pearson Education},
	author = {Love, R.},
	year = {2010},
}

@book{murphy_machine_2012,
	address = {Cambridge, MA},
	series = {Adaptive computation and machine learning series},
	title = {Machine learning: a probabilistic perspective},
	isbn = {978-0-262-01802-9},
	shorttitle = {Machine learning},
	publisher = {MIT Press},
	author = {Murphy, Kevin P.},
	year = {2012},
	keywords = {Probabilities, machine learning},
}

@article{su_longterm_1992a,
	title = {Long-{Term} {Predictions} of {Chemical} {Processes} {Using} {Recurrent} {Neural} {Networks}: {A} {Parallel} {Training} {Approach}},
	volume = {31},
	doi = {10/cc9djd},
	number = {5},
	journal = {Industrial \& Engineering Chemistry Research},
	author = {Su, Hong Te and McAvoy, Thomas J and Werbos, Paul},
	year = {1992},
	pages = {1338--1352},
}

@article{leonov_lyapunov_1992,
	title = {Lyapunov's direct method in the estimation of the {Hausdorff} dimension of attractors},
	volume = {26},
	issn = {0167-8019, 1572-9036},
	url = {http://link.springer.com/10.1007/BF00046607},
	doi = {10.1007/BF00046607},
	abstract = {This paper surveys results of the authors and others concerningestimatesfor the Hausdorffdimensionof strange attractors,particularlyin the case of (generalized)Lorenz systems and ROsslersystems. A key idea is the interpretationof Hausdorffmeasure as an analogue of a Lyapunovfunction.},
	language = {en},
	number = {1},
	urldate = {2019-12-27},
	journal = {Acta Applicandae Mathematicae},
	author = {Leonov, G. A. and Boichenko, V. A.},
	month = jan,
	year = {1992},
	pages = {1--60},
}

@article{ghorbani_linearized_2020,
	title = {Linearized two-layers neural networks in high dimension},
	url = {http://arxiv.org/abs/1904.12191},
	abstract = {We consider the problem of learning an unknown function \$f\_\{{\textbackslash}star\}\$ on the \$d\$-dimensional sphere with respect to the square loss, given i.i.d. samples \${\textbackslash}\{(y\_i,\{{\textbackslash}boldsymbol x\}\_i){\textbackslash}\}\_\{i{\textbackslash}le n\}\$ where \$\{{\textbackslash}boldsymbol x\}\_i\$ is a feature vector uniformly distributed on the sphere and \$y\_i=f\_\{{\textbackslash}star\}(\{{\textbackslash}boldsymbol x\}\_i)+{\textbackslash}varepsilon\_i\$. We study two popular classes of models that can be regarded as linearizations of two-layers neural networks around a random initialization: the random features model of Rahimi-Recht (RF); the neural tangent kernel model of Jacot-Gabriel-Hongler (NT). Both these approaches can also be regarded as randomized approximations of kernel ridge regression (with respect to different kernels), and enjoy universal approximation properties when the number of neurons \$N\$ diverges, for a fixed dimension \$d\$. We consider two specific regimes: the approximation-limited regime, in which \$n={\textbackslash}infty\$ while \$d\$ and \$N\$ are large but finite; and the sample size-limited regime in which \$N={\textbackslash}infty\$ while \$d\$ and \$n\$ are large but finite. In the first regime we prove that if \$d{\textasciicircum}\{{\textbackslash}ell + {\textbackslash}delta\} {\textbackslash}le N{\textbackslash}le d{\textasciicircum}\{{\textbackslash}ell+1-{\textbackslash}delta\}\$ for small \${\textbackslash}delta {\textgreater} 0\$, then {\textbackslash}RF{\textbackslash}, effectively fits a degree-\${\textbackslash}ell\$ polynomial in the raw features, and {\textbackslash}NT{\textbackslash}, fits a degree-\$({\textbackslash}ell+1)\$ polynomial. In the second regime, both RF and NT reduce to kernel methods with rotationally invariant kernels. We prove that, if the number of samples is \$d{\textasciicircum}\{{\textbackslash}ell + {\textbackslash}delta\} {\textbackslash}le n {\textbackslash}le d{\textasciicircum}\{{\textbackslash}ell +1-{\textbackslash}delta\}\$, then kernel methods can fit at most a a degree-\${\textbackslash}ell\$ polynomial in the raw features. This lower bound is achieved by kernel ridge regression. Optimal prediction error is achieved for vanishing ridge regularization.},
	urldate = {2021-04-23},
	journal = {arXiv:1904.12191 [cs, math, stat]},
	author = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
	month = feb,
	year = {2020},
	note = {arXiv: 1904.12191},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory},
}

@article{bhagoji_lower_2019,
	title = {Lower {Bounds} on {Adversarial} {Robustness} from {Optimal} {Transport}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/02bf86214e264535e3412283e817deaa-Abstract.html},
	language = {en},
	urldate = {2021-05-16},
	journal = {Advances in Neural Information Processing Systems},
	author = {Bhagoji, Arjun Nitin and Cullina, Daniel and Mittal, Prateek},
	year = {2019},
}

@inproceedings{rudi_less_2015,
	title = {Less is more: {Nyström} computational regularization},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/file/03e0704b5690a2dee1861dc3ad3316c9-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
	editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
}

@inproceedings{huster_limitations_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Limitations of the {Lipschitz} {Constant} as a {Defense} {Against} {Adversarial} {Examples}},
	isbn = {978-3-030-13453-2},
	doi = {10.1007/978-3-030-13453-2_2},
	abstract = {Several recent papers have discussed utilizing Lipschitz constants to limit the susceptibility of neural networks to adversarial examples. We analyze recently proposed methods for computing the Lipschitz constant. We show that the Lipschitz constant may indeed enable adversarially robust neural networks. However, the methods currently employed for computing it suffer from theoretical and practical limitations. We argue that addressing this shortcoming is a promising direction for future research into certified adversarial defenses.},
	language = {en},
	booktitle = {{ECML} {PKDD} 2018 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Huster, Todd and Chiang, Cho-Yu Jason and Chadha, Ritu},
	editor = {Alzate, Carlos and Monreale, Anna and Assem, Haytham and Bifet, Albert and Buda, Teodora Sandra and Caglayan, Bora and Drury, Brett and García-Martín, Eva and Gavaldà, Ricard and Koprinska, Irena and Kramer, Stefan and Lavesson, Niklas and Madden, Michael and Molloy, Ian and Nicolae, Maria-Irina and Sinn, Mathieu},
	year = {2019},
	keywords = {Adversarial examples, Lipschitz constant},
	pages = {16--29},
}

@article{frankle_linear_2020,
	title = {Linear {Mode} {Connectivity} and the {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1912.05671},
	abstract = {We introduce "instability analysis," which assesses whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise. We find that standard vision models become "stable" in this way early in training. From then on, the outcome of optimization is determined to within a linearly connected region. We use instability to study "iterative magnitude pruning" (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained to full accuracy from initialization. We find that these subnetworks only reach full accuracy when they are stable, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (Resnet-50 and Inception-v3 on ImageNet). This submission subsumes 1903.01611 ("Stabilizing the Lottery Ticket Hypothesis" and "The Lottery Ticket Hypothesis at Scale")},
	urldate = {2020-07-04},
	journal = {arXiv:1912.05671 [cs, stat]},
	author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
	month = feb,
	year = {2020},
	note = {arXiv: 1912.05671},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{wong_learning_2020,
	title = {Learning perturbation sets for robust machine learning},
	url = {http://arxiv.org/abs/2007.08450},
	abstract = {Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline digit transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to learn models which have improved generalization performance and are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at https://github.com/locuslab/perturbation\_learning.},
	urldate = {2020-07-27},
	journal = {arXiv:2007.08450 [cs, stat]},
	author = {Wong, Eric and Kolter, J. Zico},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.08450},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{carratino_learning_2019,
	title = {Learning with {SGD} and {Random} {Features}},
	url = {http://arxiv.org/abs/1807.06343},
	abstract = {Sketching and stochastic gradient methods are arguably the most common techniques to derive efficient large scale learning algorithms. In this paper, we investigate their application in the context of nonparametric statistical learning. More precisely, we study the estimator defined by stochastic gradient with mini batches and random features. The latter can be seen as form of nonlinear sketching and used to define approximate kernel methods. The considered estimator is not explicitly penalized/constrained and regularization is implicit. Indeed, our study highlights how different parameters, such as number of features, iterations, step-size and mini-batch size control the learning properties of the solutions. We do this by deriving optimal finite sample bounds, under standard assumptions. The obtained results are corroborated and illustrated by numerical experiments.},
	urldate = {2020-08-10},
	journal = {arXiv:1807.06343 [cs, stat]},
	author = {Carratino, Luigi and Rudi, Alessandro and Rosasco, Lorenzo},
	month = jan,
	year = {2019},
	note = {arXiv: 1807.06343},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{li_learning_2019,
	title = {Learning {Overparameterized} {Neural} {Networks} via {Stochastic} {Gradient} {Descent} on {Structured} {Data}},
	url = {http://arxiv.org/abs/1808.01204},
	abstract = {Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to fit arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset.},
	urldate = {2020-08-10},
	journal = {arXiv:1808.01204 [cs, stat]},
	author = {Li, Yuanzhi and Liang, Yingyu},
	month = aug,
	year = {2019},
	note = {arXiv: 1808.01204},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{li_limit_2011,
	title = {Limit {Distributions} of {Eigenvalues} for {Random} {Block} {Toeplitz} and {Hankel} {Matrices}},
	volume = {24},
	issn = {0894-9840, 1572-9230},
	url = {http://link.springer.com/10.1007/s10959-010-0326-3},
	doi = {10.1007/s10959-010-0326-3},
	abstract = {Block Toeplitz and Hankel matrices arise in many aspects of applications. In this paper, we will research the distributions of eigenvalues for some models and get the semicircle law. Firstly we will give trace formulas of block Toeplitz and Hankel matrix. Then we will prove that the almost sure limit γT(m) (γH(m)) of eigenvalue distributions of random block Toeplitz (Hankel) matrices exist and give the moments of the limit distributions where m is the order of the blocks. Then we will prove the existence of almost sure limit of eigenvalue distributions of random block Toeplitz and Hankel band matrices and give the moments of the limit distributions. Finally we will prove that γT(m) (γH(m)) converges weakly to the semicircle law as m → ∞.},
	language = {en},
	number = {4},
	urldate = {2020-11-14},
	journal = {Journal of Theoretical Probability},
	author = {Li, Yi-Ting and Liu, Dang-Zheng and Wang, Zheng-Dong},
	month = dec,
	year = {2011},
	pages = {1063--1086},
}

@article{valko_lectures_,
	title = {Lectures 6 – 7 : {Marchenko}-{Pastur} {Law}},
	language = {en},
	author = {Valko, B},
	pages = {7},
}

@incollection{li_learning_2018,
	title = {Learning {Overparameterized} {Neural} {Networks} via {Stochastic} {Gradient} {Descent} on {Structured} {Data}},
	url = {http://papers.nips.cc/paper/8038-learning-overparameterized-neural-networks-via-stochastic-gradient-descent-on-structured-data.pdf},
	urldate = {2018-12-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Li, Yuanzhi and Liang, Yingyu},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {8167--8176},
}

@article{umenberger_linear_2016,
	title = {Linear {System} {Identification} via {EM} with {Latent} {Disturbances} and {Lagrangian} {Relaxation}},
	url = {http://arxiv.org/abs/1603.09157},
	abstract = {In the application of the Expectation Maximization algorithm to identification of dynamical systems, internal states are typically chosen as latent variables, for simplicity. In this work, we propose a different choice of latent variables, namely, system disturbances. Such a formulation elegantly handles the problematic case of singular state space models, and is shown, under certain circumstances, to improve the fidelity of bounds on the likelihood, leading to convergence in fewer iterations. To access these benefits we develop a Lagrangian relaxation of the nonconvex optimization problems that arise in the latent disturbances formulation, and proceed via semidefinite programming.},
	journal = {arXiv:1603.09157 [cs, stat]},
	author = {Umenberger, Jack and Wågberg, Johan and Manchester, Ian R. and Schön, Thomas B.},
	month = mar,
	year = {2016},
	note = {00000 
arXiv: 1603.09157},
	keywords = {Computer Science - Systems and Control, Statistics - Computation, 🔍No DOI found},
}

@article{falck_leastsquares_2012,
	series = {Special {Section}: {Wiener}-{Hammerstein} {System} {Identification} {Benchmark}},
	title = {Least-{Squares} {Support} {Vector} {Machines} for the identification of {Wiener}–{Hammerstein} systems},
	volume = {20},
	issn = {0967-0661},
	url = {http://www.sciencedirect.com/science/article/pii/S0967066112001098},
	doi = {10.1016/j.conengprac.2012.05.006},
	abstract = {This paper considers the identification of Wiener–Hammerstein systems using Least-Squares Support Vector Machines based models. The power of fully black-box NARX-type models is evaluated and compared with models incorporating information about the structure of the systems. For the NARX models it is shown how to extend the kernel-based estimator to large data sets. For the structured model the emphasis is on preserving the convexity of the estimation problem through a suitable relaxation of the original problem. To develop an empirical understanding of the implications of the different model design choices, all considered models are compared on an artificial system under a number of different experimental conditions. The obtained results are then validated on the Wiener–Hammerstein benchmark data set and the final models are presented. It is illustrated that black-box models are a suitable technique for the identification of Wiener–Hammerstein systems. The incorporation of structural information results in significant improvements in modeling performance.},
	number = {11},
	journal = {Control Engineering Practice},
	author = {Falck, Tillmann and Dreesen, Philippe and De Brabanter, Kris and Pelckmans, Kristiaan and De Moor, Bart and Suykens, Johan A. K.},
	month = nov,
	year = {2012},
	keywords = {Kernel-based models, LS-SVMs, Large-scale data processing, Nonlinear system identification, Overparameterization},
	pages = {1165--1174},
}

@article{efron_least_2004,
	title = {Least angle regression},
	volume = {32},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1083178935},
	doi = {10.1214/009053604000000067},
	abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
	number = {2},
	urldate = {2017-09-04},
	journal = {The Annals of Statistics},
	author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
	month = apr,
	year = {2004},
	mrnumber = {MR2060166},
	zmnumber = {1091.62054},
	keywords = {Lasso, boosting, coefficient paths, linear regression, variable selection},
	pages = {407--499},
}

@article{zoph_learning_2017,
	title = {Learning {Transferable} {Architectures} for {Scalable} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1707.07012},
	abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4\% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.},
	journal = {arXiv:1707.07012 [cs, stat]},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	month = jul,
	year = {2017},
	note = {00171 
arXiv: 1707.07012},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Statistics - Machine Learning, 🔍No DOI found},
}

@book{strang_linear_2006,
	title = {Linear {Algebra} and {Its} {Applications}},
	isbn = {978-0-03-010567-8},
	url = {https://books.google.com.br/books?id=8QVdcRJyL2oC},
	publisher = {Thomson, Brooks/Cole},
	author = {Strang, G.},
	year = {2006},
	note = {00000},
}

@book{luenberger_linear_2008,
	address = {New York, NY},
	edition = {3rd ed},
	series = {International series in operations research and management science},
	title = {Linear and nonlinear programming},
	isbn = {978-0-387-74502-2},
	publisher = {Springer},
	author = {Luenberger, David G. and Ye, Yinyu},
	year = {2008},
	keywords = {Linear programming, Nonlinear programming},
}

@inproceedings{martens_learning_2011,
	address = {USA},
	series = {{ICML}'11},
	title = {Learning {Recurrent} {Neural} {Networks} with {Hessian}-free {Optimization}},
	isbn = {978-1-4503-0619-5},
	url = {http://dl.acm.org/citation.cfm?id=3104482.3104612},
	abstract = {In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-the-art method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of Schraudolph (2002) which is used within the HF approach of Martens.},
	urldate = {2019-04-23},
	booktitle = {Proceedings of the 28th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Martens, James and Sutskever, Ilya},
	year = {2011},
	note = {event-place: Bellevue, Washington, USA},
	pages = {1033--1040},
}

@book{suykens_least_2002,
	title = {Least squares support vector machines},
	publisher = {World Scientific},
	author = {Suykens, Johan AK and Van Gestel, Tony and De Brabanter, Jos},
	year = {2002},
	note = {00000},
}

@inproceedings{cho_learning_2014,
	address = {Doha, Qatar},
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}–{Decoder} for {Statistical} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/D14-1179},
	doi = {10.3115/v1/D14-1179},
	urldate = {2019-11-22},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Cho, Kyunghyun and van Merriënboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = oct,
	year = {2014},
	pages = {1724--1734},
}

@article{rumelhart_learning_1988,
	title = {Learning representations by back-propagating errors},
	volume = {5},
	number = {3},
	journal = {Cognitive modeling},
	author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	year = {1988},
	pages = {1},
}

@book{rigollet_lecturenotes_2015,
	title = {{LectureNotes}: 18.{S997}- {HighDimensionalStatistics}},
	author = {Rigollet, Philippe},
	year = {2015},
}

@inproceedings{masti_learning_2018,
	title = {Learning {Nonlinear} {State}-{Space} {Models} {Using} {Deep} {Autoencoders}},
	isbn = {2576-2370},
	doi = {10/gfwtwq},
	booktitle = {2018 {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Masti, D. and Bemporad, A.},
	year = {2018},
	keywords = {Computational modeling, Neural networks, Nonlinear systems, Observers, State-space methods, Training, Tuning, deep autoencoders, dimensionality reduction, direct acyclic computational graph, directed graphs, learning (artificial intelligence), machine-learning techniques, model order reduction, neural networks, neural state observer, neurocontrollers, nonlinear control systems, nonlinear model predictive control, nonlinear state-space models, nonlinear system, observers, predictive control, reduced order systems, state-space methods, state-update maps},
	pages = {3862--3867},
}

@article{wu_learning_2019,
	title = {Learning a {Compressed} {Sensing} {Measurement} {Matrix} via {Gradient} {Unrolling}},
	url = {http://arxiv.org/abs/1806.10175},
	abstract = {Linear encoding of sparse vectors is widely popular, but is commonly data-independent -- missing any possible extra (but a priori unknown) structure beyond sparsity. In this paper we present a new method to learn linear encoders that adapt to data, while still performing well with the widely used \${\textbackslash}ell\_1\$ decoder. The convex \${\textbackslash}ell\_1\$ decoder prevents gradient propagation as needed in standard gradient-based training. Our method is based on the insight that unrolling the convex decoder into \$T\$ projected subgradient steps can address this issue. Our method can be seen as a data-driven way to learn a compressed sensing measurement matrix. We compare the empirical performance of 10 algorithms over 6 sparse datasets (3 synthetic and 3 real). Our experiments show that there is indeed additional structure beyond sparsity in the real datasets; our method is able to discover it and exploit it to create excellent reconstructions with fewer measurements (by a factor of 1.1-3x) compared to the previous state-of-the-art methods. We illustrate an application of our method in learning label embeddings for extreme multi-label classification, and empirically show that our method is able to match or outperform the precision scores of SLEEC, which is one of the state-of-the-art embedding-based approaches.},
	urldate = {2020-07-20},
	journal = {arXiv:1806.10175 [cs, math, stat]},
	author = {Wu, Shanshan and Dimakis, Alexandros G. and Sanghavi, Sujay and Yu, Felix X. and Holtmann-Rice, Daniel and Storcheus, Dmitry and Rostamizadeh, Afshin and Kumar, Sanjiv},
	month = jul,
	year = {2019},
	note = {arXiv: 1806.10175},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{banerjee_largest_2017,
	title = {Largest eigenvalue of large random block matrices: {A} combinatorial approach},
	volume = {06},
	issn = {2010-3263, 2010-3271},
	shorttitle = {Largest eigenvalue of large random block matrices},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S2010326317500083},
	doi = {10.1142/S2010326317500083},
	abstract = {We study the largest eigenvalue of certain block matrices where the number of blocks and the block size both increase with suitable conditions on their relative growth. In one of them, we employ a symmetric block structure with large independent Wigner blocks and in the other we have the Wigner block structure with large independent symmetric blocks. The entries are assumed to be independent and identically distributed with mean [Formula: see text] variance [Formula: see text] with an appropriate growth condition on the moments. Under our conditions the limit spectral distribution of these matrices is the standard semi-circle law. It is natural to ask if the extreme eigenvalues converge to the extreme points of its support, namely [Formula: see text]. We exhibit models where this indeed happens as well as models where the spectral norm converges to [Formula: see text]. Our proofs are based on combinatorial analysis of the behavior of the trace of large powers of the matrix.},
	language = {en},
	number = {02},
	urldate = {2020-11-23},
	journal = {Random Matrices: Theory and Applications},
	author = {Banerjee, Debapratim and Bose, Arup},
	month = apr,
	year = {2017},
	pages = {1750008},
}

@inproceedings{veloso_lazy_2006,
	title = {Lazy {Associative} {Classification}},
	doi = {10/bhq7p6},
	abstract = {Decision tree classifiers perform a greedy search for rules by heuristically selecting the most promising features. Such greedy (local) search may discard important rules. Associative classifiers, on the other hand, perform a global search for rules satisfying some quality constraints (i.e., minimum support). This global search, however, may generate a large number of rules. Further, many of these rules may be useless during classification, and worst, important rules may never be mined. Lazy (non-eager) associative classification overcomes this problem by focusing on the features of the given test instance, increasing the chance of generating more rules that are useful for classifying the test instance. In this paper we assess the performance of lazy associative classification. First we demonstrate that an associative classifier performs no worse than the corresponding decision tree classifier. Also we demonstrate that lazy classifiers outperform the corresponding eager ones. Our claims are empirically confirmed by an extensive set of experimental results. We show that our proposed lazy associative classifier is responsible for an error rate reduction of approximately 10\% when compared against its eager counterpart, and for a reduction of 20\% when compared against a decision tree classifier. A simple caching mechanism makes lazy associative classification fast, and thus improvements in the execution time are also observed.},
	booktitle = {Proceedingsof the 6th {International} {Conference} on {Data} {Mining} ({ICDM})},
	author = {Veloso, Adriano and Meira Jr, Wagner and Zaki, M. J.},
	year = {2006},
	keywords = {Classification tree analysis, Computer science, Data mining, Decision trees, Error analysis, Genetic algorithms, Neural networks, Predictive models, Testing, Training data, associative rule mining, caching mechanism, data mining, decision tree classifier, decision trees, greedy algorithms, greedy search, lazy associative classification, lazy learning, learning (artificial intelligence), pattern classification, tree searching},
	pages = {645--654},
}

@article{calafiore_leading_2017,
	title = {Leading impulse response identification via the {Elastic} {Net} criterion},
	volume = {80},
	issn = {0005-1098},
	url = {http://www.sciencedirect.com/science/article/pii/S0005109817300213},
	doi = {10.1016/j.automatica.2017.01.011},
	abstract = {This paper deals with the problem of finding a low-complexity estimate of the impulse response of a linear time-invariant discrete-time dynamic system from noise-corrupted input–output data. To this purpose, we introduce an identification criterion formed by the average (over the input perturbations) of a standard prediction error cost, plus an ℓ1 regularization term which promotes sparse solutions. While it is well known that such criteria do provide solutions with many zeros, a critical issue in our identification context is where these zeros are located, since sensible low-order models should be zero in the tail of the impulse response. The flavor of the key results in this paper is that, under quite standard assumptions (such as i.i.d. input and noise sequences and system stability), the estimate of the impulse response resulting from the proposed criterion is indeed identically zero from a certain time index nl (named the leading order) onwards, with arbitrarily high probability, for a sufficiently large data cardinality N. Numerical experiments are reported that support the theoretical results, and comparisons are made with some other state-of-the-art methodologies.},
	number = {Supplement C},
	journal = {Automatica},
	author = {Calafiore, Giuseppe C. and Novara, Carlo and Taragna, Michele},
	month = jun,
	year = {2017},
	keywords = {Elastic net, FIR identification, Lasso, Regularization, Sparsity},
	pages = {75--87},
}

@book{bradski_learning_2008,
	title = {Learning {OpenCV}: {Computer} vision with the {OpenCV} library},
	publisher = {" O'Reilly Media, Inc."},
	author = {Bradski, Gary and Kaehler, Adrian},
	year = {2008},
}

@article{wong_lasso_2016,
	title = {Lasso {Guarantees} for {Time} {Series} {Estimation} {Under} {Subgaussian} {Tails} and \$ {\textbackslash}beta \$-{Mixing}},
	url = {http://arxiv.org/abs/1602.04265},
	abstract = {Many theoretical results on estimation of high dimensional time series require specifying an underlying data generating model (DGM). Instead, along the footsteps of{\textasciitilde}{\textbackslash}cite\{wong2017lasso\}, this paper relies only on (strict) stationarity and \$ {\textbackslash}beta \$-mixing condition to establish consistency of lasso when data comes from a \${\textbackslash}beta\$-mixing process with marginals having subgaussian tails. Because of the general assumptions, the data can come from DGMs different than standard time series models such as VAR or ARCH. When the true DGM is not VAR, the lasso estimates correspond to those of the best linear predictors using the past observations. We establish non-asymptotic inequalities for estimation and prediction errors of the lasso estimates. Together with{\textasciitilde}{\textbackslash}cite\{wong2017lasso\}, we provide lasso guarantees that cover full spectrum of the parameters in specifications of \$ {\textbackslash}beta \$-mixing subgaussian time series. Applications of these results potentially extend to non-Gaussian, non-Markovian and non-linear times series models as the examples we provide demonstrate. In order to prove our results, we derive a novel Hanson-Wright type concentration inequality for \${\textbackslash}beta\$-mixing subgaussian random vectors that may be of independent interest.},
	journal = {arXiv:1602.04265 [cs, stat]},
	author = {Wong, Kam Chung and Li, Zifan and Tewari, Ambuj},
	month = feb,
	year = {2016},
	note = {00000 
arXiv: 1602.04265},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, 🔍No DOI found},
}

@article{umenberger_learning_2018,
	title = {Learning convex bounds for linear quadratic control policy synthesis},
	url = {http://arxiv.org/abs/1806.00319},
	abstract = {Learning to make decisions from observed data in dynamic environments remains a problem of fundamental importance in a number of fields, from artificial intelligence and robotics, to medicine and finance. This paper concerns the problem of learning control policies for unknown linear dynamical systems so as to maximize a quadratic reward function. We present a method to optimize the expected value of the reward over the posterior distribution of the unknown system parameters, given data. The algorithm involves sequential convex programing, and enjoys reliable local convergence and robust stability guarantees. Numerical simulations and stabilization of a real-world inverted pendulum are used to demonstrate the approach, with strong performance and robustness properties observed in both.},
	journal = {arXiv:1806.00319 [cs, math, stat]},
	author = {Umenberger, Jack and Schön, Thomas B.},
	month = jun,
	year = {2018},
	note = {00000 
arXiv: 1806.00319},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, 🔍No DOI found},
}

@inproceedings{calafiore_leading_2016,
	title = {Leading impulse response identification via the weighted elastic net criterion},
	doi = {10.1109/CDC.2016.7798705},
	abstract = {This paper deals with the problem of finding a low-complexity estimate of the impulse response of a linear time-invariant discrete-time dynamic system from noise-corrupted input-output data. To this purpose, we introduce an identification criterion formed by the average (over the input perturbations) of a standard prediction error cost, plus a weighted ℓ1 regularization term which promotes sparse solutions. While it is well known that such criteria do provide solutions with many zeros, a critical issue in our identification context is where these zeros are located, since sensible low-order models should be zero in the tail of the impulse response. The flavor of the key results in this paper is that, under quite standard assumptions (such as i.i.d. input and noise sequences and system stability), the estimate of the impulse response resulting from the proposed criterion is indeed identically zero from a certain time index nl (named the leading order) onwards, with arbitrarily high probability, for a sufficiently large data cardinality N. Numerical experiments are reported that support the theoretical results, and comparisons are made with some other state-of-the-art methodologies.},
	booktitle = {2016 {IEEE} 55th {Conference} on {Decision} and {Control} ({CDC})},
	author = {Calafiore, G. C. and Novara, C. and Taragna, M.},
	month = dec,
	year = {2016},
	keywords = {Context, Convergence, Noise measurement, Random variables, Standards, Time measurement, data cardinality, discrete time systems, impulse response estimation, input perturbation, leading impulse response identification, linear systems, linear time-invariant discrete-time dynamic system, noise sequence, noise-corrupted input-output data, nonparametric method, nonparametric statistics, parameter estimation, poles and zeros, prediction error cost, probability, stability, system stability, time index, transient response, weighted elastic net criterion, weighted ℓ1 regularization term, zeros},
	pages = {2926--2931},
}

@article{lin_learning_1996,
	title = {Learning long-term dependencies in {NARX} recurrent neural networks},
	volume = {7},
	issn = {1045-9227},
	doi = {10.1109/72.548162},
	abstract = {It has previously been shown that gradient-descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies, i.e. those problems for which the desired output depends on inputs presented at times far in the past. We show that the long-term dependencies problem is lessened for a class of architectures called nonlinear autoregressive models with exogenous (NARX) recurrent neural networks, which have powerful representational capabilities. We have previously reported that gradient descent learning can be more effective in NARX networks than in recurrent neural network architectures that have “hidden states” on problems including grammatical inference and nonlinear system identification. Typically, the network converges much faster and generalizes better than other networks. The results in this paper are consistent with this phenomenon. We present some experimental results which show that NARX networks can often retain information for two to three times as long as conventional recurrent neural networks. We show that although NARX networks do not circumvent the problem of long-term dependencies, they can greatly improve performance on long-term dependency problems. We also describe in detail some of the assumptions regarding what it means to latch information robustly and suggest possible ways to loosen these assumptions},
	number = {6},
	journal = {IEEE Transactions on Neural Networks},
	author = {Lin, Tsungnan and Horne, B. G. and Tino, P. and Giles, C. L.},
	month = nov,
	year = {1996},
	keywords = {Computer science, Ear, Intelligent networks, NARX recurrent neural networks, National electric code, Neural networks, Nonlinear system identification, Power system modeling, Recurrent neural networks, Robustness, autoregressive processes, generalisation (artificial intelligence), grammatical inference, information latching, information retention, learning (artificial intelligence), long-term dependencies, nonlinear autoregressive models with exogenous recurrent neural networks, nonlinear dynamical systems, recurrent neural nets, representational capabilities, system identification},
	pages = {1329--1338},
}

@article{bengio_learning_1994,
	title = {Learning long-term dependencies with gradient descent is difficult},
	volume = {5},
	issn = {1045-9227},
	doi = {10.1109/72.279181},
	abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered},
	number = {2},
	journal = {IEEE Transactions on Neural Networks},
	author = {Bengio, Y. and Simard, P. and Frasconi, P.},
	month = mar,
	year = {1994},
	keywords = {Computer networks, Cost function, Delay effects, Discrete transforms, Displays, Intelligent networks, Neural networks, Neurofeedback, Production, Recurrent neural networks, efficient learning, gradient descent, input/output sequence mapping, learning (artificial intelligence), long-term dependencies, numerical analysis, prediction problems, production problems, recognition, recurrent neural nets, recurrent neural network training, temporal contingencies},
	pages = {157--166},
}

@book{bradski_learning_2011,
	address = {Beijing},
	edition = {1. ed., [Nachdr.]},
	series = {Software that sees},
	title = {Learning {OpenCV}: computer vision with the {OpenCV} library},
	isbn = {978-0-596-51613-0},
	shorttitle = {Learning {OpenCV}},
	language = {eng},
	publisher = {O'Reilly},
	author = {Bradski, Gary R. and Kaehler, Adrian},
	year = {2011},
	note = {OCLC: 838472784},
}

@techreport{rumelhart_learning_1985,
	title = {Learning internal representations by error propagation},
	url = {http://www.dtic.mil/docs/citations/ADA164453},
	urldate = {2017-09-10},
	institution = {California Univ San Diego La Jolla Inst for Cognitive Science},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	year = {1985},
	note = {00000},
}

@inproceedings{terzi_learning_2018,
	title = {Learning multi-step prediction models for receding horizon control},
	doi = {10/gfxx69},
	abstract = {In this paper, the derivation of multi-step-ahead prediction models from sampled input-output data of a linear system is considered. Specifically, a dedicated prediction model is built for each future time step of interest. Each model is linearly parametrized in a suitable regressor vector, composed of past output values and past and future input values. In addition to a nominal model, the set of all models consistent with data and prior information is derived as well, making the approach suitable for robust control design within a Model Predictive Control framework. The resulting parameter identification problem is solved through a sequence of convex programs. Convergence of the identified error bounds to their theoretical minimum is demonstrated, under suitable assumptions on the measured data, and features like worst-case accuracy computation are illustrated in a numerical example.},
	booktitle = {2018 {European} {Control} {Conference} ({ECC})},
	author = {Terzi, E. and Fagiano, L. and Farina, M. and Scattolini, R.},
	month = jun,
	year = {2018},
	keywords = {Computational modeling, Data models, Linear systems, Noise measurement, Numerical models, Predictive models, Uncertainty, control system synthesis, convex programming, dedicated prediction model, future input values, horizon control, learning (artificial intelligence), learning multistep prediction models, linear system, linear systems, model predictive control framework, multistep-ahead prediction models, nominal model, output values, parameter estimation, predictive control, robust control, robust control design, sampled input-output data, suitable assumptions, suitable regressor vector},
	pages = {1335--1340},
}

@article{ba_layer_2016,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2019-05-29},
	journal = {arXiv:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.06450},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{bonin_lassoenhanced_2010,
	title = {{LASSO}-enhanced simulation error minimization method for {NARX} model selection},
	url = {http://ieeexplore.ieee.org/abstract/document/5530859/},
	urldate = {2017-09-13},
	booktitle = {American {Control} {Conference} ({ACC}), 2010},
	publisher = {IEEE},
	author = {Bonin, Mariangela and Seghezza, Valerio and Piroddi, Luigi},
	year = {2010},
	pages = {4522--4527},
}

@inproceedings{hinton_learning_1986,
	title = {Learning distributed representations of concepts},
	volume = {1},
	publisher = {Amherst, MA},
	author = {Hinton, Geoffrey E},
	year = {1986},
	pages = {12},
}

@phdthesis{ribeiro_learning_2020,
	address = {Belo Horizonte, Brazil},
	type = {{PhD} {Thesis}},
	title = {Learning nonlinear differentiable models for signals and systems:  with applications},
	copyright = {All rights reserved},
	school = {Universidade Federal de Minas Gerais},
	author = {Ribeiro, Antônio H.},
	year = {2020},
}

@inproceedings{cho_kernel_2009,
	title = {Kernel methods for deep learning},
	volume = {22},
	url = {https://proceedings.neurips.cc/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Cho, Youngmin and Saul, Lawrence},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. and Williams, C. and Culotta, A.},
	year = {2009},
}

@article{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2019},
}

@article{falck_kernel_2014,
	title = {Kernel based identification of systems with multiple outputs using nuclear norm regularization},
	issn = {1482241390},
	journal = {Regularization, Optimization, Kernels, and Support Vector Machines},
	author = {Falck, Tillmann and De Moor, Bart and Suykens, Johan AK},
	year = {2014},
	keywords = {🔍No DOI found},
	pages = {371},
}

@article{espinoza_kernel_2005,
	title = {Kernel based partially linear models and nonlinear identification},
	volume = {50},
	issn = {0018-9286},
	doi = {10.1109/TAC.2005.856656},
	abstract = {In this note, we propose partially linear models with least squares support vector machines (LS-SVMs) for nonlinear ARX models. We illustrate how full black-box models can be improved when prior information about model structure is available. A real-life example, based on the Silverbox benchmark data, shows significant improvements in the generalization ability of the structured model with respect to the full black-box model, reflected also by a reduction in the effective number of parameters.},
	number = {10},
	journal = {IEEE Transactions on Automatic Control},
	author = {Espinoza, M. and Suykens, J. A. K. and Moor, Bart De},
	month = oct,
	year = {2005},
	keywords = {Artificial neural networks, Councils, Kernel, Kernels, Least squares methods, Nonlinear equations, Nonlinear system identification, Parametric statistics, Predictive models, Silverbox benchmark data, autoregressive processes, full black box model, identification, kernel based partially linear model, least squares approximations, least squares support vector machine, least squares support vector machine (LS-SVM), linear systems, nonlinear ARX model, nonlinear control systems, nonlinear identification, nonlinear systems, partially linear models, polynomials, support vector machines},
	pages = {1602--1606},
}

@incollection{byrd_knitro_2006,
	title = {{KNITRO}: {An} integrated package for nonlinear optimization},
	booktitle = {Large-scale nonlinear optimization},
	publisher = {Springer},
	author = {Byrd, Richard H and Nocedal, Jorge and Waltz, Richard A},
	year = {2006},
	pages = {35--59},
}

@article{newey_large_1994a,
	title = {Large sample estimation and hypothesis testing},
	volume = {4},
	doi = {10.1016/S1573-4412(05)80005-4},
	journal = {Handbook of econometrics},
	author = {Newey, Whitney K. and McFadden, Daniel},
	year = {1994},
	pages = {2111--2245},
}

@article{voros_iterative_2015,
	title = {Iterative identification of nonlinear dynamic systems with output backlash using three-block cascade models},
	volume = {79},
	issn = {0924-090X},
	doi = {10.1007/s11071-014-1804-4},
	number = {3},
	journal = {Nonlinear Dynamics},
	author = {Vörös, Jozef},
	year = {2015},
	note = {00000},
	pages = {2187--2195},
}

@incollection{newey_large_1994,
	title = {Large sample estimation and hypothesis testing},
	abstract = {null},
	booktitle = {of {Handbook} of {Econometrics}},
	author = {Newey, Whitney K. and Mcfadden, Daniel},
	year = {1994},
	pages = {2111},
}

@article{clarke_jeffreys_1994,
	title = {Jeffreys' prior is asymptotically least favorable under entropy risk},
	volume = {41},
	issn = {03783758},
	url = {http://linkinghub.elsevier.com/retrieve/pii/0378375894901538},
	doi = {10.1016/0378-3758(94)90153-8},
	abstract = {We provide a rigorous proof that Jeffreys’ prior asymptotically maximizes Shannon’s mutual information between a sample of size n and the parameter. This was conjectured by Bernard0 (1979) and, despite the absence of a proof, forms the basis of the reference prior method in Bayesian statistical analysis. Our proof rests on an examination of large sample decision theoretic properties associated with the relative entropy or the Kullback-Leibler distance between probability density functions for independent and identically distributed random variables. For smooth finite-dimensional parametric families we derive an asymptotic expression for the minimax risk and for the related maximin risk. As a result, we show that, among continuous positive priors, Jeffreys’ prior uniquely achieves the asymptotic maximin value. In the discrete parameter case we show that, asymptotically, the Bayes risk reduces to the entropy of the prior so that the reference prior is seen to be the maximum entropy prior. We identify the physical significance of the risks by giving two information-theoretic interpretations in terms of probabilistic coding.},
	language = {en},
	number = {1},
	urldate = {2018-10-07},
	journal = {Journal of Statistical Planning and Inference},
	author = {Clarke, Bertrand S. and Barron, Andrew R.},
	month = aug,
	year = {1994},
	pages = {37--60},
}

@article{petrovic_kalman_2013,
	title = {Kalman filter and {NARX} neural network for robot vision based human tracking},
	volume = {12},
	number = {1},
	journal = {Facta Universitatis, Series: Automatic Control And Robotics},
	author = {Petrović, Emina and Ćojbašić, Žarko and Ristić-Durrant, Danijela and Nikolić, Vlastimir and Ćirić, Ivan and Matić, Sr{\textbackslash}d jan},
	year = {2013},
	note = {00000},
	keywords = {🔍No DOI found},
	pages = {43--51},
}

@inproceedings{dauphin_language_2017,
	title = {Language modeling with gated convolutional networks},
	publisher = {JMLR. org},
	author = {Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
	year = {2017},
	pages = {933--941},
}

@incollection{byrd_knitro_2006a,
	title = {{KNITRO}: {An} {Integrated} {Package} for {Nonlinear} {Optimization}},
	booktitle = {Large-{Scale} {Nonlinear} {Optimization}},
	publisher = {Springer},
	author = {Byrd, Richard H and Nocedal, Jorge and Waltz, Richard A},
	year = {2006},
	pages = {35--59},
}

@article{petrovic_kalman_2013a,
	title = {Kalman {Filter} and {NARX} {Neural} {Network} for {Robot} {Vision} {Based} {Human} {Tracking}},
	volume = {12},
	number = {1},
	journal = {Facta Universitatis, Series: Automatic Control And Robotics},
	author = {Petrović, Emina and Ćojbašić, Žarko and Ristić-Durrant, Danijela and Nikolić, Vlastimir and Ćirić, Ivan and jan Matić, Sr{\textbackslash}d},
	year = {2013},
	note = {00009},
	keywords = {🔍No DOI found},
	pages = {43--51},
}

@book{yu_kernelbased_2013,
	title = {Kernel-based data fusion for machine learning},
	publisher = {Springer},
	author = {Yu, Shi and Tranchevent, Léon-Charles and De Moor, Bart and Moreau, Yves},
	year = {2013},
	note = {00045},
}

@article{geiger_jamming_2019,
	title = {Jamming transition as a paradigm to understand the loss landscape of deep neural networks},
	volume = {100},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.100.012115},
	doi = {10.1103/PhysRevE.100.012115},
	abstract = {Deep learning has been immensely successful at a variety of tasks, ranging from classification to artificial intelligence. Learning corresponds to fitting training data, which is implemented by descending a very high-dimensional loss function. Understanding under which conditions neural networks do not get stuck in poor minima of the loss, and how the landscape of that loss evolves as depth is increased, remains a challenge. Here we predict, and test empirically, an analogy between this landscape and the energy landscape of repulsive ellipses. We argue that in fully connected deep networks a phase transition delimits the over- and underparametrized regimes where fitting can or cannot be achieved. In the vicinity of this transition, properties of the curvature of the minima of the loss (the spectrum of the Hessian) are critical. This transition shares direct similarities with the jamming transition by which particles form a disordered solid as the density is increased, which also occurs in certain classes of computational optimization and learning problems such as the perceptron. Our analysis gives a simple explanation as to why poor minima of the loss cannot be encountered in the overparametrized regime. Interestingly, we observe that the ability of fully connected networks to fit random data is independent of their depth, an independence that appears to also hold for real data. We also study a quantity Δ which characterizes how well (Δ{\textless}0) or badly (Δ{\textgreater}0) a datum is learned. At the critical point it is power-law distributed on several decades, P+(Δ)∼Δθ for Δ{\textgreater}0 and P−(Δ)∼(−Δ)−γ for Δ{\textless}0, with exponents that depend on the choice of activation function. This observation suggests that near the transition the loss landscape has a hierarchical structure and that the learning dynamics is prone to avalanche-like dynamics, with abrupt changes in the set of patterns that are learned.},
	number = {1},
	urldate = {2021-05-23},
	journal = {Physical Review E},
	author = {Geiger, Mario and Spigler, Stefano and d'Ascoli, Stéphane and Sagun, Levent and Baity-Jesi, Marco and Biroli, Giulio and Wyart, Matthieu},
	month = jul,
	year = {2019},
	note = {Publisher: American Physical Society},
	pages = {012115},
}

@book{poljak_introduction_1987,
	title = {Introduction to optimization},
	publisher = {Optimization Software},
	author = {Poljak, Boris T.},
	year = {1987},
}

@article{livan_introduction_2018,
	title = {Introduction to {Random} {Matrices} - {Theory} and {Practice}},
	volume = {26},
	url = {http://arxiv.org/abs/1712.07903},
	doi = {10.1007/978-3-319-70885-0},
	abstract = {This is a book for absolute beginners. If you have heard about random matrix theory, commonly denoted RMT, but you do not know what that is, then welcome!, this is the place for you. Our aim is to provide a truly accessible introductory account of RMT for physicists and mathematicians at the beginning of their research career. We tried to write the sort of text we would have loved to read when we were beginning Ph.D. students ourselves. Our book is structured with light and short chapters, and the style is informal. The calculations we found most instructive are spelt out in full. Particular attention is paid to the numerical verification of most analytical results. Our book covers standard material - classical ensembles, orthogonal polynomial techniques, spectral densities and spacings - but also more advanced and modern topics - replica approach and free probability - that are not normally included in elementary accounts on RMT. This book is dedicated to the fond memory of Oriol Bohigas.},
	urldate = {2020-11-12},
	journal = {arXiv:1712.07903 [cond-mat, physics:math-ph]},
	author = {Livan, Giacomo and Novaes, Marcel and Vivo, Pierpaolo},
	year = {2018},
	note = {arXiv: 1712.07903},
	keywords = {Condensed Matter - Statistical Mechanics, Mathematical Physics},
}

@article{arbabi_introduction_,
	title = {Introduction to {Koopman} operator theory of dynamical systems},
	language = {en},
	author = {Arbabi, Hassan},
	pages = {32},
}

@book{trucco_introductory_1998,
	title = {Introductory techniques for 3-{D} computer vision},
	volume = {201},
	publisher = {Prentice Hall Englewood Cliffs},
	author = {Trucco, Emanuele and Verri, Alessandro},
	year = {1998},
	note = {00000},
}

@book{stoer_introduction_1980,
	title = {Introduction to {Numerical} {Analysis}},
	publisher = {Springer-Verlag, New York},
	author = {Stoer, J and Bulirsch, R},
	year = {1980},
	note = {00000},
}

@book{strang_introduction_2009,
	title = {Introduction to {Linear} {Algebra}},
	isbn = {978-0-9802327-1-4},
	url = {https://books.google.com.br/books?id=M19gPgAACAAJ},
	publisher = {Wellesley-Cambridge Press},
	author = {Strang, G.},
	year = {2009},
	note = {00000},
}

@book{mattuck_introduction_1999,
	title = {Introduction to {Analysis}},
	isbn = {978-0-13-081132-5},
	url = {https://books.google.com.br/books?id=N0FkQgAACAAJ},
	publisher = {Prentice Hall},
	author = {Mattuck, A.},
	year = {1999},
}

@book{cormen_introduction_2009,
	address = {Cambridge, Mass},
	edition = {3rd ed},
	title = {Introduction to algorithms},
	isbn = {978-0-262-03384-8 978-0-262-53305-8},
	publisher = {MIT Press},
	editor = {Cormen, Thomas H.},
	year = {2009},
	note = {OCLC: ocn311310321},
	keywords = {Computer algorithms, Computer programming},
}

@book{sipser_introduction_2006,
	address = {Boston},
	edition = {2nd ed},
	title = {Introduction to the theory of computation},
	isbn = {978-0-534-95097-2},
	publisher = {Thomson Course Technology},
	author = {Sipser, Michael},
	year = {2006},
	keywords = {Computational complexity, Machine theory},
}

@book{eiben_introduction_2003,
	title = {Introduction to evolutionary computing},
	volume = {53},
	publisher = {Springer},
	author = {Eiben, Agoston E and Smith, James E},
	year = {2003},
}

@book{bertsimas_introduction_1997,
	title = {Introduction to linear optimization},
	volume = {6},
	publisher = {Athena Scientific Belmont, MA},
	author = {Bertsimas, Dimitris and Tsitsiklis, John N},
	year = {1997},
}

@book{orfanidis_introduction_1996,
	address = {Englewood Cliffs, N.J},
	series = {Prentice {Hall} signal processing series},
	title = {Introduction to signal processing},
	isbn = {978-0-13-209172-5},
	publisher = {Prentice Hall},
	author = {Orfanidis, Sophocles J.},
	year = {1996},
	keywords = {Digital techniques, signal processing},
}

@book{kay_intuitive_2006,
	address = {New York},
	title = {Intuitive probability and random processes using {MATLAB}},
	isbn = {978-0-387-24157-9 978-0-387-24158-6},
	publisher = {Springer},
	author = {Kay, Steven M.},
	year = {2006},
	keywords = {Computer Simulation, MATLAB, Probabilities, Textbooks, stochastic processes},
}

@book{nesterov_introductory_1998,
	title = {Introductory {Lectures} {On} {Convex} {Programming}},
	abstract = {1.1.1 General formulation of the problem................... 9},
	publisher = {Springer Science \& Business Media},
	author = {Nesterov, Yu},
	year = {1998},
}

@book{stoer_introduction_1980a,
	title = {Introduction to {Numerical} {Analysis}},
	publisher = {Springer-Verlag, New York},
	author = {Stoer, J and Bulirsch, R},
	year = {1980},
}

@article{lee_introduction_2001,
	title = {Introduction to {Smooth} {Manifolds}},
	language = {en},
	author = {Lee, John M},
	year = {2001},
}

@article{mckinney_international_2020,
	title = {International evaluation of an {AI} system for breast cancer screening},
	volume = {577},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/s41586-019-1799-6},
	doi = {10.1038/s41586-019-1799-6},
	abstract = {Screening mammography aims to identify breast cancer at earlier stages of the disease, when treatment can be more successful1. Despite the existence of screening programmes worldwide, the interpretation of mammograms is affected by high rates of false positives and false negatives2. Here we present an artificial intelligence (AI) system that is capable of surpassing human experts in breast cancer prediction. To assess its performance in the clinical setting, we curated a large representative dataset from the UK and a large enriched dataset from the USA. We show an absolute reduction of 5.7\% and 1.2\% (USA and UK) in false positives and 9.4\% and 2.7\% in false negatives. We provide evidence of the ability of the system to generalize from the UK to the USA. In an independent study of six radiologists, the AI system outperformed all of the human readers: the area under the receiver operating characteristic curve (AUC-ROC) for the AI system was greater than the AUC-ROC for the average radiologist by an absolute margin of 11.5\%. We ran a simulation in which the AI system participated in the double-reading process that is used in the UK, and found that the AI system maintained non-inferior performance and reduced the workload of the second reader by 88\%. This robust assessment of the AI system paves the way for clinical trials to improve the accuracy and efficiency of breast cancer screening.},
	number = {7788},
	journal = {Nature},
	author = {McKinney, Scott Mayer and Sieniek, Marcin and Godbole, Varun and Godwin, Jonathan and Antropova, Natasha and Ashrafian, Hutan and Back, Trevor and Chesus, Mary and Corrado, Greg C. and Darzi, Ara and Etemadi, Mozziyar and Garcia-Vicente, Florencia and Gilbert, Fiona J. and Halling-Brown, Mark and Hassabis, Demis and Jansen, Sunny and Karthikesalingam, Alan and Kelly, Christopher J. and King, Dominic and Ledsam, Joseph R. and Melnick, David and Mostofi, Hormuz and Peng, Lily and Reicher, Joshua Jay and Romera-Paredes, Bernardino and Sidebottom, Richard and Suleyman, Mustafa and Tse, Daniel and Young, Kenneth C. and De Fauw, Jeffrey and Shetty, Shravya},
	month = jan,
	year = {2020},
	pages = {89--94},
}

@article{xia_internet_2012,
	title = {Internet of things},
	volume = {25},
	issn = {1074-5351},
	number = {9},
	journal = {International journal of communication systems},
	author = {Xia, Feng and Yang, Laurence T and Wang, Lizhe and Vinel, Alexey},
	year = {2012},
	pages = {1101},
}

@article{radford_improving_,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	pages = {12},
}

@article{alkmim_improving_2012,
	title = {Improving patient access to specialized health care: the {Telehealth} {Network} of {Minas} {Gerais}, {Brazil}},
	volume = {90},
	issn = {1564-0604},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/22589571},
	doi = {10/f3x7px},
	abstract = {PROBLEM: The Brazilian population lacks equitable access to specialized health care and diagnostic tests, especially in remote municipalities, where health professionals often feel isolated and staff turnover is high. Telehealth has the potential to improve patients' access to specialized health care, but little is known about it in terms of cost-effectiveness, access to services or user satisfaction. APPROACH: In 2005, the State Government of Minas Gerais, Brazil, funded the establishment of the Telehealth Network, intended to connect university hospitals with the state's remote municipal health departments; support professionals in providing tele-assistance; and perform tele-electrocardiography and teleconsultations. The network uses low-cost equipment and has employed various strategies to overcome the barriers to telehealth use. LOCAL SETTING: The Telehealth Network connects specialists in state university hospitals with primary health-care professionals in 608 municipalities of the large state of Minas Gerais, many of them in remote areas. RELEVANT CHANGES: From June 2006 to October 2011, 782,773 electrocardiograms and 30 883 teleconsultations were performed through the network, and 6000 health professionals were trained in its use. Most of these professionals (97\%) were satisfied with the system, which was cost-effective, economically viable and averted 81\% of potential case referrals to distant centres. LESSONS LEARNT: To succeed, a telehealth service must be part of a collaborative network, meet the real needs of local health professionals, use simple technology and have at least some face-to-face components. If applied to health problems for which care is in high demand, this type of service can be economically viable and can help to improve patient access to specialized health care.},
	number = {5},
	journal = {Bulletin of the World Health Organization},
	author = {Alkmim, Maria Beatriz and Figueira, Renato Minelli and Marcolino, Milena Soriano and Cardoso, Clareci Silva and Pena de Abreu, Monica and Cunha, Lemuel Rodrigues and da Cunha, Daniel Ferreira and Antunes, Andre Pires and Resende, Adélson Geraldo de A and Resende, Elmiro Santos and Ribeiro, Antonio Luiz Pinho},
	month = may,
	year = {2012},
	pages = {373--378},
}

@article{kim_interpretability_,
	title = {Interpretability {Beyond} {Feature} {Attribution}:  {Quantitative} {Testing} with {Concept} {Activation} {Vectors} ({TCAV})},
	abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classiﬁers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-deﬁned concept is important to a classiﬁcation result–for example, how sensitive a prediction of zebra is to the presence of stripes. Using the domain of image classiﬁcation as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classiﬁcation network as well as a medical application.},
	language = {en},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
	note = {00006},
	keywords = {deep learning reading group, 🔍No DOI found},
	pages = {10},
}

@book{aguirre_introducao_2004,
	title = {Introdução à identificação de sistemas–{Técnicas} lineares e não-lineares aplicadas a sistemas reais},
	publisher = {Editora UFMG},
	author = {Aguirre, Luis Antonio},
	year = {2004},
	note = {00654},
}

@article{carraro_indirect_2014,
	title = {Indirect multiple shooting for nonlinear parabolic optimal control problems with control constraints},
	volume = {36},
	issn = {1064-8275},
	doi = {10.1137/120895809},
	number = {2},
	journal = {SIAM Journal on Scientific Computing},
	author = {Carraro, T and Geiger, Michael and Rannacher, Rolf},
	year = {2014},
	pages = {A452--A481},
}

@inproceedings{kim_interpretability_2018,
	title = {Interpretability {Beyond} {Feature} {Attribution}: {Quantitative} {Testing} with {Concept} {Activation} {Vectors} ({TCAV})},
	shorttitle = {Interpretability {Beyond} {Feature} {Attribution}},
	url = {http://proceedings.mlr.press/v80/kim18d.html},
	abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level ...},
	language = {en},
	urldate = {2018-11-15},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
	month = jul,
	year = {2018},
	pages = {2668--2677},
}

@book{mackay_information_2003,
	title = {Information theory, inference and learning algorithms},
	publisher = {Cambridge university press},
	author = {MacKay, David JC and Mac Kay, David JC},
	year = {2003},
}

@inproceedings{manchester_input_2010,
	title = {Input design for system identification via convex relaxation},
	doi = {10.1109/CDC.2010.5717097},
	abstract = {We consider the problem of designing an excitation input for a system idenfication experiment. The optimization problem considered is to maximize a reduced Fisher information matrix in any of the classical D-, E-, or A-optimal senses. In contrast to the majority of published work on this topic, we consider the problem in the time domain and subject to constraints on the amplitude of the input signal. This optimization problem is nonconvex. The main result of the paper is a convex relaxation that gives an upper bound accurate to within 2/π of the true maximum. A randomized algorithm is presented for finding a feasible solution which, in a certain sense is expected to be at least 2/π as informative as the globally optimal input signal. In the case of a single constraint on input power, the proposed approach recovers the true global optimum exactly. Extensions to situations with both power and amplitude constraints on both inputs and outputs are given. A simple simulation example illustrates the technique.},
	booktitle = {49th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Manchester, I. R.},
	month = dec,
	year = {2010},
	keywords = {Binary sequences, Computational modeling, Fisher information matrix, Frequency domain analysis, Optimization, Time domain analysis, Tin, convex relaxation, excitation input, globally optimal input signal, identification, input design, matrix algebra, optimisation, optimization problem, randomised algorithms, randomized algorithm, system idenfication, system identification, time domain, upper bound},
	pages = {2041--2046},
}

@article{jansson_input_2005,
	title = {Input design via {LMIs} admitting frequency-wise model specifications in confidence regions},
	volume = {50},
	issn = {0018-9286},
	doi = {10.1109/TAC.2005.856652},
	abstract = {A framework for reformulating input design problems in prediction error identification as convex optimization problems is presented. For linear time-invariant single input/single output systems, this framework unifies and extends existing results on open-loop input design that are based on the finite dimensional asymptotic covariance matrix of the parameter estimates. Basic methods for parametrizing the input spectrum are provided and conditions on these parametrizations that guarantee that all possible covariance matrices for the asymptotic distribution of the parameter estimates can be generated are provided. A wide range of model quality constraints can be handled. In particular, different frequency-by-frequency constraints can be used. This opens up new applications of input design in areas such as robust control. Furthermore, quality specifications can be imposed on all models in a confidence region. Thus, allowing for statements such as "with at least 99\% probability the model quality specifications will be satisfied".},
	number = {10},
	journal = {IEEE Transactions on Automatic Control},
	author = {Jansson, H. and Hjalmarsson, H.},
	month = oct,
	year = {2005},
	keywords = {Design optimization, Eigenvalues and eigenfunctions, Frequency, LMI, Predictive models, Transfer functions, Vectors, confidence regions, convex optimization, covariance matrices, covariance matrix, finite dimensional asymptotic covariance matrix, frequency wise model specification, input design, linear matrix inequalities, linear systems, linear time invariant single input single output system, multidimensional systems, open loop input design, open loop systems, parameter estimation, prediction error identification, robust control, system identification},
	pages = {1534--1549},
}

@article{langone_incremental_2014,
	title = {Incremental kernel spectral clustering for online learning of non-stationary data},
	volume = {139},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231214004433},
	doi = {10.1016/j.neucom.2014.02.036},
	abstract = {In this work a new model for online clustering named Incremental kernel spectral clustering (IKSC) is presented. It is based on kernel spectral clustering (KSC), a model designed in the Least Squares Support Vector Machines (LS-SVMs) framework, with primal-dual setting. The IKSC model is developed to quickly adapt itself to a changing environment, in order to learn evolving clusters with high accuracy. In contrast with other existing incremental spectral clustering approaches, the eigen-updating is performed in a model-based manner, by exploiting one of the Karush–Kuhn–Tucker (KKT) optimality conditions of the KSC problem. We test the capacities of IKSC with some experiments conducted on computer-generated data and a real-world data-set of PM10 concentrations registered during a pollution episode occurred in Northern Europe in January 2010. We observe that our model is able to precisely recognize the dynamics of shifting patterns in a non-stationary context.},
	journal = {Neurocomputing},
	author = {Langone, Rocco and Mauricio Agudelo, Oscar and De Moor, Bart and Suykens, Johan A. K.},
	month = sep,
	year = {2014},
	keywords = {Incremental kernel spectral clustering, LS-SVMs, Non-stationary data, Online clustering, Out-of-sample eigenvectors, PM concentrations},
	pages = {246--260},
}

@article{carraro_indirect_2014a,
	title = {Indirect {Multiple} {Shooting} for {Nonlinear} {Parabolic} {Optimal} {Control} {Problems} with {Control} {Constraints}},
	volume = {36},
	issn = {1064-8275},
	doi = {10/f54rgr},
	number = {2},
	journal = {SIAM Journal on Scientific Computing},
	author = {Carraro, T and Geiger, Michael and Rannacher, Rolf},
	year = {2014},
	pages = {A452--A481},
}

@article{paduart_identification_2010,
	title = {Identification of nonlinear systems using {Polynomial} {Nonlinear} {State} {Space} models},
	volume = {46},
	issn = {0005-1098},
	url = {http://www.sciencedirect.com/science/article/pii/S000510981000021X},
	doi = {10.1016/j.automatica.2010.01.001},
	abstract = {In this paper, we propose a method to model nonlinear systems using polynomial nonlinear state space equations. Obtaining good initial estimates is a major problem in nonlinear modelling. It is solved here by identifying first the best linear approximation of the system under test. The proposed identification procedure is successfully applied to measurements of two physical systems.},
	number = {4},
	journal = {Automatica},
	author = {Paduart, Johan and Lauwers, Lieve and Swevers, Jan and Smolders, Kris and Schoukens, Johan and Pintelon, Rik},
	month = apr,
	year = {2010},
	keywords = {Best linear approximation, Multivariable systems, nonlinear systems, system identification},
	pages = {647--656},
}

@article{finlay_improved_2019,
	title = {Improved robustness to adversarial examples using {Lipschitz} regularization of the loss},
	url = {http://arxiv.org/abs/1810.00953},
	abstract = {We augment adversarial training (AT) with worst case adversarial training (WCAT) which improves adversarial robustness by 11\% over the current state-of-the-art result in the \${\textbackslash}ell\_2\$ norm on CIFAR-10. We obtain verifiable average case and worst case robustness guarantees, based on the expected and maximum values of the norm of the gradient of the loss. We interpret adversarial training as Total Variation Regularization, which is a fundamental tool in mathematical image processing, and WCAT as Lipschitz regularization.},
	urldate = {2020-07-07},
	journal = {arXiv:1810.00953 [cs, stat]},
	author = {Finlay, Chris and Oberman, Adam and Abbasi, Bilal},
	month = sep,
	year = {2019},
	note = {arXiv: 1810.00953},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{rossetto_improving_,
	title = {Improving {Classification} with {CNNs} using {Wavelet} {Pooling} with {Nesterov}-{Accelerated} {Adam}},
	url = {https://easychair.org/publications/paper/6Mbb},
	doi = {10.29007/9c5j},
	abstract = {Wavelet pooling methods can improve the classiﬁcation accuracy of Convolutional Neural Networks (CNNs). Combining wavelet pooling with the Nesterov-accelerated Adam (NAdam) gradient calculation method can improve both the accuracy of the CNN. We have implemented wavelet pooling with NAdam in this work using both a Haar wavelet (WavPool-NH ) and a Shannon wavelet (WavPool-NS ). The WavPool-NH and WavPoolNS methods are most accurate of the methods we considered for the MNIST and LIDCIDRI lung tumor data-sets. The WavPool-NH and WavPool-NS implementations have an accuracy of 95.92\% and 95.52\%, respectively, on the LIDC-IDRI data-set. This is an improvement from the 92.93\% accuracy obtained on this data-set with the max pooling method. The WavPool methods also avoid overﬁtting which is a concern with max pooling. We also found WavPool performed fairly well on the CIFAR-10 data-set, however, overﬁtting was an issue with all the methods we considered. Wavelet pooling, especially when combined with an adaptive gradient and wavelets chosen speciﬁcally for the data, has the potential to outperform current methods.},
	language = {en},
	urldate = {2020-07-07},
	author = {Rossetto, Allison and Zhou, Wenjin},
	pages = {84--73},
}

@article{gunasekar_implicit_2017,
	title = {Implicit {Regularization} in {Matrix} {Factorization}},
	url = {http://arxiv.org/abs/1705.09280},
	abstract = {We study implicit regularization when optimizing an underdetermined quadratic objective over a matrix \$X\$ with gradient descent on a factorization of \$X\$. We conjecture and provide empirical and theoretical evidence that with small enough step sizes and initialization close enough to the origin, gradient descent on a full dimensional factorization converges to the minimum nuclear norm solution.},
	urldate = {2020-08-09},
	journal = {arXiv:1705.09280 [cs, stat]},
	author = {Gunasekar, Suriya and Woodworth, Blake and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
	month = may,
	year = {2017},
	note = {arXiv: 1705.09280},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hardt_identity_2018,
	title = {Identity {Matters} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1611.04231},
	abstract = {An emerging design principle in deep learning is that each layer of a deep artiﬁcial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as batch normalization, but was also key to the immense success of residual networks.},
	language = {en},
	urldate = {2020-08-27},
	journal = {arXiv:1611.04231 [cs, stat]},
	author = {Hardt, Moritz and Ma, Tengyu},
	month = jul,
	year = {2018},
	note = {arXiv: 1611.04231},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{nascimento_implementing_2019,
	title = {Implementing myocardial infarction systems of care in low/middle-income countries},
	volume = {105},
	url = {http://heart.bmj.com/content/105/1/20.abstract},
	doi = {10/gfchxp},
	abstract = {Ischaemic heart disease is the leading cause of death worldwide, with an increasing trend from 6.1 million deaths in 1990 to 9.5 million in 2016, markedly driven by rates observed in low/middle-income countries (LMIC). Improvements in myocardial infarction (MI) care are crucial for reducing premature mortality. We aimed to evaluate the main challenges for adequate MI care in LMIC, and possible strategies to overcome these existing barriers.Reperfusion is the cornerstone of MI treatment, but worldwide around 30\% of patients are not reperfused, with even lower rates in LMIC. The main challenges are related to delays associated with patient education, late diagnosis and inadequate referral strategies, health infrastructure and insufficient funding. The implementation of regional MI systems of care in LMIC, systematising timely reperfusion strategies, access to intensive care, risk stratification and use of adjunctive medications have shown some successful strategies. Telemedicine support for remote ECG, diagnosis and organisation of referrals has proven to be useful, improving access to reperfusion even in prehospital settings. Organisation of transport and referral hubs based on anticipated delays and development of MI excellence centres have also resulted in better equality of care. Also, education of healthcare staff and task shifting may potentially widen access to optimal therapy.In conclusion, efforts have been made for the implementation of MI systems of care in LMIC, aiming to address particularities of the health systems. However, the increasing impact of MI in these countries urges the development of further strategies to improve reperfusion and reduce system delays.},
	number = {1},
	journal = {Heart},
	author = {Nascimento, Bruno R and Brant, Luisa C Caldeira and Marino, Bárbara C A and Passaglia, Luiz Guilherme and Ribeiro, Antonio Luiz P},
	month = jan,
	year = {2019},
	pages = {20},
}

@inproceedings{he_identity_2016,
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	isbn = {978-3-319-46493-0},
	abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 \% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, 🔍No DOI found},
	pages = {630--645},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {Imagenet classification with deep convolutional neural networks},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
	pages = {1097--1105},
}

@article{sutarya_identification_2014,
	title = {Identification of industrial furnace temperature for sintering process in nuclear fuel fabrication using {NARX} neural networks},
	volume = {2014},
	doi = {10.1155/2014/854569},
	journal = {Science and Technology of Nuclear Installations},
	author = {Sutarya, Dede and Kusumoputro, Benyamin},
	year = {2014},
	note = {00000},
}

@article{goethals_identification_2005,
	title = {Identification of {MIMO} {Hammerstein} models using least squares support vector machines},
	volume = {41},
	issn = {0005-1098},
	url = {http://www.sciencedirect.com/science/article/pii/S0005109805000737},
	doi = {10.1016/j.automatica.2005.02.002},
	abstract = {This paper studies a method for the identification of Hammerstein models based on least squares support vector machines (LS-SVMs). The technique allows for the determination of the memoryless static nonlinearity as well as the estimation of the model parameters of the dynamic ARX part. This is done by applying the equivalent of Bai's overparameterization method for identification of Hammerstein systems in an LS-SVM context. The SISO as well as the MIMO identification cases are elaborated. The technique can lead to significant improvements with respect to classical overparameterization methods as illustrated in a number of examples. Another important advantage is that no stringent assumptions on the nature of the nonlinearity need to be imposed except for a certain degree of smoothness.},
	number = {7},
	journal = {Automatica},
	author = {Goethals, Ivan and Pelckmans, Kristiaan and Suykens, Johan A. K. and De Moor, Bart},
	month = jul,
	year = {2005},
	keywords = {ARX, Hammerstein models, Kernel methods, LS-SVM, MIMO systems},
	pages = {1263--1272},
}

@article{farina_identification_2012,
	title = {Identification of polynomial input/output recursive models with simulation error minimisation methods},
	volume = {43},
	doi = {10.1080/00207721.2010.496055},
	number = {2},
	journal = {International Journal of Systems Science},
	author = {Farina, Marcello and Piroddi, Luigi},
	year = {2012},
	pages = {319--333},
}

@article{kayacan_identification_2015,
	title = {Identification of nonlinear dynamic systems using type-2 fuzzy neural networks—{A} novel learning algorithm and a comparative study},
	volume = {62},
	issn = {0278-0046},
	doi = {10.1109/TIE.2014.2345353},
	number = {3},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {Kayacan, Erkan and Kayacan, Erdal and Khanesar, Mojtaba Ahmadieh},
	year = {2015},
	pages = {1716--1724},
}

@article{zhang_identification_2017,
	title = {Identification of multivariable dynamic errors-in-variables system with arbitrary inputs},
	volume = {82},
	issn = {00051098},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S000510981730225X},
	doi = {10.1016/j.automatica.2017.04.031},
	language = {en},
	urldate = {2017-08-23},
	journal = {Automatica},
	author = {Zhang, Erliang and Pintelon, Rik},
	month = aug,
	year = {2017},
	note = {00006},
	pages = {69--78},
}

@article{singh_identification_2013,
	title = {Identification on non linear series-parallel model using neural network},
	volume = {3},
	number = {1},
	journal = {MIT Int. J. Electr. Instrumen. Eng},
	author = {Singh, Manu and Singh, Isha and Verma, A},
	year = {2013},
	note = {00000},
	keywords = {🔍No DOI found},
	pages = {21--23},
}

@article{horbelt_identifying_2001,
	title = {Identifying physical properties of a {CO2} {LASER} by dynamical modeling of measured time series},
	volume = {64},
	doi = {10.1103/PhysRevE.64.016222},
	number = {1},
	journal = {Physical Review E},
	author = {Horbelt, W and Timmer, J and Bünner, MJ and Meucci, R and Ciofini, M},
	year = {2001},
	pages = {016222},
}

@article{vargas_improved_2019,
	title = {Improved learning algorithm for two-layer neural networks for identification of nonlinear systems},
	volume = {329},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231218311846},
	doi = {10/gfwt99},
	abstract = {This study is concerned with the asymptotic identification of nonlinear systems based on Lyapunov theory and two-layer neural networks. An improved identification model enhanced with a feedback term and a novel adaptation law for the threshold offset, associated with the output weight matrix, is introduced to assure the convergence of the online prediction error, even in the presence of approximation error and bounded disturbances and when upper bounds for these perturbations are not known in advance. The effectiveness of the proposed method and its application to the identification of a hyperchaotic system and control of a welding system is investigated.},
	journal = {Neurocomputing},
	author = {Vargas, José A.R. and Pedrycz, Witold and Hemerly, Elder M.},
	month = feb,
	year = {2019},
	keywords = {Dynamical systems, Identification, Lyapunov methods, Neural networks},
	pages = {86--96},
}

@article{xie_identification_2013,
	title = {Identification of nonlinear hysteretic systems by artificial neural network},
	volume = {34},
	issn = {0888-3270},
	url = {http://www.sciencedirect.com/science/article/pii/S0888327012003032},
	doi = {10/f4kjsq},
	abstract = {An identification method is developed for nonlinear hysteretic systems by use of artificial neural network in the paper. Employing the Bouc–Wen differential model widely used for memory-type nonlinear hysteretic systems, the approach sets up a Bouc–Wen model-based neural network. The weights of the designed specifically network correspond to the Bouc–Wen model parameters and are thus physical ones. Taking advantage of powerful function approximation capability of neural network, the nonlinear hysteretic systems can be identified with the proposed approach by network training. The identification scheme is validated by a simulated case and thereafter applied to modeling of a wire cable vibration isolation experimental system. The results show that the presented identification method can identify the nonlinear hysteretic systems with high accuracy.},
	number = {1},
	journal = {Mechanical Systems and Signal Processing},
	author = {Xie, S.L. and Zhang, Y.H. and Chen, C.H. and Zhang, X.N.},
	month = jan,
	year = {2013},
	keywords = {Bouc–Wen model, Identification, Neural network, Nonlinear hysteretic system, Wire cable isolator},
	pages = {76--87},
}

@article{zhao_identification_2014,
	title = {Identification of k-step-ahead prediction error model and {MPC} control},
	volume = {24},
	issn = {0959-1524},
	url = {http://www.sciencedirect.com/science/article/pii/S0959152413002187},
	doi = {10/f5v69w},
	abstract = {This work studies k-step-ahead prediction error model identification and its relationship to MPC control. The use of error criteria in parameter estimation will be discussed, where the identified model is used in model predictive control (MPC). Assume that the model error is dominated by the variance part, it can be shown that a k-step-ahead prediction error model is not optimal for k-step-ahead prediction. A normal one-step-ahead prediction error criterion will be optimal for k-step-ahead prediction. Then it is argued that even when some bias exists, the result could still hold true. Therefore, for MPC identification of linear processes, one-step-ahead prediction error models fever k-step-ahead prediction models. Simulations and industrial testing data will be used to illustrate the idea.},
	number = {1},
	urldate = {2019-04-04},
	journal = {Journal of Process Control},
	author = {Zhao, Jun and Zhu, Yucai and Patwardhan, Rohit},
	month = jan,
	year = {2014},
	keywords = {Error criteria, Identification, Industrial application, MPC},
	pages = {48--56},
}

@article{radford_improving_2018,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year = {2018},
	keywords = {⛔ No DOI found},
	pages = {12},
}

@article{billings_identification_1989,
	title = {Identification of {MIMO} non-linear systems using a forward-regression orthogonal estimator},
	volume = {49},
	doi = {10.1080/00207178908559767},
	number = {6},
	journal = {International Journal of Control},
	author = {Billings, S. A. and Chen, S and Korenberg, M. J.},
	year = {1989},
	pages = {2157--2189},
}

@phdthesis{ribeiro_implementacao_2015,
	address = {Belo Horizonte, Brazil},
	type = {{BSc} {Thesis}},
	title = {Implementação de uma {Câmera} {Estéreo}},
	copyright = {All rights reserved},
	abstract = {A stereo camera is a camera that simultaneously captures two or more images 
in order to estimate depth. A stereo camera prototype was implemented using two image sensors. Its project was based on v200, a commercial camera produced by Invent Vision (iVision), company for which this prototype was developed. The major steps needed to estimate depth using a stereo camera were followed. They are: calibration (estimation of camera geometry and parameters), retification (aligning images), correspondence (finding correspondent points on the two images and the distance between them)
and reconstruction (estimation of features depth using triangulation). All algorithms needed were implemented using C++ and integrated to the prototype. The prototype is able to estimate depth in a range of 0.7 to 20 meters, it has a resolution on the order of a few centimeters and can reach a frame rate of 5 frames per second. The industrial camera v200 has all its internal processing computed by a FPGA. It is suggested, as a future work, to use the FPGA to speed up the retification and correspondence algorithms. The aim of this work was to take a first step towards a stereo camera based on v200 to be a competitive commercial product, apt to be sold by
iVision.},
	school = {Universidade Federal de Minas Gerais},
	author = {Ribeiro, Antonio H.},
	month = dec,
	year = {2015},
}

@inproceedings{gong_impact_2018,
	title = {Impact of {Aliasing} on {Deep} {CNN}-{Based} {End}-to-{End} {Acoustic} {Models}},
	url = {http://www.isca-speech.org/archive/Interspeech_2018/abstracts/1371.html},
	doi = {10.21437/Interspeech.2018-1371},
	abstract = {A recent trend in audio and speech processing is to learn target labels directly from raw waveforms rather than hand-crafted acoustic features. Previous work has shown that deep convolutional neural networks (CNNs) as front-end can learn effective representations from the raw waveform. However, due to the large dimension of raw audio waveforms, pooling layers are usually used aggressively between temporal convolutional layers. In essence, these pooling layers perform operations that are similar to signal downsampling, which may lead to temporal aliasing according to the Nyquist-Shannon sampling theorem. This paper explores, using a series of experiments, if and how this aliasing effect impacts modern deep CNN-based models.},
	language = {en},
	urldate = {2020-03-23},
	booktitle = {Interspeech},
	author = {Gong, Yuan and Poellabauer, Christian},
	year = {2018},
	pages = {2698--2702},
}

@article{jacot_implicit_,
	title = {Implicit {Regularization} of {Random} {Feature} {Models}},
	abstract = {Random Feature (RF) models are used as efﬁcient parametric approximations of kernel methods. We investigate, by means of random matrix theory, the connection between Gaussian RF models and Kernel Ridge Regression (KRR). For a Gaussian RF model with P features, N data points, and a ridge λ, we show that the average (i.e. expected) RF predictor is close to a KRR predictor with an effective ridge λ˜. We show that λ˜ {\textgreater} λ and λ˜ λ monotonically as P grows, thus revealing the implicit regularization effect of ﬁnite RF sampling. We then compare the risk (i.e. test error) of the λ˜KRR predictor with the average risk of the λ-RF predictor and obtain a precise and explicit bound on their difference. Finally, we empirically ﬁnd an extremely good agreement between the test errors of the average λ-RF predictor and λ˜-KRR predictor.},
	language = {en},
	author = {Jacot, Arthur and Simsek, Berfin and Spadaro, Francesco and Hongler, Clément and Gabriel, Franck},
	pages = {10},
}

@article{schoukens_identification_2017,
	title = {Identification of block-oriented nonlinear systems starting from linear approximations: {A} survey},
	volume = {85},
	issn = {00051098},
	shorttitle = {Identification of block-oriented nonlinear systems starting from linear approximations},
	url = {http://arxiv.org/abs/1607.01217},
	doi = {10.1016/j.automatica.2017.06.044},
	abstract = {Block-oriented nonlinear models are popular in nonlinear system identification because of their advantages of being simple to understand and easy to use. Many different identification approaches were developed over the years to estimate the parameters of a wide range of block-oriented nonlinear models. One class of these approaches uses linear approximations to initialize the identification algorithm. The best linear approximation framework and the \${\textbackslash}epsilon\$-approximation framework, or equivalent frameworks, allow the user to extract important information about the system, guide the user in selecting good candidate model structures and orders, and prove to be a good starting point for nonlinear system identification algorithms. This paper gives an overview of the different block-oriented nonlinear models that can be identified using linear approximations, and of the identification algorithms that have been developed in the past. A non-exhaustive overview of the most important other block-oriented nonlinear system identification approaches is also provided throughout this paper.},
	journal = {Automatica},
	author = {Schoukens, Maarten and Tiels, Koen},
	month = nov,
	year = {2017},
	keywords = {Computer Science - Systems and Control},
	pages = {272--292},
}

@article{gastal_highorder_2015,
	title = {High-{Order} {Recursive} {Filtering} of {Non}-{Uniformly} {Sampled} {Signals} for {Image} and {Video} {Processing}},
	volume = {34},
	issn = {01677055},
	url = {http://doi.wiley.com/10.1111/cgf.12543},
	doi = {10.1111/cgf.12543},
	abstract = {We present a discrete-time mathematical formulation for applying recursive digital ﬁlters to non-uniformly sampled signals. Our solution presents several desirable features: it preserves the stability of the original ﬁlters; is well-conditioned for low-pass, high-pass, and band-pass ﬁlters alike; its cost is linear in the number of samples and is not affected by the size of the ﬁlter support. Our method is general and works with any non-uniformly sampled signal and any recursive digital ﬁlter deﬁned by a difference equation. Since our formulation directly uses the ﬁlter coefﬁcients, it works out-of-the-box with existing methodologies for digital ﬁlter design. We demonstrate the effectiveness of our approach by ﬁltering non-uniformly sampled signals in various image and video processing tasks including edge-preserving color ﬁltering, noise reduction, stylization, and detail enhancement. Our formulation enables, for the ﬁrst time, edge-aware evaluation of any recursive inﬁnite impulse response digital ﬁlter (not only low-pass), producing high-quality ﬁltering results in real time.},
	language = {en},
	number = {2},
	urldate = {2020-07-16},
	journal = {Computer Graphics Forum},
	author = {Gastal, Eduardo S. L. and Oliveira, Manuel M.},
	month = may,
	year = {2015},
	pages = {81--93},
}

@incollection{yosinski_how_2014,
	title = {How transferable are features in deep neural networks?},
	url = {http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf},
	urldate = {2020-09-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {3320--3328},
}

@article{advani_highdimensional_2017,
	title = {High-dimensional dynamics of generalization error in neural networks},
	url = {http://arxiv.org/abs/1710.03667},
	abstract = {We perform an average case analysis of the generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant "high-dimensional" regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that naive application of worst-case theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
	urldate = {2020-11-26},
	journal = {arXiv:1710.03667 [physics, q-bio, stat]},
	author = {Advani, Madhu S. and Saxe, Andrew M.},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.03667},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{santurkar_how_2018,
	title = {How {Does} {Batch} {Normalization} {Help} {Optimization}?},
	url = {http://arxiv.org/abs/1805.11604},
	abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
	urldate = {2018-12-04},
	journal = {arXiv:1805.11604 [cs, stat]},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
	month = may,
	year = {2018},
	note = {arXiv: 1805.11604},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@incollection{wu_how_2018,
	title = {How {SGD} {Selects} the {Global} {Minima} in {Over}-parameterized {Learning}: {A} {Dynamical} {Stability} {Perspective}},
	shorttitle = {How {SGD} {Selects} the {Global} {Minima} in {Over}-parameterized {Learning}},
	url = {http://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective.pdf},
	urldate = {2018-12-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Wu, Lei and Ma, Chao and E, Weinan},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {8289--8298},
}

@article{narendra_identification_1990,
	title = {Identification and control of dynamical systems using neural networks},
	volume = {1},
	doi = {10.1109/72.80202},
	number = {1},
	journal = {IEEE Transactions on Neural Networks},
	author = {Narendra, Kumpati S and Parthasarathy, Kannan},
	year = {1990},
	pages = {4--27},
}

@article{nasrabadi_hopfield_1992,
	title = {Hopfield network for stereo vision correspondence},
	volume = {3},
	doi = {10.1109/72.105413},
	number = {1},
	journal = {Neural Networks, IEEE Transactions on},
	author = {Nasrabadi, Nasser M and Choo, Chang Y},
	year = {1992},
	pages = {5--13},
}

@article{noel_hysteretic_2016,
	title = {Hysteretic benchmark with a dynamic nonlinearity},
	url = {https://www.researchgate.net/profile/Maarten_Schoukens/publication/307569412_Hysteretic_benchmark_with_a_dynamic_nonlinearity/links/57c92fbe08ae9d640483eda0.pdf},
	urldate = {2017-08-28},
	journal = {Brussels, Belgium},
	author = {Noël, J. P. and Schoukens, M.},
	year = {2016},
	keywords = {🔍No DOI found},
}

@article{dong_identification_2017,
	title = {Identification and {Robust} {Control} of the {Nonlinear} {Photoelectrothermal} {Dynamics} of {LED} {Systems}},
	volume = {64},
	doi = {10.1109/TIE.2016.2619659},
	number = {3},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {Dong, Jianfei and Zhang, Guoqi},
	year = {2017},
	pages = {2215--2225},
}

@article{wang_hybrid_2016,
	title = {Hybrid recursive least squares algorithm for online sequential identification using data chunks},
	volume = {174},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S092523121501423X},
	doi = {10.1016/j.neucom.2015.09.090},
	abstract = {In this paper, a hybrid recursive least squares (HRLS) algorithm for online identification using sequential chunk-by-chunk observations is proposed. By employing the optimization-based least squares (O-LS), the HRLS can be initialized with any chunk of data samples and works successively in two recursive procedures for updating the inverse matrix with minimal dimension and least rank-deficiency, and thereby contributing to fast and stable online identification. Since norms of the output weight and training errors are minimized simultaneously, the HRLS achieves high accuracy in terms of both generalization and approximation. Simulation studies and comprehensive comparisons demonstrate that the HRLS is numerically more stable and superior to other algorithms in terms of accuracy and speed.},
	journal = {Neurocomputing},
	author = {Wang, Ning and Sun, Jing-Chao and Er, Meng Joo and Liu, Yan-Cheng},
	month = jan,
	year = {2016},
	note = {00014},
	keywords = {Hybrid recursive least squares (HRLS), Online sequential identification, Recursive orthogonal least squares (ROLS), Single-hidden-layer feedforward network (SLFN)},
	pages = {651--660},
}

@article{ternes_identification_2017,
	title = {Identification of biomarker-by-treatment interactions in randomized clinical trials with survival outcomes and high-dimensional spaces},
	volume = {59},
	issn = {1521-4036},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/bimj.201500234/abstract},
	doi = {10.1002/bimj.201500234},
	abstract = {Stratified medicine seeks to identify biomarkers or parsimonious gene signatures distinguishing patients that will benefit most from a targeted treatment. We evaluated 12 approaches in high-dimensional Cox models in randomized clinical trials: penalization of the biomarker main effects and biomarker-by-treatment interactions (full-lasso, three kinds of adaptive lasso, ridge+lasso and group-lasso); dimensionality reduction of the main effect matrix via linear combinations (PCA+lasso (where PCA is principal components analysis) or PLS+lasso (where PLS is partial least squares)); penalization of modified covariates or of the arm-specific biomarker effects (two-I model); gradient boosting; and univariate approach with control of multiple testing. We compared these methods via simulations, evaluating their selection abilities in null and alternative scenarios. We varied the number of biomarkers, of nonnull main effects and true biomarker-by-treatment interactions. We also proposed a novel measure evaluating the interaction strength of the developed gene signatures. In the null scenarios, the group-lasso, two-I model, and gradient boosting performed poorly in the presence of nonnull main effects, and performed well in alternative scenarios with also high interaction strength. The adaptive lasso with grouped weights was too conservative. The modified covariates, PCA+lasso, PLS+lasso, and ridge+lasso performed moderately. The full-lasso and adaptive lassos performed well, with the exception of the full-lasso in the presence of only nonnull main effects. The univariate approach performed poorly in alternative scenarios. We also illustrate the methods using gene expression data from 614 breast cancer patients treated with adjuvant chemotherapy.},
	language = {en},
	number = {4},
	journal = {Biometrical Journal},
	author = {Ternès, Nils and Rotolo, Federico and Heinze, Georg and Michiels, Stefan},
	month = jul,
	year = {2017},
	note = {00000},
	keywords = {Biomarker-by-treatment interactions, High-dimensional, Precision medicine, Stratified medicine, Survival, variable selection},
	pages = {685--701},
}

@article{lin_how_1998,
	title = {How embedded memory in recurrent neural network architectures helps learning long-term temporal dependencies},
	volume = {11},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608098000185},
	doi = {10.1016/S0893-6080(98)00018-5},
	abstract = {Learning long-term temporal dependencies with recurrent neural networks can be a difficult problem. It has recently been shown that a class of recurrent neural networks called NARX networks perform much better than conventional recurrent neural networks for learning certain simple long-term dependency problems. The intuitive explanation for this behavior is that the output memories of a NARX network can be manifested as jump-ahead connections in the time-unfolded network. These jump-ahead connections can propagate gradient information more efficiently, thus reducing the sensitivity of the network to long-term dependencies. This work gives empirical justification to our hypothesis that similar improvements in learning long-term dependencies can be achieved with other classes of recurrent neural network axchitectures simply by increasing the order of the embedded memory. In particular we explore the impact of learning simple long-term dependency problems on three classes of recurrent neural network architectures: globally recurrent networks, locally recurrent networks, and NARX (output feedback) networks. Comparing the performance of these architectures with different orders of embedded memory on two simple long-term dependencies problems shows that all of these classes of network architectures demonstrate significant improvement on learning long-term dependencies when the orders of embedded memory are increased. These results can be important to a user comfortable with a specific recurrent neural network architecture because simply increasing the embedding memory order of that architecture will make it more robust to the problem of long-term dependency learning.},
	number = {5},
	journal = {Neural Networks},
	author = {Lin, Tsungnan and Horne, Bill G. and Giles, C. Lee},
	month = jul,
	year = {1998},
	keywords = {Automata, Gradient-descent, Latching, Memory, Neural networks, Recurrent, Time-delay, Training, long-term dependencies},
	pages = {861--868},
}

@article{huyck_identification_2011,
	series = {18th {IFAC} {World} {Congress}},
	title = {Identification of a {Pilot} {Scale} {Distillation} {Column}: {A} {Kernel} {Based} {Approach}},
	volume = {44},
	issn = {1474-6670},
	shorttitle = {Identification of a {Pilot} {Scale} {Distillation} {Column}},
	url = {http://www.sciencedirect.com/science/article/pii/S1474667016436542},
	doi = {10.3182/20110828-6-IT-1002.01512},
	abstract = {This paper describes the identification of a binary distillation column with Least-Squares Support Vector Machines (LS-SVM). It is our intention to investigate whether a kernel based model, particularly an LS-SVM, can be used for the simulation of the top and bottom temperature of a binary distillation column. Furthermore, we compare the latter model with standard linear models by means of mean-squared error (MSE). It will be demonstrated that this nonlinear model class achieves higher performances in MSE than linear models in the presence of nonlinear distortions. When the system is close to linear, the performance of the LS-SVM is only slightly better than the linear models.},
	number = {1},
	journal = {IFAC Proceedings Volumes},
	author = {Huyck, B. and De Brabanter, K. and Logist, F. and De Brabanter, J. and Van Impe, J. and De Moor, B.},
	month = jan,
	year = {2011},
	keywords = {Chemical Industry, Distillation columns, kernel based system identification},
	pages = {471--476},
}

@inproceedings{hereid_hybrid_2015,
	title = {Hybrid zero dynamics based multiple shooting optimization with applications to robotic walking},
	doi = {10.1109/ICRA.2015.7140002},
	abstract = {Hybrid zero dynamics (HZD) has emerged as a popular framework for the stable control of bipedal robotic gaits, but typically designing a gait's virtual constraints is a slow and undependable optimization process. To expedite and boost the reliability of HZD gait generation, we borrow methods from trajectory optimization to formulate a smoother and more linear optimization problem. We present a multiple-shooting formulation for the optimization of virtual constraints, combining the stability-friendly properties of HZD with an optimization-conducive problem formulation. To showcase the implications of this recipe for improving gait generation, we use the same process to generate periodic planar walking gaits on two different robot models, and in one case, demonstrate stable walking on the hardware prototype, DURUS-R.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Hereid, A. and Hubicki, C. M. and Cousineau, E. A. and Hurst, J. W. and Ames, A. D.},
	month = may,
	year = {2015},
	keywords = {Control systems, DURUS-R, Foot, HZD gait generation, Joints, Optimization, Reliability, hybrid zero dynamics, legged locomotion, linear optimization problem, multiple shooting optimization, optimisation, optimization-conducive problem formulation, periodic planar walking gaits, robotic walking, stability, stability-friendly properties, trajectory optimization},
	pages = {5734--5740},
}

@inproceedings{graves_hybrid_2013,
	address = {Olomouc, Czech Republic},
	title = {Hybrid speech recognition with {Deep} {Bidirectional} {LSTM}},
	isbn = {978-1-4799-2756-2},
	url = {http://ieeexplore.ieee.org/document/6707742/},
	doi = {10.1109/ASRU.2013.6707742},
	abstract = {Deep Bidirectional LSTM (DBLSTM) recurrent neural networks have recently been shown to give state-of-the-art performance on the TIMIT speech database. However, the results in that work relied on recurrent-neural-network-speciﬁc objective functions, which are difﬁcult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of DBLSTM as an acoustic model in a standard neural network-HMM hybrid system. We ﬁnd that a DBLSTM-HMM hybrid gives equally good results on TIMIT as the previous work. It also outperforms both GMM and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates.},
	language = {en},
	urldate = {2019-11-22},
	booktitle = {2013 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding}},
	publisher = {IEEE},
	author = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
	month = dec,
	year = {2013},
	pages = {273--278},
}

@incollection{biau_highdimensional_2015,
	title = {High-{Dimensional} p-{Norms}},
	booktitle = {Mathematical statistics and limit theorems},
	publisher = {Springer},
	author = {Biau, Gérard and Mason, David M},
	year = {2015},
	pages = {21--40},
}

@inproceedings{ribeiro_how_2021,
	title = {How convolutional neural networks deal with aliasing},
	doi = {10.1109/ICASSP39728.2021.9414627},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Ribeiro, Antonio H. and Schon, Thomas B.},
	year = {2021},
	pages = {2755--2759},
}

@article{advani_highdimensional_2020,
	title = {High-dimensional dynamics of generalization error in neural networks},
	volume = {132},
	issn = {0893-6080},
	doi = {10.1016/j.neunet.2020.08.022},
	abstract = {We perform an analysis of the average generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant “high-dimensional” regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that standard application of theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
	urldate = {2021-05-28},
	journal = {Neural Networks},
	author = {Advani, Madhu S. and Saxe, Andrew M. and Sompolinsky, Haim},
	month = dec,
	year = {2020},
	keywords = {Generalization error, Neural networks, Random matrix theory},
	pages = {428--446},
}

@article{wehrmann_hierarchical_,
	title = {Hierarchical {Multi}-{Label} {Classification} {Networks}},
	abstract = {One of the most challenging machine learning problems is a particular case of data classiﬁcation in which classes are hierarchically structured and objects can be assigned to multiple paths of the class hierarchy at the same time. This task is known as hierarchical multi-label classiﬁcation (HMC), with applications in text classiﬁcation, image annotation, and in bioinformatics problems such as protein function prediction. In this paper, we propose novel neural network architectures for HMC called HMCN, capable of simultaneously optimizing local and global loss functions for discovering local hierarchical class-relationships and global information from the entire class hierarchy while penalizing hierarchical violations. We evaluate its performance in 21 datasets from four distinct domains, and we compare it against the current HMC state-of-the-art approaches. Results show that HMCN substantially outperforms all baselines with statistical signiﬁcance, arising as the novel state-of-the-art for HMC.},
	language = {en},
	author = {Wehrmann, Jônatas and Cerri, Ricardo and Barros, Rodrigo C},
	pages = {10},
}

@misc{_google_,
	title = {Google {Agenda} - 7 dias, a partir de segunda-feira, 25 de maio de 2020},
	url = {https://calendar.google.com/calendar/r},
	urldate = {2020-05-25},
}

@article{ulicny_harmonic_2018,
	title = {Harmonic {Networks}: {Integrating} {Spectral} {Information} into {CNNs}},
	shorttitle = {Harmonic {Networks}},
	url = {http://arxiv.org/abs/1812.03205},
	abstract = {Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. In contrast, in this paper we propose harmonic blocks that produce features by learning optimal combinations of spectral filters defined by the Discrete Cosine Transform. The harmonic blocks are used to replace conventional convolutional layers to construct partial or fully harmonic CNNs. We extensively validate our approach and show that the introduction of harmonic blocks into state-of-the-art CNN baseline architectures results in comparable or better performance in classification tasks on small NORB, CIFAR10 and CIFAR100 datasets.},
	urldate = {2020-07-07},
	journal = {arXiv:1812.03205 [cs]},
	author = {Ulicny, Matej and Krylov, Vladimir A. and Dahyot, Rozenn},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.03205},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{du_gradient_2019a,
	title = {Gradient {Descent} {Provably} {Optimizes} {Over}-parameterized {Neural} {Networks}},
	url = {http://arxiv.org/abs/1810.02054},
	abstract = {One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an \$m\$ hidden node shallow neural network with ReLU activation and \$n\$ training data, we show as long as \$m\$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. Our analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.},
	urldate = {2020-07-27},
	journal = {arXiv:1810.02054 [cs, math, stat]},
	author = {Du, Simon S. and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
	month = feb,
	year = {2019},
	note = {arXiv: 1810.02054},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{_google_a,
	title = {Google {Calendar} {Hours} {Calculator}},
	url = {https://google-calendar-hours.com/},
	urldate = {2020-09-16},
}

@article{dobriban_highdimensional_2018,
	title = {High-dimensional asymptotics of prediction: {Ridge} regression and classification},
	volume = {46},
	issn = {0090-5364, 2168-8966},
	shorttitle = {High-dimensional asymptotics of prediction},
	url = {https://projecteuclid.org/euclid.aos/1519268430},
	doi = {10.1214/17-AOS1549},
	abstract = {We provide a unified analysis of the predictive risk of ridge regression and regularized discriminant analysis in a dense random effects model. We work in a high-dimensional asymptotic regime where p,n→∞p,n→∞p,n{\textbackslash}to{\textbackslash}infty and p/n→γ{\textgreater}0p/n→γ{\textgreater}0p/n{\textbackslash}to{\textbackslash}gamma{\textgreater}0, and allow for arbitrary covariance among the features. For both methods, we provide an explicit and efficiently computable expression for the limiting predictive risk, which depends only on the spectrum of the feature-covariance matrix, the signal strength and the aspect ratio γγ{\textbackslash}gamma. Especially in the case of regularized discriminant analysis, we find that predictive accuracy has a nuanced dependence on the eigenvalue distribution of the covariance matrix, suggesting that analyses based on the operator norm of the covariance matrix may not be sharp. Our results also uncover an exact inverse relation between the limiting predictive risk and the limiting estimation risk in high-dimensional linear models. The analysis builds on recent advances in random matrix theory.},
	language = {EN},
	number = {1},
	urldate = {2020-11-26},
	journal = {Annals of Statistics},
	author = {Dobriban, Edgar and Wager, Stefan},
	month = feb,
	year = {2018},
	mrnumber = {MR3766952},
	zmnumber = {06865111},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {High-dimensional asymptotics, prediction error, random matrix theory, regularized discriminant analysis, ridge regression},
	pages = {247--279},
}

@article{du_gradient_2018,
	title = {Gradient {Descent} {Finds} {Global} {Minima} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1811.03804},
	abstract = {Gradient descent ﬁnds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. Our bounds also shed light on the advantage of using ResNet over the fully connected feedforward architecture; our bound requires the number of neurons per layer scaling exponentially with depth for feedforward networks whereas for ResNet the bound only requires the number of neurons per layer scaling polynomially with depth. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.},
	language = {en},
	urldate = {2018-11-13},
	journal = {arXiv:1811.03804 [cs, math, stat]},
	author = {Du, Simon S. and Lee, Jason D. and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
	month = nov,
	year = {2018},
	note = {00000 
arXiv: 1811.03804},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, 🔍No DOI found},
}

@article{bertozzi_gold_1998,
	title = {{GOLD}: {A} parallel real-time stereo vision system for generic obstacle and lane detection},
	volume = {7},
	doi = {10.1109/83.650851},
	number = {1},
	journal = {Image Processing, IEEE Transactions on},
	author = {Bertozzi, Massimo and Broggi, Alberto},
	year = {1998},
	pages = {62--81},
}

@article{noel_greybox_2017,
	title = {Grey-box state-space identification of nonlinear mechanical vibrations},
	issn = {0020-7179},
	journal = {International Journal of Control},
	author = {Noël, Jean-Philippe and Schoukens, Johan},
	year = {2017},
	keywords = {🔍No DOI found},
	pages = {1--22},
}

@article{breiman_heuristics_1996,
	title = {Heuristics of instability and stabilization in model selection},
	volume = {24},
	number = {6},
	journal = {The annals of statistics},
	author = {Breiman, Leo},
	year = {1996},
	keywords = {🔍No DOI found},
	pages = {2350--2383},
}

@article{tulleken_greybox_1993,
	title = {Grey-box modelling and identification using physical knowledge and {Bayesian} techniques},
	volume = {29},
	issn = {0005-1098},
	doi = {10.1016/0005-1098(93)90124-C},
	number = {2},
	journal = {Automatica},
	author = {Tulleken, Herbert JAF},
	year = {1993},
	note = {00000},
	pages = {285--308},
}

@article{quam_hierarchical_1984,
	title = {Hierarchical warp stereo},
	journal = {Readings in computer vision},
	author = {Quam, Lynn H and Center, Artificial Intelligence},
	year = {1984},
	note = {00000},
	keywords = {🔍No DOI found},
	pages = {80--86},
}

@article{greblicki_hammerstein_2017,
	title = {Hammerstein {System} {Identification} {With} the {Nearest} {Neighbor} {Algorithm}},
	volume = {63},
	issn = {0018-9448},
	doi = {10.1109/TIT.2017.2694013},
	abstract = {The nonlinear characteristic in a Hammerstein system, i.e., a system in which a nonlinear memoryless subsystem and a linear dynamic are connected in a cascade, is recovered with the nonparametric nearest neighbor regression estimate. The a priori information is nonparametric, both the nonlinear characteristic and the impulse response are completely unknown and can be of any form. Local and global properties of the estimate are examined. Whatever the probability density of the input signal, the estimate converges at every continuity point of the characteristic as well as in the global sense. We derive the asymptotic bias and variance of the proposed estimate. As a result, the optimal rate of convergence is established that additionally is independent of the shape of the input density. Results of numerical simulations are also presented.},
	number = {8},
	journal = {IEEE Transactions on Information Theory},
	author = {Greblicki, W. and Pawlak, M.},
	month = aug,
	year = {2017},
	keywords = {Convergence, Estimation, Hammerstein system, Hammerstein system identification, Heuristic algorithms, Kernel, Random variables, Signal processing algorithms, dependent data, impulse response, linear dynamic, nearest neighbor, nearest neighbor algorithm, nonlinear memoryless subsystem, nonlinear systems, nonparametric nearest neighbor regression estimate, nonparametric regression, numerical analysis, numerical simulations, rate of convergence, regression analysis, signal processing, system identification, transient response},
	pages = {4746--4757},
}

@article{noel_greybox_2018,
	title = {Grey-box state-space identification of nonlinear mechanical vibrations},
	volume = {91},
	issn = {0020-7179},
	url = {https://doi.org/10.1080/00207179.2017.1308557},
	doi = {10/gfzgtg},
	abstract = {The present paper deals with the identification of nonlinear mechanical vibrations. A grey-box, or semi-physical, nonlinear state-space representation is introduced, expressing the nonlinear basis functions using a limited number of measured output variables. This representation assumes that the observed nonlinearities are localised in physical space, which is a generic case in mechanics. A two-step identification procedure is derived for the grey-box model parameters, integrating nonlinear subspace initialisation and weighted least-squares optimisation. The complete procedure is applied to an electrical circuit mimicking the behaviour of a single–input, single–output (SISO) nonlinear mechanical system and to a single–input, multiple–output (SIMO) geometrically nonlinear beam structure.},
	number = {5},
	urldate = {2019-04-15},
	journal = {International Journal of Control},
	author = {Noël, J. P. and Schoukens, J.},
	month = may,
	year = {2018},
	keywords = {Nonlinear system identification, Silverbox benchmark, grey-box modelling, nonlinear beam benchmark, nonlinear mechanical vibrations, semi-physical modelling, state-space equations},
	pages = {1118--1139},
}

@article{kanuparthi_hdetach_2019,
	title = {h-{DETACH}: {Modifying} the {LSTM} {Gradient} {Towards} {Better} {Optimization}},
	abstract = {Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP). This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because EVGP prevents important gradient components from being back-propagated adequately over a large number of steps. We introduce a simple stochastic algorithm (h-detach) that is speciﬁc to LSTM optimization and targeted towards addressing this problem. Speciﬁcally, we show that when the LSTM weights are large, the gradient components through the linear path (cell state) in the LSTM computational graph get suppressed. Based on the hypothesis that these components carry information about long term dependencies (which we show empirically), their suppression can prevent LSTMs from capturing them. Our algorithm1 prevents gradients ﬂowing through this path from getting suppressed, thus allowing the LSTM to capture such dependencies better. We show signiﬁcant improvements over vanilla LSTM gradient based training in terms of convergence speed, robustness to seed and learning rate, and generalization using our modiﬁcation of LSTM gradient on various benchmark datasets.},
	language = {en},
	journal = {Proceedings of the International Conference for Learning Representations (ICLR)},
	author = {Kanuparthi, Bhargav and Arpit, Devansh and Kerg, Giancarlo and Ke, Nan Rosemary and Mitliagkas, Ioannis and Bengio, Yoshua},
	year = {2019},
	keywords = {⛔ No DOI found},
	pages = {19},
}

@article{lecun_gradientbased_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {0018-9219},
	doi = {10.1109/5.726791},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {14818},
	keywords = {2D shape variability, Character recognition, Feature extraction, GTN, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis, back-propagation, backpropagation, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, handwritten character recognition, handwritten digit recognition task, high-dimensional patterns, language modeling, multilayer neural networks, multilayer perceptrons, multimodule systems, optical character recognition, performance measure minimization, segmentation recognition},
	pages = {2278--2324},
}

@article{jaeger_harnessing_2008,
	title = {Harnessing {Nonlinearity}: {Predicting} {Chaotic} {Systems} and {Saving} {Energy} in {Wireless} {Communication}},
	volume = {304},
	journal = {Science},
	author = {Jaeger, Herbert and Haas, Harald},
	year = {2008},
	keywords = {Image resolution, Nonlinear system, Web of Science},
	pages = {78--80},
}

@inproceedings{lecun_handwritten_1990,
	title = {Handwritten zip code recognition with multilayer networks},
	volume = {2},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Le Cun, Yann and Matan, Ofer and Boser, Bernhard and Denker, John S and Henderson, Don and Howard, Richard E and Hubbard, Wayne and Jacket, LD and Baird, Henry S},
	year = {1990},
	note = {tex.organization: IEEE},
	pages = {35--40},
}

@article{selvaraju_gradcam_2020,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Grad-{CAM}},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
	number = {2},
	urldate = {2021-03-24},
	journal = {International Journal of Computer Vision},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = feb,
	year = {2020},
	note = {arXiv: 1610.02391},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {336--359},
}

@inproceedings{cohen_gradient_2020,
	title = {Gradient {Descent} on {Neural} {Networks} {Typically} {Occurs} at the {Edge} of {Stability}},
	url = {https://openreview.net/forum?id=jh-rTtvkGeM},
	abstract = {We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum...},
	language = {en},
	urldate = {2021-03-19},
	author = {Cohen, Jeremy and Kaur, Simran and Li, Yuanzhi and Kolter, J. Zico and Talwalkar, Ameet},
	month = sep,
	year = {2020},
}

@inproceedings{du_gradient_2019,
	series = {Proceedings of machine learning research},
	title = {Gradient descent finds global minima of deep neural networks},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/du19c.html},
	abstract = {Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.},
	booktitle = {Proceedings of the 36th international conference on machine learning},
	publisher = {PMLR},
	author = {Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	pages = {1675--1685},
}

@article{muthukumar_harmless_2020,
	title = {Harmless {Interpolation} of {Noisy} {Data} in {Regression}},
	volume = {1},
	issn = {2641-8770},
	doi = {10.1109/JSAIT.2020.2984716},
	abstract = {A continuing mystery in understanding the empirical success of deep neural networks is their ability to achieve zero training error and generalize well, even when the training data is noisy and there are more parameters than data points. We investigate this overparameterized regime in linear regression, where all solutions that minimize training error interpolate the data, including noise. We lower-bound the fundamental generalization (mean-squared) error of any interpolating solution in the presence of noise, and show that this bound decays to zero with the number of features. Thus, overparameterization can be beneficial in ensuring harmless interpolation of noise. We discuss two root causes for poor generalization that are complementary in nature - signal “bleeding” into a large number of alias features, and overfitting of noise by parsimonious feature selectors. For the sparse linear model with noise, we provide a hybrid interpolating scheme that mitigates both these issues and achieves order-optimal MSE over all possible interpolating solutions.},
	number = {1},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Muthukumar, Vidya and Vodrahalli, Kailas and Subramanian, Vignesh and Sahai, Anant},
	month = may,
	year = {2020},
	keywords = {Information theory, Interpolation, Kernel, Linear regression, Neural networks, Optimization, Statistical learning, Training data, function approximation, interpolation, supervised learning},
	pages = {67--83},
}

@inproceedings{rudi_generalization_2017,
	title = {Generalization properties of learning with random features},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Rudi, Alessandro and Rosasco, Lorenzo},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@article{qian_generalized_2017,
	title = {Generalized {Hybrid} {Constructive} {Learning} {Algorithm} for {Multioutput} {RBF} {Networks}},
	volume = {47},
	issn = {2168-2267},
	doi = {10/gfwvbc},
	number = {11},
	journal = {IEEE Transactions on Cybernetics},
	author = {Qian, X. and Huang, H. and Chen, X. and Huang, T.},
	month = nov,
	year = {2017},
	keywords = {Algorithm design and analysis, Approximation algorithms, Convergence, GHC learning algorithm, Generalized hidden matrix, LM training, Levenberg-Marquardt algorithm, Neurons, Optimization, Radial basis function networks, Training, compact network, computational complexity, efficient generalized hybrid constructive learning algorithm, generalisation (artificial intelligence), generalization capability, generalized hidden matrix, generalized hybrid constructive (GHC) learning algorithm, growing algorithm, improved incremental constructive scheme, initialization method, learning (artificial intelligence), least squares approximations, least-square method, local minima, matrix algebra, memory limitation problem, memory reduction, multioutput RBF network training, multioutput radial basis function (RBF) networks, multioutput radial basis function networks, optimal network structure, optimisation, pruning algorithm, radial basis function networks, structured parameter optimization (SPO), structured parameter optimization algorithm},
	pages = {3634--3648},
}

@inproceedings{lee_generalizing_2016,
	title = {Generalizing {Pooling} {Functions} in {Convolutional} {Neural} {Networks}: {Mixed}, {Gated}, and {Tree}},
	abstract = {We seek to improve deep neural networks by generalizing the pooling operations that play a central role in current architectures. We pursue a careful exploration of approaches to allow pooling to learn and to adapt to complex and variable patterns. The two primary directions lie in (1) learning a pooling function via (two strategies of) combining of max and average pooling, and (2) learning a pooling function in the form of a tree-structured fusion of pooling ﬁlters that are themselves learned. In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets; they are also easy to implement, and can be applied within various deep neural network architectures. These beneﬁts come with only a light increase in computational overhead during training and a very modest increase in the number of model parameters.},
	language = {en},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {Lee, Chen-Yu and Gallagher, Patrick W and Tu, Zhuowen},
	year = {2016},
	pages = {9},
}

@inproceedings{henaff_geodesics_2016,
	title = {Geodesics of learned representations},
	url = {http://arxiv.org/abs/1511.06394},
	abstract = {We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a "representational geodesic"). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.},
	urldate = {2020-03-23},
	booktitle = {Proceedings of the {International} {Conference} for {Learning} {Representations} ({ICLR})},
	author = {Hénaff, Olivier J. and Simoncelli, Eero P.},
	month = feb,
	year = {2016},
	note = {arXiv: 1511.06394},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{zhang_generalized_2018,
	title = {Generalized {Cross} {Entropy} {Loss} for {Training} {Deep} {Neural} {Networks} with {Noisy} {Labels}},
	url = {http://arxiv.org/abs/1805.07836},
	abstract = {Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.},
	urldate = {2018-12-06},
	journal = {arXiv:1805.07836 [cs, stat]},
	author = {Zhang, Zhilu and Sabuncu, Mert R.},
	month = may,
	year = {2018},
	note = {arXiv: 1805.07836},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{roth_global_2018,
	title = {Global, regional, and national age-sex-specific mortality for 282 causes of death in 195 countries and territories, 1980–2017: a systematic analysis for the {Global} {Burden} of {Disease} {Study} 2017},
	volume = {392},
	issn = {0140-6736, 1474-547X},
	shorttitle = {Global, regional, and national age-sex-specific mortality for 282 causes of death in 195 countries and territories, 1980–2017},
	url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(18)32203-7/abstract},
	doi = {10/gfhx3t},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}h3{\textgreater}Background{\textless}/h3{\textgreater}{\textless}p{\textgreater}Global development goals increasingly rely on country-specific estimates for benchmarking a nation's progress. To meet this need, the Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2016 estimated global, regional, national, and, for selected locations, subnational cause-specific mortality beginning in the year 1980. Here we report an update to that study, making use of newly available data and improved methods. GBD 2017 provides a comprehensive assessment of cause-specific mortality for 282 causes in 195 countries and territories from 1980 to 2017.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Methods{\textless}/h3{\textgreater}{\textless}p{\textgreater}The causes of death database is composed of vital registration (VR), verbal autopsy (VA), registry, survey, police, and surveillance data. GBD 2017 added ten VA studies, 127 country-years of VR data, 502 cancer-registry country-years, and an additional surveillance country-year. Expansions of the GBD cause of death hierarchy resulted in 18 additional causes estimated for GBD 2017. Newly available data led to subnational estimates for five additional countries—Ethiopia, Iran, New Zealand, Norway, and Russia. Deaths assigned International Classification of Diseases (ICD) codes for non-specific, implausible, or intermediate causes of death were reassigned to underlying causes by redistribution algorithms that were incorporated into uncertainty estimation. We used statistical modelling tools developed for GBD, including the Cause of Death Ensemble model (CODEm), to generate cause fractions and cause-specific death rates for each location, year, age, and sex. Instead of using UN estimates as in previous versions, GBD 2017 independently estimated population size and fertility rate for all locations. Years of life lost (YLLs) were then calculated as the sum of each death multiplied by the standard life expectancy at each age. All rates reported here are age-standardised.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Findings{\textless}/h3{\textgreater}{\textless}p{\textgreater}At the broadest grouping of causes of death (Level 1), non-communicable diseases (NCDs) comprised the greatest fraction of deaths, contributing to 73·4\% (95\% uncertainty interval [UI] 72·5–74·1) of total deaths in 2017, while communicable, maternal, neonatal, and nutritional (CMNN) causes accounted for 18·6\% (17·9–19·6), and injuries 8·0\% (7·7–8·2). Total numbers of deaths from NCD causes increased from 2007 to 2017 by 22·7\% (21·5–23·9), representing an additional 7·61 million (7·20–8·01) deaths estimated in 2017 versus 2007. The death rate from NCDs decreased globally by 7·9\% (7·0–8·8). The number of deaths for CMNN causes decreased by 22·2\% (20·0–24·0) and the death rate by 31·8\% (30·1–33·3). Total deaths from injuries increased by 2·3\% (0·5–4·0) between 2007 and 2017, and the death rate from injuries decreased by 13·7\% (12·2–15·1) to 57·9 deaths (55·9–59·2) per 100 000 in 2017. Deaths from substance use disorders also increased, rising from 284 000 deaths (268 000–289 000) globally in 2007 to 352 000 (334 000–363 000) in 2017. Between 2007 and 2017, total deaths from conflict and terrorism increased by 118·0\% (88·8–148·6). A greater reduction in total deaths and death rates was observed for some CMNN causes among children younger than 5 years than for older adults, such as a 36·4\% (32·2–40·6) reduction in deaths from lower respiratory infections for children younger than 5 years compared with a 33·6\% (31·2–36·1) increase in adults older than 70 years. Globally, the number of deaths was greater for men than for women at most ages in 2017, except at ages older than 85 years. Trends in global YLLs reflect an epidemiological transition, with decreases in total YLLs from enteric infections, respiratory infections and tuberculosis, and maternal and neonatal disorders between 1990 and 2017; these were generally greater in magnitude at the lowest levels of the Socio-demographic Index (SDI). At the same time, there were large increases in YLLs from neoplasms and cardiovascular diseases. YLL rates decreased across the five leading Level 2 causes in all SDI quintiles. The leading causes of YLLs in 1990—neonatal disorders, lower respiratory infections, and diarrhoeal diseases—were ranked second, fourth, and fifth, in 2017. Meanwhile, estimated YLLs increased for ischaemic heart disease (ranked first in 2017) and stroke (ranked third), even though YLL rates decreased. Population growth contributed to increased total deaths across the 20 leading Level 2 causes of mortality between 2007 and 2017. Decreases in the cause-specific mortality rate reduced the effect of population growth for all but three causes: substance use disorders, neurological disorders, and skin and subcutaneous diseases.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Interpretation{\textless}/h3{\textgreater}{\textless}p{\textgreater}Improvements in global health have been unevenly distributed among populations. Deaths due to injuries, substance use disorders, armed conflict and terrorism, neoplasms, and cardiovascular disease are expanding threats to global health. For causes of death such as lower respiratory and enteric infections, more rapid progress occurred for children than for the oldest adults, and there is continuing disparity in mortality rates by sex across age groups. Reductions in the death rate of some common diseases are themselves slowing or have ceased, primarily for NCDs, and the death rate for selected causes has increased in the past decade.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Funding{\textless}/h3{\textgreater}{\textless}p{\textgreater}Bill \& Melinda Gates Foundation.{\textless}/p{\textgreater}},
	language = {English},
	number = {10159},
	urldate = {2019-01-14},
	journal = {The Lancet},
	author = {Roth, Gregory A. and Abate, Degu and Abate, Kalkidan Hassen and Abay, Solomon M. and Abbafati, Cristiana and Abbasi, Nooshin and Abbastabar, Hedayat and Abd-Allah, Foad and Abdela, Jemal and Abdelalim, Ahmed and Abdollahpour, Ibrahim and Abdulkader, Rizwan Suliankatchi and Abebe, Haftom Temesgen and Abebe, Molla and Abebe, Zegeye and Abejie, Ayenew Negesse and Abera, Semaw F. and Abil, Olifan Zewdie and Abraha, Haftom Niguse and Abrham, Aklilu Roba and Abu-Raddad, Laith Jamal and Accrombessi, Manfred Mario Kokou and Acharya, Dilaram and Adamu, Abdu A. and Adebayo, Oladimeji M. and Adedoyin, Rufus Adesoji and Adekanmbi, Victor and Adetokunboh, Olatunji O. and Adhena, Beyene Meressa and Adib, Mina G. and Admasie, Amha and Afshin, Ashkan and Agarwal, Gina and Agesa, Kareha M. and Agrawal, Anurag and Agrawal, Sutapa and Ahmadi, Alireza and Ahmadi, Mehdi and Ahmed, Muktar Beshir and Ahmed, Sayem and Aichour, Amani Nidhal and Aichour, Ibtihel and Aichour, Miloud Taki Eddine and Akbari, Mohammad Esmaeil and Akinyemi, Rufus Olusola and Akseer, Nadia and Al-Aly, Ziyad and Al-Eyadhy, Ayman and Al-Raddadi, Rajaa M. and Alahdab, Fares and Alam, Khurshid and Alam, Tahiya and Alebel, Animut and Alene, Kefyalew Addis and Alijanzadeh, Mehran and Alizadeh-Navaei, Reza and Aljunid, Syed Mohamed and Alkerwi, Ala'a and Alla, François and Allebeck, Peter and Alonso, Jordi and Altirkawi, Khalid and Alvis-Guzman, Nelson and Amare, Azmeraw T. and Aminde, Leopold N. and Amini, Erfan and Ammar, Walid and Amoako, Yaw Ampem and Anber, Nahla Hamed and Andrei, Catalina Liliana and Androudi, Sofia and Animut, Megbaru Debalkie and Anjomshoa, Mina and Ansari, Hossein and Ansha, Mustafa Geleto and Antonio, Carl Abelardo T. and Anwari, Palwasha and Aremu, Olatunde and Ärnlöv, Johan and Arora, Amit and Arora, Monika and Artaman, Al and Aryal, Krishna K. and Asayesh, Hamid and Asfaw, Ephrem Tsegay and Ataro, Zerihun and Atique, Suleman and Atre, Sachin R. and Ausloos, Marcel and Avokpaho, Euripide F. G. A. and Awasthi, Ashish and Quintanilla, Beatriz Paulina Ayala and Ayele, Yohanes and Ayer, Rakesh and Azzopardi, Peter S. and Babazadeh, Arefeh and Bacha, Umar and Badali, Hamid and Badawi, Alaa and Bali, Ayele Geleto and Ballesteros, Katherine E. and Banach, Maciej and Banerjee, Kajori and Bannick, Marlena S. and Banoub, Joseph Adel Mattar and Barboza, Miguel A. and Barker-Collo, Suzanne Lyn and Bärnighausen, Till Winfried and Barquera, Simon and Barrero, Lope H. and Bassat, Quique and Basu, Sanjay and Baune, Bernhard T. and Baynes, Habtamu Wondifraw and Bazargan-Hejazi, Shahrzad and Bedi, Neeraj and Beghi, Ettore and Behzadifar, Masoud and Behzadifar, Meysam and Béjot, Yannick and Bekele, Bayu Begashaw and Belachew, Abate Bekele and Belay, Ezra and Belay, Yihalem Abebe and Bell, Michelle L. and Bello, Aminu K. and Bennett, Derrick A. and Bensenor, Isabela M. and Berman, Adam E. and Bernabe, Eduardo and Bernstein, Robert S. and Bertolacci, Gregory J. and Beuran, Mircea and Beyranvand, Tina and Bhalla, Ashish and Bhattarai, Suraj and Bhaumik, Soumyadeeep and Bhutta, Zulfiqar A. and Biadgo, Belete and Biehl, Molly H. and Bijani, Ali and Bikbov, Boris and Bilano, Ver and Bililign, Nigus and Sayeed, Muhammad Shahdaat Bin and Bisanzio, Donal and Biswas, Tuhin and Blacker, Brigette F. and Basara, Berrak Bora and Borschmann, Rohan and Bosetti, Cristina and Bozorgmehr, Kayvan and Brady, Oliver J. and Brant, Luisa C. and Brayne, Carol and Brazinova, Alexandra and Breitborde, Nicholas J. K. and Brenner, Hermann and Briant, Paul Svitil and Britton, Gabrielle and Brugha, Traolach and Busse, Reinhard and Butt, Zahid A. and Callender, Charlton S. K. H. and Campos-Nonato, Ismael R. and Rincon, Julio Cesar Campuzano and Cano, Jorge and Car, Mate and Cárdenas, Rosario and Carreras, Giulia and Carrero, Juan J. and Carter, Austin and Carvalho, Félix and Castañeda-Orjuela, Carlos A. and Rivas, Jacqueline Castillo and Castle, Chris D. and Castro, Clara and Castro, Franz and Catalá-López, Ferrán and Cerin, Ester and Chaiah, Yazan and Chang, Jung-Chen and Charlson, Fiona J. and Chaturvedi, Pankaj and Chiang, Peggy Pei-Chia and Chimed-Ochir, Odgerel and Chisumpa, Vesper Hichilombwe and Chitheer, Abdulaal and Chowdhury, Rajiv and Christensen, Hanne and Christopher, Devasahayam J. and Chung, Sheng-Chia and Cicuttini, Flavia M. and Ciobanu, Liliana G. and Cirillo, Massimo and Cohen, Aaron J. and Cooper, Leslie Trumbull and Cortesi, Paolo Angelo and Cortinovis, Monica and Cousin, Ewerton and Cowie, Benjamin C. and Criqui, Michael H. and Cromwell, Elizabeth A. and Crowe, Christopher Stephen and Crump, John A. and Cunningham, Matthew and Daba, Alemneh Kabeta and Dadi, Abel Fekadu and Dandona, Lalit and Dandona, Rakhi and Dang, Anh Kim and Dargan, Paul I. and Daryani, Ahmad and Das, Siddharth K. and Gupta, Rajat Das and Neves, José Das and Dasa, Tamirat Tesfaye and Dash, Aditya Prasad and Davis, Adrian C. and Weaver, Nicole Davis and Davitoiu, Dragos Virgil and Davletov, Kairat and Hoz, Fernando Pio De La and Neve, Jan-Walter De and Degefa, Meaza Girma and Degenhardt, Louisa and Degfie, Tizta T. and Deiparine, Selina and Demoz, Gebre Teklemariam and Demtsu, Balem Betsu and Denova-Gutiérrez, Edgar and Deribe, Kebede and Dervenis, Nikolaos and Jarlais, Don C. Des and Dessie, Getenet Ayalew and Dey, Subhojit and Dharmaratne, Samath D. and Dicker, Daniel and Dinberu, Mesfin Tadese and Ding, Eric L. and Dirac, M. Ashworth and Djalalinia, Shirin and Dokova, Klara and Doku, David Teye and Donnelly, Christl A. and Dorsey, E. Ray and Doshi, Pratik P. and Douwes-Schultz, Dirk and Doyle, Kerrie E. and Driscoll, Tim R. and Dubey, Manisha and Dubljanin, Eleonora and Duken, Eyasu Ejeta and Duncan, Bruce B. and Duraes, Andre R. and Ebrahimi, Hedyeh and Ebrahimpour, Soheil and Edessa, Dumessa and Edvardsson, David and Eggen, Anne Elise and Bcheraoui, Charbel El and Zaki, Maysaa El Sayed and El-Khatib, Ziad and Elkout, Hajer and Ellingsen, Christian Lycke and Endres, Matthias and Endries, Aman Yesuf and Er, Benjamin and Erskine, Holly E. and Eshrati, Babak and Eskandarieh, Sharareh and Esmaeili, Reza and Esteghamati, Alireza and Fakhar, Mahdi and Fakhim, Hamed and Faramarzi, Mahbobeh and Fareed, Mohammad and Farhadi, Farzaneh and Farinha, Carla Sofia E. sá and Faro, Andre and Farvid, Maryam S. and Farzadfar, Farshad and Farzaei, Mohammad Hosein and Feigin, Valery L. and Feigl, Andrea B. and Fentahun, Netsanet and Fereshtehnejad, Seyed-Mohammad and Fernandes, Eduarda and Fernandes, Joao C. and Ferrari, Alize J. and Feyissa, Garumma Tolu and Filip, Irina and Finegold, Samuel and Fischer, Florian and Fitzmaurice, Christina and Foigt, Nataliya A. and Foreman, Kyle J. and Fornari, Carla and Frank, Tahvi D. and Fukumoto, Takeshi and Fuller, John E. and Fullman, Nancy and Fürst, Thomas and Furtado, João M. and Futran, Neal D. and Gallus, Silvano and Garcia-Basteiro, Alberto L. and Garcia-Gordillo, Miguel A. and Gardner, William M. and Gebre, Abadi Kahsu and Gebrehiwot, Tsegaye Tewelde and Gebremedhin, Amanuel Tesfay and Gebremichael, Bereket and Gebremichael, Teklu Gebrehiwo and Gelano, Tilayie Feto and Geleijnse, Johanna M. and Genova-Maleras, Ricard and Geramo, Yilma Chisha Dea and Gething, Peter W. and Gezae, Kebede Embaye and Ghadami, Mohammad Rasoul and Ghadimi, Reza and Falavarjani, Khalil Ghasemi and Ghasemi-Kasman, Maryam and Ghimire, Mamata and Gibney, Katherine B. and Gill, Paramjit Singh and Gill, Tiffany K. and Gillum, Richard F. and Ginawi, Ibrahim Abdelmageed and Giroud, Maurice and Giussani, Giorgia and Goenka, Shifalika and Goldberg, Ellen M. and Goli, Srinivas and Gómez-Dantés, Hector and Gona, Philimon N. and Gopalani, Sameer Vali and Gorman, Taren M. and Goto, Atsushi and Goulart, Alessandra C. and Gnedovskaya, Elena V. and Grada, Ayman and Grosso, Giuseppe and Gugnani, Harish Chander and Guimaraes, Andre Luiz Sena and Guo, Yuming and Gupta, Prakash C. and Gupta, Rahul and Gupta, Rajeev and Gupta, Tanush and Gutiérrez, Reyna Alma and Gyawali, Bishal and Haagsma, Juanita A. and Hafezi-Nejad, Nima and Hagos, Tekleberhan B. and Hailegiyorgis, Tewodros Tesfa and Hailu, Gessessew Bugssa and Haj-Mirzaian, Arvin and Haj-Mirzaian, Arya and Hamadeh, Randah R. and Hamidi, Samer and Handal, Alexis J. and Hankey, Graeme J. and Harb, Hilda L. and Harikrishnan, Sivadasanpillai and Haro, Josep Maria and Hasan, Mehedi and Hassankhani, Hadi and Hassen, Hamid Yimam and Havmoeller, Rasmus and Hay, Roderick J. and Hay, Simon I. and He, Yihua and Hedayatizadeh-Omran, Akbar and Hegazy, Mohamed I. and Heibati, Behzad and Heidari, Mohsen and Hendrie, Delia and Henok, Andualem and Henry, Nathaniel J. and Herteliu, Claudiu and Heydarpour, Fatemeh and Heydarpour, Pouria and Heydarpour, Sousan and Hibstu, Desalegn Tsegaw and Hoek, Hans W. and Hole, Michael K. and Rad, Enayatollah Homaie and Hoogar, Praveen and Hosgood, H. Dean and Hosseini, Seyed Mostafa and Hosseinzadeh, Mehdi and Hostiuc, Mihaela and Hostiuc, Sorin and Hotez, Peter J. and Hoy, Damian G. and Hsiao, Thomas and Hu, Guoqing and Huang, John J. and Husseini, Abdullatif and Hussen, Mohammedaman Mama and Hutfless, Susan and Idrisov, Bulat and Ilesanmi, Olayinka Stephen and Iqbal, Usman and Irvani, Seyed Sina Naghibi and Irvine, Caleb Mackay Salpeter and Islam, Nazrul and Islam, Sheikh Mohammed Shariful and Islami, Farhad and Jacobsen, Kathryn H. and Jahangiry, Leila and Jahanmehr, Nader and Jain, Sudhir Kumar and Jakovljevic, Mihajlo and Jalu, Moti Tolera and James, Spencer L. and Javanbakht, Mehdi and Jayatilleke, Achala Upendra and Jeemon, Panniyammakal and Jenkins, Kathy J. and Jha, Ravi Prakash and Jha, Vivekanand and Johnson, Catherine O. and Johnson, Sarah C. and Jonas, Jost B. and Joshi, Ankur and Jozwiak, Jacek Jerzy and Jungari, Suresh Banayya and Jürisson, Mikk and Kabir, Zubair and Kadel, Rajendra and Kahsay, Amaha and Kalani, Rizwan and Karami, Manoochehr and Matin, Behzad Karami and Karch, André and Karema, Corine and Karimi-Sari, Hamidreza and Kasaeian, Amir and Kassa, Dessalegn H. and Kassa, Getachew Mullu and Kassa, Tesfaye Dessale and Kassebaum, Nicholas J. and Katikireddi, Srinivasa Vittal and Kaul, Anil and Kazemi, Zhila and Karyani, Ali Kazemi and Kazi, Dhruv Satish and Kefale, Adane Teshome and Keiyoro, Peter Njenga and Kemp, Grant Rodgers and Kengne, Andre Pascal and Keren, Andre and Kesavachandran, Chandrasekharan Nair and Khader, Yousef Saleh and Khafaei, Behzad and Khafaie, Morteza Abdullatif and Khajavi, Alireza and Khalid, Nauman and Khalil, Ibrahim A. and Khan, Ejaz Ahmad and Khan, Muhammad Shahzeb and Khan, Muhammad Ali and Khang, Young-Ho and Khater, Mona M. and Khoja, Abdullah T. and Khosravi, Ardeshir and Khosravi, Mohammad Hossein and Khubchandani, Jagdish and Kiadaliri, Aliasghar A. and Kibret, Getiye D. and Kidanemariam, Zelalem Teklemariam and Kiirithio, Daniel N. and Kim, Daniel and Kim, Young-Eun and Kim, Yun Jin and Kimokoti, Ruth W. and Kinfu, Yohannes and Kisa, Adnan and Kissimova-Skarbek, Katarzyna and Kivimäki, Mika and Knudsen, Ann Kristin Skrindo and Kocarnik, Jonathan M. and Kochhar, Sonali and Kokubo, Yoshihiro and Kolola, Tufa and Kopec, Jacek A. and Koul, Parvaiz A. and Koyanagi, Ai and Kravchenko, Michael A. and Krishan, Kewal and Defo, Barthelemy Kuate and Bicer, Burcu Kucuk and Kumar, G. Anil and Kumar, Manasi and Kumar, Pushpendra and Kutz, Michael J. and Kuzin, Igor and Kyu, Hmwe Hmwe and Lad, Deepesh P. and Lad, Sheetal D. and Lafranconi, Alessandra and Lal, Dharmesh Kumar and Lalloo, Ratilal and Lallukka, Tea and Lam, Jennifer O. and Lami, Faris Hasan and Lansingh, Van C. and Lansky, Sonia and Larson, Heidi J. and Latifi, Arman and Lau, Kathryn Mei-Ming and Lazarus, Jeffrey V. and Lebedev, Georgy and Lee, Paul H. and Leigh, James and Leili, Mostafa and Leshargie, Cheru Tesema and Li, Shanshan and Li, Yichong and Liang, Juan and Lim, Lee-Ling and Lim, Stephen S. and Limenih, Miteku Andualem and Linn, Shai and Liu, Shiwei and Liu, Yang and Lodha, Rakesh and Lonsdale, Chris and Lopez, Alan D. and Lorkowski, Stefan and Lotufo, Paulo A. and Lozano, Rafael and Lunevicius, Raimundas and Ma, Stefan and Macarayan, Erlyn Rachelle King and Mackay, Mark T. and MacLachlan, Jennifer H. and Maddison, Emilie R. and Madotto, Fabiana and Razek, Hassan Magdy Abd El and Razek, Muhammed Magdy Abd El and Maghavani, Dhaval P. and Majdan, Marek and Majdzadeh, Reza and Majeed, Azeem and Malekzadeh, Reza and Malta, Deborah Carvalho and Manda, Ana-Laura and Mandarano-Filho, Luiz Garcia and Manguerra, Helena and Mansournia, Mohammad Ali and Mapoma, Chabila Christopher and Marami, Dadi and Maravilla, Joemer C. and Marcenes, Wagner and Marczak, Laurie and Marks, Ashley and Marks, Guy B. and Martinez, Gabriel and Martins-Melo, Francisco Rogerlândio and Martopullo, Ira and März, Winfried and Marzan, Melvin B. and Masci, Joseph R. and Massenburg, Benjamin Ballard and Mathur, Manu Raj and Mathur, Prashant and Matzopoulos, Richard and Maulik, Pallab K. and Mazidi, Mohsen and McAlinden, Colm and McGrath, John J. and McKee, Martin and McMahon, Brian J. and Mehata, Suresh and Mehndiratta, Man Mohan and Mehrotra, Ravi and Mehta, Kala M. and Mehta, Varshil and Mekonnen, Tefera C. and Melese, Addisu and Melku, Mulugeta and Memiah, Peter T. N. and Memish, Ziad A. and Mendoza, Walter and Mengistu, Desalegn Tadese and Mengistu, Getnet and Mensah, George A. and Mereta, Seid Tiku and Meretoja, Atte and Meretoja, Tuomo J. and Mestrovic, Tomislav and Mezgebe, Haftay Berhane and Miazgowski, Bartosz and Miazgowski, Tomasz and Millear, Anoushka I. and Miller, Ted R. and Miller-Petrie, Molly Katherine and Mini, G. K. and Mirabi, Parvaneh and Mirarefin, Mojde and Mirica, Andreea and Mirrakhimov, Erkin M. and Misganaw, Awoke Temesgen and Mitiku, Habtamu and Moazen, Babak and Mohammad, Karzan Abdulmuhsin and Mohammadi, Moslem and Mohammadifard, Noushin and Mohammed, Mohammed A. and Mohammed, Shafiu and Mohan, Viswanathan and Mokdad, Ali H. and Molokhia, Mariam and Monasta, Lorenzo and Moradi, Ghobad and Moradi-Lakeh, Maziar and Moradinazar, Mehdi and Moraga, Paula and Morawska, Lidia and Velásquez, Ilais Moreno and Morgado-Da-Costa, Joana and Morrison, Shane Douglas and Moschos, Marilita M. and Mouodi, Simin and Mousavi, Seyyed Meysam and Muchie, Kindie Fentahun and Mueller, Ulrich Otto and Mukhopadhyay, Satinath and Muller, Kate and Mumford, John Everett and Musa, Jonah and Musa, Kamarul Imran and Mustafa, Ghulam and Muthupandian, Saravanan and Nachega, Jean B. and Nagel, Gabriele and Naheed, Aliya and Nahvijou, Azin and Naik, Gurudatta and Nair, Sanjeev and Najafi, Farid and Naldi, Luigi and Nam, Hae Sung and Nangia, Vinay and Nansseu, Jobert Richie and Nascimento, Bruno Ramos and Natarajan, Gopalakrishnan and Neamati, Nahid and Negoi, Ionut and Negoi, Ruxandra Irina and Neupane, Subas and Newton, Charles R. J. and Ngalesoni, Frida N. and Ngunjiri, Josephine W. and Nguyen, Anh Quynh and Nguyen, Grant and Nguyen, Ha Thu and Nguyen, Huong Thanh and Nguyen, Long Hoang and Nguyen, Minh and Nguyen, Trang Huyen and Nichols, Emma and Ningrum, Dina Nur Anggraini and Nirayo, Yirga Legesse and Nixon, Molly R. and Nolutshungu, Nomonde and Nomura, Shuhei and Norheim, Ole F. and Noroozi, Mehdi and Norrving, Bo and Noubiap, Jean Jacques and Nouri, Hamid Reza and Shiadeh, Malihe Nourollahpour and Nowroozi, Mohammad Reza and Nyasulu, Peter S. and Odell, Christopher M. and Ofori-Asenso, Richard and Ogbo, Felix Akpojene and Oh, In-Hwan and Oladimeji, Olanrewaju and Olagunju, Andrew T. and Olivares, Pedro R. and Olsen, Helen Elizabeth and Olusanya, Bolajoko Olubukunola and Olusanya, Jacob Olusegun and Ong, Kanyin L. and Ong, Sok King Sk and Oren, Eyal and Orpana, Heather M. and Ortiz, Alberto and Ortiz, Justin R. and Otstavnov, Stanislav S. and Øverland, Simon and Owolabi, Mayowa Ojo and Özdemir, Raziye and A, Mahesh P. and Pacella, Rosana and Pakhale, Smita and Pakhare, Abhijit P. and Pakpour, Amir H. and Pana, Adrian and Panda-Jonas, Songhomitra and Pandian, Jeyaraj Durai and Parisi, Andrea and Park, Eun-Kee and Parry, Charles D. H. and Parsian, Hadi and Patel, Shanti and Pati, Sanghamitra and Patton, George C. and Paturi, Vishnupriya Rao and Paulson, Katherine R. and Pereira, Alexandre and Pereira, David M. and Perico, Norberto and Pesudovs, Konrad and Petzold, Max and Phillips, Michael R. and Piel, Frédéric B. and Pigott, David M. and Pillay, Julian David and Pirsaheb, Meghdad and Pishgar, Farhad and Polinder, Suzanne and Postma, Maarten J. and Pourshams, Akram and Poustchi, Hossein and Pujar, Ashwini and Prakash, Swayam and Prasad, Narayan and Purcell, Caroline A. and Qorbani, Mostafa and Quintana, Hedley and Quistberg, D. Alex and Rade, Kirankumar Waman and Radfar, Amir and Rafay, Anwar and Rafiei, Alireza and Rahim, Fakher and Rahimi, Kazem and Rahimi-Movaghar, Afarin and Rahman, Mahfuzar and Rahman, Mohammad Hifz Ur and Rahman, Muhammad Aziz and Rai, Rajesh Kumar and Rajsic, Sasa and Ram, Usha and Ranabhat, Chhabi Lal and Ranjan, Prabhat and Rao, Puja C. and Rawaf, David Laith and Rawaf, Salman and Razo-García, Christian and Reddy, K. Srinath and Reiner, Robert C. and Reitsma, Marissa B. and Remuzzi, Giuseppe and Renzaho, Andre M. N. and Resnikoff, Serge and Rezaei, Satar and Rezaeian, Shahab and Rezai, Mohammad Sadegh and Riahi, Seyed Mohammad and Ribeiro, Antonio Luiz P. and Rios-Blancas, Maria Jesus and Roba, Kedir Teji and Roberts, Nicholas L. S. and Robinson, Stephen R. and Roever, Leonardo and Ronfani, Luca and Roshandel, Gholamreza and Rostami, Ali and Rothenbacher, Dietrich and Roy, Ambuj and Rubagotti, Enrico and Sachdev, Perminder S. and Saddik, Basema and Sadeghi, Ehsan and Safari, Hosein and Safdarian, Mahdi and Safi, Sare and Safiri, Saeid and Sagar, Rajesh and Sahebkar, Amirhossein and Sahraian, Mohammad Ali and Salam, Nasir and Salama, Joseph S. and Salamati, Payman and Saldanha, Raphael De Freitas and Saleem, Zikria and Salimi, Yahya and Salvi, Sundeep Santosh and Salz, Inbal and Sambala, Evanson Zondani and Samy, Abdallah M. and Sanabria, Juan and Sanchez-Niño, Maria Dolores and Santomauro, Damian Francesco and Santos, Itamar S. and Santos, João Vasco and Milicevic, Milena M. Santric and Jose, Bruno Piassi Sao and Sarker, Abdur Razzaque and Sarmiento-Suárez, Rodrigo and Sarrafzadegan, Nizal and Sartorius, Benn and Sarvi, Shahabeddin and Sathian, Brijesh and Satpathy, Maheswar and Sawant, Arundhati R. and Sawhney, Monika and Saxena, Sonia and Sayyah, Mehdi and Schaeffner, Elke and Schmidt, Maria Inês and Schneider, Ione J. C. and Schöttker, Ben and Schutte, Aletta Elisabeth and Schwebel, David C. and Schwendicke, Falk and Scott, James G. and Sekerija, Mario and Sepanlou, Sadaf G. and Serván-Mori, Edson and Seyedmousavi, Seyedmojtaba and Shabaninejad, Hosein and Shackelford, Katya Anne and Shafieesabet, Azadeh and Shahbazi, Mehdi and Shaheen, Amira A. and Shaikh, Masood Ali and Shams-Beyranvand, Mehran and Shamsi, Mohammadbagher and Shamsizadeh, Morteza and Sharafi, Kiomars and Sharif, Mehdi and Sharif-Alhoseini, Mahdi and Sharma, Rajesh and She, Jun and Sheikh, Aziz and Shi, Peilin and Shiferaw, Mekonnen Sisay and Shigematsu, Mika and Shiri, Rahman and Shirkoohi, Reza and Shiue, Ivy and Shokraneh, Farhad and Shrime, Mark G. and Si, Si and Siabani, Soraya and Siddiqi, Tariq J. and Sigfusdottir, Inga Dora and Sigurvinsdottir, Rannveig and Silberberg, Donald H. and Silva, Diego Augusto Santos and Silva, João Pedro and Silva, Natacha Torres Da and Silveira, Dayane Gabriele Alves and Singh, Jasvinder A. and Singh, Narinder Pal and Singh, Prashant Kumar and Singh, Virendra and Sinha, Dhirendra Narain and Sliwa, Karen and Smith, Mari and Sobaih, Badr Hasan and Sobhani, Soheila and Sobngwi, Eugène and Soneji, Samir S. and Soofi, Moslem and Sorensen, Reed J. D. and Soriano, Joan B. and Soyiri, Ireneous N. and Sposato, Luciano A. and Sreeramareddy, Chandrashekhar T. and Srinivasan, Vinay and Stanaway, Jeffrey D. and Starodubov, Vladimir I. and Stathopoulou, Vasiliki and Stein, Dan J. and Steiner, Caitlyn and Stewart, Leo G. and Stokes, Mark A. and Subart, Michelle L. and Sudaryanto, Agus and Sufiyan, Mu'awiyyah Babale and Sur, Patrick John and Sutradhar, Ipsita and Sykes, Bryan L. and Sylaja, P. N. and Sylte, Dillon O. and Szoeke, Cassandra E. I. and Tabarés-Seisdedos, Rafael and Tabuchi, Takahiro and Tadakamadla, Santosh Kumar and Takahashi, Ken and Tandon, Nikhil and Tassew, Segen Gebremeskel and Taveira, Nuno and Tehrani-Banihashemi, Arash and Tekalign, Tigist Gashaw and Tekle, Merhawi Gebremedhin and Temsah, Mohamad-Hani and Temsah, Omar and Terkawi, Abdullah Sulieman and Teshale, Manaye Yihune and Tessema, Belay and Tessema, Gizachew Assefa and Thankappan, Kavumpurathu Raman and Thirunavukkarasu, Sathish and Thomas, Nihal and Thrift, Amanda G. and Thurston, George D. and Tilahun, Binyam and To, Quyen G. and Tobe-Gai, Ruoyan and Tonelli, Marcello and Topor-Madry, Roman and Torre, Anna E. and Tortajada-Girbés, Miguel and Touvier, Mathilde and Tovani-Palone, Marcos Roberto and Tran, Bach Xuan and Tran, Khanh Bao and Tripathi, Suryakant and Troeger, Christopher E. and Truelsen, Thomas Clement and Truong, Nu Thi and Tsadik, Afewerki Gebremeskel and Tsoi, Derrick and Car, Lorainne Tudor and Tuzcu, E. Murat and Tyrovolas, Stefanos and Ukwaja, Kingsley N. and Ullah, Irfan and Undurraga, Eduardo A. and Updike, Rachel L. and Usman, Muhammad Shariq and Uthman, Olalekan A. and Uzun, Selen Begüm and Vaduganathan, Muthiah and Vaezi, Afsane and Vaidya, Gaurang and Valdez, Pascual R. and Varavikova, Elena and Vasankari, Tommi Juhani and Venketasubramanian, Narayanaswamy and Villafaina, Santos and Violante, Francesco S. and Vladimirov, Sergey Konstantinovitch and Vlassov, Vasily and Vollset, Stein Emil and Vos, Theo and Wagner, Gregory R. and Wagnew, Fasil Shiferaw and Waheed, Yasir and Wallin, Mitchell Taylor and Walson, Judd L. and Wang, Yanping and Wang, Yuan-Pang and Wassie, Molla Mesele and Weiderpass, Elisabete and Weintraub, Robert G. and Weldegebreal, Fitsum and Weldegwergs, Kidu Gidey and Werdecker, Andrea and Werkneh, Adhena Ayaliew and West, T. Eoin and Westerman, Ronny and Whiteford, Harvey A. and Widecka, Justyna and Wilner, Lauren B. and Wilson, Shadrach and Winkler, Andrea Sylvia and Wiysonge, Charles Shey and Wolfe, Charles D. A. and Wu, Shouling and Wu, Yun-Chun and Wyper, Grant M. A. and Xavier, Denis and Xu, Gelin and Yadgir, Simon and Yadollahpour, Ali and Jabbari, Seyed Hossein Yahyazadeh and Yakob, Bereket and Yan, Lijing L. and Yano, Yuichiro and Yaseri, Mehdi and Yasin, Yasin Jemal and Yentür, Gökalp Kadri and Yeshaneh, Alex and Yimer, Ebrahim M. and Yip, Paul and Yirsaw, Biruck Desalegn and Yisma, Engida and Yonemoto, Naohiro and Yonga, Gerald and Yoon, Seok-Jun and Yotebieng, Marcel and Younis, Mustafa Z. and Yousefifard, Mahmoud and Yu, Chuanhua and Zadnik, Vesna and Zaidi, Zoubida and Zaman, Sojib Bin and Zamani, Mohammad and Zare, Zohreh and Zeleke, Ayalew Jejaw and Zenebe, Zerihun Menlkalew and Zhang, Anthony Lin and Zhang, Kai and Zhou, Maigeng and Zodpey, Sanjay and Zuhlke, Liesl Joanna and Naghavi, Mohsen and Murray, Christopher J. L.},
	month = nov,
	year = {2018},
	pmid = {30496103},
	pages = {1736--1788},
}

@article{reyland_generalized_2014,
	title = {Generalized {Wiener} system identification: general backlash nonlinearity and finite impulse response linear part},
	volume = {28},
	issn = {1099-1115},
	doi = {10.1002/acs.2437},
	number = {11},
	journal = {International Journal of Adaptive Control and Signal Processing},
	author = {Reyland, John and Bai, Er‐Wei},
	year = {2014},
	note = {00000},
	pages = {1174--1188},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {http://www.aclweb.org/anthology/D14-1162},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	month = oct,
	year = {2014},
	note = {00000},
	pages = {1532--1543},
}

@book{worldhealthorganization_global_2014,
	address = {Geneva},
	title = {Global status report on noncommunicable diseases 2014: attaining the nine global noncommunicable diseases targets; a shared responsibility.},
	isbn = {978-92-4-156485-4},
	shorttitle = {Global status report on noncommunicable diseases 2014},
	language = {en},
	publisher = {World Health Organization},
	author = {{World Health Organization}},
	year = {2014},
	note = {OCLC: 907517003},
}

@article{friedman_glmnet_2009,
	title = {glmnet: {Lasso} and elastic-net regularized generalized linear models},
	volume = {1},
	number = {4},
	journal = {R package version},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
	year = {2009},
	keywords = {🔍No DOI found},
}

@article{wang_generalized_2015,
	title = {Generalized {Single}-{Hidden} {Layer} {Feedforward} {Networks} for {Regression} {Problems}},
	volume = {26},
	issn = {2162-237X},
	doi = {10.1109/TNNLS.2014.2334366},
	abstract = {In this paper, traditional single-hidden layer feedforward network (SLFN) is extended to novel generalized SLFN (GSLFN) by employing polynomial functions of inputs as output weights connecting randomly generated hidden units with corresponding output nodes. The significant contributions of this paper are as follows: 1) a primal GSLFN (P-GSLFN) is implemented using randomly generated hidden nodes and polynomial output weights whereby the regression matrix is augmented by full or partial input variables and only polynomial coefficients are to be estimated; 2) a simplified GSLFN (S-GSLFN) is realized by decomposing the polynomial output weights of the P-GSLFN into randomly generated polynomial nodes and tunable output weights; 3) both P- and S-GSLFN are able to achieve universal approximation if the output weights are tuned by ridge regression estimators; and 4) by virtue of the developed batch and online sequential ridge ELM (BR-ELM and OSR-ELM) learning algorithms, high performance of the proposed GSLFNs in terms of generalization and learning speed is guaranteed. Comprehensive simulation studies and comparisons with standard SLFNs are carried out on real-world regression benchmark data sets. Simulation results demonstrate that the innovative GSLFNs using BR-ELM and OSR-ELM are superior to standard SLFNs in terms of accuracy, training speed, and structure compactness.},
	number = {6},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Wang, N. and Er, M. J. and Han, M.},
	month = jun,
	year = {2015},
	note = {00073},
	keywords = {Approximation capability, Approximation methods, BR-ELM, Educational institutions, GSLFN, Joining processes, OSR-ELM, Standards, batch ridge ELM learning algorithms, extreme learning machine (ELM), feedforward neural nets, feedforward neural networks, generalized single-hidden layer feedforward networks, generalized single-hidden layer feedforward networks (GSLFN), learning (artificial intelligence), matrix algebra, novel generalized SLFN, online sequential ridge ELM learning algorithms, polynomial functions, polynomial output weights, polynomials, randomly generated hidden nodes, regression analysis, regression problems, ridge regression, ridge regression., universal approximation},
	pages = {1161--1176},
}

@inproceedings{szegedy_going_2015,
	title = {Going deeper with convolutions},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	year = {2015},
	note = {00000},
	pages = {1--9},
}

@book{mccullagh_generalized_1989,
	series = {Chapman \& {Hall}/{CRC} {Monographs} on {Statistics} \& {Applied} {Probability}},
	title = {Generalized {Linear} {Models}, {Second} {Edition}},
	isbn = {978-0-412-31760-6},
	url = {https://books.google.com.br/books?id=h9kFH2_FfBkC},
	publisher = {Taylor \& Francis},
	author = {McCullagh, P. and Nelder, J.A.},
	year = {1989},
}

@book{rasmussen_gaussian_2006,
	address = {Cambridge, Mass},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2006},
	note = {OCLC: ocm61285753},
	keywords = {Data processing, Gaussian processes, Mathematical models, machine learning},
}

@incollection{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Nets}},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	urldate = {2018-10-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {2672--2680},
}

@article{gbd2016causesofdeathcollaborators_global_2017,
	title = {Global, regional, and national age-sex specific mortality for 264 causes of death, 1980-2016: a systematic analysis for the {Global} {Burden} of {Disease} {Study} 2016},
	volume = {390},
	issn = {1474-547X},
	shorttitle = {Global, regional, and national age-sex specific mortality for 264 causes of death, 1980-2016},
	doi = {10.1016/S0140-6736(17)32152-9},
	abstract = {BACKGROUND: Monitoring levels and trends in premature mortality is crucial to understanding how societies can address prominent sources of early death. The Global Burden of Disease 2016 Study (GBD 2016) provides a comprehensive assessment of cause-specific mortality for 264 causes in 195 locations from 1980 to 2016. This assessment includes evaluation of the expected epidemiological transition with changes in development and where local patterns deviate from these trends.
METHODS: We estimated cause-specific deaths and years of life lost (YLLs) by age, sex, geography, and year. YLLs were calculated from the sum of each death multiplied by the standard life expectancy at each age. We used the GBD cause of death database composed of: vital registration (VR) data corrected for under-registration and garbage coding; national and subnational verbal autopsy (VA) studies corrected for garbage coding; and other sources including surveys and surveillance systems for specific causes such as maternal mortality. To facilitate assessment of quality, we reported on the fraction of deaths assigned to GBD Level 1 or Level 2 causes that cannot be underlying causes of death (major garbage codes) by location and year. Based on completeness, garbage coding, cause list detail, and time periods covered, we provided an overall data quality rating for each location with scores ranging from 0 stars (worst) to 5 stars (best). We used robust statistical methods including the Cause of Death Ensemble model (CODEm) to generate estimates for each location, year, age, and sex. We assessed observed and expected levels and trends of cause-specific deaths in relation to the Socio-demographic Index (SDI), a summary indicator derived from measures of average income per capita, educational attainment, and total fertility, with locations grouped into quintiles by SDI. Relative to GBD 2015, we expanded the GBD cause hierarchy by 18 causes of death for GBD 2016.
FINDINGS: The quality of available data varied by location. Data quality in 25 countries rated in the highest category (5 stars), while 48, 30, 21, and 44 countries were rated at each of the succeeding data quality levels. Vital registration or verbal autopsy data were not available in 27 countries, resulting in the assignment of a zero value for data quality. Deaths from non-communicable diseases (NCDs) represented 72·3\% (95\% uncertainty interval [UI] 71·2-73·2) of deaths in 2016 with 19·3\% (18·5-20·4) of deaths in that year occurring from communicable, maternal, neonatal, and nutritional (CMNN) diseases and a further 8·43\% (8·00-8·67) from injuries. Although age-standardised rates of death from NCDs decreased globally between 2006 and 2016, total numbers of these deaths increased; both numbers and age-standardised rates of death from CMNN causes decreased in the decade 2006-16-age-standardised rates of deaths from injuries decreased but total numbers varied little. In 2016, the three leading global causes of death in children under-5 were lower respiratory infections, neonatal preterm birth complications, and neonatal encephalopathy due to birth asphyxia and trauma, combined resulting in 1·80 million deaths (95\% UI 1·59 million to 1·89 million). Between 1990 and 2016, a profound shift toward deaths at older ages occurred with a 178\% (95\% UI 176-181) increase in deaths in ages 90-94 years and a 210\% (208-212) increase in deaths older than age 95 years. The ten leading causes by rates of age-standardised YLL significantly decreased from 2006 to 2016 (median annualised rate of change was a decrease of 2·89\%); the median annualised rate of change for all other causes was lower (a decrease of 1·59\%) during the same interval. Globally, the five leading causes of total YLLs in 2016 were cardiovascular diseases; diarrhoea, lower respiratory infections, and other common infectious diseases; neoplasms; neonatal disorders; and HIV/AIDS and tuberculosis. At a finer level of disaggregation within cause groupings, the ten leading causes of total YLLs in 2016 were ischaemic heart disease, cerebrovascular disease, lower respiratory infections, diarrhoeal diseases, road injuries, malaria, neonatal preterm birth complications, HIV/AIDS, chronic obstructive pulmonary disease, and neonatal encephalopathy due to birth asphyxia and trauma. Ischaemic heart disease was the leading cause of total YLLs in 113 countries for men and 97 countries for women. Comparisons of observed levels of YLLs by countries, relative to the level of YLLs expected on the basis of SDI alone, highlighted distinct regional patterns including the greater than expected level of YLLs from malaria and from HIV/AIDS across sub-Saharan Africa; diabetes mellitus, especially in Oceania; interpersonal violence, notably within Latin America and the Caribbean; and cardiomyopathy and myocarditis, particularly in eastern and central Europe. The level of YLLs from ischaemic heart disease was less than expected in 117 of 195 locations. Other leading causes of YLLs for which YLLs were notably lower than expected included neonatal preterm birth complications in many locations in both south Asia and southeast Asia, and cerebrovascular disease in western Europe.
INTERPRETATION: The past 37 years have featured declining rates of communicable, maternal, neonatal, and nutritional diseases across all quintiles of SDI, with faster than expected gains for many locations relative to their SDI. A global shift towards deaths at older ages suggests success in reducing many causes of early death. YLLs have increased globally for causes such as diabetes mellitus or some neoplasms, and in some locations for causes such as drug use disorders, and conflict and terrorism. Increasing levels of YLLs might reflect outcomes from conditions that required high levels of care but for which effective treatments remain elusive, potentially increasing costs to health systems.
FUNDING: Bill \& Melinda Gates Foundation.},
	language = {eng},
	number = {10100},
	journal = {Lancet (London, England)},
	author = {{GBD 2016 Causes of Death Collaborators}},
	month = sep,
	year = {2017},
	pmid = {28919116},
	pmcid = {PMC5605883},
	keywords = {Adolescent, Adult, Age Distribution, Aged, Aged, 80 and over, Cause of Death, Child, Child, Preschool, Communicable Diseases, Disasters, Female, Global Burden of Disease, Global Health, Humans, Infant, Infant, Newborn, Male, Middle Aged, Noncommunicable Diseases, Nutrition Disorders, Pregnancy, Pregnancy Complications, Socioeconomic Factors, Wounds and Injuries, Young Adult},
	pages = {1151--1210},
}

@article{kaminnski_genetic_1996,
	title = {Genetic algorithms and artificial neural networks for description of thermal deterioration processes},
	volume = {14},
	doi = {10.1080/07373939608917198},
	number = {9},
	journal = {Drying Technology},
	author = {Kamińnski, W and Strumitto, P and Tomczak, E},
	year = {1996},
	pages = {2117--2133},
}

@book{goldberg_genetic_2000,
	title = {Genetic {Algorithms}–{In} {Search}, {Optimization} \& {Machine} {Learning}, {Revised} ed},
	publisher = {Addison Wesley, New Delhi},
	author = {Goldberg, David E},
	year = {2000},
}

@article{menick_generating_2019,
	title = {{GENERATING} {HIGH} {FIDELITY} {IMAGES} {WITH} {SUBSCALE} {PIXEL} {NETWORKS} {AND} {MULTIDIMENSIONAL} {UPSCALING}},
	abstract = {The unconditional generation of high ﬁdelity images is a longstanding benchmark for testing the performance of image decoders. Autoregressive image models have been able to generate small images unconditionally, but the extension of these methods to large images where ﬁdelity can be more readily assessed has remained an open problem. Among the major challenges are the capacity to encode the vast previous context and the sheer difﬁculty of learning a distribution that preserves both global semantic coherence and exactness of detail. To address the former challenge, we propose the Subscale Pixel Network (SPN), a conditional decoder architecture that generates an image as a sequence of sub-images of equal size. The SPN compactly captures image-wide spatial dependencies and requires a fraction of the memory and the computation required by other fully autoregressive models. To address the latter challenge, we propose to use Multidimensional Upscaling to grow an image in both size and depth via intermediate stages utilising distinct SPNs. We evaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of ImageNet from size 32 to 256. We achieve state-of-the-art likelihood results in multiple settings, set up new benchmark results in previously unexplored settings and are able to generate very high ﬁdelity large scale samples on the basis of both datasets.},
	language = {en},
	author = {Menick, Jacob and Kalchbrenner, Nal},
	year = {2019},
	keywords = {⛔ No DOI found},
	pages = {15},
}

@article{pennington_geometry_,
	title = {Geometry of {Neural} {Network} {Loss} {Surfaces} via {Random} {Matrix} {Theory}},
	language = {en},
	author = {Pennington, Jeffrey and Bahri, Yasaman},
	pages = {9},
}

@inproceedings{ifju_flexiblewingbased_2002,
	title = {Flexible-wing-based micro air vehicles},
	author = {Ifju, P and Jenkins, D and Ettinger, Scott and Lian, Yongsheng and Shyy, Wei and Waszak, M},
	year = {2002},
	pages = {705},
}

@article{lensink_fully_2019,
	title = {Fully {Hyperbolic} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1905.10484},
	abstract = {Convolutional Neural Networks (CNN) have recently seen tremendous success in various computer vision tasks. However, their application to problems with high dimensional input and output, such as high-resolution image and video segmentation or 3D medical imaging, has been limited by various factors. Primarily, in the training stage, it is necessary to store network activations for back propagation. In these settings, the memory requirements associated with storing activations can exceed what is feasible with current hardware, especially for problems in 3D. Previously proposed reversible architectures allow one to recalculate activations in the backwards pass instead of storing them. For computer visions tasks, only block reversible networks have been possible because pooling operations are not reversible. Block-reversibility still requires storing a number of activations that grows with the number of blocks. Motivated by the propagation of signals over physical networks, that are governed by the hyperbolic Telegraph equation, in this work we introduce a fully conservative hyperbolic network for problems with high dimensional input and output. We introduce a coarsening operation that allows completely reversible CNNs by using the Discrete Wavelet Transform and its inverse to both coarsen and interpolate the network state and change the number of channels. This means that during training we do not need to store any of the activations from the forward pass, and can train arbitrarily deep networks. We show that fully reversible networks are able to achieve results comparable to the state of the art in image depth estimation and full 3D video segmentation, with a much lower memory footprint that is a constant independent of the network depth.},
	urldate = {2020-07-07},
	journal = {arXiv:1905.10484 [cs]},
	author = {Lensink, Keegan and Haber, Eldad and Peters, Bas},
	month = sep,
	year = {2019},
	note = {arXiv: 1905.10484},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@incollection{liu_frequencydomain_2018,
	title = {Frequency-{Domain} {Dynamic} {Pruning} for {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/7382-frequency-domain-dynamic-pruning-for-convolutional-neural-networks.pdf},
	urldate = {2018-12-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Zhenhua and Xu, Jizheng and Peng, Xiulian and Xiong, Ruiqin},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {1051--1061},
}

@inproceedings{long_fully_2015,
	title = {Fully convolutional networks for semantic segmentation},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	year = {2015},
	pages = {3431--3440},
}

@article{fletcher_function_1964,
	title = {Function minimization by conjugate gradients},
	volume = {7},
	doi = {10.1093/comjnl/7.2.149},
	number = {2},
	journal = {The computer journal},
	author = {Fletcher, Reeves and Reeves, Colin M},
	year = {1964},
	pages = {149--154},
}

@article{zhangjeffrey_fully_2018,
	title = {Fully {Automated} {Echocardiogram} {Interpretation} in {Clinical} {Practice}},
	volume = {138},
	url = {https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.118.034338},
	doi = {10/gfg9fn},
	abstract = {Background:Automated cardiac image interpretation has the potential to transform clinical practice in multiple ways, including enabling serial assessment of cardiac function by nonexperts in primary care and rural settings. We hypothesized that advances in computer vision could enable building a fully automated, scalable analysis pipeline for echocardiogram interpretation, including (1) view identification, (2) image segmentation, (3) quantification of structure and function, and (4) disease detection.Methods:Using 14 035 echocardiograms spanning a 10-year period, we trained and evaluated convolutional neural network models for multiple tasks, including automated identification of 23 viewpoints and segmentation of cardiac chambers across 5 common views. The segmentation output was used to quantify chamber volumes and left ventricular mass, determine ejection fraction, and facilitate automated determination of longitudinal strain through speckle tracking. Results were evaluated through comparison to manual segmentation and measurements from 8666 echocardiograms obtained during the routine clinical workflow. Finally, we developed models to detect 3 diseases: hypertrophic cardiomyopathy, cardiac amyloid, and pulmonary arterial hypertension.Results:Convolutional neural networks accurately identified views (eg, 96\% for parasternal long axis), including flagging partially obscured cardiac chambers, and enabled the segmentation of individual cardiac chambers. The resulting cardiac structure measurements agreed with study report values (eg, median absolute deviations of 15\% to 17\% of observed values for left ventricular mass, left ventricular diastolic volume, and left atrial volume). In terms of function, we computed automated ejection fraction and longitudinal strain measurements (within 2 cohorts), which agreed with commercial software-derived values (for ejection fraction, median absolute deviation=9.7\% of observed, N=6407 studies; for strain, median absolute deviation=7.5\%, n=419, and 9.0\%, n=110) and demonstrated applicability to serial monitoring of patients with breast cancer for trastuzumab cardiotoxicity. Overall, we found automated measurements to be comparable or superior to manual measurements across 11 internal consistency metrics (eg, the correlation of left atrial and ventricular volumes). Finally, we trained convolutional neural networks to detect hypertrophic cardiomyopathy, cardiac amyloidosis, and pulmonary arterial hypertension with C statistics of 0.93, 0.87, and 0.85, respectively.Conclusions:Our pipeline lays the groundwork for using automated interpretation to support serial patient tracking and scalable analysis of millions of echocardiograms archived within healthcare systems.},
	number = {16},
	urldate = {2018-11-27},
	journal = {Circulation},
	author = {{Zhang Jeffrey} and {Gajjala Sravani} and {Agrawal Pulkit} and {Tison Geoffrey H.} and {Hallock Laura A.} and {Beussink-Nelson Lauren} and {Lassen Mats H.} and {Fan Eugene} and {Aras Mandar A.} and {Jordan ChaRandle} and {Fleischmann Kirsten E.} and {Melisko Michelle} and {Qasim Atif} and {Shah Sanjiv J.} and {Bajcsy Ruzena} and {Deo Rahul C.}},
	month = oct,
	year = {2018},
	pages = {1623--1635},
}

@article{matthews_gaussian_2018,
	title = {Gaussian {Process} {Behaviour} in {Wide} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1804.11271},
	abstract = {Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between random, wide, fully connected, feedforward networks with more than one hidden layer and Gaussian processes with a recursive kernel definition. We show that, under broad conditions, as we make the architecture increasingly wide, the implied random function converges in distribution to a Gaussian process, formalising and extending existing results by Neal (1996) to deep networks. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then compare finite Bayesian deep networks from the literature to Gaussian processes in terms of the key predictive quantities of interest, finding that in some cases the agreement can be very close. We discuss the desirability of Gaussian process behaviour and review non-Gaussian alternative models from the literature.},
	urldate = {2018-10-24},
	journal = {arXiv:1804.11271 [cs, stat]},
	author = {Matthews, Alexander G. de G. and Rowland, Mark and Hron, Jiri and Turner, Richard E. and Ghahramani, Zoubin},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.11271},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, 🔍No DOI found},
}

@article{liu_fuzzy_2016,
	title = {Fuzzy approximation-based adaptive backstepping optimal control for a class of nonlinear discrete-time systems with dead-zone},
	volume = {24},
	issn = {1063-6706},
	doi = {10.1109/TFUZZ.2015.2418000},
	number = {1},
	journal = {IEEE Transactions on Fuzzy Systems},
	author = {Liu, Yan-Jun and Gao, Ying and Tong, Shaocheng and Li, Yongming},
	year = {2016},
	pages = {16--28},
}

@book{dubey_fundamentals_2002,
	title = {Fundamentals of {Electrical} {Drives}},
	isbn = {978-0-8493-2422-2},
	url = {https://books.google.com.br/books?id=2NsGKpLolsQC},
	publisher = {Alpha Science International},
	author = {Dubey, G.K.},
	year = {2002},
}

@article{zhang_forward_2015,
	title = {Forward and backward least angle regression for nonlinear system identification},
	volume = {53},
	issn = {0005-1098},
	url = {http://www.sciencedirect.com/science/article/pii/S0005109814005895},
	doi = {10.1016/j.automatica.2014.12.010},
	abstract = {A forward and backward least angle regression (LAR) algorithm is proposed to construct the nonlinear autoregressive model with exogenous inputs (NARX) that is widely used to describe a large class of nonlinear dynamic systems. The main objective of this paper is to improve model sparsity and generalization performance of the original forward LAR algorithm. This is achieved by introducing a replacement scheme using an additional backward LAR stage. The backward stage replaces insignificant model terms selected by forward LAR with more significant ones, leading to an improved model in terms of the model compactness and performance. A numerical example to construct four types of NARX models, namely polynomials, radial basis function (RBF) networks, neuro fuzzy and wavelet networks, is presented to illustrate the effectiveness of the proposed technique in comparison with some popular methods.},
	journal = {Automatica},
	author = {Zhang, Long and Li, Kang},
	month = mar,
	year = {2015},
	note = {00013},
	keywords = {Backward refinement, Forward selection, Least angle regression (LAR), Nonlinear system identification},
	pages = {94--102},
}

@inproceedings{ruslan_flood_2014,
	title = {Flood water level modeling and prediction using {NARX} neural network: {Case} study at {Kelang} river},
	booktitle = {Signal {Processing} \& its {Applications} ({CSPA}), 2014 {IEEE} 10th {International} {Colloquium} on},
	publisher = {IEEE},
	author = {Ruslan, Fazlina Ahmat and Samad, Abd Manan and Zain, Zainazlan Md and Adnan, Ramli},
	year = {2014},
	note = {00000},
	pages = {204--207},
}

@article{jin_fpga_2010,
	title = {{FPGA} design and implementation of a real-time stereo vision system},
	volume = {20},
	number = {1},
	journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
	author = {Jin, Seunghun and Cho, Junguk and Pham, Xuan Dai and Lee, Kyoung Mu and Park, S-K and Kim, Munsang and Jeon, Jae Wook},
	year = {2010},
	keywords = {🔍No DOI found},
	pages = {15--26},
}

@article{khan_forecasting_2015,
	title = {Forecasting the {Number} of {Muslim} {Pilgrims} {Using} {NARX} {Neural} {Networks} with a {Comparison} {Study} with {Other} {Modern} {Methods}},
	volume = {6},
	doi = {10.9734/BJMCS/2015/14563},
	number = {5},
	journal = {British Journal of Mathematics \& Computer Science},
	author = {Khan, Esam A and Elgamal, Mahmoud A and Shaarawy, Sameer M},
	year = {2015},
	pages = {394},
}

@article{hastie_forward_2007,
	title = {Forward stagewise regression and the monotone lasso},
	volume = {1},
	issn = {1935-7524},
	url = {http://projecteuclid.org/euclid.ejs/1177687773},
	doi = {10.1214/07-EJS004},
	language = {en},
	number = {0},
	urldate = {2017-09-12},
	journal = {Electronic Journal of Statistics},
	author = {Hastie, Trevor and Taylor, Jonathan and Tibshirani, Robert and Walther, Guenther},
	year = {2007},
	pages = {1--29},
}

@incollection{wisdom_fullcapacity_2016,
	title = {Full-{Capacity} {Unitary} {Recurrent} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/6327-full-capacity-unitary-recurrent-neural-networks.pdf},
	urldate = {2019-09-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Wisdom, Scott and Powers, Thomas and Hershey, John and Le Roux, Jonathan and Atlas, Les},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {4880--4888},
}

@book{lax_functional_2002,
	address = {New York},
	title = {Functional analysis},
	isbn = {978-0-471-55604-6},
	language = {en},
	publisher = {Wiley},
	author = {Lax, Peter D.},
	year = {2002},
	keywords = {Functional analysis},
}

@article{akiyama_first_2019,
	title = {First {M87} event horizon telescope results. iii. data processing and calibration},
	volume = {875},
	issn = {2041-8205},
	number = {1},
	journal = {The Astrophysical Journal Letters},
	author = {Akiyama, Kazunori and Alberdi, Antxon and Alef, Walter and Asada, Keiichi and Azulay, Rebecca and Baczko, Anne-Kathrin and Ball, David and Baloković, Mislav and Barrett, John and Bintley, Dan},
	year = {2019},
	pages = {L3},
}

@misc{_first_,
	title = {First {M87} {Event} {Horizon} {Telescope} {Results}. {III}. {Data} {Processing} and {Calibration} - {IOPscience}},
	url = {https://iopscience.iop.org/article/10.3847/2041-8213/ab0c57},
	urldate = {2020-02-21},
}

@misc{_first_a,
	title = {First {M87} {Event} {Horizon} {Telescope} {Results}. {III}. {Data} {Processing} and {Calibration} - {IOPscience}},
	url = {https://iopscience.iop.org/article/10.3847/2041-8213/ab0c57},
	urldate = {2020-02-21},
}

@article{yang_feedback_2013,
	title = {Feedback {Particle} {Filter}},
	volume = {58},
	issn = {0018-9286},
	doi = {10/f5bvxs},
	abstract = {The feedback particle filter introduced in this paper is a new approach to approximate nonlinear filtering, motivated by techniques from mean-field game theory. The filter is defined by an ensemble of controlled stochastic systems (the particles). Each particle evolves under feedback control based on its own state, and features of the empirical distribution of the ensemble. The feedback control law is obtained as the solution to an optimal control problem, in which the optimization criterion is the Kullback-Leibler divergence between the actual posterior, and the common posterior of any particle. The following conclusions are obtained for diffusions with continuous observations: 1) The optimal control solution is exact: The two posteriors match exactly, provided they are initialized with identical priors. 2) The optimal filter admits an innovation error-based gain feedback structure. 3) The optimal feedback gain is obtained via a solution of an Euler-Lagrange boundary value problem; the feedback gain equals the Kalman gain in the linear Gaussian case. Numerical algorithms are introduced and implemented in two general examples, and a neuroscience application involving coupled oscillators. In some cases it is found that the filter exhibits significantly lower variance when compared to the bootstrap particle filter.},
	number = {10},
	journal = {IEEE Transactions on Automatic Control},
	author = {Yang, T. and Mehta, P. G. and Meyn, S. P.},
	month = oct,
	year = {2013},
	keywords = {Approximation methods, Equations, Euler-Lagrange boundary value problem, Gaussian processes, Kalman filters, Kalman gain, Kullback- Leibler divergence, Mathematical model, Mean-field games, Optimal control, Particle filters, Technological innovation, boundary-value problems, empirical ensemble distribution, error-based gain feedback structure, feedback, feedback control law, feedback particle filter, game theory, linear Gaussian, mean field game theory, nonlinear filtering, nonlinear filters, numerical algorithm, optimal control, optimal control problem, optimal feedback gain, optimal transportation, optimisation, optimization criterion, particle filtering, particle filtering (numerical methods), stochastic control system, stochastic systems},
	pages = {2465--2480},
}

@article{arora_finegrained_2019,
	title = {Fine-{Grained} {Analysis} of {Optimization} and {Generalization} for {Overparameterized} {Two}-{Layer} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1901.08584},
	abstract = {Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR'17]. (ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.},
	urldate = {2019-02-22},
	journal = {arXiv:1901.08584 [cs, stat]},
	author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.08584},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} real-time object detection with region proposal networks},
	shorttitle = {Faster {R}-{CNN}},
	booktitle = {Advances in neural information processing systems},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2015},
	note = {00000},
	pages = {91--99},
}

@article{baake_fitting_1992,
	title = {Fitting ordinary differential equations to chaotic data},
	volume = {45},
	doi = {10.1103/PhysRevA.45.5524},
	number = {8},
	journal = {Physical Review A},
	author = {Baake, Ellen and Baake, Michael and Bock, HG and Briggs, KM},
	year = {1992},
	pages = {5524},
}

@article{battiti_first_1992,
	title = {First- and {Second}-{Order} {Methods} for {Learning}: {Between} {Steepest} {Descent} and {Newton}'s {Method}},
	volume = {4},
	issn = {0899-7667},
	shorttitle = {First- and {Second}-{Order} {Methods} for {Learning}},
	doi = {10.1162/neco.1992.4.2.141},
	abstract = {On-line first-order backpropagation is sufficiently fast and effective for many large-scale classification problems but for very high precision mappings, batch processing may be the method of choice. This paper reviews first- and second-order optimization methods for learning in feedforward neural networks. The viewpoint is that of optimization: many methods can be cast in the language of optimization techniques, allowing the transfer to neural nets of detailed results about computational complexity and safety procedures to ensure convergence and to avoid numerical problems. The review is not intended to deliver detailed prescriptions for the most appropriate methods in specific applications, but to illustrate the main characteristics of the different methods and their mutual relations.},
	number = {2},
	journal = {Neural Computation},
	author = {Battiti, R.},
	month = mar,
	year = {1992},
	pages = {141--166},
}

@article{gupta_feedback_2018,
	title = {Feedback {GAN} ({FBGAN}) for {DNA}: a {Novel} {Feedback}-{Loop} {Architecture} for {Optimizing} {Protein} {Functions}},
	shorttitle = {Feedback {GAN} ({FBGAN}) for {DNA}},
	url = {http://arxiv.org/abs/1804.01694},
	abstract = {Generative Adversarial Networks (GANs) represent an attractive and novel approach to generate realistic data, such as genes, proteins, or drugs, in synthetic biology. Here, we apply GANs to generate synthetic DNA sequences encoding for proteins of variable length. We propose a novel feedback-loop architecture, called Feedback GAN (FBGAN), to optimize the synthetic gene sequences for desired properties using an external function analyzer. The proposed architecture also has the advantage that the analyzer need not be differentiable. We apply the feedback-loop mechanism to two examples: 1) generating synthetic genes coding for antimicrobial peptides, and 2) optimizing synthetic genes for the secondary structure of their resulting peptides. A suite of metrics demonstrate that the GAN generated proteins have desirable biophysical properties. The FBGAN architecture can also be used to optimize GAN-generated datapoints for useful properties in domains beyond genomics.},
	urldate = {2019-04-01},
	journal = {arXiv:1804.01694 [cs, q-bio]},
	author = {Gupta, Anvita and Zou, James},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.01694},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Genomics},
}

@incollection{simard_fixed_1989,
	title = {Fixed {Point} {Analysis} for {Recurrent} {Networks}},
	url = {http://papers.nips.cc/paper/181-fixed-point-analysis-for-recurrent-networks.pdf},
	urldate = {2019-10-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 1},
	publisher = {Morgan-Kaufmann},
	author = {Simard, Patrice Y. and Ottaway, Mary B. and Ballard, Dana H.},
	editor = {Touretzky, D. S.},
	year = {1989},
	pages = {149--159},
}

@inproceedings{oliveira_explaining_2020,
	title = {Explaining black-box automated electrocardiogram classiﬁcation to cardiologists},
	volume = {47},
	copyright = {All rights reserved},
	doi = {10.22489/CinC.2020.452},
	abstract = {In this work, we present a method to explain “end-toend” electrocardiogram (ECG) signal classiﬁers, where the explanations were built along with seniors cardiologist to provide meaningful features to the ﬁnal users. Our method focuses exclusively on automated ECG diagnosis and analyzes the explanation in terms of clinical accuracy for interpretability and robustness. The proposed method uses a noise-insertion strategy to quantify the impact of intervals and segments of the ECG signals on the automated classiﬁcation outcome. An ECG segmentation method was applied to ECG tracings, to obtain: (1) Intervals, Segments and Axis; (2) Rate, and (3) Rhythm. Noise was added to the signal to disturb the ECG features in a realistic way. The method was tested using Monte Carlo simulation and the feature impact is estimated by the change in the model prediction averaged over 499 executions and a feature is deﬁned as important if its mean value changes the result of the classiﬁer. We demonstrate our method by explaining diagnoses generated by a deep convolutional neural network. The proposed method is particularly effective and useful for modern deep learning models that take raw data as input.},
	booktitle = {2020 {Computing} in {Cardiology} ({CinC})},
	author = {Oliveira, Derick M and Ribeiro, Antonio H and Pedrosa, Joao A O and Paixao, Gabriela M M and Ribeiro, Antonio L and Jr, Wagner Meira},
	year = {2020},
}

@inproceedings{hutchison_evaluation_2010,
	title = {Evaluation of {Pooling} {Operations} in {Convolutional} {Architectures} for {Object} {Recognition}},
	volume = {6354},
	isbn = {978-3-642-15824-7 978-3-642-15825-4},
	url = {http://link.springer.com/10.1007/978-3-642-15825-4_10},
	doi = {10.1007/978-3-642-15825-4_10},
	abstract = {A common practice to gain invariant features in object recognition models is to aggregate multiple low-level features over a small neighborhood. However, the diﬀerences between those models makes a comparison of the properties of diﬀerent aggregation functions hard. Our aim is to gain insight into diﬀerent functions by directly comparing them on a ﬁxed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation signiﬁcantly outperforms subsampling operations. Despite their shift-invariant properties, overlapping pooling windows are no signiﬁcant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57\% on the NORB normalized-uniform dataset and 5.6\% on the NORB jittered-cluttered dataset.},
	urldate = {2019-02-13},
	booktitle = {Artificial {Neural} {Networks} – {ICANN} 2010},
	author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Scherer, Dominik and Müller, Andreas and Behnke, Sven},
	editor = {Diamantaras, Konstantinos and Duch, Wlodek and Iliadis, Lazaros S.},
	year = {2010},
	pages = {92--101},
}

@incollection{diehl_fast_2006,
	title = {Fast direct multiple shooting algorithms for optimal robot control},
	booktitle = {Fast motions in biomechanics and robotics},
	publisher = {Springer},
	author = {Diehl, Moritz and Bock, Hans Georg and Diedam, Holger and Wieber, P-B},
	year = {2006},
	pages = {65--93},
}

@article{noel_f16_,
	title = {F-16 aircraft benchmark based on ground vibration test data},
	language = {en},
	author = {Noel, J P and Schoukens, M},
	keywords = {🔍No DOI found},
	pages = {5},
}

@article{bollapragada_exact_2016,
	title = {Exact and {Inexact} {Subsampled} {Newton} {Methods} for {Optimization}},
	url = {http://arxiv.org/abs/1609.08502},
	abstract = {The paper studies the solution of stochastic optimization problems in which approximations to the gradient and Hessian are obtained through subsampling. We first consider Newton-like methods that employ these approximations and discuss how to coordinate the accuracy in the gradient and Hessian to yield a superlinear rate of convergence in expectation. The second part of the paper analyzes an inexact Newton method that solves linear systems approximately using the conjugate gradient (CG) method, and that samples the Hessian and not the gradient (the gradient is assumed to be exact). We provide a complexity analysis for this method based on the properties of the CG iteration and the quality of the Hessian approximation, and compare it with a method that employs a stochastic gradient iteration instead of the CG method. We report preliminary numerical results that illustrate the performance of inexact subsampled Newton methods on machine learning applications based on logistic regression.},
	journal = {arXiv:1609.08502 [math, stat]},
	author = {Bollapragada, Raghu and Byrd, Richard and Nocedal, Jorge},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.08502},
	keywords = {Mathematics - Optimization and Control, Statistics - Machine Learning, 🔍No DOI found},
}

@inproceedings{schroff_facenet_2015,
	title = {{FaceNet}: {A} {Unified} {Embedding} for {Face} {Recognition} and {Clustering}},
	shorttitle = {{FaceNet}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html},
	urldate = {2018-01-26},
	author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	year = {2015},
	note = {00000},
	pages = {815--823},
}

@article{friedman_fast_2012,
	title = {Fast sparse regression and classification},
	volume = {28},
	issn = {0169-2070},
	url = {http://www.sciencedirect.com/science/article/pii/S0169207012000490},
	doi = {10.1016/j.ijforecast.2012.05.001},
	abstract = {Many present day applications of statistical learning involve large numbers of predictor variables. Often, that number is much larger than the number of cases or observations available for training the learning algorithm. In such situations, traditional methods fail. Recently, new techniques have been developed, based on regularization, which can often produce accurate models in these settings. This paper describes the basic principles underlying the method of regularization, then focuses on those methods which exploit the sparsity of the predicting model. The potential merits of these methods are then explored by example.},
	number = {3},
	journal = {International Journal of Forecasting},
	author = {Friedman, Jerome H.},
	month = jul,
	year = {2012},
	keywords = {-norm penalization, Bridge-regression, Classification, Elastic net, Lasso, Regression, Regularization, Sparsity, variable selection},
	pages = {722--738},
}

@inproceedings{girshick_fast_2015,
	title = {Fast r-cnn},
	booktitle = {Proceedings of the {IEEE} international conference on computer vision},
	author = {Girshick, Ross},
	year = {2015},
	pages = {1440--1448},
}

@phdthesis{kors_expert_1992,
	address = {S.l.},
	title = {Expert knowledge for computerized {ECG} interpretation},
	language = {English},
	school = {s.n.]},
	author = {Kors, Jan},
	year = {1992},
	note = {OCLC: 65910805},
}

@article{lai_extended_1986,
	title = {Extended least squares and their applications to adaptive control and prediction in linear systems},
	volume = {31},
	issn = {0018-9286},
	doi = {10.1109/TAC.1986.1104138},
	abstract = {Herein strong consistency of recursive extended least squares is established under considerably weaker assumptions than previously assumed in the literature. The argument used to establish consistency also leads to certain basic properties of adaptive predictors based on these recursive estimators. Making use of these properties of the adaptive predictors, simple modifications of the Åström-Wittenmark self-tuning regulator are proposed and shown to be asymptotically optimal.},
	number = {10},
	journal = {IEEE Transactions on Automatic Control},
	author = {Lai, Tze and Wei, Ching-Zong},
	month = oct,
	year = {1986},
	keywords = {Adaptive control, Adaptive control, linear systems, Adaptive estimation, linear systems, Context modeling, Delay, Least squares approximation, Least squares methods, Least-squares methods, Linear systems, Polynomials, Prediction methods, Recursive estimation, Stochastic systems, Tin},
	pages = {898--906},
}

@article{williams_experimental_1989,
	title = {Experimental analysis of the real-time recurrent learning algorithm},
	volume = {1},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09540098908915631},
	doi = {10.1080/09540098908915631},
	number = {1},
	urldate = {2017-09-10},
	journal = {Connection Science},
	author = {Williams, Ronald J. and Zipser, David},
	year = {1989},
	note = {00369},
	pages = {87--111},
}

@article{peng_eventtriggered_2017,
	title = {Event-triggered fault detection framework based on subspace identification method for the networked control systems},
	volume = {239},
	doi = {10.1016/j.neucom.2017.02.027},
	journal = {Neurocomputing},
	author = {Peng, Kaixiang and Wang, Mengyuan and Dong, Jie},
	year = {2017},
	note = {00000},
	pages = {257--267},
}

@article{leontaritis_experimental_1987,
	title = {Experimental design and identifiability for non-linear systems},
	volume = {18},
	doi = {10.1080/00207728708963958},
	number = {1},
	journal = {International Journal of Systems Science},
	author = {Leontaritis, I. J. and Billings, S. A.},
	year = {1987},
	pages = {189--202},
}

@inproceedings{gorelick_fast_2013,
	title = {Fast trust region for segmentation},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Gorelick, Lena and Schmidt, Frank R and Boykov, Yuri},
	year = {2013},
	pages = {1714--1721},
}

@article{boyd_fading_1985,
	title = {Fading memory and the problem of approximating nonlinear operators with {Volterra} series},
	volume = {32},
	issn = {0098-4094},
	doi = {10/fkqmc5},
	abstract = {Using the notion of fading memory we prove very strong versions of two folk theorems. The first is that any time-invariant (TI) continuous nonlinear operator can be approximated by a Volterra series operator, and the second is that the approximating operator can be realized as a finite-dimensional linear dynamical system with a nonlinear readout map. While previous approximation results are valid over finite time intervals and for signals in compact sets, the approximations presented here hold for all time and for signals in useful (noncompact) sets. The discretetime analog of the second theorem asserts that any TI operator with fading memory can be approximated (in our strong sense) by a nonlinear moving- average operator. Some further discussion of the notion of fading memory is given.},
	number = {11},
	journal = {IEEE Transactions on Circuits and Systems},
	author = {Boyd, S. and Chua, L.},
	month = nov,
	year = {1985},
	keywords = {Approximation methods, Control systems, Convolution, Fading, Fasteners, Mathematics, Nonlinear circuits and systems, Nonlinear control systems, Nonlinear equations, Nonlinear systems, Operational amplifiers, Operator theory, Polynomials, Volterra series},
	pages = {1150--1161},
}

@book{markovsky_exact_2006,
	title = {Exact and approximate modeling of linear systems: {A} behavioral approach},
	publisher = {SIAM},
	author = {Markovsky, Ivan and Willems, Jan C and Van Huffel, Sabine and De Moor, Bart},
	year = {2006},
}

@incollection{scherer_evaluation_2010,
	address = {Berlin, Heidelberg},
	title = {Evaluation of {Pooling} {Operations} in {Convolutional} {Architectures} for {Object} {Recognition}},
	volume = {6354},
	abstract = {A common practice to gain invariant features in object recognition models is to aggregate multiple low-level features over a small neighborhood. However, the diﬀerences between those models makes a comparison of the properties of diﬀerent aggregation functions hard. Our aim is to gain insight into diﬀerent functions by directly comparing them on a ﬁxed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation signiﬁcantly outperforms subsampling operations. Despite their shift-invariant properties, overlapping pooling windows are no signiﬁcant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57\% on the NORB normalized-uniform dataset and 5.6\% on the NORB jittered-cluttered dataset.},
	urldate = {2020-03-23},
	booktitle = {Artificial {Neural} {Networks} – {ICANN}},
	publisher = {Springer},
	author = {Scherer, Dominik and Muller, Andreas and Behnke, Sven},
	year = {2010},
	doi = {10.1007/978-3-642-15825-4_10},
	pages = {92--101},
}

@article{paixao_evaluation_2020,
	title = {Evaluation of {Mortality} in {Atrial} {Fibrillation}: {Clinical} {Outcomes} in {Digital} {Electrocardiography} ({CODE}) {Study}},
	volume = {15},
	copyright = {All rights reserved},
	issn = {2211-8179},
	shorttitle = {Evaluation of {Mortality} in {Atrial} {Fibrillation}},
	url = {https://globalheartjournal.com/articles/10.5334/gh.772/},
	doi = {10.5334/gh.772},
	abstract = {Methods: This observational retrospective study of primary care patients was developed with the digital ECG database from the Telehealth Network of Minas Gerais, Brazil. ECGs performed from 2010 to 2017 were interpreted by cardiologists and the University of Glasgow automated analysis software. An electronic cohort was obtained linking data from ECG exams and those from a national mortality information system, using standard probabilistic linkage methods. We considered only the first ECG of each patient. Patients under 16 years were excluded. Hazard ratios (HR) for mortality were adjusted for demographic and self-reported clinical factors and estimated with Cox regression.
Results: From a dataset of 1,773,689 patients, 1,558,421 were included, mean age 51.6 years; 40.2\% male. There were 3.34\% deaths from all causes in 3.68 years of median follow up. The prevalence of AF was 1.33\%. AF was an independent risk factor for all-cause mortality (HR 2.10, 95\%CI 2.03–2.17) and cardiovascular mortality (HR 2.06, 95\%CI 1.86–2.29). Females with AF had a higher risk of overall and cardiovascular mortality compared with males (p {\textless} 0.001).
Conclusions: AF was a strong predictor of cardiovascular and all-cause mortality in a primary care population, with increased risk in women.},
	number = {1},
	urldate = {2020-08-07},
	journal = {Global Heart},
	author = {Paixão, Gabriela M. M. and Silva, Luis Gustavo S. and Gomes, Paulo R. and Lima, Emilly M. and Ferreira, Milton P. F. and Oliveira, Derick M. and Ribeiro, Manoel H. and Ribeiro, Antonio H. and Nascimento, Jamil S. and Canazart, Jéssica A. and Ribeiro, Leonardo B. and Benjamin, Emelia J. and Macfarlane, Peter W. and Marcolino, Milena S. and Ribeiro, Antonio L.},
	month = jul,
	year = {2020},
	pages = {48},
}

@inproceedings{kawaguchi_elimination_2020,
	title = {Elimination of {All} {Bad} {Local} {Minima} in {Deep} {Learning}},
	url = {http://proceedings.mlr.press/v108/kawaguchi20b.html},
	abstract = {In this paper, we theoretically prove that adding one special neuron per output unit eliminates all suboptimal local minima of any deep neural network, for multi-class classification, binary classi...},
	language = {en},
	urldate = {2020-08-27},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Kawaguchi, Kenji and Kaelbling, Leslie},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {853--863},
}

@article{tan_efficientnet_2019,
	series = {{PMLR}},
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	volume = {97},
	url = {http://arxiv.org/abs/1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	journal = {Proceedings of the 36th International Conference on Machine Learning,},
	author = {Tan, Mingxing and Le, Quoc V.},
	year = {2019},
	note = {arXiv: 1905.11946},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{shah_errors_2007,
	title = {Errors in the computerized electrocardiogram interpretation of cardiac rhythm.},
	volume = {40},
	issn = {1532-8430 0022-0736},
	doi = {10.1016/j.jelectrocard.2007.03.008},
	abstract = {BACKGROUND: More than 100 million computer-interpreted electrocardiograms (ECG-C) are obtained annually. However, there are few contemporary published data on the  accuracy of cardiac rhythm interpretation by this method. PURPOSE: The purpose of this study is to determine the accuracy of ECG-C rhythm interpretation in a typical patient population. METHODS: We compared the ECG-C rhythm interpretation  to that of 2 expert overreaders in 2112 randomly selected standard 12-lead ECGs.  RESULTS: The ECG-C correctly interpreted the rhythm in 1858 and incorrectly identified the rhythm in 254 (overall accuracy, 88.0\%). Sinus rhythm was correctly interpreted in 95.0\% of the ECGs (1666/1753) with this rhythm, whereas  nonsinus rhythms were correctly interpreted with an accuracy of only 53.5\% (192/359) (P {\textless} .0001). The ECG-C interpreted sinus rhythm with a sensitivity of 95\% (confidence interval, 93.8-96.7), specificity of 66.3\%, and positive predictive value of 93.2\%. The ECG-C interpreted nonsinus rhythms with a sensitivity of 72\%, (confidence interval, 68.7-73.7), a specificity of 93\%, and a positive predictive value of 59.3\%. Of the 254 ECGs that had incorrect rhythm interpretation, additional major errors were noted in 137 (54\%). CONCLUSIONS: The},
	language = {eng},
	number = {5},
	journal = {Journal of Electrocardiology},
	author = {Shah, Atman P. and Rubin, Stanley A.},
	month = oct,
	year = {2007},
	pmid = {17531257},
	keywords = {*Artifacts, Arrhythmias, Cardiac/*diagnosis/epidemiology, California, Diagnosis, Computer-Assisted/*methods, Electrocardiography/*methods/*statistics \& numerical data, Humans, Observer Variation, Quality Assurance, Health Care/*methods, Reproducibility of Results, Retrospective Studies, Sensitivity and Specificity},
	pages = {385--390},
}

@inproceedings{hong_encase_2017,
	title = {{ENCASE}: an {ENsemble} {ClASsifiEr} for {ECG} {Classification} {Using} {Expert} {Features} and {Deep} {Neural} {Networks}},
	shorttitle = {{ENCASE}},
	url = {http://www.cinc.org/archives/2017/pdf/178-245.pdf},
	doi = {10/gftsxw},
	abstract = {We propose ENCASE to combine expert features and DNNs (Deep Neural Networks) together for ECG classiﬁcation. We ﬁrst explore and implement expert features from statistical area, signal processing area and medical area. Then, we build DNNs to automatically extract deep features. Besides, we propose a new algorithm to ﬁnd the most representative wave (called centerwave) among long ECG record, and extract features from centerwave. Finally, we combine these features together and put them into ensemble classiﬁers. Experiment on 4-class ECG data classiﬁcation reports 0.84 F1 score, which is much better than any of the single model.},
	language = {en},
	urldate = {2019-01-24},
	author = {Hong, Shenda and Wu, Meng and Zhou, Yuxi and Wang, Qingyun and Shang, Junyuan and Li, Hongyan and Xie, Junqing},
	month = sep,
	year = {2017},
}

@article{samek_evaluating_2015,
	title = {Evaluating the visualization of what a {Deep} {Neural} {Network} has learned},
	url = {http://arxiv.org/abs/1509.06321},
	abstract = {Deep Neural Networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multi-layer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the ''importance'' of individual pixels wrt the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012 and MIT Places data sets. Our main result is that the recently proposed Layer-wise Relevance Propagation (LRP) algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of neural network performance.},
	journal = {arXiv:1509.06321 [cs]},
	author = {Samek, Wojciech and Binder, Alexander and Montavon, Grégoire and Bach, Sebastian and Müller, Klaus-Robert},
	month = sep,
	year = {2015},
	note = {00000 
arXiv: 1509.06321},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 🔍No DOI found},
}

@phdthesis{schon_estimation_2006,
	address = {Linköping},
	title = {Estimation of {Nonlinear} dynamic systems: theory and applications},
	shorttitle = {Estimation of {Nonlinear} dynamic systems},
	language = {Med sammanfattning på svenska.},
	school = {Univ.},
	author = {Schön, Thomas B},
	year = {2006},
	note = {00000 
OCLC: 185211029},
}

@inproceedings{braun_enzoiia_1994,
	title = {{ENZO}-{II}-{A} powerful design tool to evolve multilayer feed forward networks},
	booktitle = {Evolutionary {Computation}, 1994. {IEEE} {World} {Congress} on {Computational} {Intelligence}., {Proceedings} of the {First} {IEEE} {Conference} on},
	publisher = {IEEE},
	author = {Braun, Heinrich and Zagorski, Peter},
	year = {1994},
	pages = {278--283},
}

@book{hastie_elements_2009,
	edition = {Second},
	title = {Elements of {Statistical} {Learning}},
	publisher = {Springer Science \& Business Media},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
}

@article{veronese_emergency_2016,
	title = {Emergency physician accuracy in interpreting electrocardiograms with potential {ST}-segment elevation myocardial infarction: {Is} it enough?},
	volume = {18},
	issn = {1748-295X},
	shorttitle = {Emergency physician accuracy in interpreting electrocardiograms with potential {ST}-segment elevation myocardial infarction},
	doi = {10.1080/17482941.2016.1234058},
	abstract = {BACKGROUND: Electrocardiogram (ECG) interpretation is widely performed by emergency physicians. We aimed to determine the accuracy of interpretation of potential ST-segment elevation myocardial infarction (STEMI) ECGs by emergency physicians.
METHODS: Thirty-six ECGs resulted in putative STEMI diagnoses were selected. Participants were asked to focus on whether or not the ECG in question met the diagnostic criteria for an acutely blocked coronary artery causing a STEMI. Based on the coronary angiogram, a binary outcome of accurate versus inaccurate ECG interpretation was defined. We computed the overall sensitivity, specificity, accuracy and 95\% confidence intervals (95\%CIs) for ECG interpretation. Data on participant training level, working experience and place were collected.
RESULTS: 135 participants interpreted 4603 ECGs. Overall sensitivity to identify 'true' STEMI ECGs was 64.5\% (95\%CI: 62.8-66.3); specificity in determining 'false' ECGs was 78\% (95\%CI: 76-80.1). Overall accuracy was modest (69.1, 95\%CI: 67.8-70.4). Higher accuracy in ECG interpretation was observed for attending physicians, participants working in tertiary care hospitals and those more experienced.
CONCLUSION: The accuracy of interpretation of potential STEMI ECGs was modest among emergency physicians. The study supports the notion that ECG interpretation for establishing a STEMI diagnosis lacks the necessary sensitivity and specificity to be considered a reliable 'stand-alone' diagnostic test.},
	language = {eng},
	number = {1},
	journal = {Acute Cardiac Care},
	author = {Veronese, Giacomo and Germini, Federico and Ingrassia, Stella and Cutuli, Ombretta and Donati, Valeria and Bonacchini, Luca and Marcucci, Maura and Fabbri, Andrea and {Italian Society of Emergency Medicine (SIMEU)}},
	month = mar,
	year = {2016},
	pmid = {27759433},
	keywords = {Accuracy, Clinical Competence, Coronary Angiography, Diagnostic Errors, Dimensional Measurement Accuracy, Electrocardiography, Emergency Medical Services, Health Care Surveys, Humans, Italy, Physicians, Reproducibility of Results, ST Elevation Myocardial Infarction, Sensitivity and Specificity, electrocardiogram, emergency medicine, myocardial infarction},
	pages = {7--10},
}

@article{chung_empirical_2014,
	title = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
	journal = {arXiv:1412.3555 [cs]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.3555},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, 🔍No DOI found},
}

@article{schwarz_estimating_1978,
	title = {Estimating the dimension of a model},
	volume = {6},
	issn = {0090-5364},
	number = {2},
	journal = {The annals of statistics},
	author = {Schwarz, Gideon},
	year = {1978},
	note = {00000},
	keywords = {❓Multiple DOI},
	pages = {461--464},
}

@book{anton_elementary_2010,
	title = {Elementary {Linear} {Algebra}: {Applications} {Version}},
	isbn = {978-0-470-43205-1},
	url = {https://books.google.com.br/books?id=1PJ-WHepeBsC},
	publisher = {John Wiley \& Sons},
	author = {Anton, H. and Rorres, C.},
	year = {2010},
}

@book{cover_elements_2006,
	address = {Hoboken, N.J},
	edition = {2nd ed},
	title = {Elements of information theory},
	isbn = {978-0-471-24195-9},
	publisher = {Wiley-Interscience},
	author = {Cover, T. M. and Thomas, Joy A.},
	year = {2006},
	note = {OCLC: ocm59879802},
	keywords = {Information theory},
}

@book{rao_engineering_2009,
	address = {Hoboken, N.J},
	edition = {4th ed},
	title = {Engineering optimization: theory and practice},
	isbn = {978-0-470-18352-6},
	shorttitle = {Engineering optimization},
	publisher = {John Wiley \& Sons},
	author = {Rao, Singiresu S.},
	year = {2009},
	note = {OCLC: ocn320352991},
	keywords = {Engineering, Mathematical models, Mathematical optimization},
}

@book{macedo_eletromagnetismo_1988,
	title = {Eletromagnetismo},
	isbn = {978-85-277-0100-6},
	url = {https://books.google.com.br/books?id=XpvCPgAACAAJ},
	publisher = {GUANABARA},
	author = {Macedo, Annita},
	year = {1988},
}

@article{vaicenavicius_evaluating_2019,
	title = {Evaluating model calibration in classification},
	url = {http://arxiv.org/abs/1902.06977},
	abstract = {Probabilistic classifiers output a probability distribution on target classes rather than just a class prediction. Besides providing a clear separation of prediction and decision making, the main advantage of probabilistic models is their ability to represent uncertainty about predictions. In safety-critical applications, it is pivotal for a model to possess an adequate sense of uncertainty, which for probabilistic classifiers translates into outputting probability distributions that are consistent with the empirical frequencies observed from realized outcomes. A classifier with such a property is called calibrated. In this work, we develop a general theoretical calibration evaluation framework grounded in probability theory, and point out subtleties present in model calibration evaluation that lead to refined interpretations of existing evaluation techniques. Lastly, we propose new ways to quantify and visualize miscalibration in probabilistic classification, including novel multidimensional reliability diagrams.},
	urldate = {2019-03-20},
	journal = {arXiv:1902.06977 [cs, stat]},
	author = {Vaicenavicius, Juozas and Widmann, David and Andersson, Carl and Lindsten, Fredrik and Roll, Jacob and Schön, Thomas B.},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.06977},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{young_entropy_,
	title = {Entropy in {Dynamical} {Systems}},
	doi = {10/gfz8cs},
	abstract = {Both hµ and htop measure the exponential rates of growth of n-orbits: – hµ counts the number of typical n-orbits, while – htop counts all distinguishable n-orbits.},
	language = {en},
	author = {Young, Lai-Sang},
	pages = {13},
}

@inproceedings{mhammedi_efficient_2017,
	title = {Efficient orthogonal parametrisation of recurrent neural networks using householder reflections},
	publisher = {JMLR. org},
	author = {Mhammedi, Zakaria and Hellicar, Andrew and Rahman, Ashfaqur and Bailey, James},
	year = {2017},
	pages = {2401--2409},
}

@article{mhammedi_efficient_2016,
	title = {Efficient {Orthogonal} {Parametrisation} of {Recurrent} {Neural} {Networks} {Using} {Householder} {Reflections}},
	url = {http://arxiv.org/abs/1612.00188},
	abstract = {The problem of learning long-term dependencies in sequences using Recurrent Neural Networks (RNNs) is still a major challenge. Recent methods have been suggested to solve this problem by constraining the transition matrix to be unitary during training which ensures that its norm is equal to one and prevents exploding gradients. These methods either have limited expressiveness or scale poorly with the size of the network when compared with the simple RNN case, especially when using stochastic gradient descent with a small mini-batch size. Our contributions are as follows; we ﬁrst show that constraining the transition matrix to be unitary is a special case of an orthogonal constraint. Then we present a new parametrisation of the transition matrix which allows efﬁcient training of an RNN while ensuring that the matrix is always orthogonal. Our results show that the orthogonal constraint on the transition matrix applied through our parametrisation gives similar beneﬁts to the unitary constraint, without the time complexity limitations.},
	language = {en},
	urldate = {2019-09-19},
	journal = {arXiv:1612.00188 [cs]},
	author = {Mhammedi, Zakaria and Hellicar, Andrew and Rahman, Ashfaqur and Bailey, James},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.00188},
	keywords = {Computer Science - Machine Learning, ⛔ No DOI found},
}

@article{sarode_embedded_2015,
	title = {Embedded {Multiple} {Shooting} {Methodology} in a {Genetic} {Algorithm} {Framework} for {Parameter} {Estimation} and {State} {Identification} of {Complex} {Systems}},
	volume = {134},
	issn = {0009-2509},
	doi = {10/f7mqwg},
	journal = {Chemical Engineering Science},
	author = {Sarode, Ketan Dinkar and Kumar, V Ravi and Kulkarni, BD},
	year = {2015},
	pages = {605--618},
}

@article{kuznetsov_elements_2004,
	title = {Elements of applied bifurcation theory},
	author = {Kuznetsov, Yu A},
	year = {2004},
}

@article{paixao_ecgage_2020,
	title = {{ECG}-{AGE} {FROM} {ARTIFICIAL} {INTELLIGENCE}: {A} {NEW} {PREDICTOR} {FOR} {MORTALITY}? {THE} {CODE} ({CLINICAL} {OUTCOMES} {IN} {DIGITAL} {ELECTROCARDIOGRAPHY}) {STUDY}},
	volume = {75},
	copyright = {All rights reserved},
	issn = {0735-1097},
	doi = {10.1016/S0735-1097(20)34299-6},
	number = {11 Supplement 1},
	journal = {Journal of the American College of Cardiology},
	author = {Paixão, Gabriela Miana and Ribeiro, Antonio Horta and Lima, Emilly and Seewald, Bruna and Ribeiro, Manoel Horta and Oliveira, Derick and Gomes, Paulo and Castro, Nathalia and Meira, Wagner and Schön, Thomas and Ribeiro, Antonio L.},
	year = {2020},
	pages = {3672},
}

@article{you_drawing_2020,
	title = {Drawing early-bird tickets: {Towards} more efficient training of deep networks},
	shorttitle = {Drawing early-bird tickets},
	url = {http://arxiv.org/abs/1909.11957},
	abstract = {(Frankle \& Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at the very early training stage, which we term as early-bird (EB) tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. Our finding of EB tickets is consistent with recently reported observations that the key connectivity patterns of neural networks emerge early. Furthermore, we propose a mask distance metric that can be used to identify EB tickets with low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, we leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy. Experiments based on various deep networks and datasets validate: 1) the existence of EB tickets, and the effectiveness of mask distance in efficiently identifying them; and 2) that the proposed efficient training via EB tickets can achieve up to 4.7x energy savings while maintaining comparable or even better accuracy, demonstrating a promising and easily adopted method for tackling cost-prohibitive deep network training. Code available at https://github.com/RICE-EIC/Early-Bird-Tickets.},
	urldate = {2020-07-05},
	journal = {arXiv:1909.11957 [cs, stat]},
	author = {You, Haoran and Li, Chaojian and Xu, Pengfei and Fu, Yonggan and Wang, Yue and Chen, Xiaohan and Baraniuk, Richard G. and Wang, Zhangyang and Lin, Yingyan},
	month = feb,
	year = {2020},
	note = {arXiv: 1909.11957},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{agrawal_differentiable_2019,
	title = {Differentiable {Convex} {Optimization} {Layers}},
	abstract = {Recent work has shown how to embed differentiable optimization problems (that is, problems whose solutions can be backpropagated through) as layers within deep learning architectures. This method provides a useful inductive bias for certain problems, but existing software for differentiable optimization layers is rigid and difﬁcult to apply to new settings. In this paper, we propose an approach to differentiating through disciplined convex programs, a subclass of convex optimization problems used by domain-speciﬁc languages (DSLs) for convex optimization. We introduce disciplined parametrized programming, a subset of disciplined convex programming, and we show that every disciplined parametrized program can be represented as the composition of an afﬁne map from parameters to problem data, a solver, and an afﬁne map from the solver’s solution to a solution of the original problem (a new form we refer to as afﬁne-solver-afﬁne form). We then demonstrate how to efﬁciently differentiate through each of these components, allowing for end-to-end analytical differentiation through the entire convex program. We implement our methodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex optimization, and additionally implement differentiable layers for disciplined convex programs in PyTorch and TensorFlow 2.0. Our implementation signiﬁcantly lowers the barrier to using convex optimization problems in differentiable programs. We present applications in linear machine learning models and in stochastic control, and we show that our layer is competitive (in execution time) compared to specialized differentiable solvers from past work.},
	language = {en},
	author = {Agrawal, Akshay and Amos, Brandon and Barratt, Shane and Boyd, Stephen},
	year = {2019},
	pages = {19},
}

@article{salman_adversarially_2020,
	title = {Do {Adversarially} {Robust} {ImageNet} {Models} {Transfer} {Better}?},
	url = {http://arxiv.org/abs/2007.08489},
	abstract = {Transfer learning is a widely-used paradigm in deep learning, where models pre-trained on standard datasets can be efficiently adapted to downstream tasks. Typically, better pre-trained models yield better transfer results, suggesting that initial accuracy is a key aspect of transfer learning performance. In this work, we identify another such aspect: we find that adversarially robust models, while less accurate, often perform better than their standard-trained counterparts when used for transfer learning. Specifically, we focus on adversarially robust ImageNet classifiers, and show that they yield improved accuracy on a standard suite of downstream classification tasks. Further analysis uncovers more differences between robust and standard models in the context of transfer learning. Our results are consistent with (and in fact, add to) recent hypotheses stating that robustness leads to improved feature representations. Our code and models are available at https://github.com/Microsoft/robust-models-transfer .},
	urldate = {2020-07-26},
	journal = {arXiv:2007.08489 [cs, stat]},
	author = {Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Kapoor, Ashish and Madry, Aleksander},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.08489},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{oppenheim_discretetime_1999,
	title = {Discrete-time signal processing},
	publisher = {Pearson Education India},
	author = {Oppenheim, Alan V},
	year = {1999},
}

@misc{deazevedo_does_2020,
	title = {Does gradient descent converge to a minimum-norm solution in least-squares problems?},
	url = {https://math.stackexchange.com/q/3499305},
	author = {de Azevedo, Rodrigo},
	month = oct,
	year = {2020},
	note = {tex.eprint: https://math.stackexchange.com/q/3499305
tex.howpublished: Mathematics Stack Exchange},
}

@article{srivastava_dropout_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting.},
	volume = {15},
	shorttitle = {Dropout},
	number = {1},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	keywords = {🔍No DOI found},
	pages = {1929--1958},
}

@article{bejnordi_diagnostic_2017,
	title = {Diagnostic {Assessment} of {Deep} {Learning} {Algorithms} for {Detection} of {Lymph} {Node} {Metastases} in {Women} {With} {Breast} {Cancer}},
	volume = {318},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2017.14585},
	doi = {10.1001/jama.2017.14585},
	language = {en},
	number = {22},
	urldate = {2017-12-13},
	journal = {JAMA},
	author = {Bejnordi, Babak Ehteshami and Veta, Mitko and Johannes van Diest, Paul and van Ginneken, Bram and Karssemeijer, Nico and Litjens, Geert and van der Laak, Jeroen A. W. M. and {and the CAMELYON16 Consortium} and Hermsen, Meyke and Manson, Quirine F and Balkenhol, Maschenka and Geessink, Oscar and Stathonikos, Nikolaos and van Dijk, Marcory CRF and Bult, Peter and Beca, Francisco and Beck, Andrew H and Wang, Dayong and Khosla, Aditya and Gargeya, Rishab and Irshad, Humayun and Zhong, Aoxiao and Dou, Qi and Li, Quanzheng and Chen, Hao and Lin, Huang-Jing and Heng, Pheng-Ann and Haß, Christian and Bruni, Elia and Wong, Quincy and Halici, Ugur and Öner, Mustafa Ümit and Cetin-Atalay, Rengul and Berseth, Matt and Khvatkov, Vitali and Vylegzhanin, Alexei and Kraus, Oren and Shaban, Muhammad and Rajpoot, Nasir and Awan, Ruqayya and Sirinukunwattana, Korsuk and Qaiser, Talha and Tsang, Yee-Wah and Tellez, David and Annuscheit, Jonas and Hufnagl, Peter and Valkonen, Mira and Kartasalo, Kimmo and Latonen, Leena and Ruusuvuori, Pekka and Liimatainen, Kaisa and Albarqouni, Shadi and Mungal, Bharti and George, Ami and Demirci, Stefanie and Navab, Nassir and Watanabe, Seiryo and Seno, Shigeto and Takenaka, Yoichi and Matsuda, Hideo and Ahmady Phoulady, Hady and Kovalev, Vassili and Kalinovsky, Alexander and Liauchuk, Vitali and Bueno, Gloria and Fernandez-Carrobles, M. Milagro and Serrano, Ismael and Deniz, Oscar and Racoceanu, Daniel and Venâncio, Rui},
	month = dec,
	year = {2017},
	pages = {2199},
}

@article{powell_direct_1998,
	title = {Direct search algorithms for optimization calculations},
	volume = {7},
	issn = {1474-0508},
	doi = {10.1017/S0962492900002841},
	journal = {Acta numerica},
	author = {Powell, MJD},
	year = {1998},
	note = {00000},
	pages = {287--336},
}

@article{storn_differential_1997,
	title = {Differential evolution–a simple and efficient heuristic for global optimization over continuous spaces},
	volume = {11},
	doi = {10.1023/A:1008202821328},
	number = {4},
	journal = {Journal of global optimization},
	author = {Storn, Rainer and Price, Kenneth},
	year = {1997},
	note = {00000},
	pages = {341--359},
}

@article{aguirre_dynamical_1995,
	title = {Dynamical effects of overparametrization in nonlinear models},
	volume = {80},
	doi = {10.1016/0167-2789(95)90053-5},
	number = {1-2},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Aguirre, Luis Antonio and Billings, Stephen A},
	year = {1995},
	pages = {26--40},
}

@article{barbosa_downhole_2015,
	title = {Downhole {Pressure} {Estimation} {Using} {Committee} {Machines} and {Neural} {Networks}},
	volume = {48},
	doi = {10.1016/j.ifacol.2015.08.045},
	number = {6},
	journal = {IFAC-PapersOnLine},
	author = {Barbosa, Bruno HG and Gomes, Lucas P and Teixeira, Alex F and Aguirre, Luis A},
	year = {2015},
	pages = {286--291},
}

@article{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.3781},
	keywords = {Computer Science - Computation and Language, 🔍No DOI found},
}

@book{yansongwang_discrete_2011,
	title = {Discrete {Wavelet} {Transfom} for {Nonstationary} {Signal} {Processing}.},
	isbn = {978-953-307-185-5},
	language = {en.},
	publisher = {INTECH Open Access Publisher},
	author = {{Yansong Wang} and {Gongqi Shen} and {Qiang Zhu} and {Weiwei Wu}},
	year = {2011},
	note = {00006 
OCLC: 884041491},
}

@book{vukosavic_digital_2007,
	series = {Power {Electronics} and {Power} {Systems}},
	title = {Digital {Control} of {Electrical} {Drives}},
	isbn = {978-0-387-48598-0},
	url = {https://books.google.com.br/books?id=lmvb2gzE0OEC},
	publisher = {Springer US},
	author = {Vukosavic, S.N.},
	year = {2007},
	note = {00093},
}

@inproceedings{mikolov_distributed_2013,
	title = {Distributed representations of words and phrases and their compositionality},
	booktitle = {Advances in neural information processing systems},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
	year = {2013},
	pages = {3111--3119},
}

@book{greene_econometric_2003,
	address = {Upper Saddle River, N.J},
	edition = {5th ed},
	title = {Econometric analysis},
	isbn = {978-0-13-066189-0},
	publisher = {Prentice Hall},
	author = {Greene, William H.},
	year = {2003},
	keywords = {Econometrics},
}

@techreport{wesely_dry_1979,
	title = {Dry deposition and emission of small particles at the surface of the earth},
	url = {https://www.osti.gov/scitech/biblio/6231439},
	urldate = {2017-09-11},
	institution = {Argonne National Lab., IL (USA)},
	author = {Wesely, M. L. and Hicks, B. B.},
	year = {1979},
}

@inproceedings{sinha_efficient_2014,
	title = {Efficient high-resolution stereo matching using local plane sweeps},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} ({CVPR}), 2014 {IEEE} {Conference} on},
	publisher = {IEEE},
	author = {Sinha, Sudipta N and Scharstein, Daniel and Szeliski, Richard},
	year = {2014},
	note = {00000},
	pages = {1582--1589},
}

@incollection{lecun_efficient_1998,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Efficient {BackProp}},
	isbn = {978-3-540-65311-0 978-3-540-49430-0},
	url = {https://link.springer.com/chapter/10.1007/3-540-49430-8_2},
	abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
	language = {en},
	urldate = {2017-09-11},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {LeCun, Yann and Bottou, Leon and Orr, Genevieve B. and Müller, Klaus-Robert},
	year = {1998},
	doi = {10.1007/3-540-49430-8_2},
	pages = {9--50},
}

@inproceedings{luong_effective_2015,
	address = {Lisbon, Portugal},
	title = {Effective {Approaches} to {Attention}-based {Neural} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/D15-1166},
	doi = {10/gdpd6w},
	urldate = {2019-05-29},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Luong, Thang and Pham, Hieu and Manning, Christopher D.},
	month = sep,
	year = {2015},
	pages = {1412--1421},
}

@article{barbosa_downhole_2015a,
	title = {Downhole {Pressure} {Estimation} {Using} {Committee} {Machines} and {Neural} {Networks}},
	volume = {48},
	doi = {10/gfjwq5},
	number = {6},
	journal = {IFAC-PapersOnLine},
	author = {Barbosa, Bruno HG and Gomes, Lucas P and Teixeira, Alex F and Aguirre, Luis A},
	year = {2015},
	note = {00003},
	pages = {286--291},
}

@article{wright_direct_1996,
	title = {Direct search methods: {Once} scorned, now respectable},
	issn = {0269-3674},
	journal = {Pitman Research Notes in Mathematics Series},
	author = {Wright, Margaret H},
	year = {1996},
	note = {00398},
	keywords = {🔍No DOI found},
	pages = {191--208},
}

@phdthesis{araujo_desenvolvimento_2012,
	type = {Projeto de {Final} de {Curso}},
	title = {Desenvolvimento de uma {Ferramenta} para {Identificação} de {Sinais} {Correlacionados}: {Aplicação} em {Dados} {Provenientes} de uma {Plataforma} de {Petróleo}},
	language = {pt},
	school = {Universidade Federal de Minas Gerais},
	author = {Araujo, Kim Cândido Pereira},
	month = jun,
	year = {2012},
}

@article{aguirre_development_2017,
	title = {Development of soft sensors for permanent downhole {Gauges} in deepwater oil wells},
	volume = {65},
	issn = {09670661},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0967066117301284},
	doi = {10.1016/j.conengprac.2017.06.002},
	abstract = {Downhole pressure is an important process variable in the operation of gas-lifted oil wells. The device installed in order to measure this variable is often called a Permanent Downhole Gauge (PDG). Replacing a faulty PDG is often not economically viable and to have an alternative estimate of the downhole pressure is an important goal. Using data from operating PDGs, this paper describes a number of issues dealt with in the development of soft sensors for several deepwater gas-lifted oil wells. Some of the tested models include nonlinear polynomials, neural networks, committee machines, unscented Kalman filters and filter banks. The variety of model classes used in addition to the diversity of oil wells considered brings to light some of the key-problems that have to be faced and reveal the strengths and weaknesses of each alternative solution. A major constraint throughout the work was the use of historical data, hence no specific tests were performed at any time. The aim of this work is to discuss the procedures, pros and cons of the tested solutions and to point to possible future directions of research.},
	language = {en},
	urldate = {2020-02-17},
	journal = {Control Engineering Practice},
	author = {Aguirre, Luis A. and Teixeira, Bruno O.S. and Barbosa, Bruno H.G. and Teixeira, Alex F. and Campos, Mario C.M.M. and Mendes, Eduardo M.A.M.},
	month = aug,
	year = {2017},
	pages = {83--99},
}

@article{chen_detection_2020,
	title = {Detection and {Classification} of {Cardiac} {Arrhythmias} by a {Challenge}-{Best} {Deep} {Learning} {Neural} {Network} {Model}},
	volume = {23},
	issn = {25890042},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2589004220300705},
	doi = {10.1016/j.isci.2020.100886},
	abstract = {Electrocardiograms (ECGs) are widely used to clinically detect cardiac arrhythmias (CAs). They are also being used to develop computer-assisted methods for heart disease diagnosis. We have developed a convolution neural network model to detect and classify CAs, using a large 12-lead ECG dataset (6,877 recordings) provided by the China Physiological Signal Challenge (CPSC) 2018. Our model, which was ranked ﬁrst in the challenge competition, achieved a median overall F1-score of 0.84 for the nine-type CA classiﬁcation of CPSC2018’s hidden test set of 2,954 ECG recordings. Further analysis showed that concurrent CAs were adequately predictive for 476 patients with multiple types of CA diagnoses in the dataset. Using only single-lead data yielded a performance that was only slightly worse than using the full 12-lead data, with leads aVR and V1 being the most prominent. We extensively consider these results in the context of their agreement with and relevance to clinical observations.},
	language = {en},
	number = {3},
	urldate = {2020-06-25},
	journal = {iScience},
	author = {Chen, Tsai-Min and Huang, Chih-Han and Shih, Edward S.C. and Hu, Yu-Feng and Hwang, Ming-Jing},
	month = mar,
	year = {2020},
	pages = {100886},
}

@article{zhang_depth_2020,
	title = {Depth creates no more spurious local minima},
	url = {http://arxiv.org/abs/1901.09827},
	abstract = {We show that for any convex differentiable loss, a deep linear network has no spurious local minima as long as it is true for the two layer case. This reduction greatly simplifies the study on the existence of spurious local minima in deep linear networks. When applied to the quadratic loss, our result immediately implies the powerful result in [Kawaguchi 2016]. Further, with the work in [Zhou and Liang 2018], we can remove all the assumptions in [Kawaguchi 2016]. This property holds for more general "multi-tower" linear networks too. Our proof builds on [Laurent and von Brecht 2018] and develops a new perturbation argument to show that any spurious local minimum must have full rank, a structural property which can be useful more generally.},
	urldate = {2020-08-27},
	journal = {arXiv:1901.09827 [cs, stat]},
	author = {Zhang, Li},
	month = jan,
	year = {2020},
	note = {arXiv: 1901.09827},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gedon_deep_2020,
	title = {Deep {State} {Space} {Models} for {Nonlinear} {System} {Identification}},
	abstract = {An actively evolving model class for generative temporal models developed in the deep learning community are deep state space models (SSMs) which have a close connection to classic SSMs. In this work six new deep SSMs are implemented and evaluated for the identification of established nonlinear dynamic system benchmarks. The models and their parameter learning algorithms are elaborated rigorously. The usage of deep SSMs as a black-box identification model can describe a wide range of dynamics due to the flexibility of deep neural networks. Additionally, the uncertainty of the system is modelled and therefore one obtains a much richer representation and a whole class of systems to describe the underlying dynamics.},
	journal = {Proceedings of the 2020 IEEE  59th Conference on Decision and Control},
	author = {Gedon, Daniel and Wahlström, Niklas and Schön, Thomas B. and Ljung, Lennart},
	year = {2020},
	note = {arXiv: 2003.14162},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning},
}

@article{rubin_densely_2017,
	title = {Densely {Connected} {Convolutional} {Networks} and {Signal} {Quality} {Analysis} to {Detect} {Atrial} {Fibrillation} {Using} {Short} {Single}-{Lead} {ECG} {Recordings}},
	url = {http://arxiv.org/abs/1710.05817},
	abstract = {The development of new technology such as wearables that record high-quality single channel ECG, provides an opportunity for ECG screening in a larger population, especially for atrial fibrillation screening. The main goal of this study is to develop an automatic classification algorithm for normal sinus rhythm (NSR), atrial fibrillation (AF), other rhythms (O), and noise from a single channel short ECG segment (9-60 seconds). For this purpose, signal quality index (SQI) along with dense convolutional neural networks was used. Two convolutional neural network (CNN) models (main model that accepts 15 seconds ECG and secondary model that processes 9 seconds shorter ECG) were trained using the training data set. If the recording is determined to be of low quality by SQI, it is immediately classified as noisy. Otherwise, it is transformed to a time-frequency representation and classified with the CNN as NSR, AF, O, or noise. At the final step, a feature-based post-processing algorithm classifies the rhythm as either NSR or O in case the CNN model's discrimination between the two is indeterminate. The best result achieved at the official phase of the PhysioNet/CinC challenge on the blind test set was 0.80 (F1 for NSR, AF, and O were 0.90, 0.80, and 0.70, respectively).},
	urldate = {2018-10-21},
	journal = {arXiv:1710.05817},
	author = {Rubin, Jonathan and Parvaneh, Saman and Rahman, Asif and Conroy, Bryan and Babaeizadeh, Saeed},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.05817},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning, 🔍No DOI found},
}

@inproceedings{shashikumar_detection_2018,
	address = {New York, NY, USA},
	series = {{KDD} '18},
	title = {Detection of {Paroxysmal} {Atrial} {Fibrillation} {Using} {Attention}-based {Bidirectional} {Recurrent} {Neural} {Networks}},
	isbn = {978-1-4503-5552-0},
	url = {http://doi.acm.org/10.1145/3219819.3219912},
	doi = {10.1145/3219819.3219912},
	abstract = {Detection of atrial fibrillation (AF), a type of cardiac arrhythmia, is difficult since many cases of AF are usually clinically silent and undiagnosed. In particular paroxysmal AF is a form of AF that occurs occasionally, and has a higher probability of being undetected. In this work, we present an attention based deep learning framework for detection of paroxysmal AF episodes from a sequence of windows. Time-frequency representation of 30 seconds recording windows, over a 10 minute data segment, are fed sequentially into a deep convolutional neural network for image-based feature extraction, which are then presented to a bidirectional recurrent neural network with an attention layer for AF detection. To demonstrate the effectiveness of the proposed framework for transient AF detection, we use a database of 24 hour Holter Electrocardiogram (ECG) recordings acquired from 2850 patients at the University of Virginia heart station. The algorithm achieves an AUC of 0.94 on the testing set, which exceeds the performance of baseline models. We also demonstrate the cross-domain generalizablity of the approach by adapting the learned model parameters from one recording modality (ECG) to another (photoplethysmogram) with improved AF detection performance. The proposed high accuracy, low false alarm algorithm for detecting paroxysmal AF has potential applications in long-term monitoring using wearable sensors.},
	urldate = {2018-10-20},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Shashikumar, Supreeth P. and Shah, Amit J. and Clifford, Gari D. and Nemati, Shamim},
	year = {2018},
	keywords = {atrial fibrillation, convolutional neural network, deep learning, recurrent neural network, transfer learning},
	pages = {715--723},
}

@inproceedings{he_delving_2015,
	title = {Delving deep into rectifiers: {Surpassing} human-level performance on imagenet classification},
	shorttitle = {Delving deep into rectifiers},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
	pages = {1026--1034},
}

@incollection{sacramento_dendritic_2018,
	title = {Dendritic cortical microcircuits approximate the backpropagation algorithm},
	url = {http://papers.nips.cc/paper/8089-dendritic-cortical-microcircuits-approximate-the-backpropagation-algorithm.pdf},
	urldate = {2018-12-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Sacramento, João and Ponte Costa, Rui and Bengio, Yoshua and Senn, Walter},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {8734--8745},
}

@incollection{rangapuram_deep_2018,
	title = {Deep {State} {Space} {Models} for {Time} {Series} {Forecasting}},
	url = {http://papers.nips.cc/paper/8004-deep-state-space-models-for-time-series-forecasting.pdf},
	urldate = {2018-12-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Rangapuram, Syama Sundar and Seeger, Matthias W and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {7796--7805},
}

@article{chua_deep_2018,
	title = {Deep {Reinforcement} {Learning} in a {Handful} of {Trials} using {Probabilistic} {Dynamics} {Models}},
	url = {http://arxiv.org/abs/1805.12114},
	abstract = {Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).},
	urldate = {2019-02-06},
	journal = {arXiv:1805.12114 [cs, stat]},
	author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
	month = may,
	year = {2018},
	note = {arXiv: 1805.12114},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 🔍No DOI found},
	pages = {770--778},
}

@article{birchfield_depth_1999,
	title = {Depth discontinuities by pixel-to-pixel stereo},
	volume = {35},
	doi = {10.1023/A:1008160311296},
	number = {3},
	journal = {International Journal of Computer Vision},
	author = {Birchfield, Stan and Tomasi, Carlo},
	year = {1999},
	pages = {269--293},
}

@article{he_delving_2015a,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://arxiv.org/abs/1502.01852},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	journal = {arXiv:1502.01852 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.01852},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, 🔍No DOI found},
}

@inproceedings{taigman_deepface_2014,
	title = {{DeepFace}: {Closing} the {Gap} to {Human}-{Level} {Performance} in {Face} {Verification}},
	shorttitle = {{DeepFace}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Taigman_DeepFace_Closing_the_2014_CVPR_paper.html},
	urldate = {2018-01-26},
	author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
	year = {2014},
	note = {00000},
	pages = {1701--1708},
}

@article{banerjee_diagnosis_2017,
	title = {Diagnosis of prostate cancer by desorption electrospray ionization mass spectrometric imaging of small metabolites and lipids},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/114/13/3334},
	doi = {10.1073/pnas.1700677114},
	abstract = {Accurate identification of prostate cancer in frozen sections at the time of surgery can be challenging, limiting the surgeon’s ability to best determine resection margins during prostatectomy. We performed desorption electrospray ionization mass spectrometry imaging (DESI-MSI) on 54 banked human cancerous and normal prostate tissue specimens to investigate the spatial distribution of a wide variety of small metabolites, carbohydrates, and lipids. In contrast to several previous studies, our method included Krebs cycle intermediates (m/z {\textless}200), which we found to be highly informative in distinguishing cancer from benign tissue. Malignant prostate cells showed marked metabolic derangements compared with their benign counterparts. Using the “Least absolute shrinkage and selection operator” (Lasso), we analyzed all metabolites from the DESI-MS data and identified parsimonious sets of metabolic profiles for distinguishing between cancer and normal tissue. In an independent set of samples, we could use these models to classify prostate cancer from benign specimens with nearly 90\% accuracy per patient. Based on previous work in prostate cancer showing that glucose levels are high while citrate is low, we found that measurement of the glucose/citrate ion signal ratio accurately predicted cancer when this ratio exceeds 1.0 and normal prostate when the ratio is less than 0.5. After brief tissue preparation, the glucose/citrate ratio can be recorded on a tissue sample in 1 min or less, which is in sharp contrast to the 20 min or more required by histopathological examination of frozen tissue specimens.},
	language = {en},
	number = {13},
	urldate = {2017-09-18},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Banerjee, Shibdas and Zare, Richard N. and Tibshirani, Robert J. and Kunder, Christian A. and Nolley, Rosalie and Fan, Richard and Brooks, James D. and Sonn, Geoffrey A.},
	month = mar,
	year = {2017},
	pmid = {28292895},
	keywords = {Krebs cycle, desorption electrospray ionization, mass spectrometry, metabolism, prostate cancer},
	pages = {3334--3339},
}

@book{franco_design_2002,
	series = {{McGraw}-{Hill} series in electrical and computer engineering},
	title = {Design with operational amplifiers and analog integrated circuits},
	isbn = {978-0-07-232084-8},
	url = {https://books.google.com.br/books?id=LXMeAQAAIAAJ},
	publisher = {McGraw-Hill},
	author = {Franco, S.},
	year = {2002},
}

@inproceedings{zabihi_detection_2017,
	title = {Detection of atrial fibrillation in {ECG} hand-held devices using a random forest classifier},
	volume = {44},
	url = {https://www.scholars.northwestern.edu/en/publications/detection-of-atrial-fibrillation-in-ecg-hand-held-devices-using-a},
	doi = {10/gf2597},
	language = {English (US)},
	urldate = {2019-05-27},
	booktitle = {Computing in {Cardiology}},
	publisher = {IEEE Computer Society},
	author = {Zabihi, Morteza and Rad, Ali Bahrami and Katsaggelos, Aggelos K. and Kiranyaz, Serkan and Narkilahti, Susanna and Gabbouj, Moncef},
	month = jan,
	year = {2017},
	pages = {1--4},
}

@article{huang_densely_2016,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.06993},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
	urldate = {2019-06-13},
	journal = {arXiv:1608.06993 [cs]},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.06993},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{zou_delving_2020,
	title = {Delving {Deeper} into {Anti}-aliasing in {ConvNets}},
	language = {en},
	booktitle = {Proceedings of the 31st {British} {Machine} {Vision} {Virtual} {Conference} ({BMVC})},
	author = {Zou, Xueyan},
	year = {2020},
}

@article{lorenz_deterministic_1963,
	title = {Deterministic nonperiodic flow},
	volume = {20},
	issn = {1520-0469},
	number = {2},
	journal = {Journal of atmospheric sciences},
	author = {Lorenz, Edward N},
	year = {1963},
	pages = {130--141},
}

@article{fort_deep_2020,
	title = {Deep {Ensembles}: {A} {Loss} {Landscape} {Perspective}},
	shorttitle = {Deep {Ensembles}},
	url = {http://arxiv.org/abs/1912.02757},
	abstract = {Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.},
	urldate = {2020-07-13},
	journal = {arXiv:1912.02757 [cs, stat]},
	author = {Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
	month = jun,
	year = {2020},
	note = {arXiv: 1912.02757},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{huijben_deep_2019,
	title = {Deep probabilistic subsampling for task-adaptive compressed sensing},
	url = {https://openreview.net/forum?id=SJeq9JBFvH},
	abstract = {The field of deep learning is commonly concerned with optimizing predictive models using large pre-acquired datasets of densely sampled datapoints or signals. In this work, we demonstrate that the...},
	urldate = {2020-07-20},
	author = {Huijben, Iris A. M. and Veeling, Bastiaan S. and Sloun, Ruud J. G. van},
	month = sep,
	year = {2019},
}

@article{strodthoff_deep_2020,
	title = {Deep {Learning} for {ECG} {Analysis}: {Benchmarks} and {Insights} from {PTB}-{XL}},
	shorttitle = {Deep {Learning} for {ECG} {Analysis}},
	url = {http://arxiv.org/abs/2004.13701},
	abstract = {Electrocardiography is a very common, non-invasive diagnostic procedure and its interpretation is increasingly supported by automatic interpretation algorithms. The progress in the field of automatic ECG interpretation has up to now been hampered by a lack of appropriate datasets for training as well as a lack of well-defined evaluation procedures to ensure comparability of different algorithms. To alleviate these issues, we put forward first benchmarking results for the recently published, freely accessible PTB-XL dataset, covering a variety of tasks from different ECG statement prediction tasks over age and gender prediction to signal quality assessment. We find that convolutional neural networks, in particular resnet- and inception-based architectures, show the strongest performance across all tasks outperforming feature-based algorithms by a large margin. These results are complemented by deeper insights into the classification algorithm in terms of hidden stratification, model uncertainty and an exploratory interpretability analysis. We also put forward benchmarking results for the ICBEB2018 challenge ECG dataset and discuss prospects of transfer learning using classifiers pretrained on PTB-XL. With this resource, we aim to establish the PTB-XL dataset as a resource for structured benchmarking of ECG analysis algorithms and encourage other researchers in the field to join these efforts.},
	urldate = {2020-07-14},
	journal = {arXiv:2004.13701 [cs, stat]},
	author = {Strodthoff, Nils and Wagner, Patrick and Schaeffter, Tobias and Samek, Wojciech},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.13701},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{kawaguchi_deep_2016,
	title = {Deep {Learning} without {Poor} {Local} {Minima}},
	url = {http://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima.pdf},
	urldate = {2020-08-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Kawaguchi, Kenji},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {586--594},
}

@article{laurent_deep_,
	title = {Deep {Linear} {Networks} with {Arbitrary} {Loss}: {All} {Local} {Minima} {Are} {Global}},
	abstract = {We consider deep linear networks with arbitrary convex differentiable loss. We provide a short and elementary proof of the fact that all local minima are global minima if the hidden layers are either 1) at least as wide as the input layer, or 2) at least as wide as the output layer. This result is the strongest possible in the following sense: If the loss is convex and Lipschitz but not differentiable then deep linear networks can have sub-optimal local minima.},
	language = {en},
	author = {Laurent, Thomas},
	pages = {6},
}

@article{lecun_deep_2015a,
	title = {Deep learning},
	volume = {521},
	url = {https://doi.org/10.1038/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	number = {7553},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	year = {2015},
	note = {ISBN: 1476-4687
tex.da: 2015/05/01
tex.date-added: 2020-09-23 19:36:56 -0300
tex.date-modified: 2020-09-23 19:36:56 -0300
tex.ty: JOUR},
	pages = {436--444},
}

@inproceedings{nakkiran_deep_2020,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	year = {2020},
	note = {arXiv: 1912.02292},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {1476-4687},
	doi = {10/bmqp},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	number = {7553},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	year = {2015},
	pages = {436--444},
}

@article{han_deep_2020,
	title = {Deep learning models for electrocardiograms are susceptible to adversarial attack},
	volume = {26},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-020-0791-x},
	doi = {10.1038/s41591-020-0791-x},
	abstract = {Electrocardiogram (ECG) acquisition is increasingly widespread in medical and commercial devices, necessitating the development of automated interpretation strategies. Recently, deep neural networks have been used to automatically analyze ECG tracings and outperform physicians in detecting certain rhythm irregularities1. However, deep learning classifiers are susceptible to adversarial examples, which are created from raw data to fool the classifier such that it assigns the example to the wrong class, but which are undetectable to the human eye2,3. Adversarial examples have also been created for medical-related tasks4,5. However, traditional attack methods to create adversarial examples do not extend directly to ECG signals, as such methods introduce square-wave artefacts that are not physiologically plausible. Here we develop a method to construct smoothed adversarial examples for ECG tracings that are invisible to human expert evaluation and show that a deep learning model for arrhythmia detection from single-lead ECG6 is vulnerable to this type of attack. Moreover, we provide a general technique for collating and perturbing known adversarial examples to create multiple new ones. The susceptibility of deep learning ECG algorithms to adversarial misclassification implies that care should be taken when evaluating these models on ECGs that may have been altered, particularly when incentives for causing misclassification exist.},
	language = {en},
	number = {3},
	urldate = {2020-10-28},
	journal = {Nature Medicine},
	author = {Han, Xintian and Hu, Yuxuan and Foschini, Luca and Chinitz, Larry and Jankelson, Lior and Ranganath, Rajesh},
	month = mar,
	year = {2020},
	note = {Number: 3
Publisher: Nature Publishing Group},
	pages = {360--363},
}

@inproceedings{ljung_deep_2020,
	title = {Deep {Learning} and {System} {Identiﬁcation}},
	abstract = {Deep learning is a topic of considerable interest today. Since it deals with estimating – or learning – models, there are connections to the area of System Identiﬁcation developed in the Automatic Control community. Such connections are explored and exploited in this contribution. It is stressed that common deep nets such as feedforward and cascadeforward nets are nonlinear ARX (NARX) models, and can thus be easily incorporated in System Identiﬁcation code and practice. The case of LSTM nets is an example of NonLinear State-Space (NLSS) models. It performs worse than the cascadeforwardnet for a standard benchmark example.},
	language = {en},
	booktitle = {Proceedings of the {IFAC} {Congress}, {Berlin}},
	author = {Ljung, Lennart and Andersson, Carl and Tiels, Koen and Schon, Thomas B},
	year = {2020},
	pages = {8},
}

@article{rahhal_deep_2016,
	title = {Deep learning approach for active classification of electrocardiogram signals},
	volume = {345},
	issn = {00200255},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025516300184},
	doi = {10.1016/j.ins.2016.01.082},
	language = {en},
	urldate = {2018-10-21},
	journal = {Information Sciences},
	author = {Rahhal, M.M. Al and Bazi, Yakoub and AlHichri, Haikel and Alajlan, Naif and Melgani, Farid and Yager, R.R.},
	month = jun,
	year = {2016},
	pages = {340--354},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@article{simonyan_deep_2013,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1312.6034},
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	urldate = {2018-11-15},
	journal = {arXiv:1312.6034 [cs]},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6034},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{hinton_deep_2018,
	title = {Deep learning—a technology with the potential to transform health care},
	volume = {320},
	issn = {0098-7484},
	url = {http://dx.doi.org/10.1001/jama.2018.11100},
	doi = {10/gfkhr6},
	abstract = {Widespread application of artificial intelligence in health care has been anticipated for half a century. For most of that time, the dominant approach to artificial intelligence was inspired by logic: researchers assumed that the essence of intelligence was manipulating symbolic expressions, using rules of inference. This approach produced expert systems and graphical models that attempted to automate the reasoning processes of experts. In the last decade, however, a radically different approach to artificial intelligence, called deep learning, has produced major breakthroughs and is now used on billions of digital devices for complex tasks such as speech recognition, image interpretation, and language translation. The purpose of this Viewpoint is to give health care professionals an intuitive understanding of the technology underlying deep learning. In an accompanying Viewpoint, Naylor1 outlines some of the factors propelling adoption of this technology in medicine and health care.},
	number = {11},
	journal = {JAMA},
	author = {Hinton, Geoffrey},
	month = sep,
	year = {2018},
	pages = {1101--1102},
}

@article{hinton_deep_2012,
	title = {Deep {Neural} {Networks} for {Acoustic} {Modeling} in {Speech} {Recognition}: {The} {Shared} {Views} of {Four} {Research} {Groups}},
	volume = {29},
	issn = {1053-5888},
	shorttitle = {Deep {Neural} {Networks} for {Acoustic} {Modeling} in {Speech} {Recognition}},
	doi = {10/gc8z3r},
	abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Hinton, G. and Deng, L. and Yu, D. and Dahl, G. E. and Mohamed, A. and Jaitly, N. and Senior, A. and Vanhoucke, V. and Nguyen, P. and Sainath, T. N. and Kingsbury, B.},
	month = nov,
	year = {2012},
	keywords = {Acoustics, Automatic speech recognition, Data models, Gaussian mixture models, Gaussian processes, HMM states, Hidden Markov models, Neural networks, Speech recognition, Training, acoustic modeling, deep neural networks, feed-forward neural network, feedforward neural nets, hidden Markov models, posterior probabilities, speech recognition, temporal variability},
	pages = {82--97},
}

@book{topol_deep_2019,
	title = {Deep medicine: how artificial intelligence can make healthcare human again},
	isbn = {1-5416-4464-6},
	publisher = {Hachette UK},
	author = {Topol, Eric},
	year = {2019},
}

@article{bai_deep_2019,
	title = {Deep {Equilibrium} {Models}},
	url = {http://arxiv.org/abs/1909.01377},
	abstract = {We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some fixed point, we propose the DEQ approach that directly finds these equilibrium points via root-finding. Such a method is equivalent to running an infinite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective "depth" of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these state-of-the-art models (for similar parameter counts); 2) have similar computational requirements as existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88\% memory reduction in our experiments. The code is available at https://github. com/locuslab/deq .},
	urldate = {2019-10-14},
	journal = {arXiv:1909.01377 [cs, stat]},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.01377},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hendriks_deep_2021a,
	title = {Deep {Energy}-{Based} {NARX} {Models}},
	abstract = {This paper is directed towards the problem of learning nonlinear ARX models based on system input–output data. In particular, our interest is in learning a conditional distribution of the current output based on a finite window of past inputs and outputs. To achieve this, we consider the use of so-called energy-based models, which have been developed in allied fields for learning unknown distributions based on data. This energy-based model relies on a general function to describe the distribution, and here we consider a deep neural network for this purpose. The primary benefit of this approach is that it is capable of learning both simple and highly complex noise models, which we demonstrate on simulated and experimental data.},
	journal = {Workshop on Nonlinear System Identification},
	author = {Hendriks, Johannes N. and Gustafsson, Fredrik K. and Ribeiro, Antônio H. and Wills, Adrian G. and Schön, Thomas B.},
	year = {2021},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
}

@article{teixeira_datadriven_2014,
	title = {Data-{Driven} {Soft} {Sensor} of {Downhole} {Pressure} for a {Gas}-{Lift} {Oil} {Well}},
	volume = {22},
	doi = {10/f5nhgb},
	journal = {Control Engineering Practice},
	author = {Teixeira, Bruno O. S. and Castro, Walace S and Teixeira, Alex F and Aguirre, Luis A},
	year = {2014},
	pages = {34--43},
}

@article{zhou_deconstructing_2020,
	title = {Deconstructing {Lottery} {Tickets}: {Zeros}, {Signs}, and the {Supermask}},
	shorttitle = {Deconstructing {Lottery} {Tickets}},
	url = {http://arxiv.org/abs/1905.01067},
	abstract = {The recent "Lottery Ticket Hypothesis" paper by Frankle \& Carbin showed that a simple approach to creating sparse networks (keeping the large weights) results in models that are trainable from scratch, but only when starting from the same initial weights. The performance of these networks often exceeds the performance of the non-sparse base model, but for reasons that were not well understood. In this paper we study the three critical components of the Lottery Ticket (LT) algorithm, showing that each may be varied significantly without impacting the overall results. Ablating these factors leads to new insights for why LT networks perform as well as they do. We show why setting weights to zero is important, how signs are all you need to make the reinitialized network train, and why masking behaves like training. Finally, we discover the existence of Supermasks, masks that can be applied to an untrained, randomly initialized network to produce a model with performance far better than chance (86\% on MNIST, 41\% on CIFAR-10).},
	urldate = {2020-06-29},
	journal = {arXiv:1905.01067 [cs, stat]},
	author = {Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
	month = mar,
	year = {2020},
	note = {arXiv: 1905.01067},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{candes_decoding_2005,
	title = {Decoding by linear programming},
	volume = {51},
	number = {12},
	journal = {IEEE transactions on information theory},
	author = {Candes, Emmanuel J and Tao, Terence},
	year = {2005},
	note = {Publisher: IEEE},
	pages = {4203--4215},
}

@article{hernan_data_2018,
	title = {Data science is science's second chance to get causal inference right: {A} classification of data science tasks},
	shorttitle = {Data science is science's second chance to get causal inference right},
	url = {http://arxiv.org/abs/1804.10846},
	abstract = {Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term "data science" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: description, prediction, and causal inference. An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.},
	urldate = {2018-12-13},
	journal = {arXiv:1804.10846 [cs, stat]},
	author = {Hernán, Miguel A. and Hsu, John and Healy, Brian},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.10846},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{dai_coupled_2018,
	title = {Coupled {Variational} {Bayes} via {Optimization} {Embedding}},
	url = {http://papers.nips.cc/paper/8177-coupled-variational-bayes-via-optimization-embedding.pdf},
	urldate = {2019-03-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Dai, Bo and Dai, Hanjun and He, Niao and Liu, Weiyang and Liu, Zhen and Chen, Jianshu and Xiao, Lin and Song, Le},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {9690--9700},
}

@article{billings_correlation_1986,
	title = {Correlation based model validity tests for non-linear models},
	volume = {44},
	doi = {10.1080/00207179108934155},
	number = {1},
	journal = {International journal of Control},
	author = {Billings, S. A. and Voon, W. S. F.},
	year = {1986},
	pages = {235--244},
}

@article{wigren_coupled_,
	title = {Coupled {Electric} {Drives} {Data} {Set} and {Reference} {Models}},
	abstract = {The following report provides a description of the CE8 coupled electric drives laboratory process. A first set of continuous time model structures that are suitable to describe the nonlinear dynamics are presented. The data sets, which are available in .mat and .csv file formats, are then described in detail. The available data sets are short, which constitute a challenge when performing identification. In support of future work, Wiener models are identified with a recursive algorithm that is parameterized in continuous time. This approach reduces the number of parameters to four for identification of third order dynamics.},
	language = {en},
	author = {Wigren, Torbjörn and Schoukens, Maarten},
	keywords = {🔍No DOI found},
	pages = {11},
}

@article{wu_coordinate_2008,
	title = {Coordinate {Descent} {Algorithms} for {Lasso} {Penalized} {Regression}},
	volume = {2},
	issn = {1932-6157},
	url = {http://www.jstor.org/stable/30244184},
	doi = {10.1214/07-AOAS147},
	abstract = {Imposition of a lasso penalty shrinks parameter estimates toward zero and performs continuous model selection. Lasso penalized regression is capable of handling linear regression problems where the number of predictors far exceeds the number of cases. This paper tests two exceptionally fast algorithms for estimating regression coefficients with a lasso penalty. The previously known ℓ₂ algorithm is based on cyclic coordinate descent. Our new ℓ₁ algorithm is based on greedy coordinate descent and Edgeworth's algorithm for ordinary ℓ₁ regression. Each algorithm relies on a tuning constant that can be chosen by cross-validation. In some regression problems it is natural to group parameters and penalize parameters group by group rather than separately. If the group penalty is proportional to the Euclidean norm of the parameters of the group, then it is possible to majorize the norm and reduce parameter estimation to ℓ₂ regression with a lasso penalty. Thus, the existing algorithm can be extended to novel settings. Each of the algorithms discussed is tested via either simulated or real data or both. The Appendix proves that a greedy form of the ℓ₂ algorithm converges to the minimum value of the objective function.},
	number = {1},
	journal = {The Annals of Applied Statistics},
	author = {Wu, Tong Tong and Lange, Kenneth},
	year = {2008},
	note = {00622},
	pages = {224--244},
}

@article{kadlec_datadriven_2009,
	title = {Data-driven {Soft} {Sensors} in the process industry},
	volume = {33},
	issn = {00981354},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0098135409000076},
	doi = {10.1016/j.compchemeng.2008.12.012},
	language = {en},
	number = {4},
	urldate = {2017-12-24},
	journal = {Computers \& Chemical Engineering},
	author = {Kadlec, Petr and Gabrys, Bogdan and Strandt, Sibylle},
	month = apr,
	year = {2009},
	pages = {795--814},
}

@book{zaki_data_2014,
	address = {New York, NY},
	title = {Data mining and analysis: fundamental concepts and algorithms},
	isbn = {978-0-521-76633-3},
	shorttitle = {Data mining and analysis},
	publisher = {Cambridge University Press},
	author = {Zaki, Mohammed J. and Meira, Wagner},
	year = {2014},
	keywords = {Data mining},
}

@article{relan_datadriven_2017,
	title = {Data-{Driven} {Nonlinear} {Identification} of {Li}-{Ion} {Battery} {Based} on a {Frequency} {Domain} {Nonparametric} {Analysis}},
	volume = {25},
	issn = {1063-6536},
	doi = {10.1109/TCST.2016.2616380},
	abstract = {Lithium ion batteries are attracting significant and growing interest, because their high energy and high power density render them an excellent option for energy storage, particularly in hybrid and electric vehicles. In this brief, a data-driven polynomial nonlinear state-space model is proposed for the operating points at the cusp of linear and nonlinear regimes of the battery’s electrical operation, based on the thorough nonparametric frequency domain characterization and quantification of the battery’s behavior in terms of its linear and nonlinear behavior at different levels of the state of charge.},
	number = {5},
	journal = {IEEE Transactions on Control Systems Technology},
	author = {Relan, R. and Firouz, Y. and Timmermans, J. M. and Schoukens, J.},
	month = sep,
	year = {2017},
	note = {00000},
	keywords = {Analytical models, Batteries, Computational modeling, Frequency-domain analysis, Hidden Markov models, Input–output response, Integrated circuit modeling, Mathematical model, Nonlinear system identification, lithium ion (Li-ion) battery, nonparametric characterization, polynomial nonlinear state space (PNLSS)},
	pages = {1825--1832},
}

@article{gould_cutest_2015,
	title = {{CUTEst}: a constrained and unconstrained testing environment with safe threads for mathematical optimization},
	volume = {60},
	issn = {0926-6003},
	doi = {10.1007/s10589-014-9687-3},
	number = {3},
	journal = {Computational Optimization and Applications},
	author = {Gould, Nicholas IM and Orban, Dominique and Toint, Philippe L},
	year = {2015},
	pages = {545--557},
}

@article{eckhard_cost_2017,
	title = {Cost function shaping of the output error criterion},
	volume = {76},
	issn = {0005-1098},
	url = {http://www.sciencedirect.com/science/article/pii/S0005109816304198},
	doi = {10/f9pgk7},
	abstract = {Identification of an output error model using the prediction error method leads to an optimization problem built on input/output data collected from the system to be identified. It is often hard to find the global solution of this optimization problem because in most cases both the corresponding objective function and the search space are nonconvex. The difficulty in solving the optimization problem depends mainly on the experimental conditions, more specifically on the spectra of the input/output data collected from the system. It is therefore possible to improve the convergence of the algorithms by properly choosing the data prefilters; in this paper we show how to perform this choice. We present the application of the proposed approach to case studies where the standard algorithms tend to fail to converge to the global minimum.},
	urldate = {2019-04-14},
	journal = {Automatica},
	author = {Eckhard, Diego and Bazanella, Alexandre S. and Rojas, Cristian R. and Hjalmarsson, Håkan},
	month = feb,
	year = {2017},
	keywords = {Identification methods, Model fitting},
	pages = {53--60},
}

@article{achille_critical_2017,
	title = {Critical {Learning} {Periods} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.08856},
	abstract = {Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training. To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training. Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of "Information Plasticity". Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.},
	urldate = {2019-06-01},
	journal = {arXiv:1711.08856 [cs, q-bio, stat]},
	author = {Achille, Alessandro and Rovere, Matteo and Soatto, Stefano},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.08856},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@article{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	urldate = {2019-06-08},
	journal = {arXiv:1802.05365 [cs]},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.05365},
	keywords = {Computer Science - Computation and Language},
}

@misc{ng_cs230_2018,
	title = {{CS230} {Deep} {Learning} ({Stanford})},
	url = {https://cs230-stanford.github.io/train-dev-test-split.html},
	abstract = {Short tutorial detailing the best practices to split your dataset into train, dev and test sets},
	language = {en},
	urldate = {2019-07-18},
	journal = {CS230 Deep Learning},
	author = {Ng, Andrew Y. and Katanforoosh, Kian},
	month = jan,
	year = {2018},
}

@article{billings_correlation_1986a,
	title = {Correlation {Based} {Model} {Validity} {Tests} for {Non}-{Linear} {Models}},
	volume = {44},
	doi = {10/dkj53r},
	number = {1},
	journal = {International journal of Control},
	author = {Billings, S. A. and Voon, W. S. F.},
	year = {1986},
	note = {00002},
	pages = {235--244},
}

@article{gould_cutest_2015a,
	title = {{CUTEst}: {A} {Constrained} and {Unconstrained} {Testing} {Environment} with {Safe} {Threads} for {Mathematical} {Optimization}},
	volume = {60},
	issn = {0926-6003},
	doi = {10/gfjwmh},
	number = {3},
	journal = {Computational Optimization and Applications},
	author = {Gould, Nicholas IM and Orban, Dominique and Toint, Philippe L},
	year = {2015},
	pages = {545--557},
}

@article{teixeira_datadriven_2014a,
	title = {Data-{Driven} {Soft} {Sensor} of {Downhole} {Pressure} for a {Gas}-{Lift} {Oil} {Well}},
	volume = {22},
	doi = {10/f5nhgb},
	journal = {Control Engineering Practice},
	author = {Teixeira, Bruno O. S. and Castro, Walace S and Teixeira, Alex F and Aguirre, Luis A},
	year = {2014},
	pages = {34--43},
}

@article{chattopadhyay_datadriven_2020,
	title = {Data-driven prediction of a multi-scale {Lorenz} 96 chaotic system using deep learning methods: {Reservoir} computing, {ANN}, and {RNN}-{LSTM}},
	volume = {27},
	issn = {1607-7946},
	shorttitle = {Data-driven prediction of a multi-scale {Lorenz} 96 chaotic system using deep learning methods},
	url = {http://arxiv.org/abs/1906.08829},
	doi = {10.5194/npg-27-373-2020},
	abstract = {In this paper, the performance of three deep learning methods for predicting short-term evolution and for reproducing the long-term statistics of a multi-scale spatio-temporal Lorenz 96 system is examined. The methods are: echo state network (a type of reservoir computing, RC-ESN), deep feed-forward artificial neural network (ANN), and recurrent neural network with long short-term memory (RNN-LSTM). This Lorenz 96 system has three tiers of nonlinearly interacting variables representing slow/large-scale (\$X\$), intermediate (\$Y\$), and fast/small-scale (\$Z\$) processes. For training or testing, only \$X\$ is available; \$Y\$ and \$Z\$ are never known or used. We show that RC-ESN substantially outperforms ANN and RNN-LSTM for short-term prediction, e.g., accurately forecasting the chaotic trajectories for hundreds of numerical solver's time steps, equivalent to several Lyapunov timescales. The RNN-LSTM and ANN show some prediction skills as well; RNN-LSTM bests ANN. Furthermore, even after losing the trajectory, data predicted by RC-ESN and RNN-LSTM have probability density functions (PDFs) that closely match the true PDF, even at the tails. The PDF of the data predicted using ANN, however, deviates from the true PDF. Implications, caveats, and applications to data-driven and data-assisted surrogate modeling of complex nonlinear dynamical systems such as weather/climate are discussed.},
	number = {3},
	urldate = {2021-04-01},
	journal = {Nonlinear Processes in Geophysics},
	author = {Chattopadhyay, Ashesh and Hassanzadeh, Pedram and Subramanian, Devika},
	month = jul,
	year = {2020},
	note = {arXiv: 1906.08829},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Nonlinear Sciences - Chaotic Dynamics, Statistics - Machine Learning},
	pages = {373--389},
}

@article{ribeiro_deep_2019,
	title = {Deep {Convolutional} {Networks} are {Useful} in {System} {Identiﬁcation}},
	copyright = {All rights reserved},
	journal = {Workshop on Nonlinear System Identification},
	author = {Ribeiro, Antonio H and Andersson, Carl and Tiels, Koen and Wahlstrom, Niklas and Schon, Thomas B},
	year = {2019},
}

@article{meirajr_contextualized_2020,
	title = {Contextualized {Interpretable} {Machine} {Learning} for {Medical} {Diagnosis}},
	copyright = {All rights reserved},
	doi = {10.1145/3416965},
	journal = {Communications of the ACM},
	author = {Meira Jr, Wagner and Ribeiro, Antonio L. P. and Oliveira, Derick M. and Ribeiro, Antonio H.},
	year = {2020},
}

@article{he_control_2017,
	title = {Control {Design} for {Nonlinear} {Flexible} {Wings} of a {Robotic} {Aircraft}},
	volume = {25},
	issn = {2374-0159},
	doi = {10.1109/TCST.2016.2536708},
	abstract = {In this brief, the control problem for flexible wings of a robotic aircraft is addressed by using boundary control schemes. Inspired by birds and bats, the wing with flexibility and articulation is modeled as a distributed parameter system described by hybrid partial differential equations and ordinary differential equations. Boundary control for both wing twist and bending is proposed on the original coupled dynamics, and bounded stability is proved by introducing a proper Lyapunov function. The effectiveness of the proposed control is verified by simulations.},
	number = {1},
	journal = {IEEE Transactions on Control Systems Technology},
	author = {He, Wei and Zhang, Shuang},
	month = jan,
	year = {2017},
	keywords = {Aerospace control, Aircraft, Boundary conditions, Boundary control, Control design, Distributed parameter systems, Lyapunov function, Lyapunov methods, Mathematical model, Robots, aerospace components, aerospace simulation, articulation, autonomous aerial vehicles, bats, bending, birds, boundary control schemes, bounded stability, control design, control system synthesis, distributed parameter system, distributed parameter systems, flexible wings, hybrid partial differential equations, microaerial vehicle (MAV), nonlinear flexible wings, nonlinear systems, ordinary differential equations, original coupled dynamics, partial differential equations, robot, robotic aircraft, simulations, stability, vehicle dynamics, vibration control, wing twist},
	pages = {351--357},
}

@article{gluch_constructing_,
	title = {Constructing a provably adversarially-robust classiﬁer from a high accuracy one},
	language = {en},
	author = {Głuch, Grzegorz and Urbanke, Rüdiger},
	pages = {10},
}

@article{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	urldate = {2018-12-04},
	journal = {arXiv:1411.1784 [cs, stat]},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.1784},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{loop_computing_1999,
	title = {Computing rectifying homographies for stereo vision},
	volume = {1},
	booktitle = {Computer {Vision} and {Pattern} {Recognition}, 1999. {IEEE} {Computer} {Society} {Conference} on.},
	publisher = {IEEE},
	author = {Loop, Charles and Zhang, Zhengyou},
	year = {1999},
}

@article{karikov_construction_2013,
	title = {Construction of a {Dynamic} {Neural} {Network} {Model} as a {Stage} of {Grate} {Cooler} {Automation}},
	volume = {25},
	number = {2},
	journal = {World Applied Sciences Journal},
	author = {Karikov, Evgeny Borisovich and Rubanov, Vasily Grigorievich and Klassen, Victor Korneevich},
	year = {2013},
	keywords = {🔍No DOI found},
	pages = {227--232},
}

@article{garnelo_conditional_,
	title = {Conditional {Neural} {Processes}},
	abstract = {Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet GPs are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes (CNPs), that combine the beneﬁts of both. CNPs are inspired by the ﬂexibility of stochastic processes such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classiﬁcation and image completion.},
	language = {en},
	author = {Garnelo, Marta and Rosenbaum, Dan and Maddison, Chris J and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo J and Eslami, S M Ali},
	keywords = {🔍No DOI found},
	pages = {10},
}

@inproceedings{tassa_controllimited_2014,
	title = {Control-limited differential dynamic programming},
	booktitle = {Robotics and {Automation} ({ICRA}), 2014 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Tassa, Yuval and Mansard, Nicolas and Todorov, Emo},
	year = {2014},
	note = {00000},
	pages = {1168--1175},
}

@article{ljung_convergence_1978,
	title = {Convergence analysis of parametric identification methods},
	volume = {23},
	issn = {0018-9286},
	doi = {10.1109/TAC.1978.1101840},
	abstract = {A certain class of methods to select suitable models of dynamical stochastic systems from measured input-output data is considered. The methods are based on a comparison between the measured outputs and the outputs of a candidate model. Depending on the set of models that is used, such methods are known under a variety of names, like output-error methods, equation-error methods, maximum-likelihood methods, etc. General results are proved concerning the models that are selected asymptotically as the number of observed data tends to infinity. For these results it is not assumed that the true system necessarily can be exactly represented within the chosen set of models. In the particular case when the model set contains the system, general consistency results are obtained and commented upon. Rather than to seek an exact description of the system, it is usually more realistic to be content with a suitable approximation of the true system with reasonable complexity properties. Here, the consequences of such a viewpoint are discussed.},
	number = {5},
	journal = {IEEE Transactions on Automatic Control},
	author = {Ljung, L.},
	month = oct,
	year = {1978},
	keywords = {Convergence, Entropy, Equations, H infinity control, Helium, Parameter identification, Predictive models, Random processes, Testing, maximum likelihood estimation, stochastic systems},
	pages = {770--783},
}

@inproceedings{graves_connectionist_2006,
	title = {Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
	shorttitle = {Connectionist temporal classification},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning},
	publisher = {ACM},
	author = {Graves, Alex and Fernández, Santiago and Gomez, Faustino and Schmidhuber, Jürgen},
	year = {2006},
	pages = {369--376},
}

@book{graham_concrete_1994,
	address = {Reading, Mass},
	edition = {2nd ed},
	title = {Concrete mathematics: a foundation for computer science},
	isbn = {978-0-201-55802-9},
	shorttitle = {Concrete mathematics},
	publisher = {Addison-Wesley},
	author = {Graham, Ronald L. and Knuth, Donald Ervin and Patashnik, Oren},
	year = {1994},
	keywords = {Computer science, Mathematics},
}

@book{leonhard_control_2001,
	title = {Control of electrical drives},
	publisher = {Springer Science \& Business Media},
	author = {Leonhard, Werner},
	year = {2001},
}

@book{normey-rico_control_2007,
	address = {London},
	series = {Advanced textbooks in control and signal processing},
	title = {Control of dead-time processes},
	isbn = {978-1-84628-828-9 978-1-84628-829-6},
	publisher = {Springer},
	author = {Normey-Rico, J. E. and Camacho, E. F.},
	year = {2007},
	note = {OCLC: ocm82672359},
	keywords = {Automatic control},
}

@article{wolfe_convergence_1969,
	title = {Convergence conditions for ascent methods},
	volume = {11},
	number = {2},
	journal = {SIAM review},
	author = {Wolfe, Philip},
	year = {1969},
	note = {00926},
	keywords = {❓Multiple DOI},
	pages = {226--235},
}

@phdthesis{umenberger_convex_2017,
	address = {Sydney},
	title = {Convex {Identifcation} of {Stable} {Dynamical} {Systems}},
	abstract = {This thesis concerns the scalable application of convex optimization to data-driven mod- eling of dynamical systems, termed system identification in the control community. Two problems commonly arising in system identification are model instability (e.g. unreliability of long-term, open-loop predictions), and nonconvexity of quality-of-fit criteria, such as sim- ulation error (a.k.a. output error). To address these problems, this thesis presents convex parametrizations of stable dynamical systems, convex quality-of-fit criteria, and e cient algorithms to optimize the latter over the former.
In particular, this thesis makes extensive use of Lagrangian relaxation, a technique for gen- erating convex approximations to nonconvex optimization problems. Recently, Lagrangian relaxation has been used to approximate simulation error and guarantee nonlinear model stability via semidefinite programming (SDP), however, the resulting SDPs have large di- mension, limiting their practical utility. The first contribution of this thesis is a custom interior point algorithm that exploits structure in the problem to significantly reduce com- putational complexity. The new algorithm enables empirical comparisons to established methods including Nonlinear ARX, in which superior generalization to new data is demon- strated.
Equipped with this algorithmic machinery, the second contribution of this thesis is the in- corporation of model stability constraints into the maximum likelihood framework. Specifi- cally, Lagrangian relaxation is combined with the expectation maximization (EM) algorithm to derive tight bounds on the likelihood function, that can be optimized over a convex parametrization of all stable linear dynamical systems. Two di↵erent formulations are pre- sented, one of which gives higher fidelity bounds when disturbances (a.k.a. process noise) dominate measurement noise, and vice versa.
Finally, identification of positive systems is considered. Such systems enjoy substantially simpler stability and performance analysis compared to the general linear time-invariant(LTI) case, and appear frequently in applications where physical constraints imply nonneg- ativity of the quantities of interest. Lagrangian relaxation is used to derive new convex parametrizations of stable positive systems and quality-of-fit criteria, and substantial im- provements in accuracy of the identified models, compared to existing approaches based on weighted equation error, are demonstrated. Furthermore, the convex parametrizations of stable systems based on linear Lyapunov functions are shown to be amenable to distributed optimization, which is useful for identification of large-scale networked dynamical systems.},
	language = {English},
	school = {The University of Sydney},
	author = {Umenberger, Jack},
	year = {2017},
	note = {00000},
}

@article{tseng_convergence_2001,
	title = {Convergence of a block coordinate descent method for nondifferentiable minimization},
	volume = {109},
	url = {http://www.springerlink.com/index/q327675221126243.pdf},
	doi = {10.1023/A:1017501703105},
	number = {3},
	urldate = {2017-09-13},
	journal = {Journal of Optimization Theory and Applications},
	author = {Tseng, Paul},
	year = {2001},
	note = {00000},
	pages = {475--494},
}

@article{orovic_compressive_2016,
	title = {Compressive {Sensing} in {Signal} {Processing}: {Algorithms} and {Transform} {Domain} {Formulations}},
	volume = {2016},
	issn = {1024-123X, 1563-5147},
	shorttitle = {Compressive {Sensing} in {Signal} {Processing}},
	url = {https://www.hindawi.com/journals/mpe/2016/7616393/},
	doi = {10.1155/2016/7616393},
	abstract = {Compressive sensing has emerged as an area that opens new perspectives in signal acquisition and processing. It appears as an alternative to the traditional sampling theory, endeavoring to reduce the required number of samples for successful signal reconstruction. In practice, compressive sensing aims to provide saving in sensing resources, transmission, and storage capacities and to facilitate signal processing in the circumstances when certain data are unavailable. To that end, compressive sensing relies on the mathematical algorithms solving the problem of data reconstruction from a greatly reduced number of measurements by exploring the properties of sparsity and incoherence. Therefore, this concept includes the optimization procedures aiming to provide the sparsest solution in a suitable representation domain. This work, therefore, offers a survey of the compressive sensing idea and prerequisites, together with the commonly used reconstruction methods. Moreover, the compressive sensing problem formulation is considered in signal processing applications assuming some of the commonly used transformation domains, namely, the Fourier transform domain, the polynomial Fourier transform domain, Hermite transform domain, and combined time-frequency domain.},
	language = {en},
	urldate = {2020-07-16},
	journal = {Mathematical Problems in Engineering},
	author = {Orović, Irena and Papić, Vladan and Ioana, Cornel and Li, Xiumei and Stanković, Srdjan},
	year = {2016},
	pages = {1--16},
}

@inproceedings{candes_compressive_2006,
	title = {Compressive sampling},
	volume = {3},
	booktitle = {Proceedings of the international congress of mathematicians},
	author = {Candès, Emmanuel J},
	year = {2006},
	note = {tex.organization: Madrid, Spain},
	pages = {1433--1452},
}

@article{schlapfer_computerinterpreted_2017,
	title = {Computer-{Interpreted} {Electrocardiograms}: {Benefits} and {Limitations}},
	volume = {70},
	url = {http://www.onlinejacc.org/content/70/9/1183.abstract},
	doi = {10/gbs2q5},
	abstract = {Computerized interpretation of the electrocardiogram (CIE) was introduced to improve the correct interpretation of the electrocardiogram (ECG), facilitating health care decision making and reducing costs. Worldwide, millions of ECGs are recorded annually, with the majority automatically analyzed, followed by an immediate interpretation. Limitations in the diagnostic accuracy of CIE were soon recognized and still persist, despite ongoing improvement in ECG algorithms. Unfortunately, inexperienced physicians ordering the ECG may fail to recognize interpretation mistakes and accept the automated diagnosis without criticism. Clinical mismanagement may result, with the risk of exposing patients to useless investigations or potentially dangerous treatment. Consequently, CIE over-reading and confirmation by an experienced ECG reader are essential and are repeatedly recommended in published reports. Implementation of new ECG knowledge is also important. The current status of automated ECG interpretation is reviewed, with suggestions for improvement.},
	number = {9},
	journal = {Journal of the American College of Cardiology},
	author = {Schläpfer, Jürg and Wellens, Hein J.},
	month = aug,
	year = {2017},
	pages = {1183},
}

@book{patterson_computer_2005,
	address = {Amsterdam},
	edition = {3. ed},
	title = {Computer organization and design: the hardware/software interface},
	isbn = {978-1-55860-604-3 978-0-12-088433-9},
	shorttitle = {Computer organization and design},
	language = {eng},
	publisher = {Kaufmann},
	author = {Patterson, David A. and Hennessy, John L.},
	year = {2005},
	note = {00000 
OCLC: 249857976},
}

@article{more_computing_1983,
	title = {Computing a trust region step},
	volume = {4},
	doi = {10.1137/0904038},
	number = {3},
	journal = {SIAM Journal on Scientific and Statistical Computing},
	author = {Moré, Jorge J and Sorensen, Danny C},
	year = {1983},
	pages = {553--572},
}

@article{delathauwer_computation_2004,
	title = {Computation of the {Canonical} {Decomposition} by {Means} of a {Simultaneous} {Generalized} {Schur} {Decomposition}},
	volume = {26},
	issn = {0895-4798},
	url = {http://epubs.siam.org/doi/abs/10.1137/S089547980139786X},
	doi = {10.1137/S089547980139786X},
	abstract = {The canonical decomposition of higher-order tensors is a key tool in multilinear algebra. First we review the state of the art. Then we show that, under certain conditions, the problem can be rephrased as the simultaneous diagonalization, by equivalence or congruence, of a set of matrices. Necessary and sufficient conditions for the uniqueness of these simultaneous matrix decompositions are derived. In a next step, the problem can be translated into a simultaneous generalized Schur decomposition, with orthogonal unknowns [A.-J. van der Veen and A. Paulraj, IEEE Trans. Signal Process., 44 (1996), pp. 1136--1155]. A first-order perturbation analysis of the simultaneous generalized Schur decomposition is carried out. We discuss some computational techniques (including a new Jacobi algorithm) and illustrate their behavior by means of a number of numerical experiments.},
	number = {2},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {De Lathauwer, L. and De Moor, B. and Vandewalle, J.},
	month = jan,
	year = {2004},
	pages = {295--327},
}

@book{brown_complex_2009,
	address = {Boston},
	edition = {8th ed},
	series = {Brown and {Churchill} series},
	title = {Complex variables and applications},
	isbn = {978-0-07-305194-9},
	publisher = {McGraw-Hill Higher Education},
	author = {Brown, James Ward and Churchill, Ruel V.},
	year = {2009},
	note = {OCLC: ocn176648981},
	keywords = {Functions of complex variables},
}

@book{forsyth_computer_2012,
	series = {Always learning},
	title = {Computer {Vision}: {A} {Modern} {Approach}},
	isbn = {978-0-13-608592-8},
	url = {https://books.google.com.br/books?id=gM63QQAACAAJ},
	publisher = {Pearson},
	author = {Forsyth, D. and Ponce, J.},
	year = {2012},
}

@book{ahlfors_complex_1966,
	title = {Complex {Analysis}},
	url = {https://books.google.com.br/books?id=RfYK28TcZEwC},
	author = {Ahlfors, L.V.},
	year = {1966},
}

@book{tanenbaum_computer_2011,
	address = {Boston},
	edition = {5th ed},
	title = {Computer networks},
	isbn = {978-0-13-212695-3},
	publisher = {Pearson Prentice Hall},
	author = {Tanenbaum, Andrew S. and Wetherall, D.},
	year = {2011},
	note = {OCLC: ocn660087726},
	keywords = {Computer networks},
}

@book{hennessy_computer_2012,
	address = {Waltham, MA},
	edition = {5th ed},
	title = {Computer architecture: a quantitative approach},
	isbn = {978-0-12-383872-8},
	shorttitle = {Computer architecture},
	publisher = {Morgan Kaufmann/Elsevier},
	author = {Hennessy, John L. and Patterson, David A. and Asanović, Krste},
	year = {2012},
	note = {OCLC: ocn755102367},
	keywords = {Computer architecture},
}

@book{boolos_computability_2002,
	title = {Computability and logic},
	publisher = {Cambridge university press},
	author = {Boolos, George S. and Burgess, John P. and Jeffrey, Richard C.},
	year = {2002},
}

@book{cooper_computability_2017,
	series = {Chapman {Hall}/{CRC} {Mathematics} {Series}},
	title = {Computability {Theory}},
	isbn = {978-1-351-99196-4},
	url = {https://books.google.com.br/books?id=hlc0DwAAQBAJ},
	publisher = {CRC Press},
	author = {Cooper, S.B.},
	year = {2017},
}

@book{forsythe_computer_1977,
	title = {Computer methods for mathematical computations},
	author = {Forsythe, George Elmer and Moler, Cleve B and Malcolm, Michael A},
	year = {1977},
}

@article{chandra_competition_2015,
	title = {Competition and {Collaboration} in {Cooperative} {Coevolution} of {Elman} {Recurrent} {Neural} {Networks} for {Time}-{Series} {Prediction}},
	volume = {26},
	issn = {2162-237X, 2162-2388},
	url = {http://ieeexplore.ieee.org/document/7055352/},
	doi = {10.1109/TNNLS.2015.2404823},
	number = {12},
	urldate = {2018-05-07},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Chandra, Rohitash},
	month = dec,
	year = {2015},
	pages = {3123--3136},
}

@article{estes_computerized_2013,
	title = {Computerized interpretation of {ECGs}: supplement not a substitute},
	volume = {6},
	issn = {1941-3084},
	shorttitle = {Computerized interpretation of {ECGs}},
	doi = {10.1161/CIRCEP.111.000097},
	language = {eng},
	number = {1},
	journal = {Circulation. Arrhythmia and Electrophysiology},
	author = {Estes, N. A. Mark},
	month = feb,
	year = {2013},
	pmid = {23424219},
	keywords = {Electrocardiography, Female, Heart Conduction System, Humans, Long QT Syndrome, Male, Signal Processing, Computer-Assisted},
	pages = {2--4},
}

@article{lyon_computational_2018,
	title = {Computational techniques for {ECG} analysis and interpretation in light of their contribution to medical advances},
	volume = {15},
	issn = {1742-5689},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5805987/},
	doi = {10.1098/rsif.2017.0821},
	abstract = {Widely developed for clinical screening, electrocardiogram (ECG) recordings capture the cardiac electrical activity from the body surface. ECG analysis can therefore be a crucial first step to help diagnose, understand and predict cardiovascular disorders responsible for 30\% of deaths worldwide. Computational techniques, and more specifically machine learning techniques and computational modelling are powerful tools for classification, clustering and simulation, and they have recently been applied to address the analysis of medical data, especially ECG data. This review describes the computational methods in use for ECG analysis, with a focus on machine learning and 3D computer simulations, as well as their accuracy, clinical implications and contributions to medical advances. The first section focuses on heartbeat classification and the techniques developed to extract and classify abnormal from regular beats. The second section focuses on patient diagnosis from whole recordings, applied to different diseases. The third section presents real-time diagnosis and applications to wearable devices. The fourth section highlights the recent field of personalized ECG computer simulations and their interpretation. Finally, the discussion section outlines the challenges of ECG analysis and provides a critical assessment of the methods presented. The computational methods reported in this review are a strong asset for medical discoveries and their translation to the clinical world may lead to promising advances.},
	number = {138},
	urldate = {2018-10-22},
	journal = {Journal of the Royal Society Interface},
	author = {Lyon, Aurore and Mincholé, Ana and Martínez, Juan Pablo and Laguna, Pablo and Rodriguez, Blanca},
	month = jan,
	year = {2018},
	pmid = {29321268},
	pmcid = {PMC5805987},
}

@article{soderstrom_comparison_1981,
	title = {Comparison of some instrumental variable methods consistency and accuracy aspects},
	volume = {17},
	doi = {10.1016/0005-1098(81)90087-X},
	number = {1},
	journal = {Automatica},
	author = {Söderström, T and Stoica, P},
	year = {1981},
	note = {00000},
	pages = {101--115},
}

@article{romerougalde_computational_2015,
	title = {Computational cost improvement of neural network models in black box nonlinear system identification},
	volume = {166},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231215004695},
	doi = {10.1016/j.neucom.2015.04.022},
	abstract = {Models play an important role in many engineering fields. Therefore, the goal in system identification is to find the good balance between the accuracy, complexity and computational cost of such identification models. In a previous work (Romero-Ugalde et al., 2013 [1]), we focused on the topic of providing balanced accuracy/complexity models by proposing a dedicated neural network design and a model complexity reduction approach. In this paper, we focus on the reduction of the computational cost required to achieve these balanced models. More precisely, the improvement of the preceding method presented here leads to a significantly computational cost reduction of the neural network training phase. Even if this reduction is achieved by a convenient choice of the activation functions and the initial conditions of the synaptic weights, the proposed architecture leads to a wide range of models among the most encountered in the literature assuring the interest of such a method. To validate the proposed approach, two different systems are identified. The first one corresponds to the unavoidable Wiener–Hammerstein system proposed in SYSID2009 as a benchmark. The second system is a flexible robot arm. Results show the interest of the proposed reduction methods.},
	journal = {Neurocomputing},
	author = {Romero Ugalde, Hector M. and Carmona, Jean-Claude and Reyes-Reyes, Juan and Alvarado, Victor M. and Mantilla, Juan},
	month = oct,
	year = {2015},
	note = {00000},
	keywords = {Black box, Computational cost reduction, Estimation quality, Neural networks, Non-linear system identification},
	pages = {96--108},
}

@inproceedings{kalarot_comparison_2010,
	title = {Comparison of {FPGA} and {GPU} implementations of real-time stereo vision},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW}), 2010 {IEEE} {Computer} {Society} {Conference} on},
	publisher = {IEEE},
	author = {Kalarot, Ratheesh and Morris, John},
	year = {2010},
	pages = {9--15},
}

@inproceedings{maduranga_complex_2019,
	title = {Complex unitary recurrent neural networks using scaled cayley transform},
	volume = {33},
	isbn = {2374-3468},
	author = {Maduranga, Kehelwala DG and Helfrich, Kyle E and Ye, Qiang},
	year = {2019},
	pages = {4528--4535},
}

@article{donoho_compressed_2006,
	title = {Compressed sensing},
	volume = {52},
	number = {4},
	journal = {IEEE Transactions on Information Theory},
	author = {Donoho, David L.},
	year = {2006},
	pages = {1289--1306},
}

@article{renda_comparing_2020,
	title = {Comparing {Rewinding} and {Fine}-tuning in {Neural} {Network} {Pruning}},
	url = {http://arxiv.org/abs/2003.02389},
	abstract = {Many neural network pruning algorithms proceed in three steps: train the network to completion, remove unwanted structure to compress the network, and retrain the remaining structure to recover lost accuracy. The standard retraining technique, fine-tuning, trains the unpruned weights from their final trained values using a small fixed learning rate. In this paper, we compare fine-tuning to alternative retraining techniques. Weight rewinding (as proposed by Frankle et al., (2019)), rewinds unpruned weights to their values from earlier in training and retrains them from there using the original training schedule. Learning rate rewinding (which we propose) trains the unpruned weights from their final values using the same learning rate schedule as weight rewinding. Both rewinding techniques outperform fine-tuning, forming the basis of a network-agnostic pruning algorithm that matches the accuracy and compression ratios of several more network-specific state-of-the-art techniques.},
	urldate = {2020-06-29},
	journal = {arXiv:2003.02389 [cs, stat]},
	author = {Renda, Alex and Frankle, Jonathan and Carbin, Michael},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.02389},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{_caixa_a,
	title = {Caixa de entrada - antonior92@gmail.com - {Gmail}},
	url = {https://mail.google.com/mail/u/0/#inbox},
	urldate = {2020-06-29},
}

@misc{_caixa_,
	title = {Caixa de entrada - antonior92@gmail.com - {Gmail}},
	url = {https://mail.google.com/mail/u/0/#inbox},
	urldate = {2020-06-28},
}

@article{alday_classification_2020,
	title = {Classification of 12-lead {ECGs}: the {PhysioNet}/{Computing} in {Cardiology} {Challenge} 2020},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Classification of 12-lead {ECGs}},
	url = {https://www.medrxiv.org/content/10.1101/2020.08.11.20172601v1},
	doi = {10.1101/2020.08.11.20172601},
	abstract = {{\textless}p{\textgreater}The subject of the PhysioNet/Computing in Cardiology Challenge 2020 was the identification of cardiac abnormalities in 12-lead electrocardiogram (ECG) recordings. A total of 66,405 recordings were sourced from hospital systems from four distinct countries and annotated with clinical diagnoses, including 43,101 annotated recordings that were posted publicly. For this Challenge, we asked participants to design working, open-source algorithms for identifying cardiac abnormalities in 12-lead ECG recordings. This Challenge provided several innovations. First, we sourced data from multiple institutions from around the world with different demographics, allowing us to assess the generalizability of the algorithms. Second, we required participants to submit both their trained models and the code for reproducing their trained models from the training data, which aids the generalizability and reproducibility of the algorithms. Third, we proposed a novel evaluation metric that considers different misclassification errors for different cardiac abnormalities, reflecting the clinical reality that some diagnoses have similar outcomes and varying risks. Over 200 teams submitted 850 algorithms (432 of which successfully ran) during the unofficial and official phases of the Challenge, representing a diversity of approaches from both academia and industry for identifying cardiac abnormalities. The official phase of the Challenge is ongoing.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-09-01},
	journal = {medRxiv},
	author = {Alday, Erick A. Perez and Gu, Annie and Shah, Amit and Robichaux, Chad and Wong, An-Kwok Ian and Liu, Chengyu and Liu, Feifei and Rad, Ali Bahrami and Elola, Andoni and Seyedi, Salman and Li, Qiao and Sharma, Ashish and Clifford, Gari D. and Reyna, Matthew A.},
	month = aug,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory Press},
	pages = {2020.08.11.20172601},
}

@article{rajpurkar_cardiologistlevel_2017,
	title = {Cardiologist-{Level} {Arrhythmia} {Detection} with {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1707.01836},
	abstract = {We develop an algorithm which exceeds the performance of board certified cardiologists in detecting a wide range of heart arrhythmias from electrocardiograms recorded with a single-lead wearable monitor. We build a dataset with more than 500 times the number of unique patients than previously studied corpora. On this dataset, we train a 34-layer convolutional neural network which maps a sequence of ECG samples to a sequence of rhythm classes. Committees of board-certified cardiologists annotate a gold standard test set on which we compare the performance of our model to that of 6 other individual cardiologists. We exceed the average cardiologist performance in both recall (sensitivity) and precision (positive predictive value).},
	journal = {arXiv:1707.01836},
	author = {Rajpurkar, Pranav and Hannun, Awni Y. and Haghpanahi, Masoumeh and Bourn, Codie and Ng, Andrew Y.},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.01836},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 🔍No DOI found},
}

@article{hannun_cardiologistlevel_2019,
	title = {Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network},
	volume = {25},
	issn = {1546-170X},
	url = {https://doi.org/10.1038/s41591-018-0268-3},
	doi = {10/gftc8p},
	abstract = {Computerized electrocardiogram (ECG) interpretation plays a critical role in the clinical ECG workflow1. Widely available digital ECG data and the algorithmic paradigm of deep learning2 present an opportunity to substantially improve the accuracy and scalability of automated ECG analysis. However, a comprehensive evaluation of an end-to-end deep learning approach for ECG analysis across a wide variety of diagnostic classes has not been previously reported. Here, we develop a deep neural network (DNN) to classify 12 rhythm classes using 91,232 single-lead ECGs from 53,549 patients who used a single-lead ambulatory ECG monitoring device. When validated against an independent test dataset annotated by a consensus committee of board-certified practicing cardiologists, the DNN achieved an average area under the receiver operating characteristic curve (ROC) of 0.97. The average F1 score, which is the harmonic mean of the positive predictive value and sensitivity, for the DNN (0.837) exceeded that of average cardiologists (0.780). With specificity fixed at the average specificity achieved by cardiologists, the sensitivity of the DNN exceeded the average cardiologist sensitivity for all rhythm classes. These findings demonstrate that an end-to-end deep learning approach can classify a broad range of distinct arrhythmias from single-lead ECGs with high diagnostic performance similar to that of cardiologists. If confirmed in clinical settings, this approach could reduce the rate of misdiagnosed computerized ECG interpretations and improve the efficiency of expert human ECG interpretation by accurately triaging or prioritizing the most urgent conditions.},
	number = {1},
	journal = {Nature Medicine},
	author = {Hannun, Awni Y. and Rajpurkar, Pranav and Haghpanahi, Masoumeh and Tison, Geoffrey H. and Bourn, Codie and Turakhia, Mintu P. and Ng, Andrew Y.},
	month = jan,
	year = {2019},
	pages = {65--69},
}

@inproceedings{jambukia_classification_2015,
	title = {Classification of {ECG} signals using machine learning techniques: {A} survey},
	isbn = {978-1-4673-6911-4},
	shorttitle = {Classification of {ECG} signals using machine learning techniques},
	url = {http://ieeexplore.ieee.org/document/7164783/},
	doi = {10.1109/ICACEA.2015.7164783},
	abstract = {Classification of electrocardiogram (ECG) signals plays an important role in diagnoses of heart diseases. An accurate ECG classification is a challenging problem. A survey of ECG classification into arrhythmia types is presented in this paper. Early and accurate detection of arrhythmia types is important in detecting heart diseases and finding treatment of a patient. Different classifiers are available for ECG classification. Amongst all classifiers, artificial neural networks have become very popular and most widely used for ECG classification. In this paper a detailed survey of preprocessing techniques, ECG databases, feature extraction techniques, classifiers and performance measures are presented. This paper also discusses issues in ECG classification, analysis of input beat selection, and output of classifiers.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the {International} {Conference} on {Advances} in {Computer} {Engineering} and {Applications} ({ICACEA})},
	publisher = {IEEE},
	author = {Jambukia, Shweta H. and Dabhi, Vipul K. and Prajapati, Harshadkumar B.},
	month = mar,
	year = {2015},
	pages = {714--721},
}

@book{stewart_calculus_2015,
	title = {Calculus: early transcendentals},
	publisher = {Cengage Learning},
	author = {Stewart, James},
	year = {2015},
	note = {00000},
}

@article{schoukens_cascaded_2016,
	title = {Cascaded tanks benchmark combining soft and hard nonlinearities},
	language = {en},
	author = {Schoukens, M and Mattsson, P and Wigren, T and Noel, J P},
	year = {2016},
	keywords = {🔍No DOI found},
	pages = {4},
}

@article{stead_clinical_2018,
	title = {Clinical implications and challenges of artificial intelligence and deep learning},
	volume = {320},
	issn = {0098-7484},
	url = {http://dx.doi.org/10.1001/jama.2018.11029},
	doi = {10/gfkhr8},
	abstract = {Artificial intelligence (AI) and deep learning are entering the mainstream of clinical medicine. For example, in December 2016, Gulshan et al1 reported development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. An accompanying editorial by Wong and Bressler2 pointed out limits of the study, the need for further validation of the algorithm in different populations, and unresolved challenges (eg, incorporating the algorithm into clinical work flows and convincing clinicians and patients to “trust a ‘black box’”). Sixteen months later, the Food and Drug Administration (FDA)3 permitted marketing of the first medical device to use AI to detect diabetic retinopathy. FDA reduced the risk of releasing the device by limiting the indication for use to screening adults who do not have visual symptoms for greater than mild retinopathy, to refer them to an eye care specialist.},
	number = {11},
	journal = {JAMA},
	author = {Stead, William W.},
	month = sep,
	year = {2018},
	pages = {1107--1108},
}

@article{hara_can_2017,
	title = {Can {Spatiotemporal} {3D} {CNNs} {Retrace} the {History} of {2D} {CNNs} and {ImageNet}?},
	url = {http://arxiv.org/abs/1711.09577},
	abstract = {The purpose of this study is to determine whether current video datasets have sufficient data for training very deep convolutional neural networks (CNNs) with spatio-temporal three-dimensional (3D) kernels. Recently, the performance levels of 3D CNNs in the field of action recognition have improved significantly. However, to date, conventional research has only explored relatively shallow 3D architectures. We examine the architectures of various 3D CNNs from relatively shallow to very deep ones on current video datasets. Based on the results of those experiments, the following conclusions could be obtained: (i) ResNet-18 training resulted in significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics. (ii) The Kinetics dataset has sufficient data for training of deep 3D CNNs, and enables training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet. ResNeXt-101 achieved 78.4\% average accuracy on the Kinetics test set. (iii) Kinetics pretrained simple 3D architectures outperforms complex 2D architectures, and the pretrained ResNeXt-101 achieved 94.5\% and 70.2\% on UCF-101 and HMDB-51, respectively. The use of 2D CNNs trained on ImageNet has produced significant progress in various tasks in image. We believe that using deep 3D CNNs together with Kinetics will retrace the successful history of 2D CNNs and ImageNet, and stimulate advances in computer vision for videos. The codes and pretrained models used in this study are publicly available. https://github.com/kenshohara/3D-ResNets-PyTorch},
	journal = {arXiv:1711.09577 [cs]},
	author = {Hara, Kensho and Kataoka, Hirokatsu and Satoh, Yutaka},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.09577},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 🔍No DOI found},
}

@book{weste_cmos_2011,
	address = {Boston},
	edition = {4th ed},
	title = {{CMOS} {VLSI} design: a circuits and systems perspective},
	isbn = {978-0-321-54774-3},
	shorttitle = {{CMOS} {VLSI} design},
	publisher = {Addison Wesley},
	author = {Weste, Neil H. E. and Harris, David Money},
	year = {2011},
	note = {OCLC: ocn473447233},
	keywords = {Integrated circuits, Metal oxide semiconductors, Complementary, Very large scale integration Design and construction},
}

@article{chandra_coevolutionary_2017,
	title = {Co-evolutionary multi-task learning with predictive recurrence for multi-step chaotic time series prediction},
	volume = {243},
	issn = {09252312},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231217303892},
	doi = {10.1016/j.neucom.2017.02.065},
	language = {en},
	urldate = {2018-05-07},
	journal = {Neurocomputing},
	author = {Chandra, Rohitash and Ong, Yew-Soon and Goh, Chi-Keong},
	month = jun,
	year = {2017},
	pages = {21--34},
}

@article{defauw_clinically_2018,
	title = {Clinically applicable deep learning for diagnosis and referral in retinal disease},
	volume = {24},
	issn = {1546-170X},
	url = {https://doi.org/10.1038/s41591-018-0107-6},
	doi = {10.1038/s41591-018-0107-6},
	abstract = {The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting.},
	number = {9},
	journal = {Nature Medicine},
	author = {De  Fauw, Jeffrey and Ledsam, Joseph R. and Romera-Paredes, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O’Donoghue, Brendan and Visentin, Daniel and van den  Driessche, George and Lakshminarayanan, Balaji and Meyer, Clemens and Mackinder, Faith and Bouton, Simon and Ayoub, Kareem and Chopra, Reena and King, Dominic and Karthikesalingam, Alan and Hughes, Cían O. and Raine, Rosalind and Hughes, Julian and Sim, Dawn A. and Egan, Catherine and Tufail, Adnan and Montgomery, Hugh and Hassabis, Demis and Rees, Geraint and Back, Trevor and Khaw, Peng T. and Suleyman, Mustafa and Cornebise, Julien and Keane, Pearse A. and Ronneberger, Olaf},
	month = sep,
	year = {2018},
	pages = {1342--1350},
}

@inproceedings{overschee_closed_1997,
	title = {Closed loop subspace system identification},
	volume = {2},
	doi = {10.1109/CDC.1997.657851},
	abstract = {We present a general framework for closed loop subspace system identification. This framework consists of two new projection theorems which allow the extraction of non-steady state Kalman filter states and of system related matrices directly from input output data. Three algorithms for the identification of the state space matrices can be derived from these theorems. The similarities between the theorems and algorithms, and the corresponding open loop theorems and algorithms in the literature are remarked on},
	booktitle = {Proceedings of the 36th {IEEE} {Conference} on {Decision} and {Control}},
	author = {Overschee, P. Van and Moor, B. De},
	month = dec,
	year = {1997},
	keywords = {Data mining, Error correction, Feedback, Hankel matrices, Instruments, Intelligent systems, Kalman filters, MIMO, Optimal control, Optimization methods, State estimation, Toeplitz matrices, closed loop subspace system identification, closed loop systems, controllability, nonsteady state Kalman filter states, projection theorems, state space matrices, state-space methods, system identification},
	pages = {1848--1853 vol.2},
}

@article{kilani_cognitive_2016,
	title = {Cognitive waveform and receiver selection mechanism for multistatic radar},
	volume = {10},
	number = {2},
	journal = {IET Radar, Sonar \& Navigation},
	author = {Kilani, Moez Ben and Nijsure, Yogesh and Gagnon, Ghyslain and Kaddoum, Georges and Gagnon, François},
	year = {2016},
	keywords = {🔍No DOI found},
	pages = {417--425},
}

@article{ernst_chromhmm_2012,
	title = {{ChromHMM}: automating chromatin-state discovery and characterization},
	volume = {9},
	copyright = {2012 Nature Publishing Group},
	issn = {1548-7105},
	shorttitle = {{ChromHMM}},
	url = {https://www.nature.com/articles/nmeth.1906},
	doi = {10/gffkzn},
	abstract = {ChromHMM: automating chromatin-state discovery and characterization},
	language = {en},
	number = {3},
	urldate = {2019-04-01},
	journal = {Nature Methods},
	author = {Ernst, Jason and Kellis, Manolis},
	month = mar,
	year = {2012},
	pages = {215--216},
}

@article{kumar_comparative_2019,
	title = {Comparative study of neural networks for dynamic nonlinear systems identification},
	volume = {23},
	issn = {1433-7479},
	url = {https://doi.org/10.1007/s00500-018-3235-5},
	doi = {10/gfwvbb},
	abstract = {In this paper, a comparative study is performed to test the approximation ability of different neural network structures. It involves three neural networks multilayer feedforward neural network (MLFFNN), diagonal recurrent neural network (DRNN), and nonlinear autoregressive with exogenous inputs (NARX) neural network. Their robustness is also tested and compared when the system is subjected to parameter variations and disturbance signals. Further, dynamic back-propagation algorithm is used to update the parameters associated with these neural networks. Four dynamical systems of different complexities including motor-driven robotic link are considered on which the comparative study is performed. The simulation results show the superior performance of DRNN identification model over NARX and MLFFNN identification models.},
	number = {1},
	journal = {Soft Computing},
	author = {Kumar, Rajesh and Srivastava, Smriti and Gupta, J. R. P. and Mohindru, Amit},
	month = jan,
	year = {2019},
	pages = {101--114},
}

@book{spivak_calculus_1998,
	address = {Cambridge, Mass},
	series = {Mathematics monograph series},
	title = {Calculus on manifolds: a modern approach to classical theorems of advanced calculus},
	isbn = {978-0-8053-9021-6},
	shorttitle = {Calculus on manifolds},
	language = {en},
	publisher = {Perseus Books},
	author = {Spivak, Michael},
	year = {1998},
}

@inproceedings{lezcano-casado_cheap_2019,
	title = {Cheap {Orthogonal} {Constraints} in {Neural} {Networks}: {A} {Simple} {Parametrization} of the {Orthogonal} and {Unitary} {Group}},
	shorttitle = {Cheap {Orthogonal} {Constraints} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1901.08428},
	abstract = {We introduce a novel approach to perform first-order optimization with orthogonal and unitary constraints. This approach is based on a parametrization stemming from Lie group theory through the exponential map. The parametrization transforms the constrained optimization problem into an unconstrained one over a Euclidean space, for which common first-order optimization methods can be used. The theoretical results presented are general enough to cover the special orthogonal group, the unitary group and, in general, any connected compact Lie group. We discuss how this and other parametrizations can be computed efficiently through an implementation trick, making numerically complex parametrizations usable at a negligible runtime cost in neural networks. In particular, we apply our results to RNNs with orthogonal recurrent weights, yielding a new architecture called expRNN. We demonstrate how our method constitutes a more robust approach to optimization with orthogonal constraints, showing faster, accurate, and more stable convergence in several tasks designed to test RNNs.},
	language = {en},
	urldate = {2019-09-19},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Lezcano-Casado, Mario and Martínez-Rubio, David},
	year = {2019},
	note = {arXiv: 1901.08428},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, ⛔ No DOI found},
	pages = {3794--3803},
}

@article{luo_companion_2014,
	title = {Companion {Matrices} and {Their} {Relations} to {Toeplitz} and {Hankel} {Matrices}},
	url = {http://arxiv.org/abs/1411.4592},
	abstract = {In this paper we describe some properties of companion matrices and demonstrate some special patterns that arise when a Toeplitz or a Hankel matrix is multiplied by a related companion matrix. We present a new condition, generalizing known results, for a Toeplitz or a Hankel matrix to be the transforming matrix for a similarity between a pair of companion matrices. A special case of our main result shows that a Toeplitz or a Hankel matrix can be extended using associated companion matrices, preserving the Toeplitz or Hankel structure respectively.},
	urldate = {2020-12-29},
	journal = {arXiv:1411.4592 [math]},
	author = {Luo, Yousong and Hill, Robin},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.4592},
	keywords = {Mathematics - General Mathematics},
}

@article{nyquist_certain_1928,
	title = {Certain topics in telegraph transmission theory},
	volume = {47},
	number = {2},
	journal = {Transactions of the American Institute of Electrical Engineers},
	author = {Nyquist, Harry},
	year = {1928},
	pages = {617--644},
}

@article{paixao_clinical_2018,
	title = {Clinical {Outcomes} in {Digital} {Electrocardiography}: {Evaluation} of {Mortality} in {Atrial} {Fibrillation} ({Code} {Study})},
	volume = {138},
	copyright = {All rights reserved},
	shorttitle = {Abstract 16594},
	url = {https://www.ahajournals.org/doi/abs/10.1161/circ.138.suppl_1.16594},
	abstract = {Introduction: Telehealth system is an important tool to improve access and quality to health assistance.Large electrocardiogram (ECG) databases, linked to mortality or hospitalization data, can be useful in determining the prognostic value of ECG markers. Atrial fibrillation (AF) is a public health problem with increasing prevalence as the population ages, associated with cardiovascular mortality and morbidity.Hypothesis: Evaluate the association between the presence of AF with overall and cardiovascular mortality in a large electronic cohort of primary care patients of Minas Gerais.Methods: This is an observational retrospective study. Patients over 16 years old who performed digital electrocardiograms by Telehealth Network of Minas Gerais from 2013 to 2016 were assessed. A probabilistic linkage between data from the national mortality information system and our ECG database was made. Clinical data were self-reported, and ECGs were interpreted by a team of trained cardiologists and automatic software (Glasgow and Minnesota).The diagnosis of AF was considered if there was concordance between the cardiologist′s report and one of the automatic systems. In cases of disagreement, ECGs were reviewed manually.Only the first ECG made was analysed. To assess the relation between AF and mortality, Cox regression was used, adjusted by age, sex and clinical conditions.Results: From a dataset of 1,773,689 patients, 1,075,531 were included. The mean age was 51.4 years, 40.5\% male.The prevalence of AF was 1.15\%. There were 2.9\% deaths for all causes in 2.69 years of mean follow up. In univariate analysis, AF was a risk factor for death from all causes (HR 6.98, 95\%CI 6.68-7.28). After adjustment for age, sex and comorbidities, AF remained an independent risk factor for all-cause mortality (HR 2.49; 95\% CI 2.39 - 2.61). AF was also a predictor of risk for cardiovascular mortality after adjustment for age, sex and clinical conditions (HR 2.35, 95\% CI 2.01-2.73). In multivariate analysis by sex, adjusted for age and comorbidities, AF women had higher risk of death for all causes (HR 3.06; 95\% CI 2.86-3.26) than men (HR 2.18; 95\% CI 2.06-2.32). There were no difference between sex in cardiovascular mortality (HR 2.34; 95\% CI 2.01-2.73 for male sex e HR 2.49; 95\% CI 2.14-2.90 for female).Conclusions: AF was a strong predictor of mortality for all causes and cardiovascular mortality in primary care population with increased risk in women for deaths for all cause.},
	number = {Suppl\_1},
	urldate = {2020-12-29},
	journal = {Circulation. Abstracts from American Heart Association's.},
	author = {Paixao, Gabriela and Silva, Luis Gustavo Silva e and Gomes, Paulo R. and Ferreira, Milton and Oliveira, Derick and Ribeiro, Manoel Horta and Ribeiro, Antonio H. and Nascimento, Jamil and Cardoso, Gustavo and Araujo, Rodrigo and Santos, Bruno and Canazart, Jessica and Ribeiro, Leonardo and Ribeiro, Antonio L.},
	month = nov,
	year = {2018},
	note = {Publisher: American Heart Association},
	pages = {A16594--A16594},
}

@article{bartlett_benign_2020,
	title = {Benign overfitting in linear regression},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	doi = {10.1073/pnas.1907378117},
	abstract = {The phenomenon of benign overfitting is one of the key mysteries uncovered by deep learning methodology: deep neural networks seem to predict well, even with a perfect fit to noisy training data. Motivated by this phenomenon, we consider when a perfect fit to training data in linear regression is compatible with accurate prediction. We give a characterization of linear regression problems for which the minimum norm interpolating prediction rule has near-optimal prediction accuracy. The characterization is in terms of two notions of the effective rank of the data covariance. It shows that overparameterization is essential for benign overfitting in this setting: the number of directions in parameter space that are unimportant for prediction must significantly exceed the sample size. By studying examples of data covariance properties that this characterization shows are required for benign overfitting, we find an important role for finite-dimensional data: the accuracy of the minimum norm interpolating prediction rule approaches the best possible accuracy for a much narrower range of properties of the data distribution when the data lie in an infinite-dimensional space vs. when the data lie in a finite-dimensional space with dimension that grows faster than the sample size.},
	number = {48},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bartlett, Peter L. and Long, Philip M. and Lugosi, Gábor and Tsigler, Alexander},
	month = apr,
	year = {2020},
	pages = {30063--30070},
}

@article{bach_breaking_2017,
	title = {Breaking the curse of dimensionality with convex neural networks},
	volume = {18},
	url = {http://jmlr.org/papers/v18/14-546.html},
	number = {19},
	journal = {Journal of Machine Learning Research},
	author = {Bach, Francis},
	year = {2017},
	pages = {1--53},
}

@book{groover_automation_2000,
	edition = {2},
	title = {Automation, production systems, and computer-integrated manufacturing (2nd edition)},
	isbn = {0-13-088978-4 978-0-13-088978-2},
	url = {http://gen.lib.rus.ec/book/index.php?md5=eaf2497c97c9ed94f2dd6cba7cfc33bb},
	publisher = {Prentice Hall},
	author = {Groover, Mikell P.},
	year = {2000},
}

@article{vandeleur_automatic_2020,
	title = {Automatic {Triage} of 12‐{Lead} {ECGs} {Using} {Deep} {Convolutional} {Neural} {Networks}},
	volume = {9},
	issn = {2047-9980},
	url = {https://www.ahajournals.org/doi/10.1161/JAHA.119.015138},
	doi = {10.1161/JAHA.119.015138},
	abstract = {BACKGROUND: The correct interpretation of the ECG is pivotal for the accurate diagnosis of many cardiac abnormalities, and conventional computerized interpretation has not been able to reach physician-­level accuracy in detecting (acute) cardiac abnormalities. This study aims to develop and validate a deep neural network for comprehensive automated ECG triage in daily practice.
METHODS AND RESULTS: We developed a 37-­layer convolutional residual deep neural network on a data set of free-­text physician-a­nnotated 12-l­ead ECGs. The deep neural network was trained on a data set with 336.835 recordings from 142.040 patients and validated on an independent validation data set (n=984), annotated by a panel of 5 cardiologists electrophysiologists. The 12-­lead ECGs were acquired in all noncardiology departments of the University Medical Center Utrecht. The algorithm learned to classify these ECGs into the following 4 triage categories: normal, abnormal not acute, subacute, and acute. Discriminative performance is presented with overall and category-s­ pecific concordance statistics, polytomous discrimination indexes, sensitivities, specificities, and positive and negative predictive values. The patients in the validation data set had a mean age of 60.4 years and 54.3\% were men. The deep neural network showed excellent overall discrimination with an overall concordance statistic of 0.93 (95\% CI, 0.92–0.95) and a polytomous discriminatory index of 0.83 (95\% CI, 0.79–0.87).
CONCLUSIONS: This study demonstrates that an end-­to-­end deep neural network can be accurately trained on unstructured free-t­ext physician annotations and used to consistently triage 12-l­ead ECGs. When further fine-­tuned with other clinical outcomes and externally validated in clinical practice, the demonstrated deep learning–based ECG interpretation can potentially improve time to treatment and decrease healthcare burden.},
	language = {en},
	number = {10},
	urldate = {2020-07-28},
	journal = {Journal of the American Heart Association},
	author = {van de Leur, Rutger R. and Blom, Lennart J. and Gavves, Efstratios and Hof, Irene E. and van der Heijden, Jeroen F. and Clappers, Nick C. and Doevendans, Pieter A. and Hassink, Rutger J. and van Es, René},
	month = may,
	year = {2020},
}

@article{breiman_bagging_1996,
	title = {Bagging predictors},
	volume = {24},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00058655},
	doi = {10.1007/BF00058655},
	abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
	language = {en},
	number = {2},
	urldate = {2020-11-26},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = aug,
	year = {1996},
	pages = {123--140},
}

@incollection{yoon_bayesian_2018,
	title = {Bayesian {Model}-{Agnostic} {Meta}-{Learning}},
	url = {http://papers.nips.cc/paper/7963-bayesian-model-agnostic-meta-learning.pdf},
	urldate = {2018-12-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Yoon, Jaesik and Kim, Taesup and Dia, Ousmane and Kim, Sungwoong and Bengio, Yoshua and Ahn, Sungjin},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {7342--7352},
}

@inproceedings{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	url = {http://proceedings.mlr.press/v37/ioffe15.html},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = jun,
	year = {2015},
	keywords = {Computer Science - Learning, 🔍No DOI found},
	pages = {448--456},
}

@inproceedings{papineni_bleu_2002,
	title = {{BLEU}: a method for automatic evaluation of machine translation},
	shorttitle = {{BLEU}},
	booktitle = {Proceedings of the 40th annual meeting on association for computational linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	year = {2002},
	note = {00000},
	pages = {311--318},
}

@techreport{dolan_benchmarking_2004,
	title = {Benchmarking optimization software with {COPS} 3.0.},
	institution = {Argonne National Lab., Argonne, IL (US)},
	author = {Dolan, Elizabeth D and Moré, Jorge J and Munson, Todd S},
	year = {2004},
}

@article{novak_bayesian_2018,
	title = {Bayesian {Convolutional} {Neural} {Networks} with {Many} {Channels} are {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1810.05148},
	abstract = {There is a previously identified equivalence between wide fully connected neural networks (FCNs) and Gaussian processes (GPs). This equivalence enables, for instance, test set predictions that would have resulted from a fully Bayesian, infinitely wide trained FCN to be computed without ever instantiating the FCN, but by instead evaluating the corresponding GP. In this work, we derive an analogous equivalence for multi-layer convolutional neural networks (CNNs) both with and without pooling layers, and achieve state of the art results on CIFAR10 for GPs without trainable kernels. We also introduce a Monte Carlo method to estimate the GP corresponding to a given neural network architecture, even in cases where the analytic form has too many terms to be computationally feasible. Surprisingly, in the absence of pooling layers, the GPs corresponding to CNNs with and without weight sharing are identical. As a consequence, translation equivariance in finite-channel CNNs trained with stochastic gradient descent (SGD) has no corresponding property in the Bayesian treatment of the infinite channel limit - a qualitative difference between the two regimes that is not present in the FCN case. We confirm experimentally, that while in some scenarios the performance of SGD-trained finite CNNs approaches that of the corresponding GPs as the channel count increases, with careful tuning SGD-trained CNNs can significantly outperform their corresponding GPs, suggesting advantages from SGD training compared to fully Bayesian parameter estimation.},
	urldate = {2018-10-24},
	journal = {arXiv:1810.05148 [cs, stat]},
	author = {Novak, Roman and Xiao, Lechao and Lee, Jaehoon and Bahri, Yasaman and Abolafia, Daniel A. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.05148},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, 🔍No DOI found},
}

@inproceedings{liu_blockwise_2009,
	title = {Blockwise coordinate descent procedures for the multi-task lasso, with applications to neural semantic basis discovery},
	url = {http://dl.acm.org/citation.cfm?id=1553458},
	urldate = {2017-09-18},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Liu, Han and Palatucci, Mark and Zhang, Jian},
	year = {2009},
	pages = {649--656},
}

@article{nardi_autoregressive_2011,
	title = {Autoregressive process modeling via the {Lasso} procedure},
	volume = {102},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X10002186},
	doi = {10.1016/j.jmva.2010.10.012},
	abstract = {The Lasso is a popular model selection and estimation procedure for linear models that enjoys nice theoretical properties. In this paper, we study the Lasso estimator for fitting autoregressive time series models. We adopt a double asymptotic framework where the maximal lag may increase with the sample size. We derive theoretical results establishing various types of consistency. In particular, we derive conditions under which the Lasso estimator for the autoregressive coefficients is model selection consistent, estimation consistent and prediction consistent. Simulation study results are reported.},
	number = {3},
	journal = {Journal of Multivariate Analysis},
	author = {Nardi, Y. and Rinaldo, A.},
	month = mar,
	year = {2011},
	keywords = {Autoregressive model, Estimation consistency, Lasso procedure, Model selection, Prediction consistency},
	pages = {528--549},
}

@book{garrels_bash_2010,
	title = {Bash {Guide} for {Beginners}},
	publisher = {Fultus Corporation},
	author = {Garrels, Machtelt},
	year = {2010},
}

@book{sarkka_bayesian_2013,
	address = {Cambridge, U.K. ; New York},
	series = {Institute of {Mathematical} {Statistics} textbooks},
	title = {Bayesian filtering and smoothing},
	isbn = {978-1-107-03065-7 978-1-107-61928-9},
	number = {3},
	publisher = {Cambridge University Press},
	author = {Särkkä, Simo},
	year = {2013},
	note = {OCLC: ocn840462877},
	keywords = {Bayesian statistical decision theory, Filters (Mathematics), Smoothing (Statistics)},
}

@book{barber_bayesian_2012,
	title = {Bayesian reasoning and machine learning},
	publisher = {Cambridge University Press},
	author = {Barber, David},
	year = {2012},
}

@inproceedings{doya_bifurcations_1993,
	title = {Bifurcations of {Recurrent} {Neural} {Networks} in {Gradient} {Descent} {Learning}},
	abstract = {Asymptotic behavior of a recurrent neural network changes qualitatively at certain points in the parameter space, which are known as {\textbackslash}bifurcation points". At bifurcation points, the output of a network can change discontinuously with the change of parameters and therefore convergence of gradient descent algorithms is not guaranteed. Furthermore, learning equations used for error gradient estimation can be unstable. However, some kinds of bifurcations are inevitable in training a recurrent network as an automaton or an oscillator. Some of the factors underlying successful training of recurrent networks are investigated, such as choice of initial connections, choice of input patterns, teacher forcing, and truncated learning equations.},
	author = {Doya, Kenji},
	year = {1993},
	keywords = {Algorithm, Artificial neural network, Automaton, Bifurcation (procedure), Bifurcation theory, Control theory, Dynamical system, Gradient descent, Neural Network Simulation, Neural Networks, Oscillator Device Component, Population Parameter, Recurrent neural network, Unstable Medical Device Problem},
}

@article{devlin_bert_2018,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2019-06-08},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
}

@techreport{dolan_benchmarking_2004a,
	title = {Benchmarking {Optimization} {Software} with {COPS} 3.0.},
	institution = {Argonne National Lab., Argonne, IL (US)},
	author = {Dolan, Elizabeth D and Moré, Jorge J and Munson, Todd S},
	year = {2004},
}

@article{aquino_brazilian_2012,
	title = {Brazilian longitudinal study of adult health ({ELSA}-{Brasil}): {Objectives} and design},
	volume = {175},
	issn = {0002-9262},
	url = {https://doi.org/10.1093/aje/kwr294},
	doi = {10.1093/aje/kwr294},
	abstract = {Although low- and middle-income countries still bear the burden of major infectious diseases, chronic noncommunicable diseases are becoming increasingly common due to rapid demographic, epidemiologic, and nutritional transitions. However, information is generally scant in these countries regarding chronic disease incidence, social determinants, and risk factors. The Brazilian Longitudinal Study of Adult Health (ELSA-Brasil) aims to contribute relevant information with respect to the development and progression of clinical and subclinical chronic diseases, particularly cardiovascular diseases and diabetes. In this report, the authors delineate the study’s objectives, principal methodological features, and timeline. At baseline, ELSA-Brasil enrolled 15,105 civil servants from 5 universities and 1 research institute. The baseline examination (2008–2010) included detailed interviews, clinical and anthropometric examinations, an oral glucose tolerance test, overnight urine collection, a 12-lead resting electrocardiogram, measurement of carotid intima-media thickness, echocardiography, measurement of pulse wave velocity, hepatic ultrasonography, retinal fundus photography, and an analysis of heart rate variability. Long-term biologic sample storage will allow investigation of biomarkers that may predict cardiovascular diseases and diabetes. Annual telephone surveillance, initiated in 2009, will continue for the duration of the study. A follow-up examination is scheduled for 2012–2013.},
	number = {4},
	journal = {American Journal of Epidemiology},
	author = {Aquino, Estela M. L. and Barreto, Sandhi Maria and Bensenor, Isabela M. and Carvalho, Marilia S. and Chor, Dóra and Duncan, Bruce B. and Lotufo, Paulo A. and Mill, José Geraldo and Molina, Maria Del Carmen and Mota, Eduardo L. A. and Azeredo Passos, Valéria Maria and Schmidt, Maria Inês and Szklo, Moyses},
	month = jan,
	year = {2012},
	note = {tex.eprint: https://academic.oup.com/aje/article-pdf/175/4/315/267831/kwr294.pdf},
	pages = {315--324},
}

@article{marcus_building_1993,
	title = {Building a large annotated corpus of {English}: {The} {Penn} {Treebank}},
	volume = {19},
	url = {https://www.aclweb.org/anthology/J93-2004},
	number = {2},
	journal = {Computational Linguistics},
	author = {Marcus, Mitchell P. and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
	year = {1993},
	pages = {313--330},
}

@inproceedings{ribeiro_occam_2021a,
	title = {Beyond {Occam}'s {Razor} in {System} {Identification}: {Double}-{Descent} when {Modeling} {Dynamics}},
	copyright = {All rights reserved},
	booktitle = {Workshop on {Nonlinear} {System} {Identification}},
	author = {Ribeiro, Antônio H. and Hendriks, Johannes N. and Wills, Adrian G. and Schön, Thomas B.},
	year = {2021},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning},
}

@article{ribeiro_automatic_2018,
	title = {Automatic {Diagnosis} of {Short}-{Duration} 12-{Lead} {ECG} using a {Deep} {Convolutional} {Network}},
	copyright = {All rights reserved},
	abstract = {We present a model for predicting electrocardiogram (ECG) abnormalities in short-duration 12-lead ECG signals which outperformed medical doctors on the 4th year of their cardiology residency. Such exams can provide a full evaluation of heart activity and have not been studied in previous end-to-end machine learning papers. Using the database of a large telehealth network, we built a novel dataset with more than 2 million ECG tracings, orders of magnitude larger than those used in previous studies. Moreover, our dataset is more realistic, as it consist of 12-lead ECGs recorded during standard in-clinics exams. Using this data, we trained a residual neural network with 9 convolutional layers to map 7 to 10 second ECG signals to 6 classes of ECG abnormalities. Future work should extend these results to cover a large range of ECG abnormalities, which could improve the accessibility of this diagnostic tool and avoid wrong diagnosis from medical doctors.},
	journal = {Machine Learning for Health (ML4H) Workshop at NeurIPS},
	author = {Ribeiro, Antônio H. and Ribeiro, Manoel Horta and Paixão, Gabriela and Oliveira, Derick and Gomes, Paulo R. and Canazart, Jéssica A. and Pifano, Milton and Meira Jr., Wagner and Schön, Thomas B. and Ribeiro, Antonio Luiz},
	year = {2018},
	note = {arXiv: 1811.12194},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
}

@book{vandervaart_asymptotic_2000,
	series = {Cambridge series in statistical and probabilistic mathematics},
	title = {Asymptotic statistics},
	isbn = {0-521-78450-6 978-0-521-78450-4 0-521-49603-9 978-0-521-49603-2},
	publisher = {Cambridge University Press},
	author = {van der Vaart, A. W.},
	year = {2000},
}

@article{ljung_asymptotic_1980,
	title = {Asymptotic normality of prediction error estimators for approximate system models},
	volume = {3},
	issn = {0090-9491},
	url = {https://doi.org/10.1080/17442507908833135},
	doi = {10.1080/17442507908833135},
	number = {1-4},
	journal = {Stochastics},
	author = {Ljung, Lennart and Caines, Peter E.},
	month = jan,
	year = {1980},
	pages = {29--46},
}

@book{fialho_automacao_2004,
	edition = {2},
	title = {Automação pneumática projetos, dimensionamento e análise de circuitos},
	url = {http://gen.lib.rus.ec/book/index.php?md5=9d2d9fd28255227c59bdbedd1ff75834},
	author = {Fialho, Arivelto Bustamante},
	year = {2004},
}

@article{bond_assessing_2014,
	title = {Assessing computerized eye tracking technology for gaining insight into expert interpretation of the 12-lead electrocardiogram: an objective quantitative approach},
	volume = {47},
	issn = {1532-8430},
	shorttitle = {Assessing computerized eye tracking technology for gaining insight into expert interpretation of the 12-lead electrocardiogram},
	doi = {10.1016/j.jelectrocard.2014.07.011},
	abstract = {INTRODUCTION: It is well known that accurate interpretation of the 12-lead electrocardiogram (ECG) requires a high degree of skill. There is also a moderate degree of variability among those who interpret the ECG. While this is the case, there are no best practice guidelines for the actual ECG interpretation process. Hence, this study adopts computerized eye tracking technology to investigate whether eye-gaze can be used to gain a deeper insight into how expert annotators interpret the ECG. Annotators were recruited in San Jose, California at the 2013 International Society of Computerised Electrocardiology (ISCE).
METHODS: Each annotator was recruited to interpret a number of 12-lead ECGs (N=12) while their eye gaze was recorded using a Tobii X60 eye tracker. The device is based on corneal reflection and is non-intrusive. With a sampling rate of 60Hz, eye gaze coordinates were acquired every 16.7ms. Fixations were determined using a predefined computerized classification algorithm, which was then used to generate heat maps of where the annotators looked. The ECGs used in this study form four groups (3=ST elevation myocardial infarction [STEMI], 3=hypertrophy, 3=arrhythmias and 3=exhibiting unique artefacts). There was also an equal distribution of difficulty levels (3=easy to interpret, 3=average and 3=difficult). ECGs were displayed using the 4x3+1 display format and computerized annotations were concealed.
RESULTS: Precisely 252 expert ECG interpretations (21 annotators×12 ECGs) were recorded. Average duration for ECG interpretation was 58s (SD=23). Fleiss' generalized kappa coefficient (Pa=0.56) indicated a moderate inter-rater reliability among the annotators. There was a 79\% inter-rater agreement for STEMI cases, 71\% agreement for arrhythmia cases, 65\% for the lead misplacement and dextrocardia cases and only 37\% agreement for the hypertrophy cases. In analyzing the total fixation duration, it was found that on average annotators study lead V1 the most (4.29s), followed by leads V2 (3.83s), the rhythm strip (3.47s), II (2.74s), V3 (2.63s), I (2.53s), aVL (2.45s), V5 (2.27s), aVF (1.74s), aVR (1.63s), V6 (1.39s), III (1.32s) and V4 (1.19s). It was also found that on average the annotator spends an equal amount of time studying leads in the frontal plane (15.89s) when compared to leads in the transverse plane (15.70s). It was found that on average the annotators fixated on lead I first followed by leads V2, aVL, V1, II, aVR, V3, rhythm strip, III, aVF, V5, V4 and V6. We found a strong correlation (r=0.67) between time to first fixation on a lead and the total fixation duration on each lead. This indicates that leads studied first are studied the longest. There was a weak negative correlation between duration and accuracy (r=-0.2) and a strong correlation between age and accuracy (r=0.67).
CONCLUSIONS: Eye tracking facilitated a deeper insight into how expert annotators interpret the 12-lead ECG. As a result, the authors recommend ECG annotators to adopt an initial first impression/pattern recognition approach followed by a conventional systematic protocol to ECG interpretation. This recommendation is based on observing misdiagnoses given due to first impression only. In summary, this research presents eye gaze results from expert ECG annotators and provides scope for future work that involves exploiting computerized eye tracking technology to further the science of ECG interpretation.},
	language = {eng},
	number = {6},
	journal = {Journal of Electrocardiology},
	author = {Bond, R. R. and Zhu, T. and Finlay, D. D. and Drew, B. and Kligfield, P. D. and Guldenring, D. and Breen, C. and Gallagher, A. G. and Daly, M. J. and Clifford, G. D.},
	month = dec,
	year = {2014},
	pmid = {25110276},
	keywords = {Adult, Arrhythmias, Cardiac, Artificial Intelligence, Clinical Competence, ECG interpretation, Electrocardiography, Eye Movements, Female, Fixation, Ocular, Humans, Male, Reading, Visual Perception, eye-tracking},
	pages = {895--906},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
	urldate = {2019-04-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, 🔍No DOI found},
	pages = {5998--6008},
}

@article{macfarlane_automated_1996,
	title = {Automated serial {ECG} comparison based on the {Minnesota} code},
	volume = {29},
	issn = {0022-0736},
	doi = {10/cdh7qw},
	journal = {Journal of Electrocardiology},
	author = {Macfarlane, Peter W and Latif, Shahid},
	year = {1996},
	pages = {29--34},
}

@article{minchole_artificial_,
	title = {Artificial intelligence for the electrocardiogram},
	doi = {10/gfsvzh},
	language = {en},
	journal = {Nature Medicine},
	author = {Mincholé, Ana and Rodriguez, Blanca},
	pages = {2},
}

@inproceedings{teijeiro_arrhythmia_2017,
	title = {Arrhythmia {Classification} from the {Abductive} {Interpretation} of {Short} {Single}-{Lead} {ECG} {Records}},
	url = {http://www.cinc.org/archives/2017/pdf/166-054.pdf},
	doi = {10.22489/CinC.2017.166-054},
	abstract = {In this work we propose a new method for the rhythm classiﬁcation of short single-lead ECG records, using a set of high-level and clinically meaningful features provided by the abductive interpretation of the records. These features include morphological and rhythm-related features that are used to build two classiﬁers: one that evaluates the record globally, using aggregated values for each feature; and another one that evaluates the record as a sequence, using a Recurrent Neural Network fed with the individual features for each detected heartbeat. The two classiﬁers are ﬁnally combined using the stacking technique, providing an answer by means of four target classes: Normal sinus rhythm (N), Atrial ﬁbrillation (A), Other anomaly (O) and Noisy ({\textasciitilde}). The approach has been validated against the 2017 Physionet/CinC Challenge dataset, obtaining a ﬁnal score of 0.83 and ranking ﬁrst in the competition.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Computing in {Cardiology}},
	author = {Teijeiro, Tomas and Garcia, Constantino A. and Castro, Daniel and Félix, Paulo},
	month = sep,
	year = {2017},
}

@article{allen_assessing_1996,
	title = {Assessing {ECG} signal quality on a coronary care unit},
	volume = {17},
	doi = {10.1088/0967-3334/17/4/002},
	number = {4},
	journal = {Physiological measurement},
	author = {Allen, John and Murray, Alan},
	year = {1996},
	pages = {249},
}

@inproceedings{delrio_assessment_2011,
	title = {Assessment of different methods to estimate electrocardiogram signal quality},
	booktitle = {Computing in {Cardiology}, 2011},
	publisher = {IEEE},
	author = {del Río, B. Aldecoa Sánchez and Lopetegi, T. and Romero, I.},
	year = {2011},
	pages = {609--612},
}

@article{zhao_automated_2017,
	title = {Automated {Model} {Construction} for {Combined} {Sewer} {Overflow} {Prediction} {Based} on {Efficient} {LASSO} {Algorithm}},
	volume = {PP},
	issn = {2168-2216},
	doi = {10.1109/TSMC.2017.2724440},
	abstract = {The prediction of combined sewer overflow (CSO) operation in urban environments presents a challenging task for water utilities. The operation of CSOs (most often in heavy rainfall conditions) prevents houses and businesses from flooding. However, sometimes, CSOs do not operate as they should, potentially bringing environmental pollution risks. Therefore, CSOs should be appropriately managed by water utilities, highlighting the need for adapted decision support systems. This paper proposes an automated CSO predictive model construction methodology using field monitoring data, as a substitute for the commonly established hydrological-hydraulic modeling approach for time-series prediction of CSO statuses. It is a systematic methodology factoring in all monitored field variables to construct time-series dependencies for CSO statuses. The model construction process is largely automated with little human intervention, and the pertinent variables together with their associated time lags for every CSO are holistically and automatically generated. A fast least absolute shrinkage and selection operator solution generating scheme is proposed to expedite the model construction process, where matrix inversions are effectively eliminated. The whole algorithm works in a stepwise manner, invoking either an incremental or decremental movement for including or excluding one model regressor into, or from, the predictive model at every step. The computational complexity is thereby analyzed with the pseudo code provided. Actual experimental results from both single-step ahead (i.e., 15 min) and multistep ahead predictions are finally produced and analyzed on a U.K. pilot area with various types of monitoring data made available, demonstrating the efficiency and effectiveness of the proposed approach.},
	number = {99},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Zhao, W. and Beach, T. H. and Rezgui, Y.},
	year = {2017},
	note = {00003},
	keywords = {Adaptation models, Analytical models, Combined sewer overflows (CSOs), Computational modeling, Data models, Mathematical model, Monitoring, Predictive models, efficient model construction, hydraulics, prediction, wastewater},
	pages = {1--16},
}

@article{dvoretzky_asymptotic_1956,
	title = {Asymptotic {Minimax} {Character} of the {Sample} {Distribution} {Function} and of the {Classical} {Multinomial} {Estimator}},
	volume = {27},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177728174},
	doi = {10/csg5cg},
	abstract = {This paper is devoted, in the main, to proving the asymptotic minimax character of the sample distribution function (d.f.) for estimating an unknown d.f. in FF{\textbackslash}mathscr\{F\} or FcFc{\textbackslash}mathscr\{F\}\_c (defined in Section 1) for a wide variety of weight functions. Section 1 contains definitions and a discussion of measurability considerations. Lemma 2 of Section 2 is an essential tool in our proofs and seems to be of interest per se; for example, it implies the convergence of the moment generating function of GnGnG\_n to that of GGG (definitions in (2.1)). In Section 3 the asymptotic minimax character is proved for a fundamental class of weight functions which are functions of the maximum deviation between estimating and true d.f. In Section 4 a device (of more general applicability in decision theory) is employed which yields the asymptotic minimax result for a wide class of weight functions of this character as a consequence of the results of Section 3 for weight functions of the fundamental class. In Section 5 the asymptotic minimax character is proved for a class of integrated weight functions. A more general class of weight functions for which the asymptotic minimax character holds is discussed in Section 6. This includes weight functions for which the risk function of the sample d.f. is not a constant over Fc.Fc.{\textbackslash}mathscr\{F\}\_c. Most weight functions of practical interest are included in the considerations of Sections 3 to 6. Section 6 also includes a discussion of multinomial estimation problems for which the asymptotic minimax character of the classical estimator is contained in our results. Finally, Section 7 includes a general discussion of minimization of symmetric convex or monotone functionals of symmetric random elements, with special consideration of the "tied-down" Wiener process, and with a heuristic proof of the results of Sections 3, 4, 5, and much of Section 6.},
	language = {EN},
	number = {3},
	urldate = {2019-04-16},
	journal = {The Annals of Mathematical Statistics},
	author = {Dvoretzky, A. and Kiefer, J. and Wolfowitz, J.},
	month = sep,
	year = {1956},
	mrnumber = {MR83864},
	zmnumber = {0073.14603},
	pages = {642--669},
}

@article{goto_artificial_2019,
	title = {Artificial intelligence to predict needs for urgent revascularization from 12-leads electrocardiography in emergency patients},
	volume = {14},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0210103},
	doi = {10/gf286h},
	abstract = {Background Patient with acute coronary syndrome benefits from early revascularization. However, methods for the selection of patients who require urgent revascularization from a variety of patients visiting the emergency room with chest symptoms is not fully established. Electrocardiogram is an easy and rapid procedure, but may contain crucial information not recognized even by well-trained physicians. Objective To make a prediction model for the needs for urgent revascularization from 12-lead electrocardiogram recorded in the emergency room. Method We developed an artificial intelligence model enabling the detection of hidden information from a 12-lead electrocardiogram recorded in the emergency room. Electrocardiograms obtained from consecutive patients visiting the emergency room at Keio University Hospital from January 2012 to April 2018 with chest discomfort was collected. These data were splitted into validation and derivation dataset with no duplication in each dataset. The artificial intelligence model was constructed to select patients who require urgent revascularization within 48 hours. The model was trained with the derivation dataset and tested using the validation dataset. Results Of the consecutive 39,619 patients visiting the emergency room with chest discomfort, 362 underwent urgent revascularization. Of them, 249 were included in the derivation dataset and the remaining 113 were included in validation dataset. For the control, 300 were randomly selected as derivation dataset and another 130 patients were randomly selected for validation dataset from the 39,317 who did not undergo urgent revascularization. On validation, our artificial intelligence model had predictive value of the c-statistics 0.88 (95\% CI 0.84–0.93) for detecting patients who required urgent revascularization. Conclusions Our artificial intelligence model provides information to select patients who need urgent revascularization from only 12-leads electrocardiogram in those visiting the emergency room with chest discomfort.},
	language = {en},
	number = {1},
	urldate = {2019-05-29},
	journal = {PLOS ONE},
	author = {Goto, Shinichi and Kimura, Mai and Katsumata, Yoshinori and Goto, Shinya and Kamatani, Takashi and Ichihara, Genki and Ko, Seien and Sasaki, Junichi and Fukuda, Keiichi and Sano, Motoaki},
	month = sep,
	year = {2019},
	keywords = {Artificial intelligence, Convolution, Coronary revascularization, Critical care and emergency medicine, Electrocardiography, Neural networks, Recurrent neural networks, Revascularization},
	pages = {e0210103},
}

@inproceedings{geiger_are_2012,
	address = {Providence, RI},
	title = {Are we ready for autonomous driving? {The} {KITTI} vision benchmark suite},
	isbn = {978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1},
	shorttitle = {Are we ready for autonomous driving?},
	url = {http://ieeexplore.ieee.org/document/6248074/},
	doi = {10/gf7nxj},
	language = {en},
	urldate = {2019-09-06},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Geiger, A. and Lenz, P. and Urtasun, R.},
	month = jun,
	year = {2012},
	pages = {3354--3361},
}

@article{winkler_association_2019,
	title = {Association {Between} {Surgical} {Skin} {Markings} in {Dermoscopic} {Images} and {Diagnostic} {Performance} of a {Deep} {Learning} {Convolutional} {Neural} {Network} for {Melanoma} {Recognition}},
	url = {https://jamanetwork.com/journals/jamadermatology/fullarticle/2740808},
	doi = {10/gf6894},
	abstract = {{\textless}h3{\textgreater}Importance{\textless}/h3{\textgreater}{\textless}p{\textgreater}Deep learning convolutional neural networks (CNNs) have shown a performance at the level of dermatologists in the diagnosis of melanoma. Accordingly, further exploring the potential limitations of CNN technology before broadly applying it is of special interest.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Objective{\textless}/h3{\textgreater}{\textless}p{\textgreater}To investigate the association between gentian violet surgical skin markings in dermoscopic images and the diagnostic performance of a CNN approved for use as a medical device in the European market.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Design and Setting{\textless}/h3{\textgreater}{\textless}p{\textgreater}A cross-sectional analysis was conducted from August 1, 2018, to November 30, 2018, using a CNN architecture trained with more than 120 000 dermoscopic images of skin neoplasms and corresponding diagnoses. The association of gentian violet skin markings in dermoscopic images with the performance of the CNN was investigated in 3 image sets of 130 melanocytic lesions each (107 benign nevi, 23 melanomas).{\textless}/p{\textgreater}{\textless}h3{\textgreater}Exposures{\textless}/h3{\textgreater}{\textless}p{\textgreater}The same lesions were sequentially imaged with and without the application of a gentian violet surgical skin marker and then evaluated by the CNN for their probability of being a melanoma. In addition, the markings were removed by manually cropping the dermoscopic images to focus on the melanocytic lesion.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Main Outcomes and Measures{\textless}/h3{\textgreater}{\textless}p{\textgreater}Sensitivity, specificity, and area under the curve (AUC) of the receiver operating characteristic (ROC) curve for the CNN’s diagnostic classification in unmarked, marked, and cropped images.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Results{\textless}/h3{\textgreater}{\textless}p{\textgreater}In all, 130 melanocytic lesions (107 benign nevi and 23 melanomas) were imaged. In unmarked lesions, the CNN achieved a sensitivity of 95.7\% (95\% CI, 79\%-99.2\%) and a specificity of 84.1\% (95\% CI, 76.0\%-89.8\%). The ROC AUC was 0.969. In marked lesions, an increase in melanoma probability scores was observed that resulted in a sensitivity of 100\% (95\% CI, 85.7\%-100\%) and a significantly reduced specificity of 45.8\% (95\% CI, 36.7\%-55.2\%,\textit{P} \&lt; .001). The ROC AUC was 0.922. Cropping images led to the highest sensitivity of 100\% (95\% CI, 85.7\%-100\%), specificity of 97.2\% (95\% CI, 92.1\%-99.0\%), and ROC AUC of 0.993. Heat maps created by vanilla gradient descent backpropagation indicated that the blue markings were associated with the increased false-positive rate.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Conclusions and Relevance{\textless}/h3{\textgreater}{\textless}p{\textgreater}This study’s findings suggest that skin markings significantly interfered with the CNN’s correct diagnosis of nevi by increasing the melanoma probability scores and consequently the false-positive rate. A predominance of skin markings in melanoma training images may have induced the CNN’s association of markings with a melanoma diagnosis. Accordingly, these findings suggest that skin markings should be avoided in dermoscopic images intended for analysis by a CNN.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Trial Registration{\textless}/h3{\textgreater}{\textless}p{\textgreater}German Clinical Trial Register (DRKS) Identifier:DRKS00013570{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2019-08-30},
	journal = {JAMA Dermatology},
	author = {Winkler, Julia K. and Fink, Christine and Toberer, Ferdinand and Enk, Alexander and Deinlein, Teresa and Hofmann-Wellenhof, Rainer and Thomas, Luc and Lallas, Aimilios and Blum, Andreas and Stolz, Wilhelm and Haenssle, Holger A.},
	month = aug,
	year = {2019},
}

@techreport{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017},
	keywords = {⛔ No DOI found},
}

@book{suykens_artificial_2012,
	title = {Artificial neural networks for modelling and control of non-linear systems},
	publisher = {Springer Science \& Business Media},
	author = {Suykens, Johan AK and Vandewalle, Joos PL and de Moor, Bart L},
	year = {2012},
	note = {00000},
}

@article{lu_attractor_2018,
	title = {Attractor reconstruction by machine learning},
	volume = {28},
	issn = {1054-1500, 1089-7682},
	url = {http://aip.scitation.org/doi/10.1063/1.5039508},
	doi = {10.1063/1.5039508},
	language = {en},
	number = {6},
	urldate = {2021-02-19},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Lu, Zhixin and Hunt, Brian R. and Ott, Edward},
	month = jun,
	year = {2018},
	pages = {061104},
}

@article{kingma_autoencoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	journal = {ICLR},
	author = {Kingma, Diederik P. and Welling, Max},
	year = {2014},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, 🔍No DOI found},
}

@misc{_antonior92_,
	title = {antonior92/code-deeplearning},
	url = {https://github.com/antonior92/code-deeplearning},
	abstract = {PyTorch deep learning projects from CODE. Contribute to antonior92/code-deeplearning development by creating an account on GitHub.},
	language = {en},
	urldate = {2019-12-28},
	journal = {GitHub},
}

@misc{_antonior92_a,
	title = {antonior92/physionet-12ecg-classification},
	url = {https://github.com/antonior92/physionet-12ecg-classification},
	abstract = {Physionet 2020 challenge. Contribute to antonior92/physionet-12ecg-classification development by creating an account on GitHub.},
	language = {en},
	urldate = {2020-06-26},
	journal = {GitHub},
	note = {Library Catalog: github.com},
}

@article{liu_open_2018,
	title = {An {Open} {Access} {Database} for {Evaluating} the {Algorithms} of {Electrocardiogram} {Rhythm} and {Morphology} {Abnormality} {Detection}},
	volume = {8},
	doi = {10.1166/jmihi.2018.2442},
	abstract = {Over the past few decades, methods for classification and detection of rhythm or morphology abnormalities in ECG signals have been widely studied. However, it lacks the comprehensive performance evaluation on an open database. This paper presents a detailed introduction for the database
used for the 1st China Physiological Signal Challenge 2018 (CPSC 2018), which will be run as a special section during the ICBEB 2018. CPSC 2018 aims to encourage the development of algorithms to identify the rhythm/morphology abnormalities from 12-lead ECGs. The data used in CPSC 2018 include
one normal ECG type and eight abnormal types. This paper details the data source, recording information, patients' clinical baseline parameters as age, gender and so on. Meanwhile, it also presents the commonly used detection/classification methods for the abovementioned abnormal ECG types.
We hope this paper could be a guide reference for the CPSC 2018, to facilitate the researchers familiar with the data and the related research advances.},
	number = {7},
	journal = {Journal of Medical Imaging and Health Informatics},
	author = {Liu, Feifei and Liu, Chengyu and Zhao, Lina and Zhang, Xiangyu and Wu, Xiaoling and Xu, Xiaoyan and Liu, Yulin and Ma, Caiyun and Wei, Shoushui and He, Zhiqiang and Li, Jianqing and Yin Kwee, Eddie Ng},
	month = sep,
	year = {2018},
	keywords = {CPSC, DATABASE, ELECTROCARDIOGRAM (ECG), RHYTHM AND MORPHOLOGY ABNORMAL},
	pages = {1368--1373},
}

@book{anderson_introduction_2009,
	title = {An {Introduction} to {Random} {Matrices}},
	author = {Anderson, Greg W. and Guionnet, Alice and Zeitouni, Ofer},
	year = {2009},
}

@article{abdalmoaty_application_2018,
	series = {18th {IFAC} {Symposium} on {System} {Identification} {SYSID} 2018},
	title = {Application of a {Linear} {PEM} {Estimator} to a {Stochastic} {Wiener}-{Hammerstein} {Benchmark} {Problem}},
	volume = {51},
	issn = {2405-8963},
	url = {http://www.sciencedirect.com/science/article/pii/S2405896318317968},
	doi = {10/gfkd8g},
	abstract = {The estimation problem of stochastic Wiener-Hammerstein models is recognized to be challenging, mainly due to the analytical intractability of the likelihood function. In this contribution, we apply a computationally attractive prediction error method estimator to a real-data stochastic Wiener-Hammerstein benchmark problem. The estimator is defined using a deterministic predictor that is nonlinear in the input. The prediction error method results in tractable expressions, and Monte Carlo approximations are not necessary. This allows us to tackle several issues considered challenging from the perspective of the current mainstream approach. Under mild conditions, the estimator can be shown to be consistent and asymptotically normal. The results of the method applied to the benchmark data are presented and discussed.},
	number = {15},
	urldate = {2018-11-26},
	journal = {IFAC-PapersOnLine},
	author = {Abdalmoaty, Mohamed Rasheed and Hjalmarsson, Håkan},
	month = jan,
	year = {2018},
	keywords = {Benchmark problem, Nonlinear systems, Stochastic systems, System identification, Wiener-Hammerstein},
	pages = {784--789},
}

@book{efron_introduction_1994,
	title = {An introduction to the bootstrap},
	isbn = {0-412-04231-2},
	publisher = {CRC press},
	author = {Efron, Bradley and Tibshirani, Robert J},
	year = {1994},
}

@book{slotine_applied_1991,
	address = {Englewood Cliffs, N.J},
	title = {Applied nonlinear control},
	isbn = {978-0-13-040890-7},
	language = {en},
	publisher = {Prentice Hall},
	author = {Slotine, J.-J. E. and Li, Weiping},
	year = {1991},
	note = {19267},
	keywords = {Nonlinear control theory},
}

@article{nocedal_interior_2014a,
	title = {An interior point method for nonlinear programming with infeasibility detection capabilities},
	volume = {29},
	issn = {1055-6788},
	doi = {10.1080/10556788.2013.858156},
	number = {4},
	journal = {Optimization Methods and Software},
	author = {Nocedal, Jorge and Öztoprak, Figen and Waltz, Richard A},
	year = {2014},
	pages = {837--854},
}

@book{norton_introduction_2009,
	title = {An {Introduction} to {Identification}},
	publisher = {Courier Corporation},
	author = {Norton, John P},
	year = {2009},
}

@article{gander_analysis_2007,
	title = {Analysis of the parareal time-parallel time-integration method},
	volume = {29},
	doi = {10.1137/05064607X},
	number = {2},
	journal = {SIAM Journal on Scientific Computing},
	author = {Gander, Martin J and Vandewalle, Stefan},
	year = {2007},
	pages = {556--578},
}

@article{farina_iterative_2010,
	title = {An iterative algorithm for simulation error based identification of polynomial input–output models using multi-step prediction},
	volume = {83},
	doi = {10.1080/00207171003793262},
	number = {7},
	journal = {International Journal of Control},
	author = {Farina, Marcello and Piroddi, Luigi},
	year = {2010},
	pages = {1442--1456},
}

@book{draper_applied_1966,
	title = {Applied regression analysis},
	volume = {3},
	publisher = {Wiley New York},
	author = {Draper, Norman Richard and Smith, Harry and Pownell, Elizabeth},
	year = {1966},
}

@article{byrd_approximate_1988,
	title = {Approximate solution of the trust region problem by minimization over two-dimensional subspaces},
	volume = {40},
	doi = {10.1007/BF01580735},
	number = {1-3},
	journal = {Mathematical Programming},
	author = {Byrd, Richard H and Schnabel, Robert B and Shultz, Gerald A},
	year = {1988},
	pages = {247--263},
}

@article{byrd_interior_1999,
	title = {An interior point algorithm for large-scale nonlinear programming},
	volume = {9},
	url = {http://epubs.siam.org/doi/abs/10.1137/S1052623497325107},
	doi = {10.1137/S1052623497325107},
	number = {4},
	urldate = {2017-08-20},
	journal = {SIAM Journal on Optimization},
	author = {Byrd, Richard H. and Hribar, Mary E. and Nocedal, Jorge},
	year = {1999},
	pages = {877--900},
}

@article{dietterich_approximate_1998,
	title = {Approximate {Statistical} {Tests} for {Comparing} {Supervised} {Classification} {Learning} {Algorithms}},
	volume = {10},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/089976698300017197},
	doi = {10/fqc9w5},
	language = {en},
	number = {7},
	urldate = {2018-11-22},
	journal = {Neural Computation},
	author = {Dietterich, Thomas G.},
	month = oct,
	year = {1998},
	pages = {1895--1923},
}

@article{acharya_application_2017,
	title = {Application of deep convolutional neural network for automated detection of myocardial infarction using {ECG} signals},
	volume = {415-416},
	issn = {00200255},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025517308009},
	doi = {10.1016/j.ins.2017.06.027},
	language = {en},
	urldate = {2018-10-21},
	journal = {Information Sciences},
	author = {Acharya, U. Rajendra and Fujita, Hamido and Oh, Shu Lih and Hagiwara, Yuki and Tan, Jen Hong and Adam, Muhammad},
	month = nov,
	year = {2017},
	pages = {190--198},
}

@article{glorot_apprentissage_2015,
	title = {Apprentissage des réseaux de neurones profonds et applications en traitement automatique de la langue naturelle},
	author = {Glorot, Xavier},
	year = {2015},
	keywords = {🔍No DOI found},
}

@book{munkres_analysis_1997,
	series = {Advanced {Books} {Classics}},
	title = {Analysis {On} {Manifolds}},
	isbn = {978-0-8133-4548-2},
	url = {https://books.google.com.br/books?id=tGT6K6HdFfwC},
	publisher = {Avalon Publishing},
	author = {Munkres, J.R.},
	year = {1997},
}

@book{dimopoulos_analog_2012,
	address = {Dordrecht ; New York},
	series = {Analog circuits and signal processing},
	title = {Analog electronic filters: theory, design and synthesis},
	isbn = {978-94-007-2189-0 978-94-007-2190-6},
	shorttitle = {Analog electronic filters},
	publisher = {Springer},
	author = {Dimopoulos, Hercules G.},
	year = {2012},
	keywords = {Analog electronic systems, Electric filters},
}

@article{jeffreys_invariant_1946,
	title = {An {Invariant} {Form} for the {Prior} {Probability} in {Estimation} {Problems}},
	volume = {186},
	issn = {1364-5021, 1471-2946},
	url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1946.0056},
	doi = {10.1098/rspa.1946.0056},
	language = {en},
	number = {1007},
	urldate = {2018-10-07},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Jeffreys, H.},
	month = sep,
	year = {1946},
	pages = {453--461},
}

@inproceedings{warwick_introduction_1996,
	title = {An introduction to radial basis functions for system identification. {A} comparison with other neural network methods},
	volume = {1},
	booktitle = {Decision and {Control}, 1996., {Proceedings} of the 35th {IEEE} {Conference} on},
	publisher = {IEEE},
	author = {Warwick, K and Craddock, R},
	year = {1996},
	note = {00038},
	pages = {464--469},
}

@article{byrd_interior_1999a,
	title = {An {Interior} {Point} {Algorithm} for {Large}-{Scale} {Nonlinear} {Programming}},
	volume = {9},
	doi = {10/b4r783},
	number = {4},
	urldate = {2017-08-20},
	journal = {SIAM Journal on Optimization},
	author = {Byrd, Richard H. and Hribar, Mary E. and Nocedal, Jorge},
	year = {1999},
	pages = {877--900},
}

@article{farina_iterative_2010a,
	title = {An {Iterative} {Algorithm} for {Simulation} {Error} {Based} {Identification} of {Polynomial} {Input}–output {Models} {Using} {Multi}-{Step} {Prediction}},
	volume = {83},
	doi = {10/b84h6q},
	number = {7},
	journal = {International Journal of Control},
	author = {Farina, Marcello and Piroddi, Luigi},
	year = {2010},
	note = {00011},
	pages = {1442--1456},
}

@article{nocedal_interior_2014,
	title = {An {Interior} {Point} {Method} for {Nonlinear} {Programming} with {Infeasibility} {Detection} {Capabilities}},
	volume = {29},
	issn = {1055-6788},
	doi = {10/gfjwmn},
	number = {4},
	journal = {Optimization Methods and Software},
	author = {Nocedal, Jorge and Öztoprak, Figen and Waltz, Richard A},
	year = {2014},
	pages = {837--854},
}

@article{huval_empirical_2015,
	title = {An empirical evaluation of deep learning on highway driving},
	volume = {abs/1504.01716},
	url = {http://arxiv.org/abs/1504.01716},
	journal = {CoRR},
	author = {Huval, Brody and Wang, Tao and Tandon, Sameep and Kiske, Jeff and Song, Will and Pazhayampallil, Joel and Andriluka, Mykhaylo and Rajpurkar, Pranav and Migimatsu, Toki and Cheng-Yue, Royce and Mujica, Fernando A. and Coates, Adam and Ng, Andrew Y.},
	year = {2015},
	note = {arXiv: 1504.01716
tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.biburl: https://dblp.org/rec/journals/corr/HuvalWTKSPARMCM15.bib
tex.timestamp: Mon, 11 Mar 2019 09:54:20 +0100},
}

@article{rautaharju_aha_2009,
	title = {{AHA}/{ACCF}/{HRS} {Recommendations} for the {Standardization} and {Interpretation} of the {Electrocardiogram}: {Part} {IV}: {The} {ST} {Segment}, {T} and {U} {Waves}, and the {QT} {Interval} {A} {Scientific} {Statement} {From} the {American} {Heart} {Association} {Electrocardiography} and {Arrhythmias} {Committee}, {Council} on {Clinical} {Cardiology}; the {American} {College} of {Cardiology} {Foundation}; and the {Heart} {Rhythm} {Society} {Endorsed} by the {International} {Society} for {Computerized} {Electrocardiology}},
	volume = {53},
	issn = {0735-1097},
	url = {http://www.sciencedirect.com/science/article/pii/S0735109708041363},
	doi = {10/c7vp73},
	number = {11},
	journal = {Journal of the American College of Cardiology},
	author = {Rautaharju, Pentti M. and Surawicz, Borys and Gettes, Leonard S.},
	month = mar,
	year = {2009},
	keywords = {ACCF Expert Consensus Documents, electrocardiography, electrophysiology, ion channels, long-QT syndrome},
	pages = {982--991},
}

@article{surawicz_aha_2009,
	title = {{AHA}/{ACCF}/{HRS} {Recommendations} for the {Standardization} and {Interpretation} of the {Electrocardiogram}},
	volume = {53},
	url = {http://www.onlinejacc.org/content/53/11/976.abstract},
	doi = {10/bmv8kz},
	number = {11},
	journal = {Journal of the American College of Cardiology},
	author = {Surawicz, Borys and Childers, Rory and Deal, Barbara J. and Gettes, Leonard S.},
	month = mar,
	year = {2009},
	pages = {976},
}

@article{powell_efficient_1964,
	title = {An efficient method for finding the minimum of a function of several variables without calculating derivatives},
	volume = {7},
	issn = {0010-4620},
	doi = {10.1093/comjnl/7.2.155},
	number = {2},
	journal = {The computer journal},
	author = {Powell, Michael JD},
	year = {1964},
	note = {00000},
	pages = {155--162},
}

@article{marquardt_algorithm_1963,
	title = {An algorithm for least-squares estimation of nonlinear parameters},
	volume = {11},
	doi = {10.1137/0111030},
	number = {2},
	journal = {Journal of the Society for Industrial and Applied Mathematics},
	author = {Marquardt, Donald W},
	year = {1963},
	pages = {431--441},
}

@article{coleman_efficient_1995,
	title = {An efficient trust region method for unconstrained discrete-time optimal control problems},
	volume = {4},
	number = {1},
	journal = {Computational Optimization and Applications},
	author = {Coleman, Thomas F and Liao, Aiping},
	year = {1995},
	keywords = {❓Multiple DOI},
	pages = {47--66},
}

@article{dennisjr_adaptive_1981,
	title = {An adaptive nonlinear least-squares algorithm},
	volume = {7},
	number = {3},
	journal = {ACM Transactions on Mathematical Software (TOMS)},
	author = {Dennis Jr, John E and Gay, David M and Walsh, Roy E},
	year = {1981},
	keywords = {❓Multiple DOI},
	pages = {348--368},
}

@book{wasserman_all_2013,
	title = {All of statistics: a concise course in statistical inference},
	shorttitle = {All of statistics},
	publisher = {Springer Science \& Business Media},
	author = {Wasserman, Larry},
	year = {2013},
	note = {03141},
}

@book{artin_algebra_1991,
	title = {Algebra},
	isbn = {978-0-13-004763-2},
	url = {https://books.google.com.br/books?id=C_juAAAAMAAJ},
	publisher = {Prentice Hall},
	author = {Artin, M.},
	year = {1991},
}

@book{kleinberg_algorithm_2006,
	address = {Boston},
	title = {Algorithm design},
	isbn = {978-0-321-29535-4},
	publisher = {Pearson/Addison-Wesley},
	author = {Kleinberg, Jon and Tardos, Éva},
	year = {2006},
	keywords = {Computer algorithms, Data structures (Computer science)},
}

@article{young_instrumental_1970,
	title = {An instrumental variable method for real-time identification of a noisy process},
	volume = {6},
	doi = {10.1016/0005-1098(70)90098-1},
	number = {2},
	journal = {Automatica},
	author = {Young, Peter C},
	year = {1970},
	note = {00477},
	pages = {271--287},
}

@article{piroddi_identification_2003a,
	title = {An identification algorithm for polynomial {NARX} models based on simulation error minimization},
	volume = {76},
	doi = {10.1080/00207170310001635419},
	number = {17},
	journal = {International Journal of Control},
	author = {Piroddi, Luigi and Spinelli, William},
	year = {2003},
	note = {00000},
	pages = {1767--1781},
}

@article{piroddi_identification_2003,
	title = {An identification algorithm for polynomial {NARX} models based on simulation error minimization},
	volume = {76},
	issn = {0020-7179, 1366-5820},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00207170310001635419},
	doi = {10/cc8w93},
	language = {en},
	number = {17},
	urldate = {2019-04-14},
	journal = {International Journal of Control},
	author = {Piroddi, L. and Spinelli, W.},
	month = nov,
	year = {2003},
	pages = {1767--1781},
}

@article{attia_artificial_2019,
	title = {An artificial intelligence-enabled {ECG} algorithm for the identification of patients with atrial fibrillation during sinus rhythm: a retrospective analysis of outcome prediction},
	issn = {0140-6736},
	shorttitle = {An artificial intelligence-enabled {ECG} algorithm for the identification of patients with atrial fibrillation during sinus rhythm},
	url = {http://www.sciencedirect.com/science/article/pii/S0140673619317210},
	doi = {10/gf7d9h},
	abstract = {Summary
Background
Atrial fibrillation is frequently asymptomatic and thus underdetected but is associated with stroke, heart failure, and death. Existing screening methods require prolonged monitoring and are limited by cost and low yield. We aimed to develop a rapid, inexpensive, point-of-care means of identifying patients with atrial fibrillation using machine learning.
Methods
We developed an artificial intelligence (AI)-enabled electrocardiograph (ECG) using a convolutional neural network to detect the electrocardiographic signature of atrial fibrillation present during normal sinus rhythm using standard 10-second, 12-lead ECGs. We included all patients aged 18 years or older with at least one digital, normal sinus rhythm, standard 10-second, 12-lead ECG acquired in the supine position at the Mayo Clinic ECG laboratory between Dec 31, 1993, and July 21, 2017, with rhythm labels validated by trained personnel under cardiologist supervision. We classified patients with at least one ECG with a rhythm of atrial fibrillation or atrial flutter as positive for atrial fibrillation. We allocated ECGs to the training, internal validation, and testing datasets in a 7:1:2 ratio. We calculated the area under the curve (AUC) of the receiver operatoring characteristic curve for the internal validation dataset to select a probability threshold, which we applied to the testing dataset. We evaluated model performance on the testing dataset by calculating the AUC and the accuracy, sensitivity, specificity, and F1 score with two-sided 95\% CIs.
Findings
We included 180 922 patients with 649 931 normal sinus rhythm ECGs for analysis: 454 789 ECGs recorded from 126 526 patients in the training dataset, 64 340 ECGs from 18 116 patients in the internal validation dataset, and 130 802 ECGs from 36 280 patients in the testing dataset. 3051 (8·4\%) patients in the testing dataset had verified atrial fibrillation before the normal sinus rhythm ECG tested by the model. A single AI-enabled ECG identified atrial fibrillation with an AUC of 0·87 (95\% CI 0·86–0·88), sensitivity of 79·0\% (77·5–80·4), specificity of 79·5\% (79·0–79·9), F1 score of 39·2\% (38·1–40·3), and overall accuracy of 79·4\% (79·0–79·9). Including all ECGs acquired during the first month of each patient's window of interest (ie, the study start date or 31 days before the first recorded atrial fibrillation ECG) increased the AUC to 0·90 (0·90–0·91), sensitivity to 82·3\% (80·9–83·6), specificity to 83·4\% (83·0–83·8), F1 score to 45·4\% (44·2–46·5), and overall accuracy to 83·3\% (83·0–83·7).
Interpretation
An AI-enabled ECG acquired during normal sinus rhythm permits identification at point of care of individuals with atrial fibrillation.
Funding
None.},
	urldate = {2019-09-02},
	journal = {The Lancet},
	author = {Attia, Zachi I and Noseworthy, Peter A and Lopez-Jimenez, Francisco and Asirvatham, Samuel J and Deshmukh, Abhishek J and Gersh, Bernard J and Carter, Rickey E and Yao, Xiaoxi and Rabinstein, Alejandro A and Erickson, Brad J and Kapa, Suraj and Friedman, Paul A},
	month = aug,
	year = {2019},
}

@article{coleman_efficient_1995a,
	title = {An {Efficient} {Trust} {Region} {Method} for {Unconstrained} {Discrete}-{Time} {Optimal} {Control} {Problems}},
	volume = {4},
	doi = {10/dxsjw9},
	number = {1},
	journal = {Computational Optimization and Applications},
	author = {Coleman, Thomas F and Liao, Aiping},
	year = {1995},
	note = {00041},
	pages = {47--66},
}

@article{dennisjr_adaptive_1981a,
	title = {An {Adaptive} {Nonlinear} {Least}-{Squares} {Algorithm}},
	volume = {7},
	number = {3},
	journal = {ACM Transactions on Mathematical Software (TOMS)},
	author = {Dennis Jr, John E and Gay, David M and Walsh, Roy E},
	year = {1981},
	note = {00000},
	keywords = {❓Multiple DOI},
	pages = {348--368},
}

@article{piroddi_identification_2003b,
	title = {An {Identification} {Algorithm} for {Polynomial} {NARX} {Models} {Based} on {Simulation} {Error} {Minimization}},
	volume = {76},
	doi = {10/cc8w93},
	number = {17},
	journal = {International Journal of Control},
	author = {Piroddi, Luigi and Spinelli, William},
	year = {2003},
	pages = {1767--1781},
}

@article{waltz_interior_2006,
	title = {An interior algorithm for nonlinear optimization that combines line search and trust region steps},
	volume = {107},
	url = {http://www.springerlink.com/index/7735777P133W3221.pdf},
	doi = {10.1007/s10107-004-0560-5},
	number = {3},
	urldate = {2017-08-20},
	journal = {Mathematical programming},
	author = {Waltz, Richard A. and Morales, José Luis and Nocedal, Jorge and Orban, Dominique},
	year = {2006},
	note = {00643},
	pages = {391--408},
}

@article{waltz_interior_2006a,
	title = {An {Interior} {Algorithm} for {Nonlinear} {Optimization} {That} {Combines} {Line} {Search} and {Trust} {Region} {Steps}},
	volume = {107},
	doi = {10/dp848r},
	number = {3},
	urldate = {2017-08-20},
	journal = {Mathematical programming},
	author = {Waltz, Richard A. and Morales, José Luis and Nocedal, Jorge and Orban, Dominique},
	year = {2006},
	pages = {391--408},
}

@article{vasconcelos_effective_2020,
	title = {An {Effective} {Anti}-{Aliasing} {Approach} for {Residual} {Networks}},
	abstract = {Image pre-processing in the frequency domain has traditionally played a vital role in computer vision and was even part of the standard pipeline in the early days of deep learning. However, with the advent of large datasets, many practitioners concluded that this was unnecessary due to the belief that these priors can be learned from the data itself. Frequency aliasing is a phenomenon that may occur when sub-sampling any signal, such as an image or feature map, causing distortion in the sub-sampled output. We show that we can mitigate this effect by placing non-trainable blur filters and using smooth activation functions at key locations, particularly where networks lack the capacity to learn them. These simple architectural changes lead to substantial improvements in out-of-distribution generalization on both image classification under natural corruptions on ImageNet-C [10] and few-shot learning on Meta-Dataset [17], without introducing additional trainable parameters and using the default hyper-parameters of open source codebases.},
	journal = {arXiv:2011.10675},
	author = {Vasconcelos, Cristina and Larochelle, Hugo and Dumoulin, Vincent and Roux, Nicolas Le and Goroshin, Ross},
	month = nov,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{rossler_equation_1976,
	title = {An equation for continuous chaos},
	volume = {57},
	issn = {0375-9601},
	url = {https://www.sciencedirect.com/science/article/pii/0375960176901018},
	doi = {10.1016/0375-9601(76)90101-8},
	abstract = {A prototype equation to the Lorenz model of turbulence contains just one (second-order) nonlinearity in one variable. The flow in state space allows for a “folded” Poincaré map (horseshoe map). Many more natural and artificial systems are governed by this type of equation.},
	language = {en},
	number = {5},
	urldate = {2021-02-22},
	journal = {Physics Letters A},
	author = {Rössler, O. E.},
	month = jul,
	year = {1976},
	pages = {397--398},
}

@article{bai_empirical_2018,
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks.},
	journal = {arXiv:1803.01271},
	author = {Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
	year = {2018},
	keywords = {🔍No DOI found},
}

@article{livne_aeroelasticity_2003,
	title = {Aeroelasticity of {Nonconventional} {Airplane} {Configurations}-{Past} and {Future}},
	volume = {40},
	url = {https://doi.org/10.2514/2.7217},
	doi = {10.2514/2.7217},
	number = {6},
	urldate = {2020-01-28},
	journal = {Journal of Aircraft},
	author = {Livne, E. and Weisshaar, Terrence A.},
	year = {2003},
	pages = {1047--1065},
}

@inproceedings{xie_adversarial_2020,
	title = {Adversarial examples improve image recognition},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition ({CVPR})},
	author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan L. and Le, Quoc V.},
	month = jun,
	year = {2020},
}

@incollection{liu_adaptive_2018,
	title = {Adaptive {Negative} {Curvature} {Descent} with {Applications} in {Non}-convex {Optimization}},
	url = {http://papers.nips.cc/paper/7734-adaptive-negative-curvature-descent-with-applications-in-non-convex-optimization.pdf},
	urldate = {2018-12-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Mingrui and Li, Zhe and Wang, Xiaoyu and Yi, Jinfeng and Yang, Tianbao},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {4858--4867},
}

@article{clifford_af_2017,
	title = {{AF} {Classification} from a {Short} {Single} {Lead} {ECG} {Recording}: the {PhysioNet}/{Computing} in {Cardiology} {Challenge} 2017},
	volume = {44},
	issn = {2325-8861},
	shorttitle = {{AF} {Classification} from a {Short} {Single} {Lead} {ECG} {Recording}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5978770/},
	abstract = {The PhysioNet/Computing in Cardiology (CinC) Challenge 2017 focused on differentiating AF from noise, normal or other rhythms in short term (from 9–61 s) ECG recordings performed by patients. A total of 12,186 ECGs were used: 8,528 in the public training set and 3,658 in the private hidden test set. Due to the high degree of inter-expert disagreement between a significant fraction of the expert labels we implemented a mid-competition bootstrap approach to expert relabeling of the data, levering the best performing Challenge entrants’ algorithms to identify contentious labels., A total of 75 independent teams entered the Challenge using a variety of traditional and novel methods, ranging from random forests to a deep learning approach applied to the raw data in the spectral domain. Four teams won the Challenge with an equal high F1 score (averaged across all classes) of 0.83, although the top 11 algorithms scored within 2\% of this. A combination of 45 algorithms identified using LASSO achieved an F1 of 0.87, indicating that a voting approach can boost performance.},
	urldate = {2018-10-21},
	journal = {Computing in Cardiology},
	author = {Clifford, Gari D and Liu, Chengyu and Moody, Benjamin and Lehman, Li-wei H. and Silva, Ikaro and Li, Qiao and Johnson, A E and Mark, Roger G.},
	month = sep,
	year = {2017},
	pmid = {29862307},
	pmcid = {PMC5978770},
}

@inproceedings{kingma_adam_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	booktitle = {Proceedings of the 3rd {International} {Conference} for {Learning} {Representations} ({ICLR})},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Learning, 🔍No DOI found},
}

@article{powell_view_2007,
	title = {A view of algorithms for optimization without derivatives},
	volume = {43},
	issn = {1361-2042},
	number = {5},
	journal = {Mathematics Today-Bulletin of the Institute of Mathematics and its Applications},
	author = {Powell, Michael JD},
	year = {2007},
	note = {00000},
	keywords = {🔍No DOI found},
	pages = {170--174},
}

@article{mohamed_acoustic_2012,
	title = {Acoustic modeling using deep belief networks},
	volume = {20},
	doi = {10.1109/TASL.2011.2109382},
	number = {1},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Mohamed, Abdel-rahman and Dahl, George E. and Hinton, Geoffrey},
	year = {2012},
	pages = {14--22},
}

@book{stevens_advanced_2013,
	series = {Addison-{Wesley} {Professional} {Computing} {Series}},
	title = {Advanced {Programming} in the {UNIX} {Environment}},
	isbn = {978-0-321-63800-7},
	url = {https://books.google.com.br/books?id=kCTMFpEcIOwC},
	publisher = {Pearson Education},
	author = {Stevens, W.R. and Rago, S.A.},
	year = {2013},
	note = {00000},
}

@inproceedings{chiarugi_adaptive_2007,
	title = {Adaptive threshold {QRS} detector with best channel selection based on a noise rating system},
	doi = {10.1109/CIC.2007.4745445},
	abstract = {QRS detection performance can depend on the type of noise present in each lead involved in the overall processing. A common approach to QRS detection is based on a QRS enhanced signal obtained from the derivatives of the pre-filtered leads. However, the signal pre-filtering cannot be able to perform a complete noise rejection and the use of derivatives can enhance the noise as well. In many cases the noise occurs only on one lead and the addition of a noisy lead to the QRS enhanced signal decreases the overall detection performances of the QRS detector. For this reason the noise estimation on each channel, providing information for the channel inclusion or rejection in building the QRS enhanced signal, can improve the overall performances of the QRS detector. The results have been evaluated on the 48 records of the MIT-BIH Arrhythmia Database where each ECG record is composed by 2 leads sampled at 360 Hz for a total duration of about 30 minutes. The annotated QRSs are 109494 in total. The results have been very satisfying on all the annotated QRSs and, with the inclusion of an automatic criterion for ventricular flutter detection, a sensitivity=99.76\% and a positive predictive value=99.81\% have been obtained.},
	booktitle = {2007 {Computers} in {Cardiology}},
	author = {Chiarugi, F. and Sakkalis, V. and Emmanouilidou, D. and Krontiris, T. and Varanini, M. and Tollis, I.},
	month = sep,
	year = {2007},
	keywords = {Band pass filters, Cardiology, Computer science, Databases, Detectors, ECG signal recording, Frequency, MIT-BIH arrhythmia database, Noise level, Noise reduction, QRS detection algorithm, QRS enhanced signal, adaptive threshold QRS detector, electrocardiogram, electrocardiography, filtering theory, frequency 360 Hz, medical information systems, medical signal detection, medical signal processing, noise rating system, nonlinear filters, prefiltered signal, signal noise estimation, time 30 min, ventricular flutter detection},
	pages = {157--160},
}

@book{savitch_absolute_2013,
	address = {Boston},
	edition = {5th ed},
	title = {Absolute {Java}: {Walter} {Savitch} ; contributor, {Kenrick} {Mock}},
	isbn = {978-0-13-283031-7},
	shorttitle = {Absolute {Java}},
	publisher = {Addison-Wesley},
	author = {Savitch, Walter J. and Mock, Kenrick},
	year = {2013},
	keywords = {Java (Computer program language)},
}

@book{kilts_advanced_2007,
	address = {Hoboken, N.J},
	title = {Advanced {FPGA} design: architecture, implementation, and optimization},
	isbn = {978-0-470-05437-6},
	shorttitle = {Advanced {FPGA} design},
	publisher = {Wiley : IEEE},
	author = {Kilts, Steve},
	year = {2007},
	note = {OCLC: ocm72799161},
	keywords = {Design and construction, Field programmable gate arrays},
}

@article{mant_accuracy_2007,
	title = {Accuracy of diagnosing atrial fibrillation on electrocardiogram by primary care practitioners and interpretative diagnostic software: analysis of data from screening for atrial fibrillation in the elderly ({SAFE}) trial},
	volume = {335},
	issn = {1756-1833},
	shorttitle = {Accuracy of diagnosing atrial fibrillation on electrocardiogram by primary care practitioners and interpretative diagnostic software},
	doi = {10.1136/bmj.39227.551713.AE},
	abstract = {OBJECTIVE: To assess the accuracy of general practitioners, practice nurses, and interpretative software in the use of different types of electrocardiogram to diagnose atrial fibrillation.
DESIGN: Prospective comparison with reference standard of assessment of electrocardiograms by two independent specialists.
SETTING: 49 general practices in central England.
PARTICIPANTS: 2595 patients aged 65 or over screened for atrial fibrillation as part of the screening for atrial fibrillation in the elderly (SAFE) study; 49 general practitioners and 49 practice nurses.
INTERVENTIONS: All electrocardiograms were read with the Biolog interpretative software, and a random sample of 12 lead, limb lead, and single lead thoracic placement electrocardiograms were assessed by general practitioners and practice nurses independently of each other and of the Biolog assessment.
MAIN OUTCOME MEASURES: Sensitivity, specificity, and positive and negative predictive values.
RESULTS: General practitioners detected 79 out of 99 cases of atrial fibrillation on a 12 lead electrocardiogram (sensitivity 80\%, 95\% confidence interval 71\% to 87\%) and misinterpreted 114 out of 1355 cases of sinus rhythm as atrial fibrillation (specificity 92\%, 90\% to 93\%). Practice nurses detected a similar proportion of cases of atrial fibrillation (sensitivity 77\%, 67\% to 85\%), but had a lower specificity (85\%, 83\% to 87\%). The interpretative software was significantly more accurate, with a specificity of 99\%, but missed 36 of 215 cases of atrial fibrillation (sensitivity 83\%). Combining general practitioners' interpretation with the interpretative software led to a sensitivity of 92\% and a specificity of 91\%. Use of limb lead or single lead thoracic placement electrocardiograms resulted in some loss of specificity.
CONCLUSIONS: Many primary care professionals cannot accurately detect atrial fibrillation on an electrocardiogram, and interpretative software is not sufficiently accurate to circumvent this problem, even when combined with interpretation by a general practitioner. Diagnosis of atrial fibrillation in the community needs to factor in the reading of electrocardiograms by appropriately trained people.},
	language = {eng},
	number = {7616},
	journal = {BMJ (Clinical research ed.)},
	author = {Mant, Jonathan and Fitzmaurice, David A. and Hobbs, F. D. Richard and Jowett, Sue and Murray, Ellen T. and Holder, Roger and Davies, Michael and Lip, Gregory Y. H.},
	month = aug,
	year = {2007},
	pmid = {17604299},
	pmcid = {PMC1952490},
	keywords = {Aged, Atrial Fibrillation, Clinical Competence, Diagnosis, Computer-Assisted, Electrocardiography, England, Family Practice, Humans, Nurse Practitioners, Predictive Value of Tests, Prospective Studies},
	pages = {380},
}

@article{saad_adaptive_1994,
	title = {Adaptive robot control using neural networks},
	volume = {41},
	doi = {10.1109/41.293877},
	number = {2},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {Saad, Maarouf and Bigras, Pascal and Dessaint, L-A and Al-Haddad, Kamal},
	year = {1994},
	note = {00000},
	pages = {173--181},
}

@inproceedings{hirschmuller_accurate_2005,
	title = {Accurate and efficient stereo processing by semi-global matching and mutual information},
	volume = {2},
	booktitle = {Computer {Vision} and {Pattern} {Recognition}, 2005. {CVPR} 2005. {IEEE} {Computer} {Society} {Conference} on},
	publisher = {IEEE},
	author = {Hirschmüller, Heiko},
	year = {2005},
	pages = {807--814},
}

@article{cheng_adaptive_2009,
	title = {Adaptive neural network tracking control for manipulators with uncertain kinematics, dynamics and actuator model},
	volume = {45},
	issn = {0005-1098},
	url = {http://www.sciencedirect.com/science/article/pii/S0005109809002982},
	doi = {10/cghh6t},
	abstract = {A neural-network-based adaptive controller is proposed for the tracking problem of manipulators with uncertain kinematics, dynamics and actuator model. The adaptive Jacobian scheme is used to estimate the unknown kinematics parameters. Uncertainties in the manipulator dynamics and actuator model are compensated by three-layer neural networks. External disturbances and approximation errors are counteracted by robust signals. The actuator controller is designed based on the backstepping scheme. Compared with the existing work, the proposed method considers the manipulator kinematics uncertainty, does not need the “linearity-in-parameters” assumption for the uncertain terms in the dynamics of manipulator and actuator, and guarantees the tracking error to be as small as desired. Finally, the performance of the proposed approach is illustrated by the simulation example.},
	number = {10},
	journal = {Automatica},
	author = {Cheng, Long and Hou, Zeng-Guang and Tan, Min},
	month = oct,
	year = {2009},
	keywords = {Manipulators, Neural networks, Tracking, Uncertainty},
	pages = {2312--2318},
}

@article{cantelmo_adaptive_2010,
	title = {Adaptive model selection for polynomial {NARX} models},
	volume = {4},
	issn = {1751-8644, 1751-8652},
	url = {http://digital-library.theiet.org/content/journals/10.1049/iet-cta.2009.0581},
	doi = {10.1049/iet-cta.2009.0581},
	language = {en},
	number = {12},
	urldate = {2017-09-13},
	journal = {IET Control Theory \& Applications},
	author = {Cantelmo, C. and Piroddi, L.},
	month = dec,
	year = {2010},
	pages = {2693--2706},
}

@techreport{widrow_adaptive_1960,
	title = {Adaptive switching circuits},
	institution = {Stanford Univ Ca Stanford Electronics Labs},
	author = {Widrow, Bernard and Hoff, Marcian E},
	year = {1960},
}

@article{ilyas_adversarial_2019,
	title = {Adversarial {Examples} {Are} {Not} {Bugs}, {They} {Are} {Features}},
	volume = {32},
	url = {http://arxiv.org/abs/1905.02175},
	abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.},
	urldate = {2020-05-18},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
	year = {2019},
	note = {arXiv: 1905.02175},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{dummit_abstract_,
	title = {Abstract {Algebra}},
	author = {Dummit, David S. and Foote, Richard M.},
}

@article{fox_tutorial_2012,
	title = {A tutorial on variational {Bayesian} inference},
	volume = {38},
	issn = {0269-2821, 1573-7462},
	url = {http://link.springer.com/10.1007/s10462-011-9236-8},
	doi = {10/fd6kdm},
	abstract = {This tutorial describes the mean-ﬁeld variational Bayesian approximation to inference in graphical models, using modern machine learning terminology rather than statistical physics concepts. It begins by seeking to ﬁnd an approximate mean-ﬁeld distribution close to the target joint in the KL-divergence sense. It then derives local node updates and reviews the recent Variational Message Passing framework.},
	language = {en},
	number = {2},
	urldate = {2018-12-05},
	journal = {Artificial Intelligence Review},
	author = {Fox, Charles W. and Roberts, Stephen J.},
	month = aug,
	year = {2012},
	pages = {85--95},
}

@article{doucet_tutorial_2009,
	title = {A tutorial on particle filtering and smoothing: {Fifteen} years later},
	volume = {12},
	number = {656-704},
	journal = {Handbook of nonlinear filtering},
	author = {Doucet, Arnaud and Johansen, Adam M},
	year = {2009},
	keywords = {⛔ No DOI found},
	pages = {3},
}

@article{recht_tour_2018,
	title = {A {Tour} of {Reinforcement} {Learning}: {The} {View} from {Continuous} {Control}},
	shorttitle = {A {Tour} of {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/1806.09460},
	language = {en},
	urldate = {2018-09-19},
	author = {Recht, Benjamin},
	month = jun,
	year = {2018},
	note = {00000},
	keywords = {🔍No DOI found},
}

@article{rabiner_tutorial_1989,
	title = {A tutorial on hidden {Markov} models and selected applications in speech recognition},
	volume = {77},
	issn = {0018-9219},
	doi = {10.1109/5.18626},
	abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
	number = {2},
	journal = {Proceedings of the IEEE},
	author = {Rabiner, L. R.},
	month = feb,
	year = {1989},
	note = {00000},
	keywords = {Distortion, Hidden Markov models, Markov processes, Mathematical model, Multiple signal classification, Statistical analysis, Temperature measurement, Tutorial, balls-in-urns system, coin-tossing, discrete Markov chains, ergodic models, hidden states, left-right models, probabilistic function, signal processing, speech recognition, stochastic processes},
	pages = {257--286},
}

@article{vanoverschee_unifying_1995,
	series = {Trends in {System} {Identification}},
	title = {A unifying theorem for three subspace system identification algorithms},
	volume = {31},
	issn = {0005-1098},
	url = {http://www.sciencedirect.com/science/article/pii/0005109895000720},
	doi = {10.1016/0005-1098(95)00072-0},
	abstract = {The aim of this paper is to indicate and explore the similarities between three different subspace algorithms for the identification of combined deterministic-stochastic systems. The similarities between these algorithms have been obscured, due to different notations and backgrounds. It is shown that all three algorithms are special cases of one unifying theorem. The comparison also reveals that the three algorithms use exactly the same subspace to determine the order and the extended observability matrix, but that the weighting matrix, used to calculate a basis for the column space of the observability matrix is different in the three cases.},
	number = {12},
	journal = {Automatica},
	author = {Van Overschee, Peter and De Moor, Bart},
	month = dec,
	year = {1995},
	note = {00000},
	keywords = {Kalman filters, Multivariable systems, difference equations, linear algebra, state-space methods, stochastic systems, subspace methods, system identification},
	pages = {1853--1864},
}

@article{tsai_versatile_1987,
	title = {A versatile camera calibration technique for high-accuracy {3D} machine vision metrology using off-the-shelf {TV} cameras and lenses},
	volume = {3},
	doi = {10.1109/JRA.1987.1087109},
	number = {4},
	journal = {Robotics and Automation, IEEE Journal of},
	author = {Tsai, Roger Y},
	year = {1987},
	note = {00000},
	pages = {323--344},
}

@article{byrd_trust_2000,
	title = {A trust region method based on interior point techniques for nonlinear programming},
	volume = {89},
	issn = {0025-5610},
	doi = {10.1007/PL00011391},
	number = {1},
	journal = {Mathematical Programming},
	author = {Byrd, Richard H and Gilbert, Jean Charles and Nocedal, Jorge},
	year = {2000},
	pages = {149--185},
}

@article{byrd_trust_1987,
	title = {A trust region algorithm for nonlinearly constrained optimization},
	volume = {24},
	doi = {10.1137/0724076},
	number = {5},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Byrd, Richard H and Schnabel, Robert B and Shultz, Gerald A},
	year = {1987},
	pages = {1152--1170},
}

@article{smola_tutorial_2004,
	title = {A tutorial on support vector regression},
	volume = {14},
	issn = {0960-3174},
	doi = {10.1023/B:STCO.0000035301.49549.88},
	number = {3},
	journal = {Statistics and computing},
	author = {Smola, Alex J and Schölkopf, Bernhard},
	year = {2004},
	note = {00000},
	pages = {199--222},
}

@article{zhang_trust_2010,
	title = {A trust region method in adaptive finite element framework for bioluminescence tomography},
	volume = {18},
	doi = {10.1364/OE.18.006477},
	number = {7},
	journal = {Optics express},
	author = {Zhang, Bo and Yang, Xin and Qin, Chenghu and Liu, Dan and Zhu, Shouping and Feng, Jinchao and Sun, Li and Liu, Kai and Han, Dong and Ma, Xibo and {others}},
	year = {2010},
	note = {00029},
	pages = {6477--6491},
}

@article{sheng_trust_2016,
	title = {A trust region {SQP} method for coordinated voltage control in smart distribution grid},
	volume = {7},
	doi = {10.1109/TSG.2014.2376197},
	number = {1},
	journal = {IEEE Transactions on Smart Grid},
	author = {Sheng, Wanxing and Liu, Ke-Yan and Cheng, Sheng and Meng, Xiaoli and Dai, Wei},
	year = {2016},
	note = {00000},
	pages = {381--391},
}

@article{scharstein_taxonomy_2002,
	title = {A taxonomy and evaluation of dense two-frame stereo correspondence algorithms},
	volume = {47},
	number = {1-3},
	journal = {International journal of computer vision},
	author = {Scharstein, Daniel and Szeliski, Richard},
	year = {2002},
	note = {00000},
	keywords = {🔍No DOI found},
	pages = {7--42},
}

@article{plantenga_trust_1998,
	title = {A trust region method for nonlinear programming based on primal interior-point techniques},
	volume = {20},
	url = {http://epubs.siam.org/doi/abs/10.1137/S1064827595284403},
	doi = {10.1137/S1064827595284403},
	number = {1},
	urldate = {2017-08-20},
	journal = {SIAM journal on Scientific Computing},
	author = {Plantenga, Todd},
	year = {1998},
	note = {00000},
	pages = {282--305},
}

@article{byrd_trust_2000a,
	title = {A {Trust} {Region} {Method} {Based} on {Interior} {Point} {Techniques} for {Nonlinear} {Programming}},
	volume = {89},
	issn = {0025-5610},
	doi = {10/dmkzpw},
	number = {1},
	journal = {Mathematical Programming},
	author = {Byrd, Richard H and Gilbert, Jean Charles and Nocedal, Jorge},
	year = {2000},
	pages = {149--185},
}

@article{plantenga_trust_1998a,
	title = {A {Trust} {Region} {Method} for {Nonlinear} {Programming} {Based} on {Primal} {Interior}-{Point} {Techniques}},
	volume = {20},
	doi = {10/cg4zbf},
	number = {1},
	urldate = {2017-08-20},
	journal = {SIAM journal on Scientific Computing},
	author = {Plantenga, Todd},
	year = {1998},
	pages = {282--305},
}

@article{rani_systematic_2018,
	title = {A {Systematic} {Review} of {Compressive} {Sensing}: {Concepts}, {Implementations} and {Applications}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {A {Systematic} {Review} of {Compressive} {Sensing}},
	doi = {10.1109/ACCESS.2018.2793851},
	abstract = {Compressive Sensing (CS) is a new sensing modality, which compresses the signal being acquired at the time of sensing. Signals can have sparse or compressible representation either in original domain or in some transform domain. Relying on the sparsity of the signals, CS allows us to sample the signal at a rate much below the Nyquist sampling rate. Also, the varied reconstruction algorithms of CS can faithfully reconstruct the original signal back from fewer compressive measurements. This fact has stimulated research interest toward the use of CS in several fields, such as magnetic resonance imaging, high-speed video acquisition, and ultrawideband communication. This paper reviews the basic theoretical concepts underlying CS. To bridge the gap between theory and practicality of CS, different CS acquisition strategies and reconstruction approaches are elaborated systematically in this paper. The major application areas where CS is currently being used are reviewed here. This paper also highlights some of the challenges and research directions in this field.},
	journal = {IEEE Access},
	author = {Rani, Meenu and Dhok, S. B. and Deshmukh, R. B.},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {CS acquisition strategies, CS applications, CS reconstruction algorithms, Compressed sensing, Compressive sensing, Image reconstruction, Mathematical model, Nyquist sampling rate, OMP, Reconstruction algorithms, Sensors, Sparse matrices, Transforms, compressed sensing, compressible representation, compressive measurements, compressive sensing, random demodulator, sensing modality, signal reconstruction, signal representation, signal sampling, sparse representation, sparsity, systematic review, transform domain, transforms, varied reconstruction algorithms},
	pages = {4875--4894},
}

@article{buda_systematic_2018,
	title = {A systematic study of the class imbalance problem in convolutional neural networks},
	volume = {106},
	issn = {08936080},
	url = {http://arxiv.org/abs/1710.05381},
	doi = {10/gfbfz3},
	abstract = {In this study, we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks (CNNs) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchmark datasets of increasing complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue: oversampling, undersampling, two-phase training, and thresholding that compensates for prior class probabilities. Our main evaluation metric is area under the receiver operating characteristic curve (ROC AUC) adjusted to multi-class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. Based on results from our experiments we conclude that (i) the effect of class imbalance on classification performance is detrimental; (ii) the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling; (iii) oversampling should be applied to the level that completely eliminates the imbalance, whereas the optimal undersampling ratio depends on the extent of imbalance; (iv) as opposed to some classical machine learning models, oversampling does not cause overfitting of CNNs; (v) thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest.},
	urldate = {2019-01-28},
	journal = {Neural Networks},
	author = {Buda, Mateusz and Maki, Atsuto and Mazurowski, Maciej A.},
	month = oct,
	year = {2018},
	note = {arXiv: 1710.05381},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	pages = {249--259},
}

@article{luo_review_2010,
	title = {A review of electrocardiogram filtering},
	volume = {43},
	issn = {0022-0736},
	url = {http://www.sciencedirect.com/science/article/pii/S0022073610002852},
	doi = {10/fp6hfc},
	abstract = {Analog filtering and digital signal processing algorithms in the preprocessing modules of an electrocardiographic device play a pivotal role in providing high-quality electrocardiogram (ECG) signals for analysis, interpretation, and presentation (display, printout, and storage). In this article, issues relating to inaccuracy of ECG preprocessing filters are investigated in the context of facilitating efficient ECG interpretation and diagnosis. The discussion covers 4 specific ECG preprocessing applications: anti-aliasing and upper-frequency cutoff, baseline wander suppression and lower-frequency cutoff, line frequency rejection, and muscle artifact reduction. Issues discussed include linear phase, aliasing, distortion, ringing, and attenuation of desired ECG signals. Due to the overlapping power spectrum of signal and noise in acquired ECG data, frequency selective filters must seek a delicate balance between noise removal and deformation of the desired signal. Most importantly, the filtering output should not adversely impact subsequent diagnosis and interpretation. Based on these discussions, several suggestions are made to improve and update existing ECG data preprocessing standards and guidelines.},
	number = {6},
	journal = {Journal of Electrocardiology},
	author = {Luo, Shen and Johnston, Paul},
	month = nov,
	year = {2010},
	keywords = {Aliasing, ECG filter, ECG standards, Frequency domain, Linear phase, Magnitude distortion, Ringing artifact},
	pages = {486--496},
}

@article{maddox_simple_2019,
	title = {A {Simple} {Baseline} for {Bayesian} {Uncertainty} in {Deep} {Learning}},
	url = {https://arxiv.org/abs/1902.02476v1},
	abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose
approach for uncertainty representation and calibration in deep learning.
Stochastic Weight Averaging (SWA), which computes the first moment of
stochastic gradient descent (SGD) iterates with a modified learning rate
schedule, has recently been shown to improve generalization in deep learning.
With SWAG, we fit a Gaussian using the SWA solution as the first moment and a
low rank plus diagonal covariance also derived from the SGD iterates, forming
an approximate posterior distribution over neural network weights; we then
sample from this Gaussian distribution to perform Bayesian model averaging. We
empirically find that SWAG approximates the shape of the true posterior, in
accordance with results describing the stationary distribution of SGD iterates.
Moreover, we demonstrate that SWAG performs well on a wide variety of computer
vision tasks, including out of sample detection, calibration, and transfer
learning, in comparison to many popular alternatives including MC dropout, KFAC
Laplace, and temperature scaling.},
	language = {en},
	urldate = {2019-02-14},
	author = {Maddox, Wesley and Garipov, Timur and Izmailov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
	month = feb,
	year = {2019},
}

@article{laurent_recurrent_2016,
	title = {A recurrent neural network without chaos},
	url = {http://arxiv.org/abs/1612.06212},
	abstract = {We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.},
	urldate = {2019-03-08},
	journal = {arXiv:1612.06212 [cs]},
	author = {Laurent, Thomas and von Brecht, James},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.06212},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{voglis_rectangular_2004,
	title = {A {Rectangular} {Trust} {Region} {Dogleg} {Approach} for {Unconstrained} and {Bound} {Constrained} {Nonlinear} {Optimization}},
	booktitle = {{WSEAS} {Conference}},
	author = {Voglis, C and Lagaris, IE},
	year = {2004},
	note = {00000},
	pages = {17--19},
}

@article{tikk_survey_2003,
	title = {A survey on universal approximation and its limits in soft computing techniques},
	volume = {33},
	doi = {10.1016/S0888-613X(03)00021-5},
	number = {2},
	journal = {International Journal of Approximate Reasoning},
	author = {Tikk, Domonkos and Kóczy, László T and Gedeon, Tamás D},
	year = {2003},
	note = {00000},
	pages = {185--202},
}

@article{maros_repository_1999,
	title = {A repository of convex quadratic programming problems},
	volume = {11},
	url = {http://www.tandfonline.com/doi/abs/10.1080/10556789908805768},
	doi = {10.1080/10556789908805768},
	number = {1-4},
	urldate = {2017-08-20},
	journal = {Optimization Methods and Software},
	author = {Maros, Istvan and Mészáros, Csaba},
	year = {1999},
	pages = {671--681},
}

@article{frank_statistical_1993,
	title = {A {Statistical} {View} of {Some} {Chemometrics} {Regression} {Tools}},
	volume = {35},
	issn = {00401706},
	url = {http://www.jstor.org/stable/1269656?origin=crossref},
	doi = {10.2307/1269656},
	number = {2},
	urldate = {2017-09-08},
	journal = {Technometrics},
	author = {Frank, Ildiko E. and Friedman, Jerome H.},
	month = may,
	year = {1993},
	pages = {109},
}

@article{branch_subspace_1999,
	title = {A subspace, interior, and conjugate gradient method for large-scale bound-constrained minimization problems},
	volume = {21},
	doi = {10.1137/S1064827595289108},
	number = {1},
	journal = {SIAM Journal on Scientific Computing},
	author = {Branch, Mary Ann and Coleman, Thomas F and Li, Yuying},
	year = {1999},
	pages = {1--23},
}

@article{bert_phantom_2005,
	title = {A phantom evaluation of a stereo-vision surface imaging system for radiotherapy patient setup},
	volume = {32},
	doi = {10.1118/1.1984263},
	number = {9},
	journal = {Medical physics},
	author = {Bert, Christoph and Metheany, Katherine G and Doppke, Karen and Chen, George TY},
	year = {2005},
	pages = {2753--2762},
}

@article{kohl_probabilistic_2018,
	title = {A {Probabilistic} {U}-{Net} for {Segmentation} of {Ambiguous} {Images}},
	url = {http://arxiv.org/abs/1806.05034},
	abstract = {Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional variational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.},
	urldate = {2018-11-26},
	journal = {arXiv:1806.05034 [cs, stat]},
	author = {Kohl, Simon A. A. and Romera-Paredes, Bernardino and Meyer, Clemens and De Fauw, Jeffrey and Ledsam, Joseph R. and Maier-Hein, Klaus H. and Eslami, S. M. Ali and Rezende, Danilo Jimenez and Ronneberger, Olaf},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.05034},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{hassani_survey_2014,
	title = {A survey on hysteresis modeling, identification and control},
	volume = {49},
	issn = {0888-3270},
	url = {http://www.sciencedirect.com/science/article/pii/S0888327014001186},
	doi = {10.1016/j.ymssp.2014.04.012},
	number = {1},
	journal = {Mechanical Systems and Signal Processing},
	author = {Hassani, Vahid and Tjahjowidodo, Tegoeh and Do, Thanh Nho},
	month = dec,
	year = {2014},
	keywords = {Hysteresis characterization, Hysteresis control, Hysteresis nonlinearity, Parameter estimation, Smart materials, system identification},
	pages = {209--233},
}

@article{arreckx_regularized_2018,
	title = {A {Regularized} {Factorization}-{Free} {Method} for {Equality}-{Constrained} {Optimization}},
	volume = {28},
	issn = {1052-6234, 1095-7189},
	url = {https://epubs.siam.org/doi/10.1137/16M1088570},
	doi = {10.1137/16M1088570},
	language = {en},
	number = {2},
	urldate = {2018-05-23},
	journal = {SIAM Journal on Optimization},
	author = {Arreckx, Sylvain and Orban, Dominique},
	month = jan,
	year = {2018},
	pages = {1613--1639},
}

@article{kraft_software_1988,
	title = {A software package for sequential quadratic programming},
	issn = {0171-1342},
	journal = {Forschungsbericht- Deutsche Forschungs- und Versuchsanstalt fur Luft- und Raumfahrt},
	author = {Kraft, Dieter},
	year = {1988},
	keywords = {🔍No DOI found},
}

@article{nelder_simplex_1965,
	title = {A simplex method for function minimization},
	volume = {7},
	issn = {0010-4620},
	doi = {10.1093/comjnl/7.4.308},
	number = {4},
	journal = {The computer journal},
	author = {Nelder, John A and Mead, Roger},
	year = {1965},
	pages = {308--313},
}

@inproceedings{yuan_review_2000,
	title = {A review of trust region algorithms for optimization},
	volume = {99},
	url = {ftp://ftp.cc.ac.cn/pub/yyx/papers/p995.pdf},
	urldate = {2017-09-11},
	booktitle = {{ICIAM}},
	author = {Yuan, Ya-xiang},
	year = {2000},
	note = {00182},
	pages = {271--282},
}

@article{lin_structured_2017,
	title = {A {Structured} {Self}-attentive {Sentence} {Embedding}},
	url = {http://arxiv.org/abs/1703.03130},
	abstract = {This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.},
	urldate = {2019-05-29},
	journal = {arXiv:1703.03130 [cs]},
	author = {Lin, Zhouhan and Feng, Minwei and Santos, Cicero Nogueira dos and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.03130},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{kamaleswaran_robust_2018,
	title = {A robust deep convolutional neural network for the classification of abnormal cardiac rhythm using single lead electrocardiograms of variable length},
	volume = {39},
	issn = {1361-6579},
	doi = {10/gf2594},
	abstract = {OBJECTIVE: Atrial fibrillation (AF) is a major cause of hospitalization and death in the United States. Moreover, as the average age of individuals increases around the world, early detection and diagnosis of AF become even more pressing. In this paper, we introduce a novel deep learning architecture for the detection of normal sinus rhythm, AF, other abnormal rhythms, and noise.
APPROACH: We have demonstrated through a systematic approach many hyperparameters, input sets, and optimization methods that yielded influence in both training time and performance accuracy. We have focused on these properties to identify an optimal 13-layer convolutional neural network (CNN) model which was trained on 8528 short single-lead ECG recordings and evaluated on a test dataset of 3658 recordings.
MAIN RESULTS: The proposed CNN architecture achieved a state-of-the-art performance in identifying normal, AF and other rhythms with an average F 1-score of 0.83.
SIGNIFICANCE: We have presented a robust deep learning-based architecture that can identify abnormal cardiac rhythms using short single-lead ECG recordings. The proposed architecture is computationally fast and can also be used in real-time cardiac arrhythmia detection applications.},
	language = {eng},
	number = {3},
	journal = {Physiological Measurement},
	author = {Kamaleswaran, Rishikesan and Mahajan, Ruhi and Akbilgic, Oguz},
	year = {2018},
	pmid = {29369044},
	keywords = {Arrhythmias, Cardiac, Electrocardiography, Heart Rate, Humans, Neural Networks (Computer), Signal Processing, Computer-Assisted},
	pages = {035006},
}

@article{maros_repository_1999a,
	title = {A {Repository} of {Convex} {Quadratic} {Programming} {Problems}},
	volume = {11},
	doi = {10/cvxwrj},
	number = {1-4},
	urldate = {2017-08-20},
	journal = {Optimization Methods and Software},
	author = {Maros, Istvan and Mészáros, Csaba},
	year = {1999},
	pages = {671--681},
}

@incollection{lukosevicius_practical_2012,
	address = {Berlin, Heidelberg},
	title = {A {Practical} {Guide} to {Applying} {Echo} {State} {Networks}},
	volume = {7700},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	url = {http://link.springer.com/10.1007/978-3-642-35289-8_36},
	abstract = {Reservoir computing has emerged in the last decade as an alternative to gradient descent methods for training recurrent neural networks. Echo State Network (ESN) is one of the key reservoir computing “ﬂavors”. While being practical, conceptually simple, and easy to implement, ESNs require some experience and insight to achieve the hailed good performance in many tasks. Here we present practical techniques and recommendations for successfully applying ESNs, as well as some more advanced application-speciﬁc modiﬁcations.},
	language = {en},
	urldate = {2021-03-30},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lukoševičius, Mantas},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_36},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {659--686},
}

@article{peche_note_2019,
	title = {A note on the {Pennington}-{Worah} distribution},
	volume = {24},
	issn = {1083-589X},
	url = {https://projecteuclid.org/journals/electronic-communications-in-probability/volume-24/issue-none/A-note-on-the-Pennington-Worah-distribution/10.1214/19-ECP262.full},
	doi = {10.1214/19-ECP262},
	abstract = {This paper is concerned with a new expression of the so-called Pennington-Worah distribution, characterizing the asymptotic empirical eigenvalue distribution of some non linear random matrix ensembles.},
	language = {en},
	number = {none},
	urldate = {2021-06-24},
	journal = {Electronic Communications in Probability},
	author = {Péché, S.},
	month = jan,
	year = {2019},
}

@inproceedings{zhang_new_2014,
	title = {A new battery modelling method based on simulation error minimization},
	booktitle = {2014 {IEEE} {PES} {General} {Meeting}},
	author = {Zhang, Cheng and Li, Kang and Yang, Zhile and Pei, Lei and Zhu, Chunbo},
	year = {2014},
}

@article{haagerup_new_2005,
	title = {A new application of random matrices: {Ext}({C} $_{\textrm{red}}$ $^{\textrm{∗}}$ ({F} $_{\textrm{2}}$ )) is not a group},
	volume = {162},
	issn = {0003-486X},
	shorttitle = {A new application of random matrices},
	url = {http://annals.math.princeton.edu/2005/162-2/p03},
	doi = {10.4007/annals.2005.162.711},
	language = {en},
	number = {2},
	urldate = {2020-12-21},
	journal = {Annals of Mathematics},
	author = {Haagerup, Uffe and Thorbjørnsen, Steen},
	month = sep,
	year = {2005},
	pages = {711--775},
}

@article{tiso_new_2017,
	series = {Recent advances in nonlinear system identification},
	title = {A new, challenging benchmark for nonlinear system identification},
	volume = {84},
	issn = {0888-3270},
	url = {http://www.sciencedirect.com/science/article/pii/S0888327016302837},
	doi = {10.1016/j.ymssp.2016.08.008},
	abstract = {The progress accomplished during the past decade in nonlinear system identification in structural dynamics is considerable. The objective of the present paper is to consolidate this progress by challenging the community through a new benchmark structure exhibiting complex nonlinear dynamics. The proposed structure consists of two offset cantilevered beams connected by a highly flexible element. For increasing forcing amplitudes, the system sequentially features linear behaviour, localised nonlinearity associated with the buckling of the connecting element, and distributed nonlinearity resulting from large elastic deformations across the structure. A finite element-based code with time integration capabilities is made available at https://sem.org/nonlinear-systems-imac-focus-group/. This code permits the numerical simulation of the benchmark dynamics in response to arbitrary excitation signals.},
	journal = {Mechanical Systems and Signal Processing},
	author = {Tiso, Paolo and Noël, Jean-Philippe},
	month = feb,
	year = {2017},
	note = {00000},
	keywords = {Benchmark, Buckling, Distributed nonlinearity, Large displacements, Localised nonlinearity, Nonlinear system identification, Structural dynamics},
	pages = {185--193},
}

@inproceedings{wang_new_2017,
	title = {A new concept using {LSTM} {Neural} {Networks} for dynamic system identification},
	doi = {10.23919/ACC.2017.7963782},
	abstract = {Recently, Recurrent Neural Network becomes a very popular research topic in machine learning field. Many new ideas and RNN structures have been generated by different authors, including long short term memory (LSTM) RNN and Gated Recurrent United (GRU) RNN ([1],[2]), a number of applications have also been developed among various research labs or industrial companies ([3]-[5]). Most of these schemes, however, are only applicable to machine learning problems, or static systems in control field. In this paper, a new concept of applying one of the most popular RNN approach - LSTM to identify and control dynamic system is to be investigated. Both identification (or learning) dynamic system and design of controller based on identification are going to be discussed. Also, a new concept of using a convex-based LSTM networks for fast learning purpose will be explained in detail. Simulation studies will be presented to demonstrated the new LSTM structure performs much better than conventional RNN and even single LSTM network.},
	booktitle = {2017 {American} {Control} {Conference} ({ACC})},
	author = {Wang, Yu},
	month = may,
	year = {2017},
	note = {00011},
	keywords = {Adaptation models, Control systems, LSTM neural networks, Logic gates, Periodic structures, Recurrent neural networks, Training, convex-based LSTM networks, dynamic system identification, gated recurrent united RNN, learning (artificial intelligence), long short term memory RNN, machine learning, recurrent neural nets, recurrent neural network},
	pages = {5324--5329},
}

@inproceedings{wang_new_2002,
	title = {A new method for evaluating {ECG} signal quality for multi-lead arrhythmia analysis},
	booktitle = {Computers in {Cardiology}, 2002},
	publisher = {IEEE},
	author = {Wang, J. Y.},
	year = {2002},
	note = {00031},
	pages = {85--88},
}

@article{akaike_new_1974,
	title = {A new look at the statistical model identification},
	volume = {19},
	issn = {0018-9286},
	doi = {10.1109/TAC.1974.1100705},
	number = {6},
	journal = {IEEE Transactions on Automatic Control},
	author = {Akaike, H},
	month = dec,
	year = {1974},
	keywords = {Art, Estimation theory, History, Linear systems, Maximum likelihood estimation, Parameter identification, Roundoff errors, Sampling methods, Stochastic processes, Testing, Time series, Time series analysis, maximum-likelihood (ML) estimation},
	pages = {716--723},
}

@article{iravanian_novel_2002,
	title = {A novel algorithm for cardiac biosignal filtering based on filtered residue method},
	volume = {49},
	issn = {0018-9294},
	doi = {10.1109/TBME.2002.804589},
	abstract = {In this paper, a new algorithm is presented for the filtering (de-noising) of cardiac bioelectrical signals. The primary target of this algorithm is the class of cardiac action potentials recorded using voltage-sensitive dyes, although the method is also applied to electrocardiographic signals High periodicity is one of the main features of cardiac biosignals. The proposed algorithm exploits this feature in filtering signals with a minimum amount of distortion. The basic idea is to use signal averaging in time to rind the stationary portion of the signal. The residue is found by subtracting the signal average from the corresponding points of the input. After passing through a low-pass filter, the filtered residue (FR) is added back to the signal average to reconstruct the output. The practical implementation of the filter residue algorithm is discussed. Stretching and shrinking operations are the basis for the conversion of quasi-periodic signals into periodic signals, which can then be subjected to the FR algorithm. Various examples are presented, and error estimation is performed to guide the selection of optimal parameters for the algorithm. The ability of the algorithm to reconstruct the variation among beats is demonstrated, and its limitations are discussed.},
	number = {11},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Iravanian, S. and Tung, L.},
	month = nov,
	year = {2002},
	keywords = {AWGN, Action Potentials, Algorithms, Animals, Animals, Newborn, Bioelectric phenomena, Computer Simulation, Electrophysiology, FIR filters, Filtering algorithms, Finite impulse response filter, Low pass filters, Membrane Potentials, Models, Cardiovascular, Models, Statistical, Muscle Cells, Noise reduction, Optical distortion, Optical filters, Optical recording, Quality Control, Rats, Reproducibility of Results, Sensitivity and Specificity, Signal Processing, Computer-Assisted, Voltage, adaptive filtering, adaptive filters, bioelectric potentials, cardiac action potentials, cardiac biosignal filtering, cellular potentials, denoising, electrocardiography, error estimation, filtered residue method, low-pass filter, low-pass filters, medical signal processing, optical mapping, optimal parameters, residue number systems, signal denoising, stationary portion, stretching/shrinking operation, transmembrane voltage recordings, voltage-sensitive dyes, white Gaussian random noise},
	pages = {1310--1317},
}

@phdthesis{oh_new_2004,
	type = {{PhD} {Thesis}},
	title = {A new quality measure in electrocardiogram signal},
	school = {University of Florida},
	author = {Oh, Sungho},
	year = {2004},
}

@article{gupta_note_2012,
	title = {A note on the asymptotic distribution of {LASSO} estimator for correlated data},
	volume = {74},
	issn = {0976-836X},
	doi = {10.1007/s13171-012-0006-8},
	number = {1},
	journal = {Sankhya A},
	author = {Gupta, Shuva},
	year = {2012},
	pages = {10--28},
}

@article{powell_new_1970,
	title = {A new algorithm for unconstrained optimization},
	doi = {10.1016/B978-0-12-597050-1.50006-3},
	journal = {Nonlinear programming},
	author = {Powell, Michael JD},
	year = {1970},
	note = {00000},
	pages = {31--65},
}

@article{osborne_new_2000,
	title = {A new approach to variable selection in least squares problems},
	volume = {20},
	issn = {0272-4979},
	url = {https://academic.oup.com/imajna/article/20/3/389/777743/A-new-approach-to-variable-selection-in-least},
	doi = {10.1093/imanum/20.3.389},
	abstract = {The title Lasso has been suggested by Tibshirani (1996) as a colourful name for a technique of variable selection which requires the minimization of a sum of squares subject to an l1 bound κ on the solution. This forces zero components in the minimizing solution for small values of κ. Thus this bound can function as a selection parameter. This paper makes two contributions to computational problems associated with implementing the Lasso: (1) a compact descent method for solving the constrained problem for a particular value of κ is formulated, and (2) a homotopy method, in which the constraint bound κ becomes the homotopy parameter, is developed to completely describe the possible selection regimes. Both algorithms have a finite termination property. It is suggested that modified Gram-Schmidt orthogonalization applied to an augmented design matrix provides an effective basis for implementing the algorithms.},
	number = {3},
	urldate = {2017-09-18},
	journal = {IMA Journal of Numerical Analysis},
	author = {Osborne, M. R. and Presnell, B. and Turlach, B. A.},
	month = jul,
	year = {2000},
	pages = {389--403},
}

@inproceedings{he_new_1993,
	title = {A new method for identifying orders of input-output models for nonlinear dynamic systems},
	booktitle = {American {Control} {Conference}, 1993},
	publisher = {IEEE},
	author = {He, Xiangdong and Asada, Haruhiko},
	year = {1993},
	pages = {2520--2523},
}

@article{hager_new_2005,
	title = {A new conjugate gradient method with guaranteed descent and an efficient line search},
	volume = {16},
	doi = {10.1137/030601880},
	number = {1},
	journal = {SIAM Journal on Optimization},
	author = {Hager, William W and Zhang, Hongchao},
	year = {2005},
	pages = {170--192},
}

@article{tripathy_novel_2019,
	title = {A {Novel} {Approach} for {Detection} of {Myocardial} {Infarction} {From} {ECG} {Signals} of {Multiple} {Electrodes}},
	volume = {19},
	issn = {1530-437X},
	doi = {10/gf286v},
	abstract = {Myocardial infarction (MI) is also called the heart attack, and it results in the death of heart muscle cells due to the lacking in the supply of oxygen and other nutrients. The early and accurate detection of MI using the 12-lead electrocardiogram (ECG) is helpful in the clinical standard for saving the lives of the patients suffering from this pathology. This paper proposes a novel approach for the detection of MI pathology using the multiresolution analysis of 12-lead ECG signals. The approach is based on the use of Fourier–Bessel series expansion-based empirical wavelet transform (FBSE-EWT) for the time-scale decomposition of 12-lead ECG signals. For each lead ECG signal, nine subband signals are evaluated using FBSE-EWT. The statistical features such as the kurtosis, the skewness, and the entropy are evaluated from the subband signals of each ECG lead. The deep neural network such as the deep layer least-square support-vector machine (DL-LSSVM) which is formulated using the hidden layers of sparse auto-encoders and the LSSVM is used for the detection of MI from the feature vector of 12-lead ECG. The experimental results demonstrate that the combination of FBSE-EWT-based entropy features and DL-LSSVM has the mean accuracy, the mean sensitivity, and the mean specificity values of 99.74\%, 99.87\%, and 99.60\%, respectively, for the detection of MI. The accuracy value of the proposed method is improved by more than 3\% as compared to the wavelet-based features for the detection of MI.},
	number = {12},
	journal = {IEEE Sensors Journal},
	author = {Tripathy, R. K. and Bhattacharyya, A. and Pachori, R. B.},
	month = jun,
	year = {2019},
	keywords = {12-lead ECG, DL-LSSVM, Electrocardiography, Entropy, FBSE-EWT, Feature extraction, Heart, Myocardial infarction, Pathology, Signal resolution, Support vector machines, clinical information},
	pages = {4509--4517},
}

@article{powell_new_1970a,
	title = {A {New} {Algorithm} for {Unconstrained} {Optimization}},
	doi = {10/gfjwq4},
	journal = {Nonlinear programming},
	author = {Powell, Michael JD},
	year = {1970},
	note = {00551},
	pages = {31--65},
}

@article{noel_nonlinear_2017,
	title = {A nonlinear state-space approach to hysteresis identification},
	volume = {84},
	issn = {0888-3270},
	doi = {10.1016/j.ymssp.2016.08.025},
	journal = {Mechanical Systems and Signal Processing},
	author = {Noël, Jean-Philippe and Esfahani, A Fakhrizadeh and Kerschen, Gaetan and Schoukens, Johan},
	year = {2017},
	pages = {171--184},
}

@article{yin_fourier_2019,
	title = {A {Fourier} {Perspective} on {Model} {Robustness} in {Computer} {Vision}},
	url = {http://arxiv.org/abs/1906.08988},
	abstract = {Achieving robustness to distributional shift is a longstanding and challenging goal of computer vision. Data augmentation is a commonly used approach for improving robustness, however robustness gains are typically not uniform across corruption types. Indeed increasing performance in the presence of random noise is often met with reduced performance on other corruptions such as contrast change. Understanding when and why these sorts of trade-offs occur is a crucial step towards mitigating them. Towards this end, we investigate recently observed trade-offs caused by Gaussian data augmentation and adversarial training. We find that both methods improve robustness to corruptions that are concentrated in the high frequency domain while reducing robustness to corruptions that are concentrated in the low frequency domain. This suggests that one way to mitigate these trade-offs via data augmentation is to use a more diverse set of augmentations. Towards this end we observe that AutoAugment, a recently proposed data augmentation policy optimized for clean accuracy, achieves state-of-the-art robustness on the CIFAR-10-C benchmark.},
	urldate = {2020-07-25},
	journal = {arXiv:1906.08988 [cs, stat]},
	author = {Yin, Dong and Lopes, Raphael Gontijo and Shlens, Jonathon and Cubuk, Ekin D. and Gilmer, Justin},
	month = oct,
	year = {2019},
	note = {arXiv: 1906.08988},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cubanski_neural_1994,
	title = {A {Neural} {Network} {System} for {Detection} of {Atrial} {Fibrillation} in {Ambulatory} {Electrocardiograms}},
	volume = {5},
	issn = {1045-3873},
	url = {https://doi.org/10.1111/j.1540-8167.1994.tb01301.x},
	doi = {10/crkd95},
	abstract = {Neural Network for Detecting AF. Introduction: A neural network classifier has been designed, which is able to distinguish atrial fibrillation (AF) from other supraventricular arrhythmias in ambulatory (Holter) ECGs. Method and Results: The classification algorithm uses a rhythm analysts that considers the ECG to be a time series of RR interval durations. This is combined with an analysis of baseline morphology that considers the morphological characteristics of the non-QRS portions of the waveform. A back propagation-based neural network has been used as part of the classifier implementation. When applied to a library consisting exclusively of 42,970 examples of AF and other supraventricular rhythm disturbances validated by an experienced cardiologist, the algorithm demonstrated a sensitivity of 82.4\% for 10-beat runs of paroxysmal atrial fibrillation (PAF) and a specificity of 96.6\%. Since this system has been implemented as a postprocessor to a conventional automated Holter system, operating only on segments of ECG that are known to contain supraventricular arrhythmias rather than ventricular arrhythmias or sinus rhythm, it can be added to most existing Holter processing systems without significantly increasing the average time to process a tape. Conclusion: A neural network system has been designed, which can potentially provide, for the first time, an accurate, quantitative technique to determine the natural history of PAF and to evaluate potential treatments for PAF.},
	number = {7},
	urldate = {2019-01-15},
	journal = {Journal of Cardiovascular Electrophysiology},
	author = {CUBANSKI, DAVID and CYGANSKI, DAVID and ANTMAN, ELLIOTT M. and FELDMAN, CHARLES L.},
	month = jul,
	year = {1994},
	keywords = {Holter monitoring, atrial fibrillation, neural networks, paroxysmal atriul fibrillation},
	pages = {602--608},
}

@article{hinton_fast_2006,
	title = {A {Fast} {Learning} {Algorithm} for {Deep} {Belief} {Nets}},
	volume = {18},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.2006.18.7.1527},
	doi = {10/cjnhxz},
	abstract = {We show how to use “complementary priors” to eliminate the explaining away effects that make inference difﬁcult in densely-connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that ﬁne-tunes the weights using a contrastive version of the wake-sleep algorithm. After ﬁne-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classiﬁcation than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modelled by long ravines in the free-energy landscape of the top-level associative memory and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
	language = {en},
	number = {7},
	urldate = {2019-02-27},
	journal = {Neural Computation},
	author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
	month = jul,
	year = {2006},
	pages = {1527--1554},
}

@article{okutomi_multiplebaseline_1993,
	title = {A multiple-baseline stereo},
	volume = {15},
	doi = {10.1109/34.206955},
	number = {4},
	journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
	author = {Okutomi, Masatoshi and Kanade, Takeo},
	year = {1993},
	pages = {353--363},
}

@book{fletcher_modified_1971,
	series = {{AERE} report},
	title = {A {Modified} {Marquardt} {Subroutine} for {Non}-linear {Least} {Squares}},
	publisher = {Theoretical Physics Division, Atomic Energy Research Establishment},
	author = {Fletcher, R. and Authority, United Kingdom Atomic Energy and {H.M.S.O.}},
	year = {1971},
}

@article{delathauwer_multilinear_2000,
	title = {A {Multilinear} {Singular} {Value} {Decomposition}},
	volume = {21},
	issn = {0895-4798},
	url = {http://epubs.siam.org/doi/abs/10.1137/S0895479896305696},
	doi = {10.1137/S0895479896305696},
	abstract = {We discuss a multilinear generalization of the singular value decomposition. There is a strong analogy between several properties of the matrix and the higher-order tensor decomposition; uniqueness, link with the matrix eigenvalue decomposition, first-order perturbation effects, etc., are analyzed. We investigate how tensor symmetries affect the decomposition and propose a multilinear generalization of the symmetric eigenvalue decomposition for pair-wise symmetric tensors.},
	number = {4},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {De Lathauwer, L. and De Moor, B. and Vandewalle, J.},
	month = jan,
	year = {2000},
	pages = {1253--1278},
}

@article{bock_multiple_1984,
	title = {A multiple shooting algorithm for direct solution of optimal control problems},
	volume = {17},
	issn = {1474-6670},
	doi = {10.1016/S1474-6670(17)61205-9},
	number = {2},
	journal = {IFAC Proceedings Volumes},
	author = {Bock, Hans Georg and Plitt, Karl-Josef},
	year = {1984},
	pages = {1603--1608},
}

@article{forsyth_modern_2003,
	title = {A {Modern} {Approach}},
	journal = {Computer Vision: A Modern Approach},
	author = {Forsyth, David A and Ponce, Jean},
	year = {2003},
	keywords = {🔍No DOI found},
}

@article{svensson_flexible_2017,
	title = {A flexible state–space model for learning nonlinear dynamical systems},
	volume = {80},
	issn = {00051098},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0005109817301048},
	doi = {10.1016/j.automatica.2017.02.030},
	language = {en},
	urldate = {2018-04-25},
	journal = {Automatica},
	author = {Svensson, Andreas and Schön, Thomas B.},
	month = jun,
	year = {2017},
	note = {00000},
	pages = {189--199},
}

@article{nowak_fused_2011,
	title = {A fused lasso latent feature model for analyzing multi-sample {aCGH} data},
	volume = {12},
	issn = {1465-4644},
	url = {https://academic.oup.com/biostatistics/article/12/4/776/249257/A-fused-lasso-latent-feature-model-for-analyzing},
	doi = {10.1093/biostatistics/kxr012},
	abstract = {Array-based comparative genomic hybridization (aCGH) enables the measurement of DNA copy number across thousands of locations in a genome. The main goals of analyzing aCGH data are to identify the regions of copy number variation (CNV) and to quantify the amount of CNV. Although there are many methods for analyzing single-sample aCGH data, the analysis of multi-sample aCGH data is a relatively new area of research. Further, many of the current approaches for analyzing multi-sample aCGH data do not appropriately utilize the additional information present in the multiple samples. We propose a procedure called the Fused Lasso Latent Feature Model (FLLat) that provides a statistical framework for modeling multi-sample aCGH data and identifying regions of CNV. The procedure involves modeling each sample of aCGH data as a weighted sum of a fixed number of features. Regions of CNV are then identified through an application of the fused lasso penalty to each feature. Some simulation analyses show that FLLat outperforms single-sample methods when the simulated samples share common information. We also propose a method for estimating the false discovery rate. An analysis of an aCGH data set obtained from human breast tumors, focusing on chromosomes 8 and 17, shows that FLLat and Significance Testing of Aberrant Copy number (an alternative, existing approach) identify similar regions of CNV that are consistent with previous findings. However, through the estimated features and their corresponding weights, FLLat is further able to discern specific relationships between the samples, for example, identifying 3 distinct groups of samples based on their patterns of CNV for chromosome 17.},
	number = {4},
	urldate = {2017-09-18},
	journal = {Biostatistics},
	author = {Nowak, Gen and Hastie, Trevor and Pollack, Jonathan R. and Tibshirani, Robert},
	month = oct,
	year = {2011},
	pages = {776--791},
}

@article{gatys_neural_2015,
	title = {A {Neural} {Algorithm} of {Artistic} {Style}},
	url = {http://arxiv.org/abs/1508.06576},
	abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
	journal = {arXiv:1508.06576 [cs, q-bio]},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.06576},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, 🔍No DOI found},
}

@article{bengio_neural_2003,
	title = {A neural probabilistic language model},
	volume = {3},
	number = {Feb},
	journal = {Journal of machine learning research},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
	year = {2003},
	keywords = {🔍No DOI found},
	pages = {1137--1155},
}

@article{zhang_flexible_2000,
	title = {A flexible new technique for camera calibration},
	volume = {22},
	doi = {10.1109/34.888718},
	number = {11},
	journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
	author = {Zhang, Zhengyou},
	year = {2000},
	note = {11804},
	pages = {1330--1334},
}

@inproceedings{roy_maximumflow_1998,
	title = {A maximum-flow formulation of the n-camera stereo correspondence problem},
	booktitle = {Computer {Vision}, 1998. {Sixth} {International} {Conference} on},
	publisher = {IEEE},
	author = {Roy, Sébastien and Cox, Ingemar J},
	year = {1998},
	note = {00000},
	pages = {492--499},
}

@article{levenberg_method_1944,
	title = {A method for the solution of certain non–linear problems in least squares},
	journal = {Journal of the Society for Industrial and Applied Mathematics},
	author = {Levenberg, Kenneth},
	year = {1944},
	keywords = {🔍No DOI found},
}

@article{kukreja_least_2006,
	title = {A least absolute shrinkage and selection operator ({LASSO}) for nonlinear system identification},
	volume = {39},
	url = {http://www.sciencedirect.com/science/article/pii/S1474667015353647},
	doi = {10.3182/20060329-3-AU-2901.00128},
	number = {1},
	urldate = {2017-09-13},
	journal = {IFAC proceedings volumes},
	author = {Kukreja, Sunil L. and Löfberg, Johan and Brenner, Martin J.},
	year = {2006},
	keywords = {Aeroelasticity, Structure Detection, nonlinear systems, system identification},
	pages = {814--819},
}

@article{irigoyen_narx_2013,
	title = {A {NARX} neural network model for enhancing cardiovascular rehabilitation therapies},
	volume = {109},
	doi = {10.1016/j.neucom.2012.07.031},
	journal = {Neurocomputing},
	author = {Irigoyen, Eloy and Miñano, Gorka},
	year = {2013},
	pages = {9--15},
}

@inproceedings{heikkila_fourstep_1997,
	title = {A four-step camera calibration procedure with implicit image correction},
	booktitle = {Computer {Vision} and {Pattern} {Recognition}, 1997. {Proceedings}., 1997 {IEEE} {Computer} {Society} {Conference} on},
	publisher = {IEEE},
	author = {Heikkila, Janne and Silvén, Olli},
	year = {1997},
	pages = {1106--1112},
}

@article{bock_multiple_1984a,
	title = {A {Multiple} {Shooting} {Algorithm} for {Direct} {Solution} of {Optimal} {Control} {Problems}},
	volume = {17},
	issn = {1474-6670},
	doi = {10/gfjwmq},
	number = {2},
	journal = {IFAC Proceedings Volumes},
	author = {Bock, Hans Georg and Plitt, Karl-Josef},
	year = {1984},
	pages = {1603--1608},
}

@book{fletcher_modified_1971a,
	series = {{AERE} report},
	title = {A {Modified} {Marquardt} {Subroutine} for {Non}-{Linear} {Least} {Squares}},
	publisher = {Theoretical Physics Division, Atomic Energy Research Establishment},
	author = {Fletcher, R. and Authority, United Kingdom Atomic Energy and {H.M.S.O.}},
	year = {1971},
	note = {00003},
}

@inproceedings{roy_maximumflow_1998a,
	title = {A {Maximum}-{Flow} {Formulation} of the n-{Camera} {Stereo} {Correspondence} {Problem}},
	booktitle = {Computer {Vision}, 1998. {Sixth} {International} {Conference} {On}},
	publisher = {IEEE},
	author = {Roy, Sébastien and Cox, Ingemar J},
	year = {1998},
	note = {00731},
	pages = {492--499},
}

@article{mcsharry_dynamical_2003,
	title = {A {Dynamical} {Model} for {Generating} {Synthetic} {Electrocardiogram} {Signals}},
	volume = {50},
	abstract = {A dynamical model based on three coupled ordinary differential equations is introduced which is capable of generating realistic synthetic electrocardiogram (ECG) signals. The operator can specify the mean and standard deviation of the heart rate, the morphology of the PQRST cycle, and the power spectrum of the RR tachogram. In particular, both respiratory sinus arrhythmia at the high frequencies (HFs) and Mayer waves at the low frequencies (LFs) together with the LF/HF ratio are incorporated in the model. Much of the beat-to-beat variation in morphology and timing of the human ECG, including QT dispersion and R-peak amplitude modulation are shown to result. This model may be employed to assess biomedical signal processing techniques which are used to compute clinical statistics from the ECG.},
	language = {en},
	number = {3},
	journal = {IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING},
	author = {McSharry, Patrick E and Clifford, Gari D and Tarassenko, Lionel and Smith, Leonard A},
	year = {2003},
	pages = {6},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {0007-4985},
	number = {4},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S and Pitts, Walter},
	year = {1943},
	pages = {115--133},
}

@article{bouc_mathematical_1971,
	title = {A mathematical model for hysteresis},
	volume = {24},
	issn = {1610-1928},
	number = {1},
	journal = {Acta Acustica united with Acustica},
	author = {Bouc, R},
	year = {1971},
	note = {Publisher: S. Hirzel Verlag},
	pages = {16--25},
}

@article{aguirre_bird_2019,
	title = {A {Bird}'s {Eye} {View} of {Nonlinear} {System} {Identification}},
	url = {http://arxiv.org/abs/1907.06803},
	abstract = {This text aims at providing a bird's eye view of system identification with special attention to nonlinear systems. The driving force is to give a feeling for the philosophical problems facing those that build mathematical models from data. Special attention will be given to grey-box approaches in nonlinear system identification. In this text, grey-box methods use auxiliary information such as the system steady-state data, possible symmetries, some bifurcations and the presence of hysteresis. The text ends with a sample of applications. No attempt is made to be thorough nor to survey such an extensive and mature field as system identification. In most parts references will be provided for a more detailed study.},
	urldate = {2020-02-13},
	journal = {arXiv:1907.06803 [cs, eess]},
	author = {Aguirre, Luis Antonio},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.06803},
	keywords = {93-02, Electrical Engineering and Systems Science - Systems and Control},
}

@article{birodkar_closedform_2019,
	title = {A {Closed}-{Form} {Learned} {Pooling} for {Deep} {Classification} {Networks}},
	url = {http://arxiv.org/abs/1906.03808},
	abstract = {In modern computer vision tasks, convolutional neural networks (CNNs) are indispensable for image classification tasks due to their efficiency and effectiveness. Part of their superiority compared to other architectures, comes from the fact that a single, local filter is shared across the entire image. However, there are scenarios where we may need to treat spatial locations in non-uniform manner. We see this in nature when considering how humans have evolved foveation to process different areas in their field of vision with varying levels of detail. In this paper we propose a way to enable CNNs to learn different pooling weights for each pixel location. We do so by introducing an extended definition of a pooling operator. This operator can learn a strict super-set of what can be learned by average pooling or convolutions. It has the benefit of being shared across feature maps and can be encouraged to be local or diffuse depending on the data. We show that for fixed network weights, our pooling operator can be computed in closed-form by spectral decomposition of matrices associated with class separability. Through experiments, we show that this operator benefits generalization for ResNets and CNNs on the CIFAR-10, CIFAR-100 and SVHN datasets and improves robustness to geometric corruptions and perturbations on the CIFAR-10-C and CIFAR-10-P test sets.},
	urldate = {2020-03-23},
	journal = {arXiv:1906.03808 [cs, stat]},
	author = {Birodkar, Vighnesh and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.03808},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{arora_convergence_2019,
	title = {A {Convergence} {Analysis} of {Gradient} {Descent} for {Deep} {Linear} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1810.02281},
	abstract = {We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network (parameterized as \$x {\textbackslash}mapsto W\_N W\_\{N-1\} {\textbackslash}cdots W\_1 x\$) by minimizing the \${\textbackslash}ell\_2\$ loss over whitened data. Convergence at a linear rate is guaranteed when the following hold: (i) dimensions of hidden layers are at least the minimum of the input and output dimensions; (ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deficient solution. The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure. Moreover, in the important case of output dimension 1, i.e. scalar regression, they are met, and thus convergence to global optimum holds, with constant probability under a random initialization scheme. Our results significantly extend previous analyses, e.g., of deep linear residual networks (Bartlett et al., 2018).},
	urldate = {2020-08-27},
	journal = {arXiv:1810.02281 [cs, stat]},
	author = {Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
	month = oct,
	year = {2019},
	note = {arXiv: 1810.02281},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{allen-zhu_convergence_2019,
	title = {A {Convergence} {Theory} for {Deep} {Learning} via {Over}-{Parameterization}},
	volume = {97},
	url = {http://arxiv.org/abs/1811.03962},
	abstract = {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works has been focusing on training neural networks with one hidden layer. The theory of multi-layer networks remains largely unsettled. In this work, we prove why stochastic gradient descent (SGD) can find \${\textbackslash}textit\{global minima\}\$ on the training objective of DNNs in \${\textbackslash}textit\{polynomial time\}\$. We only make two assumptions: the inputs are non-degenerate and the network is over-parameterized. The latter means the network width is sufficiently large: \${\textbackslash}textit\{polynomial\}\$ in \$L\$, the number of layers and in \$n\$, the number of samples. Our key technique is to derive that, in a sufficiently large neighborhood of the random initialization, the optimization landscape is almost-convex and semi-smooth even with ReLU activations. This implies an equivalence between over-parameterized neural networks and neural tangent kernel (NTK) in the finite (and polynomial) width setting. As concrete examples, starting from randomly initialized weights, we prove that SGD can attain 100\% training accuracy in classification tasks, or minimize regression loss in linear convergence speed, with running time polynomial in \$n,L\$. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).},
	journal = {Proceedings of the 36 th International Conference on Machine Learning, PMLR},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
	year = {2019},
	note = {arXiv: 1811.03962},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{ma_complete_2015,
	title = {A {Complete} {Recipe} for {Stochastic} {Gradient} {MCMC}},
	url = {http://arxiv.org/abs/1506.04696},
	abstract = {Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial. Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing MCMC samplers--including stochastic gradient versions--based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We show how previous continuous-dynamic samplers can be trivially "reinvented" in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods.},
	urldate = {2019-01-30},
	journal = {arXiv:1506.04696 [math, stat]},
	author = {Ma, Yi-An and Chen, Tianqi and Fox, Emily B.},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.04696},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
}

@inproceedings{shashikumar_deep_2017,
	address = {Orland, FL, USA},
	title = {A deep learning approach to monitoring and detecting atrial fibrillation using wearable technology},
	isbn = {978-1-5090-4179-4},
	url = {http://ieeexplore.ieee.org/document/7897225/},
	doi = {10.1109/BHI.2017.7897225},
	abstract = {Atrial Fibrillation (AF) is the most common cardiac arrhythmia in clinical practice, with a prevalence of 2\% in the community. Not only it is associated with reduced quality of life, but also increased risk of stroke and myocardial infarction. Unfortunately, many cases of AF are clinically silent and undiagnosed, but long-term monitoring is difﬁcult. Nonetheless, efforts at monitoring at-risk individuals and detecting clinically silent AF may yield signiﬁcant public health beneﬁt, as individuals with new-onset, asymptomatic AF would receive preventive therapies with anticoagulants and beta-blockers, for example.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the {IEEE} {EMBS} {International} {Conference} on {Biomedical} \& {Health} {Informatics} ({BHI})},
	publisher = {IEEE},
	author = {Shashikumar, Supreeth Prajwal and Shah, Amit J. and Li, Qiao and Clifford, Gari D. and Nemati, Shamim},
	year = {2017},
	pages = {141--144},
}

@article{cohen_coefficient_1960,
	title = {A {Coefficient} of {Agreement} for {Nominal} {Scales}},
	volume = {20},
	issn = {0013-1644, 1552-3888},
	url = {http://journals.sagepub.com/doi/10.1177/001316446002000104},
	doi = {10/dghsrr},
	language = {en},
	number = {1},
	urldate = {2019-02-19},
	journal = {Educational and Psychological Measurement},
	author = {Cohen, Jacob},
	month = apr,
	year = {1960},
	pages = {37--46},
}

@incollection{powell_direct_1994,
	title = {A direct search optimization method that models the objective and constraint functions by linear interpolation},
	booktitle = {Advances in optimization and numerical analysis},
	publisher = {Springer},
	author = {Powell, Michael JD},
	year = {1994},
	note = {00000},
	pages = {51--67},
}

@article{dacostalopes_controloriented_2015,
	title = {A control-oriented model of a {PEM} fuel cell stack based on {NARX} and {NOE} neural networks},
	volume = {62},
	doi = {10.1109/TIE.2015.2412519},
	number = {8},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {da Costa Lopes, Francisco and Watanabe, Edson H and Rolim, Luís Guilherme B},
	year = {2015},
	pages = {5155--5163},
}

@article{smith_disciplined_2018,
	title = {A disciplined approach to neural network hyper-parameters: {Part} 1 -- learning rate, batch size, momentum, and weight decay},
	shorttitle = {A disciplined approach to neural network hyper-parameters},
	url = {http://arxiv.org/abs/1803.09820},
	abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums.},
	journal = {arXiv:1803.09820 [cs, stat]},
	author = {Smith, Leslie N.},
	month = mar,
	year = {2018},
	note = {00000 
arXiv: 1803.09820},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, 🔍No DOI found},
}

@book{stroock_concise_1994,
	address = {Boston},
	edition = {2nd ed},
	title = {A concise introduction to the theory of integration},
	isbn = {978-0-8176-3759-0 978-3-7643-3759-9},
	publisher = {Birkhäuser},
	author = {Stroock, Daniel W.},
	year = {1994},
	keywords = {Integrals, Generalized, Measure theory},
}

@article{acharya_deep_2017,
	title = {A deep convolutional neural network model to classify heartbeats},
	volume = {89},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482517302810},
	doi = {10.1016/j.compbiomed.2017.08.022},
	language = {en},
	urldate = {2018-10-21},
	journal = {Computers in Biology and Medicine},
	author = {Acharya, U. Rajendra and Oh, Shu Lih and Hagiwara, Yuki and Tan, Jen Hong and Adam, Muhammad and Gertych, Arkadiusz and Tan, Ru San},
	month = oct,
	year = {2017},
	pages = {389--396},
}

@article{betancourt_conceptual_2017,
	title = {A {Conceptual} {Introduction} to {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1701.02434},
	abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
	journal = {arXiv:1701.02434 [stat]},
	author = {Betancourt, Michael},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.02434},
	keywords = {Statistics - Methodology, 🔍No DOI found},
}

@article{garcia_comprehensive_2015,
	title = {A comprehensive survey on safe reinforcement learning},
	volume = {16},
	number = {1},
	journal = {Journal of Machine Learning Research},
	author = {Garcıa, Javier and Fernández, Fernando},
	year = {2015},
	keywords = {🔍No DOI found},
	pages = {1437--1480},
}

@article{bohlin_case_1994,
	title = {A case study of grey box identification},
	volume = {30},
	issn = {0005-1098},
	doi = {10.1016/0005-1098(94)90032-9},
	number = {2},
	journal = {Automatica},
	author = {Bohlin, Torsten},
	year = {1994},
	pages = {307--318},
}

@article{pauwels_comparison_2012,
	title = {A comparison of {FPGA} and {GPU} for real-time phase-based optical flow, stereo, and local image features},
	volume = {61},
	doi = {10.1109/TC.2011.120},
	number = {7},
	journal = {Computers, IEEE Transactions on},
	author = {Pauwels, Karl and Tomasi, Matteo and Diaz Alonso, Javier and Ros, Eduardo and Van Hulle, Marc M},
	year = {2012},
	note = {00000},
	pages = {999--1012},
}

@article{hooke_direct_1961,
	title = {“{Direct} {Search}” {Solution} of {Numerical} and {Statistical} {Problems}},
	volume = {8},
	doi = {10.1145/321062.321069},
	number = {2},
	journal = {Journal of the ACM (JACM)},
	author = {Hooke, Robert and Jeeves, Terry A},
	year = {1961},
	pages = {212--229},
}

@inproceedings{gehring_convolutional_2017,
	title = {A {Convolutional} {Encoder} {Model} for {Neural} {Machine} {Translation}},
	volume = {1},
	author = {Gehring, Jonas and Auli, Michael and Grangier, David and Dauphin, Yann},
	year = {2017},
	pages = {123--135},
}

@article{smith_deep_2019,
	title = {A deep neural network learning algorithm outperforms a conventional algorithm for emergency department electrocardiogram interpretation},
	volume = {52},
	issn = {0022-0736},
	url = {http://www.sciencedirect.com/science/article/pii/S0022073618302292},
	doi = {10/gf286g},
	abstract = {Background
Cardiologs® has developed the first electrocardiogram (ECG) algorithm that uses a deep neural network (DNN) for full 12‑lead ECG analysis, including rhythm, QRS and ST-T-U waves. We compared the accuracy of the first version of Cardiologs® DNN algorithm to the Mortara/Veritas® conventional algorithm in emergency department (ED) ECGs.
Methods
Individual ECG diagnoses were prospectively mapped to one of 16 pre-specified groups of ECG diagnoses, which were further classified as “major” ECG abnormality or not. Automated interpretations were compared to blinded experts'. The primary outcome was the performance of the algorithms in finding at least one “major” abnormality. The secondary outcome was the proportion of all ECGs for which all groups were identified, with no false negative or false positive groups (“accurate ECG interpretation”). Additionally, we measured sensitivity and positive predictive value (PPV) for any abnormal group.
Results
Cardiologs® vs. Veritas® accuracy for finding a major abnormality was 92.2\% vs. 87.2\% (p {\textless} 0.0001), with comparable sensitivity (88.7\% vs. 92.0\%, p = 0.086), improved specificity (94.0\% vs. 84.7\%, p {\textless} 0.0001) and improved positive predictive value (PPV 88.2\% vs. 75.4\%, p {\textless} 0.0001). Cardiologs® had accurate ECG interpretation for 72.0\% (95\% CI: 69.6–74.2) of ECGs vs. 59.8\% (57.3–62.3) for Veritas® (P {\textless} 0.0001). Sensitivity for any abnormal group for Cardiologs® and Veritas®, respectively, was 69.6\% (95CI 66.7–72.3) vs. 68.3\% (95CI 65.3–71.1) (NS). Positive Predictive Value was 74.0\% (71.1–76.7) for Cardiologs® vs. 56.5\% (53.7–59.3) for Veritas® (P {\textless} 0.0001).
Conclusion
Cardiologs' DNN was more accurate and specific in identifying ECGs with at least one major abnormal group. It had a significantly higher rate of accurate ECG interpretation, with similar sensitivity and higher PPV.},
	urldate = {2019-05-29},
	journal = {Journal of Electrocardiology},
	author = {Smith, Stephen W. and Walsh, Brooks and Grauer, Ken and Wang, Kyuhyun and Rapin, Jeremy and Li, Jia and Fennell, William and Taboulet, Pierre},
	month = jan,
	year = {2019},
	keywords = {Artificial intelligence, Big data, Computer, Deep neural network, Electrocardiography},
	pages = {88--95},
}

@article{dacostalopes_controloriented_2015a,
	title = {A {Control}-{Oriented} {Model} of a {PEM} {Fuel} {Cell} {Stack} {Based} on {NARX} and {NOE} {Neural} {Networks}},
	volume = {62},
	doi = {10/gfjwmk},
	number = {8},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {da Costa Lopes, Francisco and Watanabe, Edson H and Rolim, Luís Guilherme B},
	year = {2015},
	pages = {5155--5163},
}
